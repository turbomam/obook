{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On becoming an OBO Semantic Engineer \u00b6 Welcome to the OBOOK and our OBO Semantic Engineering Training! Introduction to the OBOOK (Open Biological and Biomedical Ontologies Organized Knowledge) \u00b6 Documentation in the OBOOK is organised into 4 distinct sections based on the Di\u00e1taxis framework of documentation: Tutorials : Learning-oriented documentation that contains exercises to help a beginner achieve basic competence in a specific area. How-to Guides : Task-oriented documentation that functions as directions to guide the reader through the steps to achieve a specific end. Reference Guides : Information-oriented documentation that describes a single topic in a succinct, technical and orderly way. Explanations : Understanding-oriented documentation that clarifies, deepens and broadens the reader\u2019s understanding of a subject. To accommodate for the various training use cases we support, we added the following categories: Courses: A convenience content type that assembles materials from OBOOK for a specific, taught unit, such as the yearly ICBO tutorials. Pathways: A kind of course pertaining to a specific role in the OBO-sphere, such as curator or software developer. Lessons: A collection of materials (tutorials, explanations and how-to guides) that together aim to teach a well defined concept. Note : We are in the process of transforming the documentation accordingly, so please be patient if some of the documentation is not yet in the correct place. Feel free to create an issue if you find something that you suspect isn't in place. Editorial Team \u00b6 James Overton , Knocean Inc. James has been a developer of software to support ontology development in and around the OBO sphere for more than a decade and delivers services to the OBO community through his company, Knocean Inc. Becky Jackson , Bend Informatics. Becky has been a Semantic Software developer and Ontology Pipeline specialist since 2016, now working as an independent consultant. Nicole Vasilevsky , Monarch Initiative. Nicole is a Visiting Associate Research Professor at University of Colorado Anschutz Medical Campus and is an Ontology Curator for ontologies such as Mondo Disease Ontology, the Human Phenotype Ontology, Uberon Anatomy Ontology, and Cell Ontology. Nicolas Matentzoglu , Semanticly Ltd. Nico is an Ontology Engineer and Ontology Pipeline Specialist, being the Principal Ontology Pipeline Developer for the Monarch Initiative. Shawn Tan , EMBL-EBI. Shawn is an Ontology Curator for The Brain Data Standards project. Bradley Varner , EMBL-EBI. Bradley is an Ontology Curator for the Cell Annotation Platform. If you would like to contribute to this training, please find out more here . Content \u00b6 Getting started (read this first) Course overview and lessons How to contribute Getting started for OBOOK documentation writers Acknowledgements \u00b6 Critical Path Institute \u00b6 Critical Path Institute (CPI) is an independent, nonprofit organisation dedicated to bringing together experts from regulatory agencies, industry and academia to collaborate and improve the medical product development process. In April 2021, the CPI has commissioned the first version of this OBO course, contributing not only funding for the preparation and delivery of the materials, but also valuable feedback about the course contents and data for the practical exercises. We thank the CPI for contributing significantly to the OBO community and open science! https://c-path.org/","title":"About the course"},{"location":"#on-becoming-an-obo-semantic-engineer","text":"Welcome to the OBOOK and our OBO Semantic Engineering Training!","title":"On becoming an OBO Semantic Engineer"},{"location":"#introduction-to-the-obook-open-biological-and-biomedical-ontologies-organized-knowledge","text":"Documentation in the OBOOK is organised into 4 distinct sections based on the Di\u00e1taxis framework of documentation: Tutorials : Learning-oriented documentation that contains exercises to help a beginner achieve basic competence in a specific area. How-to Guides : Task-oriented documentation that functions as directions to guide the reader through the steps to achieve a specific end. Reference Guides : Information-oriented documentation that describes a single topic in a succinct, technical and orderly way. Explanations : Understanding-oriented documentation that clarifies, deepens and broadens the reader\u2019s understanding of a subject. To accommodate for the various training use cases we support, we added the following categories: Courses: A convenience content type that assembles materials from OBOOK for a specific, taught unit, such as the yearly ICBO tutorials. Pathways: A kind of course pertaining to a specific role in the OBO-sphere, such as curator or software developer. Lessons: A collection of materials (tutorials, explanations and how-to guides) that together aim to teach a well defined concept. Note : We are in the process of transforming the documentation accordingly, so please be patient if some of the documentation is not yet in the correct place. Feel free to create an issue if you find something that you suspect isn't in place.","title":"Introduction to the OBOOK (Open Biological and Biomedical Ontologies Organized Knowledge)"},{"location":"#editorial-team","text":"James Overton , Knocean Inc. James has been a developer of software to support ontology development in and around the OBO sphere for more than a decade and delivers services to the OBO community through his company, Knocean Inc. Becky Jackson , Bend Informatics. Becky has been a Semantic Software developer and Ontology Pipeline specialist since 2016, now working as an independent consultant. Nicole Vasilevsky , Monarch Initiative. Nicole is a Visiting Associate Research Professor at University of Colorado Anschutz Medical Campus and is an Ontology Curator for ontologies such as Mondo Disease Ontology, the Human Phenotype Ontology, Uberon Anatomy Ontology, and Cell Ontology. Nicolas Matentzoglu , Semanticly Ltd. Nico is an Ontology Engineer and Ontology Pipeline Specialist, being the Principal Ontology Pipeline Developer for the Monarch Initiative. Shawn Tan , EMBL-EBI. Shawn is an Ontology Curator for The Brain Data Standards project. Bradley Varner , EMBL-EBI. Bradley is an Ontology Curator for the Cell Annotation Platform. If you would like to contribute to this training, please find out more here .","title":"Editorial Team"},{"location":"#content","text":"Getting started (read this first) Course overview and lessons How to contribute Getting started for OBOOK documentation writers","title":"Content"},{"location":"#acknowledgements","text":"","title":"Acknowledgements"},{"location":"#critical-path-institute","text":"Critical Path Institute (CPI) is an independent, nonprofit organisation dedicated to bringing together experts from regulatory agencies, industry and academia to collaborate and improve the medical product development process. In April 2021, the CPI has commissioned the first version of this OBO course, contributing not only funding for the preparation and delivery of the materials, but also valuable feedback about the course contents and data for the practical exercises. We thank the CPI for contributing significantly to the OBO community and open science! https://c-path.org/","title":"Critical Path Institute"},{"location":"contributing/","text":"Contributing to OBO Semantic Engineering Tutorials \u00b6 We rely on our readers to correct our materials and add to them - the hope is to centralise all the usual teaching materials for OBO ontology professionals in one place. Feel free to: Request new lessons (video/how-to guides) on the issue tracker Make a pull request if you find errors or want to add some clarifying remarks. All files of the OBOAcademy website can be found in the docs directory . The Table of Contents is edited here . Join the OBO slack space ( #obo-training channel) to ask any questions (you can request access on the issue tracker )","title":"Contribute to the course"},{"location":"contributing/#contributing-to-obo-semantic-engineering-tutorials","text":"We rely on our readers to correct our materials and add to them - the hope is to centralise all the usual teaching materials for OBO ontology professionals in one place. Feel free to: Request new lessons (video/how-to guides) on the issue tracker Make a pull request if you find errors or want to add some clarifying remarks. All files of the OBOAcademy website can be found in the docs directory . The Table of Contents is edited here . Join the OBO slack space ( #obo-training channel) to ask any questions (you can request access on the issue tracker )","title":"Contributing to OBO Semantic Engineering Tutorials"},{"location":"getting-started-obook/","text":"Getting started for OBOOK editors \u00b6 The OBOOK is trying to centralise all OBO documentation in one place. It is, and will be, a big construction site, for years to come. The goal is to iterate and make things better. We follow two philosophies: Diataxis for organising our documentation Flipped classroom for the organisation of training materials There are three main consequences to this: Our materials are organised in a certain way (according to the four-way split suggested by Diataxis). We superimpose three more categories to organise the content across all materials and facilitate self guided studying: Pathways, courses and lessons (see below). All training materials must be self-contained to ensure that they can be studied without any further guidance by a teacher. Preparation \u00b6 Browse through this page: https://diataxis.fr/ Watch the introduction to the Diataxis framework: Beyond Diataxis: the OBOOK categories: \u00b6 We just introduced a new concept to OBOOK called pathways . The idea is that we provide a linear guide for all 6 roles mentioned on the getting started page through the materials. This will help us also complete the materials and provide a good path to reviewing them regularly. Tutorial \u00b6 A step-by-step guide to complete a well-defined mini-project. Examples: ROBOT template tutorial. DOSDP template tutorial. Protege tutorial on using the reasoner. Lesson \u00b6 A collection of materials (tutorials, explanations and how-to-guides) that together seek to teach a well defined concept. Examples: Contributing to OBO ontologies; An Introduction to templates in OBO; An Introduction to OBO Application development. While the distinction to \"tutorial\" is often fuzzy, the main distinguishing feature should be that a lesson conveys a general concept independent of some concrete technology stack . While we use concrete examples in lessons, we do always seek to generalise to problem space. Course \u00b6 A convenience content type that allows us to assemble materials from obook for a specific taught unit, such as the yearly ICBO tutorials, or the ongoing Monarch Ontology Tutorials and others. Course pages serve as go-to-pages for course participants and link to all the relevant materials in the documentation. Course usually comprise lessons, tutorials and how-to guides. Pathways \u00b6 A pathway is a kind of course, but without the expectation that it is ever taught in a concrete setting. A pathways pertains to a single concrete role (Ontology Curator, Pipeline Developer etc). It is a collection of materials (lessons, tutorials, how-to-guides) that is ordered in a linear fashion for the convenience of the student. For example, we are developing a pathway for ontology pipeline developers that start by teaching common concepts such as how to make term requests etc, and then go into depth on ROBOT pipelines, ODK and Make. Best practices: \u00b6 Items in the explanation section should conceptually start with a Why or a How question. For ordered lists, only use 1. 1. 1., ever 1. 2. 3. This makes it easier to remove and shuffle items during edits","title":"Getting started for OBOOK editors"},{"location":"getting-started-obook/#getting-started-for-obook-editors","text":"The OBOOK is trying to centralise all OBO documentation in one place. It is, and will be, a big construction site, for years to come. The goal is to iterate and make things better. We follow two philosophies: Diataxis for organising our documentation Flipped classroom for the organisation of training materials There are three main consequences to this: Our materials are organised in a certain way (according to the four-way split suggested by Diataxis). We superimpose three more categories to organise the content across all materials and facilitate self guided studying: Pathways, courses and lessons (see below). All training materials must be self-contained to ensure that they can be studied without any further guidance by a teacher.","title":"Getting started for OBOOK editors"},{"location":"getting-started-obook/#preparation","text":"Browse through this page: https://diataxis.fr/ Watch the introduction to the Diataxis framework:","title":"Preparation"},{"location":"getting-started-obook/#beyond-diataxis-the-obook-categories","text":"We just introduced a new concept to OBOOK called pathways . The idea is that we provide a linear guide for all 6 roles mentioned on the getting started page through the materials. This will help us also complete the materials and provide a good path to reviewing them regularly.","title":"Beyond Diataxis: the OBOOK categories:"},{"location":"getting-started-obook/#tutorial","text":"A step-by-step guide to complete a well-defined mini-project. Examples: ROBOT template tutorial. DOSDP template tutorial. Protege tutorial on using the reasoner.","title":"Tutorial"},{"location":"getting-started-obook/#lesson","text":"A collection of materials (tutorials, explanations and how-to-guides) that together seek to teach a well defined concept. Examples: Contributing to OBO ontologies; An Introduction to templates in OBO; An Introduction to OBO Application development. While the distinction to \"tutorial\" is often fuzzy, the main distinguishing feature should be that a lesson conveys a general concept independent of some concrete technology stack . While we use concrete examples in lessons, we do always seek to generalise to problem space.","title":"Lesson"},{"location":"getting-started-obook/#course","text":"A convenience content type that allows us to assemble materials from obook for a specific taught unit, such as the yearly ICBO tutorials, or the ongoing Monarch Ontology Tutorials and others. Course pages serve as go-to-pages for course participants and link to all the relevant materials in the documentation. Course usually comprise lessons, tutorials and how-to guides.","title":"Course"},{"location":"getting-started-obook/#pathways","text":"A pathway is a kind of course, but without the expectation that it is ever taught in a concrete setting. A pathways pertains to a single concrete role (Ontology Curator, Pipeline Developer etc). It is a collection of materials (lessons, tutorials, how-to-guides) that is ordered in a linear fashion for the convenience of the student. For example, we are developing a pathway for ontology pipeline developers that start by teaching common concepts such as how to make term requests etc, and then go into depth on ROBOT pipelines, ODK and Make.","title":"Pathways"},{"location":"getting-started-obook/#best-practices","text":"Items in the explanation section should conceptually start with a Why or a How question. For ordered lists, only use 1. 1. 1., ever 1. 2. 3. This makes it easier to remove and shuffle items during edits","title":"Best practices:"},{"location":"getting-started/","text":"How to start with the lessons \u00b6 Before you start with the lessons of this course, keep the following in mind: The materials in this course are all intended to be used for self-study. We sometimes offer flipped-classroom sessions for new members on our teams - this means that we expect them to work through the entire course themselves and then come to use with questions and requests for clarifications. There is no need to reinvent the wheel: there are a lot of great materials out there already. Providing references to these these external resources is an essential part of the course - some lessons primarily comprise external tutorials, blog articles and more - please make sure you take advantage of them. Some of the materials developed by us are a bit rough around the edges, and we need your help to fix and improve them. To that end, we appreciate anything from suggestions for improvement to pull requests . Depending on your specific role and interest, you can choose which lessons are relevant to you. There is no specific order, but if you want to start somewhere, we recommend Contributing to OBO ontologies: Protege and Github and/or Using Ontologies and Ontology Terms The different roles of OBO Semantic Engineering \u00b6 There are a wide variety of entry points into the OBO world, for example: Database Curator : You are \u00b6 using ontologies for annotating datasets, experiments and publications requesting new terms from ontologies Suggest corrections to existing ontologies, such as wrong or missing synonyms, typos and definitions Ontology Curator : You are \u00b6 developing and maintaining ontologies adding terms to ontologies performing changes to ontologies, like adding or correcting synonyms responsible for ontology releases Ontology Engineer/Developer : You are \u00b6 developing design patterns for ontologies, specifying the logical structure of terms responsible for ensuring the specification and consistent application of metadata in your ontologies (which annotation properties to use, minimal metadata standards) defining quality control checks Ontology Pipeline Specialist : You are \u00b6 developing ontology pipelines with make and ROBOT building the release and quality control architecture that Engineers and Curators need to do their work. building infrastructure for application ontologies, implementing dynamic imports modules, transformations of and mappings to other ontologies. Semantic ETL Engineer : You are \u00b6 Building ingests from public life science resources such as Bgee, Panther, UniProt and many more You use ontologies to glue together data from different sources You use ontologies to augment your the information in your data sources through inference (Semantic) Software Engineer : You are \u00b6 using ontologies to generate value to end-user applications (user interfaces, semantic facetted search) building widgets that exploit the logical and graph structure of ontologies, for example phenotypic profile matching building ontology term browsers such as OLS . Of course, many of you will occupy more than one of the above \"hats\" or roles. While they all require specialised training, many shared skill requirements exist. This course is being developed to: Provide basic training for OBO Semantic Engineers of any of the above flavours Provide an entry point for people new to the field, for example as part of onboarding activities for projects working with ontologies Capture some of the typical pitfalls and how-to's guides to address common problems across the OBO-sphere","title":"Getting started with learning"},{"location":"getting-started/#how-to-start-with-the-lessons","text":"Before you start with the lessons of this course, keep the following in mind: The materials in this course are all intended to be used for self-study. We sometimes offer flipped-classroom sessions for new members on our teams - this means that we expect them to work through the entire course themselves and then come to use with questions and requests for clarifications. There is no need to reinvent the wheel: there are a lot of great materials out there already. Providing references to these these external resources is an essential part of the course - some lessons primarily comprise external tutorials, blog articles and more - please make sure you take advantage of them. Some of the materials developed by us are a bit rough around the edges, and we need your help to fix and improve them. To that end, we appreciate anything from suggestions for improvement to pull requests . Depending on your specific role and interest, you can choose which lessons are relevant to you. There is no specific order, but if you want to start somewhere, we recommend Contributing to OBO ontologies: Protege and Github and/or Using Ontologies and Ontology Terms","title":"How to start with the lessons"},{"location":"getting-started/#the-different-roles-of-obo-semantic-engineering","text":"There are a wide variety of entry points into the OBO world, for example:","title":"The different roles of OBO Semantic Engineering"},{"location":"getting-started/#database-curator-you-are","text":"using ontologies for annotating datasets, experiments and publications requesting new terms from ontologies Suggest corrections to existing ontologies, such as wrong or missing synonyms, typos and definitions","title":"Database Curator: You are"},{"location":"getting-started/#ontology-curator-you-are","text":"developing and maintaining ontologies adding terms to ontologies performing changes to ontologies, like adding or correcting synonyms responsible for ontology releases","title":"Ontology Curator: You are"},{"location":"getting-started/#ontology-engineerdeveloper-you-are","text":"developing design patterns for ontologies, specifying the logical structure of terms responsible for ensuring the specification and consistent application of metadata in your ontologies (which annotation properties to use, minimal metadata standards) defining quality control checks","title":"Ontology Engineer/Developer: You are"},{"location":"getting-started/#ontology-pipeline-specialist-you-are","text":"developing ontology pipelines with make and ROBOT building the release and quality control architecture that Engineers and Curators need to do their work. building infrastructure for application ontologies, implementing dynamic imports modules, transformations of and mappings to other ontologies.","title":"Ontology Pipeline Specialist: You are"},{"location":"getting-started/#semantic-etl-engineer-you-are","text":"Building ingests from public life science resources such as Bgee, Panther, UniProt and many more You use ontologies to glue together data from different sources You use ontologies to augment your the information in your data sources through inference","title":"Semantic ETL Engineer: You are"},{"location":"getting-started/#semantic-software-engineer-you-are","text":"using ontologies to generate value to end-user applications (user interfaces, semantic facetted search) building widgets that exploit the logical and graph structure of ontologies, for example phenotypic profile matching building ontology term browsers such as OLS . Of course, many of you will occupy more than one of the above \"hats\" or roles. While they all require specialised training, many shared skill requirements exist. This course is being developed to: Provide basic training for OBO Semantic Engineers of any of the above flavours Provide an entry point for people new to the field, for example as part of onboarding activities for projects working with ontologies Capture some of the typical pitfalls and how-to's guides to address common problems across the OBO-sphere","title":"(Semantic) Software Engineer: You are"},{"location":"overview/","text":"Overview \u00b6 Lessons \u00b6 Using Ontologies and Ontology Terms \u00b6 Target roles : Database Curators skills know what ontologies are good for find good ontologies: ontology repositories, OBO find good terms: ontology browsers assess for use: license, quality map local terms to ontology terms identify missing terms use IRIs, prefixes, CURIEs, labels use Protege? Contributing to OBO ontologies 1: Protege and Github \u00b6 Target roles : Database Curators, Ontology Curator, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: use GitHub: issues, Pull Requests understand basic Open Source etiquette reading READMEs understand basics of ontology development workflows understand ontology design patterns use templates: ROBOT, DOS-DP basics of OWL Ontology Fundamentals \u00b6 Target roles : Ontology Curators, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: RDF RDFS OWL Reasoners basic SPARQL Turtle, JSON-LD Linked Data Analysis \u00b6 Target roles : Ontology Curators, (Semantic) Software Engineer Builds on: Ontology Fundamentals Skills: Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition more... Ontology Development \u00b6 Builds on: Ontology Fundamentals Contributing to OBO ontologies Skills: Manage GitHub Manage ontology imports Use ROBOT extract: MIREOT, SLME Use ROBOT report Pruning trees Semantic Databases \u00b6 Builds on: Ontology Development skills advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs Automating Ontology Development Workflows \u00b6 Builds on: Ontology Development Ontology Pipelines Skills: Unix shell make Advanced git, GitHub ROBOT ODK Developing an OBO Reference Ontology \u00b6 Builds on: Ontology Development Automation Skills: Detailed knowledge of OBO principles and best practises Use OBO Dashboard Use OBO Registry Use PURL system Tutorials \u00b6 ROBOT Tutorial 1: Convert, Extract and Template ROBOT Tutorial 2: Annotate, Merge, Reason and Diff Introduction to GitHub Intro to managing and tracking issues in GitHub How-to guides \u00b6 Install Elk 0.5 in Protege Getting set up with Docker and the Ontology Development Kit","title":"Overview of lessons and tutorials"},{"location":"overview/#overview","text":"","title":"Overview"},{"location":"overview/#lessons","text":"","title":"Lessons"},{"location":"overview/#using-ontologies-and-ontology-terms","text":"Target roles : Database Curators skills know what ontologies are good for find good ontologies: ontology repositories, OBO find good terms: ontology browsers assess for use: license, quality map local terms to ontology terms identify missing terms use IRIs, prefixes, CURIEs, labels use Protege?","title":"Using Ontologies and Ontology Terms"},{"location":"overview/#contributing-to-obo-ontologies-1-protege-and-github","text":"Target roles : Database Curators, Ontology Curator, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: use GitHub: issues, Pull Requests understand basic Open Source etiquette reading READMEs understand basics of ontology development workflows understand ontology design patterns use templates: ROBOT, DOS-DP basics of OWL","title":"Contributing to OBO ontologies 1: Protege and Github"},{"location":"overview/#ontology-fundamentals","text":"Target roles : Ontology Curators, Ontology Engineer/Developer Builds on: Ontology Term Use Skills: RDF RDFS OWL Reasoners basic SPARQL Turtle, JSON-LD","title":"Ontology Fundamentals"},{"location":"overview/#linked-data-analysis","text":"Target roles : Ontology Curators, (Semantic) Software Engineer Builds on: Ontology Fundamentals Skills: Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition more...","title":"Linked Data Analysis"},{"location":"overview/#ontology-development","text":"Builds on: Ontology Fundamentals Contributing to OBO ontologies Skills: Manage GitHub Manage ontology imports Use ROBOT extract: MIREOT, SLME Use ROBOT report Pruning trees","title":"Ontology Development"},{"location":"overview/#semantic-databases","text":"Builds on: Ontology Development skills advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs","title":"Semantic Databases"},{"location":"overview/#automating-ontology-development-workflows","text":"Builds on: Ontology Development Ontology Pipelines Skills: Unix shell make Advanced git, GitHub ROBOT ODK","title":"Automating Ontology Development Workflows"},{"location":"overview/#developing-an-obo-reference-ontology","text":"Builds on: Ontology Development Automation Skills: Detailed knowledge of OBO principles and best practises Use OBO Dashboard Use OBO Registry Use PURL system","title":"Developing an OBO Reference Ontology"},{"location":"overview/#tutorials","text":"ROBOT Tutorial 1: Convert, Extract and Template ROBOT Tutorial 2: Annotate, Merge, Reason and Diff Introduction to GitHub Intro to managing and tracking issues in GitHub","title":"Tutorials"},{"location":"overview/#how-to-guides","text":"Install Elk 0.5 in Protege Getting set up with Docker and the Ontology Development Kit","title":"How-to guides"},{"location":"config/template/","text":"Course title \u00b6 Prerequisites \u00b6 TBD Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: add here Learning objectives \u00b6 add learning objectives here Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 TBD Contributors \u00b6 Add contributors with ORCID here","title":"New lesson template"},{"location":"config/template/#course-title","text":"","title":"Course title"},{"location":"config/template/#prerequisites","text":"TBD","title":"Prerequisites"},{"location":"config/template/#preparation","text":"TBD","title":"Preparation"},{"location":"config/template/#what-is-delivered-as-part-of-the-course","text":"Description: add here","title":"What is delivered as part of the course"},{"location":"config/template/#learning-objectives","text":"add learning objectives here","title":"Learning objectives"},{"location":"config/template/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"config/template/#additional-materials-and-resources","text":"TBD","title":"Additional materials and resources"},{"location":"config/template/#contributors","text":"Add contributors with ORCID here","title":"Contributors"},{"location":"courses/icbo2021/","text":"ICBO Tutorial 2021 \u00b6 Wednesday, September 15, 2021 4 PM to 7 PM CEST 10 AM to 1 PM EDT 7 AM to 10 AM PDT Goal \u00b6 The goal of this tutorial is to provide a flavor of the OBO landscape, from the OBO Foundry organization to the ontology curators and OBO engineers that are doing the daily ontology development. Organizers \u00b6 James A. Overton , Knocean Inc. Chris Mungall , Lawrence Berkeley National Laboratory Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , semanticly Ltd Randi Vita , La Jolla Institute for Allergy & Immunology Agenda \u00b6 Time CEST Presenter Topic 4:00 - 4:10pm James Overton Workshop overview 4:10 - 4:20pm James Overton OBO Foundry Overview 4:20 - 4:30pm Nicole Vasilevsky Controlled Vocabularies and Ontologies 4:30 - 4:50pm Nicole Vasilevsky Using and Reusing Ontology Terms 4:50 - 5:25pm Nicole Vasilevsky A day in the life of an Ontology Curator 5:25 - 5:30pm Break 5:30pm - 5:40pm Nico Matentzoglu Ontology 201 Overview 5:40 - 6:15 pm James Overton ROBOT Tutorial 6:15 - 6:35 pm Nico Matentzoglu ODK presentation 6:35 - 6:55 pm Nico Matentzoglu A brief introduction into ontology QC using the OBO dashboard 6:55 - 7:00 pm James Overton Wrap up","title":"ICBO 2021 Tutorial"},{"location":"courses/icbo2021/#icbo-tutorial-2021","text":"Wednesday, September 15, 2021 4 PM to 7 PM CEST 10 AM to 1 PM EDT 7 AM to 10 AM PDT","title":"ICBO Tutorial 2021"},{"location":"courses/icbo2021/#goal","text":"The goal of this tutorial is to provide a flavor of the OBO landscape, from the OBO Foundry organization to the ontology curators and OBO engineers that are doing the daily ontology development.","title":"Goal"},{"location":"courses/icbo2021/#organizers","text":"James A. Overton , Knocean Inc. Chris Mungall , Lawrence Berkeley National Laboratory Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , semanticly Ltd Randi Vita , La Jolla Institute for Allergy & Immunology","title":"Organizers"},{"location":"courses/icbo2021/#agenda","text":"Time CEST Presenter Topic 4:00 - 4:10pm James Overton Workshop overview 4:10 - 4:20pm James Overton OBO Foundry Overview 4:20 - 4:30pm Nicole Vasilevsky Controlled Vocabularies and Ontologies 4:30 - 4:50pm Nicole Vasilevsky Using and Reusing Ontology Terms 4:50 - 5:25pm Nicole Vasilevsky A day in the life of an Ontology Curator 5:25 - 5:30pm Break 5:30pm - 5:40pm Nico Matentzoglu Ontology 201 Overview 5:40 - 6:15 pm James Overton ROBOT Tutorial 6:15 - 6:35 pm Nico Matentzoglu ODK presentation 6:35 - 6:55 pm Nico Matentzoglu A brief introduction into ontology QC using the OBO dashboard 6:55 - 7:00 pm James Overton Wrap up","title":"Agenda"},{"location":"courses/icbo2022/","text":"ICBO OBO Tutorial 2022: Using and Reusing Ontologies \u00b6 September 26, 2022, 9:00 am \u2013 12:30 pm ET Overview Organizers Agenda Overview \u00b6 The Open Biological and Biomedical Ontologies (OBO) community includes hundreds of open source scientific ontology projects, committed to shared principles and practices for interoperability and FAIR data. An OBO tutorial has been a regular feature of ICBO for a decade, introducing new and experienced ontology users and developers to ontologies in general, and to current OBO tools and techniques specifically. While ICBO attracts many ontology experts, it also includes an audience of ontology beginners, and of ontology users looking to become ontology developers or to further refine their skills. Our OBO tutorial will help beginner and intermediate ontology users with a combination of theory and hands-on practice. For ICBO 2022 we will host a half-day OBO tutorial consisting of two parts, with a unifying theme of ontology term reuse. The first part of our tutorial will be introductory, aimed at an audience that is new to ontologies and to the OBO Foundry. We will introduce OBO, its community, principles, resources, and best practices. We will finish the first part with a hands-on lesson in basic tools: ontology browsers, how to contribute to ontologies via GitHub (creating issues and making Pull Requests), and the Protege ontology editor. The second part will build on the first, addressing an audience that is familiar with ontologies and OBO, and wants to make better use of OBO workflows and tools in their own projects. The focus will be on making best use of OBO community open source software. We will introduce ROBOT, the command-line tool and library for automating ontology development tasks. We will show how the Ontology Development Kit (ODK) is used to standardize ontology projects with a wide range of best practices. The special emphasis of this year's tutorial will be ontology reuse, and specifically on how ROBOT and ODK can be used to manage imports from other ontologies and overcome a number of challenges to term reuse. This material for this year's OBO Tutorial will build on the content here in the OBO Academy . The OBO Academy offers free, open, online resources with self paced learning materials covering various aspects of ontology development and curation and OBO. Participants are encouraged to continue their learning using this OBO Academy website, and contribute to improving the OBO documentation. As an outcome of this workshop, we expect that new ontologists will have a clearer understanding of why we need and use ontologies, how to find ontology terms and contribute to ontologies and make basic edits using Protege. Our more advanced participants should be able to apply OBO tools and workflows to their own ontology development practices. Organizers \u00b6 James A. Overton , Knocean Inc. Becky Jackson , Bend Informatics Chris Mungall , Lawrence Berkeley National Laboratory Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , Semanticly, Athens, Greece Randi Vita , La Jolla Institute for Allergy & Immunology Agenda \u00b6 Time Topic Presenter 09:00 am ET Introduction to OBO, its community, principles, resources, and best practices James Overton 09:20 am ET Hands-on lesson in basic tools: see details below Nicole Vasilevsky 10:15 am ET Coffee break 10:30 am ET How to be an open science ontologist Nico Matentzoglu 11:00 am ET Introduction to ROBOT Becky Jackson 12:00 pm ET Introduction to the Ontology Development Kit (ODK) and Core Workflows Nico Matentzoglu Hands on lesson in basic tools \u00b6 Instructor: Nicole Vasilevsky Outline \u00b6 Protege ontology editor Protege basic functionality Plugins. See guide on installing ELK reasoner . How to contribute to ontologies via GitHub ontology maintenance and workflows practice with reasoners making Pull Requests (PRs) Example: We will work on this ticket .","title":"ICBO 2022 Tutorial"},{"location":"courses/icbo2022/#icbo-obo-tutorial-2022-using-and-reusing-ontologies","text":"September 26, 2022, 9:00 am \u2013 12:30 pm ET Overview Organizers Agenda","title":"ICBO OBO Tutorial 2022: Using and Reusing Ontologies"},{"location":"courses/icbo2022/#overview","text":"The Open Biological and Biomedical Ontologies (OBO) community includes hundreds of open source scientific ontology projects, committed to shared principles and practices for interoperability and FAIR data. An OBO tutorial has been a regular feature of ICBO for a decade, introducing new and experienced ontology users and developers to ontologies in general, and to current OBO tools and techniques specifically. While ICBO attracts many ontology experts, it also includes an audience of ontology beginners, and of ontology users looking to become ontology developers or to further refine their skills. Our OBO tutorial will help beginner and intermediate ontology users with a combination of theory and hands-on practice. For ICBO 2022 we will host a half-day OBO tutorial consisting of two parts, with a unifying theme of ontology term reuse. The first part of our tutorial will be introductory, aimed at an audience that is new to ontologies and to the OBO Foundry. We will introduce OBO, its community, principles, resources, and best practices. We will finish the first part with a hands-on lesson in basic tools: ontology browsers, how to contribute to ontologies via GitHub (creating issues and making Pull Requests), and the Protege ontology editor. The second part will build on the first, addressing an audience that is familiar with ontologies and OBO, and wants to make better use of OBO workflows and tools in their own projects. The focus will be on making best use of OBO community open source software. We will introduce ROBOT, the command-line tool and library for automating ontology development tasks. We will show how the Ontology Development Kit (ODK) is used to standardize ontology projects with a wide range of best practices. The special emphasis of this year's tutorial will be ontology reuse, and specifically on how ROBOT and ODK can be used to manage imports from other ontologies and overcome a number of challenges to term reuse. This material for this year's OBO Tutorial will build on the content here in the OBO Academy . The OBO Academy offers free, open, online resources with self paced learning materials covering various aspects of ontology development and curation and OBO. Participants are encouraged to continue their learning using this OBO Academy website, and contribute to improving the OBO documentation. As an outcome of this workshop, we expect that new ontologists will have a clearer understanding of why we need and use ontologies, how to find ontology terms and contribute to ontologies and make basic edits using Protege. Our more advanced participants should be able to apply OBO tools and workflows to their own ontology development practices.","title":"Overview"},{"location":"courses/icbo2022/#organizers","text":"James A. Overton , Knocean Inc. Becky Jackson , Bend Informatics Chris Mungall , Lawrence Berkeley National Laboratory Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , Semanticly, Athens, Greece Randi Vita , La Jolla Institute for Allergy & Immunology","title":"Organizers"},{"location":"courses/icbo2022/#agenda","text":"Time Topic Presenter 09:00 am ET Introduction to OBO, its community, principles, resources, and best practices James Overton 09:20 am ET Hands-on lesson in basic tools: see details below Nicole Vasilevsky 10:15 am ET Coffee break 10:30 am ET How to be an open science ontologist Nico Matentzoglu 11:00 am ET Introduction to ROBOT Becky Jackson 12:00 pm ET Introduction to the Ontology Development Kit (ODK) and Core Workflows Nico Matentzoglu","title":"Agenda"},{"location":"courses/icbo2022/#hands-on-lesson-in-basic-tools","text":"Instructor: Nicole Vasilevsky","title":"Hands on lesson in basic tools"},{"location":"courses/icbo2022/#outline","text":"Protege ontology editor Protege basic functionality Plugins. See guide on installing ELK reasoner . How to contribute to ontologies via GitHub ontology maintenance and workflows practice with reasoners making Pull Requests (PRs) Example: We will work on this ticket .","title":"Outline"},{"location":"courses/monarch-obo-training/","text":"Monarch OBO Training \u00b6 Goal \u00b6 The goal of this course is to provide ongoing training for the OBO community. As with previous tutorials, we follow the flipped classroom concept: as organisers, we provide you with materials to look at, and you will work through the materials on your own. During our biweekly meeting, we will answer your questions, provide you with additional demonstrations where needed and go into depth wherever you as a student are curious to learn more. This means that this course can only work if you are actually putting in the time to preparing the materials . That said, we nevertheless welcome anyone to just lurk or ask related questions. You (Students) \u00b6 Read the \"Getting started\" guide Check which lessons interest you in the upcoming schedule - just participate in the ones you care about Prepare the lessons (between 3 and 7 hours of preparation time) Prepare questions for the revision sessions (without questions, the revision sessions will be silent and awkward) Make detailed issues on our issue tracker when materials are broken, hard to follow or need more details - that is really important as we rely on you to help to improve our materials continuously. Request new course units from us using the issue tracker. We (Tutors) \u00b6 Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , Semanticly Ltd Sabrina Toro , University of Colorado Anschutz Medical Campus Prepare the materials for each lesson and provide schedules Build new training materials where needed Organise the course and rooms Schedule \u00b6 Date Lesson Notes Recordings 2022/10/18 Introduction to Medical Action Ontology (MAxO) 2022/10/04 No meeting - ISB virtual conference: register here 2022/09/20 How to be an open science ontologist Here 2022/09/06 Pull Requests: Part 2 Here 2022/07/26 Pull Requests: Part 1 Here 2022/07/12 Basic introduction to the CLI: Part 2 Due to intermitent connection issues, the first few minutes of this recording are not included. Refer to the Tutorial link for the initial directions. Here 2022/06/28 Basic introduction to the CLI: Part 1 Here 2022/06/14 Application/project ontologies Here 2022/05/31 Contributing to ontologies: annotation properties Here 2022/05/17 Introduction to managing mappings with SSSOM Here 2022/05/03 No meeting 2022/04/19 Disjointness and Unsatisfiability Here 2022/04/05 No meeting 2022/03/22 Creating an ontology from scratch Here 2022/03/08 Obsoletions in OBO ontologies Review Obsoleting an Existing Ontology Term and Merging Ontology Terms . Slides are here . Here 2022/02/22 SPARQL for OBO ontology development Here 2022/02/07 ODK/DOSDPs Here 2022/01/25 Contributing to OBO ontologies This is not new content but we'll start at the beginning again with our previous lessons. Here 2022/01/11 Office hours with Nicole and Sabrina - no formal lesson Bring any open questions. 2021/12/14 Lessons learned from troubleshooting ROBOT Open discussion, no advance preparation is needed. 2021/11/30 Semantics of object properties (including Relations Ontology) 2021/11/16 SPARQL for OBO ontology development Here 2021/11/02 Templating: DOSDPs and ROBOT 2021/10/19 Ontology Design 2021/10/05 Cancelled due to overlap with ISB conference 2021/09/21 Ontology Pipelines with ROBOT 2 2021/09/08 Migrating legacy ontology systems to ODK 2021/09/07 Ontology Pipelines with ROBOT 2021/09/01 Manage dynamic imports the ODK 2021/08/25 Ontology Release Management with the ODK Here 2021/08/24 Contributing to OBO ontologies 2 Here 2021/08/17 Contributing to OBO ontologies Upcoming courses \u00b6 Note: this is tentative and subject to change Date Lesson 2022/11/01 Contributing to OBO ontologies - Part 1 2022/11/15 OBO Academy hackathon (work on open tickets together) 2022/11/29 Contributing to OBO ontologies - Part 2 2022/12/13 Fundamentals of matching 2022/12/27 No meeting : Celebrate the holidays 2023/01/07 TBD Notes \u00b6 Most of materials used by this course were developed by James Overton, Becky Jackson, Nicole Vasilevsky and Nico Matentzoglu as part of a project with the Critical Path Institute (see here ). The materials are improved as part of an internal training program (onboarding and CPD) for the Phenomics First project (NIH / NHGRI #1RM1HG010860-01). Thanks to Sarah Gehrke for her help with project management.","title":"Monarch Ontology Training"},{"location":"courses/monarch-obo-training/#monarch-obo-training","text":"","title":"Monarch OBO Training"},{"location":"courses/monarch-obo-training/#goal","text":"The goal of this course is to provide ongoing training for the OBO community. As with previous tutorials, we follow the flipped classroom concept: as organisers, we provide you with materials to look at, and you will work through the materials on your own. During our biweekly meeting, we will answer your questions, provide you with additional demonstrations where needed and go into depth wherever you as a student are curious to learn more. This means that this course can only work if you are actually putting in the time to preparing the materials . That said, we nevertheless welcome anyone to just lurk or ask related questions.","title":"Goal"},{"location":"courses/monarch-obo-training/#you-students","text":"Read the \"Getting started\" guide Check which lessons interest you in the upcoming schedule - just participate in the ones you care about Prepare the lessons (between 3 and 7 hours of preparation time) Prepare questions for the revision sessions (without questions, the revision sessions will be silent and awkward) Make detailed issues on our issue tracker when materials are broken, hard to follow or need more details - that is really important as we rely on you to help to improve our materials continuously. Request new course units from us using the issue tracker.","title":"You (Students)"},{"location":"courses/monarch-obo-training/#we-tutors","text":"Nicole Vasilevsky , University of Colorado Anschutz Medical Campus Nico Matentzoglu , Semanticly Ltd Sabrina Toro , University of Colorado Anschutz Medical Campus Prepare the materials for each lesson and provide schedules Build new training materials where needed Organise the course and rooms","title":"We (Tutors)"},{"location":"courses/monarch-obo-training/#schedule","text":"Date Lesson Notes Recordings 2022/10/18 Introduction to Medical Action Ontology (MAxO) 2022/10/04 No meeting - ISB virtual conference: register here 2022/09/20 How to be an open science ontologist Here 2022/09/06 Pull Requests: Part 2 Here 2022/07/26 Pull Requests: Part 1 Here 2022/07/12 Basic introduction to the CLI: Part 2 Due to intermitent connection issues, the first few minutes of this recording are not included. Refer to the Tutorial link for the initial directions. Here 2022/06/28 Basic introduction to the CLI: Part 1 Here 2022/06/14 Application/project ontologies Here 2022/05/31 Contributing to ontologies: annotation properties Here 2022/05/17 Introduction to managing mappings with SSSOM Here 2022/05/03 No meeting 2022/04/19 Disjointness and Unsatisfiability Here 2022/04/05 No meeting 2022/03/22 Creating an ontology from scratch Here 2022/03/08 Obsoletions in OBO ontologies Review Obsoleting an Existing Ontology Term and Merging Ontology Terms . Slides are here . Here 2022/02/22 SPARQL for OBO ontology development Here 2022/02/07 ODK/DOSDPs Here 2022/01/25 Contributing to OBO ontologies This is not new content but we'll start at the beginning again with our previous lessons. Here 2022/01/11 Office hours with Nicole and Sabrina - no formal lesson Bring any open questions. 2021/12/14 Lessons learned from troubleshooting ROBOT Open discussion, no advance preparation is needed. 2021/11/30 Semantics of object properties (including Relations Ontology) 2021/11/16 SPARQL for OBO ontology development Here 2021/11/02 Templating: DOSDPs and ROBOT 2021/10/19 Ontology Design 2021/10/05 Cancelled due to overlap with ISB conference 2021/09/21 Ontology Pipelines with ROBOT 2 2021/09/08 Migrating legacy ontology systems to ODK 2021/09/07 Ontology Pipelines with ROBOT 2021/09/01 Manage dynamic imports the ODK 2021/08/25 Ontology Release Management with the ODK Here 2021/08/24 Contributing to OBO ontologies 2 Here 2021/08/17 Contributing to OBO ontologies","title":"Schedule"},{"location":"courses/monarch-obo-training/#upcoming-courses","text":"Note: this is tentative and subject to change Date Lesson 2022/11/01 Contributing to OBO ontologies - Part 1 2022/11/15 OBO Academy hackathon (work on open tickets together) 2022/11/29 Contributing to OBO ontologies - Part 2 2022/12/13 Fundamentals of matching 2022/12/27 No meeting : Celebrate the holidays 2023/01/07 TBD","title":"Upcoming courses"},{"location":"courses/monarch-obo-training/#notes","text":"Most of materials used by this course were developed by James Overton, Becky Jackson, Nicole Vasilevsky and Nico Matentzoglu as part of a project with the Critical Path Institute (see here ). The materials are improved as part of an internal training program (onboarding and CPD) for the Phenomics First project (NIH / NHGRI #1RM1HG010860-01). Thanks to Sarah Gehrke for her help with project management.","title":"Notes"},{"location":"explanation/annotation-properties/","text":"Contributing to ontologies: annotation properties \u00b6 Editors: Sabrina Toro (@sabrinatoro), Nicolas Matentzoglu (@matentzn) Examples with images can be found here . What are annotation properties? \u00b6 An entity such as an individual, a class, or a property can have annotations, such as labels, synonyms and definitions. An annotation property is used to link the entity to a value , which in turn can be anything from a literal (a string, number, date etc) to another entity (such as, another class). Here are some examples of frequently used annotation properties: (every element in bold is an annotation property) http://purl.obolibrary.org/obo/MONDO_0004975 rdfs:label \u2013> \u2018Alzheimer disease\u2019 oboInOwl:hasExactSynonym \u2013> Alzheimer dementia oboInOwl:hasDbXref -> NCIT:C2866 skos:exactMatch -> http://www.orpha.net/ORDO/Orphanet_238616 Some useful things to know about annotation properties \u00b6 Annotation properties have their own IRIs , just like classes and individuals. For example, the IRI of the RDFS built in label property is http://www.w3.org/2000/01/rdf-schema#label. Other examples: oboInOwl:hasExactSynonym : http://www.geneontology.org/formats/oboInOwl#hasExactSynonym oboInOwl:hasDbXref : http://www.geneontology.org/formats/oboInOwl#hasDbXref Annotation properties are just like other entities (classes, individuals) and can have their own annotations. For example, the annotation propert http://purl.obolibrary.org/obo/IAO_0000232 has an rdfs:label ('curator note') and a human readable definition (IAO:0000115): 'An administrative note of use for a curator but of no use for a user'. Annotation properties can be organised in a hierarchical structure. For example, the annotation property 'synonym_type_property' (http://www.geneontology.org/formats/oboInOwl#SynonymTypeProperty) is the parent property of other, more specific ones (such as \"abbreviation\"). Annotation properties are (usually) used with specific type of annotation values. Literal: (one can see [type: xsd:string] in the annotation) xsd:string e.g. 'definition' (http://purl.obolibrary.org/obo/IAO_0000115) xds:boolean e.g. 'owl:deprecated' (http://www.w3.org/2002/07/owl#deprecated) Entity IRI : Classes or individuals: e.g. 'has curation status' (http://purl.obolibrary.org/obo/IAO_0000114) Arbitray URIs, e.g. links to website with the 'term tracker item' (type xsd:anyURI) (http://purl.obolibrary.org/obo/IAO_0000233) property Or even other annotation properties * e.g. 'has_synonym_type' (http://www.geneontology.org/formats/oboInOwl#hasSynonymType) e.g. 'in_subset' (http://purl.obolibrary.org/obo/IAO_0000112) Note: the type of annotation required for an annotation property can be defined by adding a Range + \"select datatype\" in the Annotation Property's Description e.g. : 'scheduled for obsoletion on or after' (http://purl.obolibrary.org/obo/IAO_0006012) Annotations do not affect reasoning . No matter what values you connect with your annotation properties, the reasoner will ignore it - even if it is nonsensical. Annotation Property vs Data and Object Properties \u00b6 Some annotation properties look like data properties (connecting an entity to a literal value) and others look like object properties (connecting an entity to another entity). Other than the fact that statements involving data and object properties look very different in RDF, the key difference from a user perspective is that OWL Reasoners entirely ignore triples involving annotation properties . Data and Object Properties are taken into account by the reasoner. Object properties are different to annotation properties in that they: connect pairs of individuals in way that affects reasoning represent relationship between classes in way that affects reasoning Example property: 'has part' (http://purl.obolibrary.org/obo/BFO_0000051) Object Properties can have the following property characteristics: Inverse, Symmetric, Asymmetric, Reflexive, Irreflexive, Functional, Inverse Functional, and Transitive which effect reasoning. Annotation properties cannot have such properties (or if they had, reasoners would ignore them). Data properties are different to annotation properties in that they: connect individuals with literals in way that affects reasoning represent relation between a class and literal in way that affects reasoning You can use data properties to logically define OWL classes with data ranges. For example, you can define the class of Boomer as all people born between 1946 and 1964. If an individual would be asserted to be a Boomer, but is born earlier than 1946, the reasoner would file a complaint. Example Data Property: 'hasName', 'hasPrice', 'hasCalories', 'hasSugarContent',... More details on how to use Data Properties here Creating new Annotation Properties \u00b6 Note: before creating a new annotation property, it is always a good idea to check for an existing annotation property first. For example: OBO Metadata Ontology (https://www.ebi.ac.uk/ols/ontologies/omo), which could be imported Detailed explanations for adding a new annotation property can be found here The term \"Annotation\" in Ontologies and Data Curation means different things. \u00b6 The word \"annotation\" is used in different contexts to mean different things. For instance, \"annotation in owl\" (ie annotations to an ontology term) is different from \"annotation in the biocuration sense\" (ie gene-to-disease, gene-to-phenotype, gene-to-function annotations). It is therefore crucial to give context when using the word \"annotation\".","title":"Introduction to Annotation Properties"},{"location":"explanation/annotation-properties/#contributing-to-ontologies-annotation-properties","text":"Editors: Sabrina Toro (@sabrinatoro), Nicolas Matentzoglu (@matentzn) Examples with images can be found here .","title":"Contributing to ontologies: annotation properties"},{"location":"explanation/annotation-properties/#what-are-annotation-properties","text":"An entity such as an individual, a class, or a property can have annotations, such as labels, synonyms and definitions. An annotation property is used to link the entity to a value , which in turn can be anything from a literal (a string, number, date etc) to another entity (such as, another class). Here are some examples of frequently used annotation properties: (every element in bold is an annotation property) http://purl.obolibrary.org/obo/MONDO_0004975 rdfs:label \u2013> \u2018Alzheimer disease\u2019 oboInOwl:hasExactSynonym \u2013> Alzheimer dementia oboInOwl:hasDbXref -> NCIT:C2866 skos:exactMatch -> http://www.orpha.net/ORDO/Orphanet_238616","title":"What are annotation properties?"},{"location":"explanation/annotation-properties/#some-useful-things-to-know-about-annotation-properties","text":"Annotation properties have their own IRIs , just like classes and individuals. For example, the IRI of the RDFS built in label property is http://www.w3.org/2000/01/rdf-schema#label. Other examples: oboInOwl:hasExactSynonym : http://www.geneontology.org/formats/oboInOwl#hasExactSynonym oboInOwl:hasDbXref : http://www.geneontology.org/formats/oboInOwl#hasDbXref Annotation properties are just like other entities (classes, individuals) and can have their own annotations. For example, the annotation propert http://purl.obolibrary.org/obo/IAO_0000232 has an rdfs:label ('curator note') and a human readable definition (IAO:0000115): 'An administrative note of use for a curator but of no use for a user'. Annotation properties can be organised in a hierarchical structure. For example, the annotation property 'synonym_type_property' (http://www.geneontology.org/formats/oboInOwl#SynonymTypeProperty) is the parent property of other, more specific ones (such as \"abbreviation\"). Annotation properties are (usually) used with specific type of annotation values. Literal: (one can see [type: xsd:string] in the annotation) xsd:string e.g. 'definition' (http://purl.obolibrary.org/obo/IAO_0000115) xds:boolean e.g. 'owl:deprecated' (http://www.w3.org/2002/07/owl#deprecated) Entity IRI : Classes or individuals: e.g. 'has curation status' (http://purl.obolibrary.org/obo/IAO_0000114) Arbitray URIs, e.g. links to website with the 'term tracker item' (type xsd:anyURI) (http://purl.obolibrary.org/obo/IAO_0000233) property Or even other annotation properties * e.g. 'has_synonym_type' (http://www.geneontology.org/formats/oboInOwl#hasSynonymType) e.g. 'in_subset' (http://purl.obolibrary.org/obo/IAO_0000112) Note: the type of annotation required for an annotation property can be defined by adding a Range + \"select datatype\" in the Annotation Property's Description e.g. : 'scheduled for obsoletion on or after' (http://purl.obolibrary.org/obo/IAO_0006012) Annotations do not affect reasoning . No matter what values you connect with your annotation properties, the reasoner will ignore it - even if it is nonsensical.","title":"Some useful things to know about annotation properties"},{"location":"explanation/annotation-properties/#annotation-property-vs-data-and-object-properties","text":"Some annotation properties look like data properties (connecting an entity to a literal value) and others look like object properties (connecting an entity to another entity). Other than the fact that statements involving data and object properties look very different in RDF, the key difference from a user perspective is that OWL Reasoners entirely ignore triples involving annotation properties . Data and Object Properties are taken into account by the reasoner. Object properties are different to annotation properties in that they: connect pairs of individuals in way that affects reasoning represent relationship between classes in way that affects reasoning Example property: 'has part' (http://purl.obolibrary.org/obo/BFO_0000051) Object Properties can have the following property characteristics: Inverse, Symmetric, Asymmetric, Reflexive, Irreflexive, Functional, Inverse Functional, and Transitive which effect reasoning. Annotation properties cannot have such properties (or if they had, reasoners would ignore them). Data properties are different to annotation properties in that they: connect individuals with literals in way that affects reasoning represent relation between a class and literal in way that affects reasoning You can use data properties to logically define OWL classes with data ranges. For example, you can define the class of Boomer as all people born between 1946 and 1964. If an individual would be asserted to be a Boomer, but is born earlier than 1946, the reasoner would file a complaint. Example Data Property: 'hasName', 'hasPrice', 'hasCalories', 'hasSugarContent',... More details on how to use Data Properties here","title":"Annotation Property vs Data and Object Properties"},{"location":"explanation/annotation-properties/#creating-new-annotation-properties","text":"Note: before creating a new annotation property, it is always a good idea to check for an existing annotation property first. For example: OBO Metadata Ontology (https://www.ebi.ac.uk/ols/ontologies/omo), which could be imported Detailed explanations for adding a new annotation property can be found here","title":"Creating new Annotation Properties"},{"location":"explanation/annotation-properties/#the-term-annotation-in-ontologies-and-data-curation-means-different-things","text":"The word \"annotation\" is used in different contexts to mean different things. For instance, \"annotation in owl\" (ie annotations to an ontology term) is different from \"annotation in the biocuration sense\" (ie gene-to-disease, gene-to-phenotype, gene-to-function annotations). It is therefore crucial to give context when using the word \"annotation\".","title":"The term \"Annotation\" in Ontologies and Data Curation means different things."},{"location":"explanation/intro-to-ontologies/","text":"Introduction to ontologies \u00b6 Based on CL editors training by David Osumi-Sutherland Why do we need ontologies \u00b6 We face an every increasing deluge of biological data, analysis. Ensuring that this data and analysis is Findable, Accessible, Interoperable and Re-usable ( FAIR ) is a major challenge. Findability Interoperabiltiy and Resuability can all be enhanced by standardising metadata. Well standardised metadata can make it easy to find data and analyses despite variations in terminology ( 'Clara cell' vs 'nonciliated bronchiolar secretory cell' vs 'club cell' ) and precision ('bronchial epithelial cell' vs 'club cell'). Understanding what entities are referred to in metadata and how they relate to the annotated material can help users work out if the data or analysis they have found is of interest to them and can aid in its re-use and interoperability with other data and analyses. For example does an annotation of sample data with a term for breast cancer refer to the health status of the patient from which the sample was derived or that the sample itself comes from a breast cancer tumor? We can't find what we're looking for \u00b6 Given variation in terminology and precision, annotation with free text alone is not sufficient for findability. One very lightweight solution to this is to rely on user generated keyword systems and some system that allows users to choose from previously used keywords. This can produce some degree of annotation alignment but also results in fragmented annotation and varying levels of precision with no clear way to relate annotations. For example, trying to refer to feces, in NCBI BioSample: Query Records Feces 22,592 Faeces 1,750 Ordure 2 Dung 19 Manure 154 Excreta 153 Stool 22,756 Stool NOT faeces 21,798 Stool NOT feces 18,314 We don't know what we're talking about \u00b6 Terminology alone can be ambiguous. The same term may be used for completely unrelated or vaguely analogous structures. An insect femur and an mammalian femur are not evolutionarily or related or structurally similar. Biologists often like to use abbreviations to annotate data, but these can be extremely ambiguous. Drosophila biologists use DA1 to refer to structures in the tracheal system, musculature and nervous system. Outside of Drosophila biology it is used to refer to many other things including a rare disease , and a a neuron type in C.elegans . Some extreme examples of this ambiguity come from terminological drift in fields with a long history. For example in the male genitalia of a gasteruptiid wasp, these 5 different structures here have each been labeled \"paramere\" by different people, each studying different hymenopteran lineages. How do we know what \"paramere\" means when it is referred to? This striking example shows that even precise context is not always sufficient for disambiguation. Controlled vocabulary (CV) \u00b6 Rather than rely on users to generate lists of re-usable keywords, we can instead pre-specify a set of terms to use in annotation. This is usually refered to a controlled vocabulary or CV. Key features \u00b6 Terms are not usually defined Relationships between the terms are not usually defined Simplest form is a list Example using wines \u00b6 Pinot noir Red Chardonnay Chianti Bordeaux Riesling Hierarchical controlled vocabulary \u00b6 Definition \u00b6 Any controlled vocabulary that is arranged in a hierarchy. Key features \u00b6 Terms are arranged in a hierarchy, typically from general (top) to specific (bottom) with each term having only one parent. Terms are not usually defined. Relationships between the terms are not usually named or defined Example using wines (Taxonomy of wine) \u00b6 Red Merlot Zinfandel Cabernet Pinot Noir White Chardonnay Pinot Gris Riesling Taxonomy \u2013 a hierarchical CV in which hierarchy = classification. e.g. 'Merlot' is classified as a 'Red' (wine). Not all heirchical CVs are classifications. For example, anatomical atlases often have heirarchical CVs representing parthood. Support for Grouping and varying levels of precision \u00b6 The use of hierachical CV in which general terms group more specific terms allows for varying precision (glial cell vs some specific subtype) and simple grouping of annotated content. For example: From hierarchical CVs to ontologies \u00b6 Hierarchical CVs tend to increase in complexity in particular ways. Synonyms \u00b6 To support findability, the developers of heirarchical CVs often need to associated synonyms or closely related terms with terms in their CV. Polyhierarchy \u00b6 CV content is often driven by requests from annotators and so expansion is not driven by any unified vision of scheme. This often leads to presssure for heirarchies to support terms having multiple parents either reflecting multiple relationship types or multiple types of classification. For example in a CV with the terms 'retinal bipolar cell', retina, 'bipolar neuron' and 'glutamatergic neuron' could reasonably put 'retinal bipolar neuron' under retina based on location and under the other two terms based on classification. Named relationships \u00b6 Developers of heirarchical CVs often come to realise that multiple relationship types are represented in the heirarchy and that it can be useful to name these. For example, a heart glial cell is a (type of) glial cell , but is 'part of' the heart. What is an ontology? \u00b6 Definition \u00b6 Definitions of ontologies can be controversial. Rather than attempts a comprehensive definition, this tutorial will emphasise ontologies as: Classifications Queryable stores of knowledge Key features of well structured ontolgies: \u00b6 Terms are arranged in a classification hierarchy Terms are defined Terms are richly annotated: Textual definitions, references, synonyms, links, cross-references Relationships between terms are defined, allowing logical inference and sophisticated queries as well as graph representations. Expressed in a knowledge representation language such as RDFS, OBO, or OWL Examples \u00b6 Gene Ontology, Uberon, Cell Ontology, EFO, SNOMED Non-logical parts of onotologies \u00b6 Terminology can be ambiguous, so text definitions, references, synonyms and images are key to helping users understand the intended meaning of a term. Identifiers \u00b6 Using nonmeaningful identifiers \u00b6 Identifiers that do not hold any inherent meaning are important to ontologies. If you ever need to change the names of your terms, you're going to need identifiers that stay the same when the term name changes. For example: A microgilal cell is also known as: hortega cell, microglia, microgliocyte and brain resident macrophage. In the cell ontology, it is however referred to by a unique identifier: CL:0000129 These identifiers are short ways of referring to IRIs (e.g., CL:000129 = http://purl.obolibrary.org/obo/CL_0000129) This IRI is a unique, resolvable identifier on the web. A group of ontologies - loosely co-ordinated through the OBO Foundry, have standardised their IRIs (e.g. http://purl.obolibrary.org/obo/CL_0000129 - A term in the cell ontology; http://purl.oblibrary.org/obo/cl.owl - The cell ontology) IRIs? URIs? URLs? \u00b6 URI: Unique Resource Identifier - is a string of characters, following a standard specification, that unambiguously identifies a particular (web) resource. IRI: Internationalised Resource Identifier - a URI that can use characters in multiple languages URL: Uniform Resource Locator - a web-resolvable URI Building scalable ontologies \u00b6 Format \u00b6 OBO ontologies are mostly written in OWL2 or OBO syntax. The latter is a legacy format that maps completely to OWL. For a more in-depth explanation of formats (OWL, OBO, RDF etc.) refer to explainer on OWL format variants . In the examples below we will use OWL Manchester syntax, which allows us to express formal logic in English-like sentences. An ontology as a classification \u00b6 Ontology terms refer to classes of things in the world. For example, the class of all wings Below you will see a classification of parts of the insect and how it is represented in a simple ontology. We use a SubClassOf (or is_a in obo format) to represent that one class fully subsumes another. For example: OWL: hindwing SubClassOf wing OBO: hindwing is_a wing In English we might say: \"a hindwing is a type of wing\" or more specifically, \"all instances of hindwing are instances wing.\" 'Instance' here refers to a single wing of an individual fly. In the previous section, we talked about different types of relationships. In OWL we can define specific relations (known as object properties). One of the commonest is 'part of' which you can see used below. English: all (insect) legs are part of a thoracic segment OWL: 'leg' SubClassOf part_of some thoracic segment OBO: 'leg'; relationship: part_of thoracic segment It might seem odd at first that OWL uses subClassOf here too. The key to understanding this is the concept of an anonymous class - in OWL, we can refer to classes without giving them names. In this case, the anonymous class is the class of all things that are 'part of' (some) 'thoracic segment' (in insects). A vast array of different anatomical strctures are subclasses of this anonymous class, some of which, such as wings legs and spiracles, are visible in the diagram. Note the existential quantifier some in OWL format -- it is interpreted as \"there exists\", \"there is at least one\", or \"some\". The quantifier is important to the direction of relations. subClassOf: 'wing' SubClassOf part_of some 'thoracic segment' is correct 'thoracic segment' SubClassOf has_part some 'wing' is incorrect as it implies all thoracic segment have wings as a part. Similarly: 'claw' SubClassOf connected_to some 'tarsal segment' is correct 'tarsal segment' SubClassOf connected_to some 'claw' is incorrect as it implies all tarsal segments are connected to claws (for example some tarsal segments are connected to other tarsal segments) These relationships store knowledge in a queryable format. For more information about querying, please refer to guide on DL queries and SPARQL queries Scaling Ontologies \u00b6 There are many ways to classify things. For example, a neuron can be classified by structure, electrophysiology, neurotransmitter, lineage, etc. Manually maintaining these multiple inheritances (that occur through multiple classifications) does not scale. Problems with maintaining multiple inheritance classifications by hand Doesn\u2019t scale When adding a new class, how are human editors to know all of the relevant classifications to add? how to rearrange the existing class hierarchy? It is bad for consistency Reasons for existing classifications often opaque Hard to check for consistency with distant superclasses Doesn\u2019t allow for querying A formalized ontology can be queried for classes with arbitrary sets of properties. A manual classification can not. Automated Classifications \u00b6 The knowledge an ontology contains can be used to automate classification For example: English: Any sense organ that functions in the detection of smell is an olfactory sense organ OWL: 'olfactory sense organ' EquivalentTo \u2018sense organ\u2019 that capable_of some \u2018detection of smell\u2019 If we then have an entity nose that is subClassOf sense organ and capable_of some detection of smell , it will be automatically classified as an olfacotry sense organ. Acknowledgements \u00b6 David Osumi-Sutherland (original creator of slides) Nicole Vasilevsky (OSHU) Alex Diehl (Buffalo), Nico Matentzoglu, Matt Brush, Matt Yoder, Carlo Toriniai, Simon Jupp Chris Mungall (LNBL), Melissa Haendal (OSU), Jim Balhoff (RENCI), James Overton - slides, ideas & discussions Terry Meehan - who edited CL more than anyone Helen Parkinson (EBI) Michael Ashburner","title":"Introduction to Ontologies"},{"location":"explanation/intro-to-ontologies/#introduction-to-ontologies","text":"Based on CL editors training by David Osumi-Sutherland","title":"Introduction to ontologies"},{"location":"explanation/intro-to-ontologies/#why-do-we-need-ontologies","text":"We face an every increasing deluge of biological data, analysis. Ensuring that this data and analysis is Findable, Accessible, Interoperable and Re-usable ( FAIR ) is a major challenge. Findability Interoperabiltiy and Resuability can all be enhanced by standardising metadata. Well standardised metadata can make it easy to find data and analyses despite variations in terminology ( 'Clara cell' vs 'nonciliated bronchiolar secretory cell' vs 'club cell' ) and precision ('bronchial epithelial cell' vs 'club cell'). Understanding what entities are referred to in metadata and how they relate to the annotated material can help users work out if the data or analysis they have found is of interest to them and can aid in its re-use and interoperability with other data and analyses. For example does an annotation of sample data with a term for breast cancer refer to the health status of the patient from which the sample was derived or that the sample itself comes from a breast cancer tumor?","title":"Why do we need ontologies"},{"location":"explanation/intro-to-ontologies/#we-cant-find-what-were-looking-for","text":"Given variation in terminology and precision, annotation with free text alone is not sufficient for findability. One very lightweight solution to this is to rely on user generated keyword systems and some system that allows users to choose from previously used keywords. This can produce some degree of annotation alignment but also results in fragmented annotation and varying levels of precision with no clear way to relate annotations. For example, trying to refer to feces, in NCBI BioSample: Query Records Feces 22,592 Faeces 1,750 Ordure 2 Dung 19 Manure 154 Excreta 153 Stool 22,756 Stool NOT faeces 21,798 Stool NOT feces 18,314","title":"We can't find what we're looking for"},{"location":"explanation/intro-to-ontologies/#we-dont-know-what-were-talking-about","text":"Terminology alone can be ambiguous. The same term may be used for completely unrelated or vaguely analogous structures. An insect femur and an mammalian femur are not evolutionarily or related or structurally similar. Biologists often like to use abbreviations to annotate data, but these can be extremely ambiguous. Drosophila biologists use DA1 to refer to structures in the tracheal system, musculature and nervous system. Outside of Drosophila biology it is used to refer to many other things including a rare disease , and a a neuron type in C.elegans . Some extreme examples of this ambiguity come from terminological drift in fields with a long history. For example in the male genitalia of a gasteruptiid wasp, these 5 different structures here have each been labeled \"paramere\" by different people, each studying different hymenopteran lineages. How do we know what \"paramere\" means when it is referred to? This striking example shows that even precise context is not always sufficient for disambiguation.","title":"We don't know what we're talking about"},{"location":"explanation/intro-to-ontologies/#controlled-vocabulary-cv","text":"Rather than rely on users to generate lists of re-usable keywords, we can instead pre-specify a set of terms to use in annotation. This is usually refered to a controlled vocabulary or CV.","title":"Controlled vocabulary (CV)"},{"location":"explanation/intro-to-ontologies/#key-features","text":"Terms are not usually defined Relationships between the terms are not usually defined Simplest form is a list","title":"Key features"},{"location":"explanation/intro-to-ontologies/#example-using-wines","text":"Pinot noir Red Chardonnay Chianti Bordeaux Riesling","title":"Example using wines"},{"location":"explanation/intro-to-ontologies/#hierarchical-controlled-vocabulary","text":"","title":"Hierarchical controlled vocabulary"},{"location":"explanation/intro-to-ontologies/#definition","text":"Any controlled vocabulary that is arranged in a hierarchy.","title":"Definition"},{"location":"explanation/intro-to-ontologies/#key-features_1","text":"Terms are arranged in a hierarchy, typically from general (top) to specific (bottom) with each term having only one parent. Terms are not usually defined. Relationships between the terms are not usually named or defined","title":"Key features"},{"location":"explanation/intro-to-ontologies/#example-using-wines-taxonomy-of-wine","text":"Red Merlot Zinfandel Cabernet Pinot Noir White Chardonnay Pinot Gris Riesling Taxonomy \u2013 a hierarchical CV in which hierarchy = classification. e.g. 'Merlot' is classified as a 'Red' (wine). Not all heirchical CVs are classifications. For example, anatomical atlases often have heirarchical CVs representing parthood.","title":"Example using wines (Taxonomy of wine)"},{"location":"explanation/intro-to-ontologies/#support-for-grouping-and-varying-levels-of-precision","text":"The use of hierachical CV in which general terms group more specific terms allows for varying precision (glial cell vs some specific subtype) and simple grouping of annotated content. For example:","title":"Support for Grouping and varying levels of precision"},{"location":"explanation/intro-to-ontologies/#from-hierarchical-cvs-to-ontologies","text":"Hierarchical CVs tend to increase in complexity in particular ways.","title":"From hierarchical CVs to ontologies"},{"location":"explanation/intro-to-ontologies/#synonyms","text":"To support findability, the developers of heirarchical CVs often need to associated synonyms or closely related terms with terms in their CV.","title":"Synonyms"},{"location":"explanation/intro-to-ontologies/#polyhierarchy","text":"CV content is often driven by requests from annotators and so expansion is not driven by any unified vision of scheme. This often leads to presssure for heirarchies to support terms having multiple parents either reflecting multiple relationship types or multiple types of classification. For example in a CV with the terms 'retinal bipolar cell', retina, 'bipolar neuron' and 'glutamatergic neuron' could reasonably put 'retinal bipolar neuron' under retina based on location and under the other two terms based on classification.","title":"Polyhierarchy"},{"location":"explanation/intro-to-ontologies/#named-relationships","text":"Developers of heirarchical CVs often come to realise that multiple relationship types are represented in the heirarchy and that it can be useful to name these. For example, a heart glial cell is a (type of) glial cell , but is 'part of' the heart.","title":"Named relationships"},{"location":"explanation/intro-to-ontologies/#what-is-an-ontology","text":"","title":"What is an ontology?"},{"location":"explanation/intro-to-ontologies/#definition_1","text":"Definitions of ontologies can be controversial. Rather than attempts a comprehensive definition, this tutorial will emphasise ontologies as: Classifications Queryable stores of knowledge","title":"Definition"},{"location":"explanation/intro-to-ontologies/#key-features-of-well-structured-ontolgies","text":"Terms are arranged in a classification hierarchy Terms are defined Terms are richly annotated: Textual definitions, references, synonyms, links, cross-references Relationships between terms are defined, allowing logical inference and sophisticated queries as well as graph representations. Expressed in a knowledge representation language such as RDFS, OBO, or OWL","title":"Key features of well structured ontolgies:"},{"location":"explanation/intro-to-ontologies/#examples","text":"Gene Ontology, Uberon, Cell Ontology, EFO, SNOMED","title":"Examples"},{"location":"explanation/intro-to-ontologies/#non-logical-parts-of-onotologies","text":"Terminology can be ambiguous, so text definitions, references, synonyms and images are key to helping users understand the intended meaning of a term.","title":"Non-logical parts of onotologies"},{"location":"explanation/intro-to-ontologies/#identifiers","text":"","title":"Identifiers"},{"location":"explanation/intro-to-ontologies/#using-nonmeaningful-identifiers","text":"Identifiers that do not hold any inherent meaning are important to ontologies. If you ever need to change the names of your terms, you're going to need identifiers that stay the same when the term name changes. For example: A microgilal cell is also known as: hortega cell, microglia, microgliocyte and brain resident macrophage. In the cell ontology, it is however referred to by a unique identifier: CL:0000129 These identifiers are short ways of referring to IRIs (e.g., CL:000129 = http://purl.obolibrary.org/obo/CL_0000129) This IRI is a unique, resolvable identifier on the web. A group of ontologies - loosely co-ordinated through the OBO Foundry, have standardised their IRIs (e.g. http://purl.obolibrary.org/obo/CL_0000129 - A term in the cell ontology; http://purl.oblibrary.org/obo/cl.owl - The cell ontology)","title":"Using nonmeaningful identifiers"},{"location":"explanation/intro-to-ontologies/#iris-uris-urls","text":"URI: Unique Resource Identifier - is a string of characters, following a standard specification, that unambiguously identifies a particular (web) resource. IRI: Internationalised Resource Identifier - a URI that can use characters in multiple languages URL: Uniform Resource Locator - a web-resolvable URI","title":"IRIs? URIs? URLs?"},{"location":"explanation/intro-to-ontologies/#building-scalable-ontologies","text":"","title":"Building scalable ontologies"},{"location":"explanation/intro-to-ontologies/#format","text":"OBO ontologies are mostly written in OWL2 or OBO syntax. The latter is a legacy format that maps completely to OWL. For a more in-depth explanation of formats (OWL, OBO, RDF etc.) refer to explainer on OWL format variants . In the examples below we will use OWL Manchester syntax, which allows us to express formal logic in English-like sentences.","title":"Format"},{"location":"explanation/intro-to-ontologies/#an-ontology-as-a-classification","text":"Ontology terms refer to classes of things in the world. For example, the class of all wings Below you will see a classification of parts of the insect and how it is represented in a simple ontology. We use a SubClassOf (or is_a in obo format) to represent that one class fully subsumes another. For example: OWL: hindwing SubClassOf wing OBO: hindwing is_a wing In English we might say: \"a hindwing is a type of wing\" or more specifically, \"all instances of hindwing are instances wing.\" 'Instance' here refers to a single wing of an individual fly. In the previous section, we talked about different types of relationships. In OWL we can define specific relations (known as object properties). One of the commonest is 'part of' which you can see used below. English: all (insect) legs are part of a thoracic segment OWL: 'leg' SubClassOf part_of some thoracic segment OBO: 'leg'; relationship: part_of thoracic segment It might seem odd at first that OWL uses subClassOf here too. The key to understanding this is the concept of an anonymous class - in OWL, we can refer to classes without giving them names. In this case, the anonymous class is the class of all things that are 'part of' (some) 'thoracic segment' (in insects). A vast array of different anatomical strctures are subclasses of this anonymous class, some of which, such as wings legs and spiracles, are visible in the diagram. Note the existential quantifier some in OWL format -- it is interpreted as \"there exists\", \"there is at least one\", or \"some\". The quantifier is important to the direction of relations. subClassOf: 'wing' SubClassOf part_of some 'thoracic segment' is correct 'thoracic segment' SubClassOf has_part some 'wing' is incorrect as it implies all thoracic segment have wings as a part. Similarly: 'claw' SubClassOf connected_to some 'tarsal segment' is correct 'tarsal segment' SubClassOf connected_to some 'claw' is incorrect as it implies all tarsal segments are connected to claws (for example some tarsal segments are connected to other tarsal segments) These relationships store knowledge in a queryable format. For more information about querying, please refer to guide on DL queries and SPARQL queries","title":"An ontology as a classification"},{"location":"explanation/intro-to-ontologies/#scaling-ontologies","text":"There are many ways to classify things. For example, a neuron can be classified by structure, electrophysiology, neurotransmitter, lineage, etc. Manually maintaining these multiple inheritances (that occur through multiple classifications) does not scale. Problems with maintaining multiple inheritance classifications by hand Doesn\u2019t scale When adding a new class, how are human editors to know all of the relevant classifications to add? how to rearrange the existing class hierarchy? It is bad for consistency Reasons for existing classifications often opaque Hard to check for consistency with distant superclasses Doesn\u2019t allow for querying A formalized ontology can be queried for classes with arbitrary sets of properties. A manual classification can not.","title":"Scaling Ontologies"},{"location":"explanation/intro-to-ontologies/#automated-classifications","text":"The knowledge an ontology contains can be used to automate classification For example: English: Any sense organ that functions in the detection of smell is an olfactory sense organ OWL: 'olfactory sense organ' EquivalentTo \u2018sense organ\u2019 that capable_of some \u2018detection of smell\u2019 If we then have an entity nose that is subClassOf sense organ and capable_of some detection of smell , it will be automatically classified as an olfacotry sense organ.","title":"Automated Classifications"},{"location":"explanation/intro-to-ontologies/#acknowledgements","text":"David Osumi-Sutherland (original creator of slides) Nicole Vasilevsky (OSHU) Alex Diehl (Buffalo), Nico Matentzoglu, Matt Brush, Matt Yoder, Carlo Toriniai, Simon Jupp Chris Mungall (LNBL), Melissa Haendal (OSU), Jim Balhoff (RENCI), James Overton - slides, ideas & discussions Terry Meehan - who edited CL more than anyone Helen Parkinson (EBI) Michael Ashburner","title":"Acknowledgements"},{"location":"explanation/logical-axiomatization/","text":"Logical axiomatization of classes & use of reasoning \u00b6 This explainer requires understanding of ontology classifications. Please see \"an ontology as a classification\" section of the introduction to ontologies documentation if you are unfamiliar with these concepts. What are logical axioms \u00b6 Logical axioms are relational information about classes that are primarily aimed at machines. This is opposed to annotations like textual definitions which are primarily aimed at humans. These logical axioms allow reasoners to assist in and verify classification, lessening the development burden and enabling expressive queries. What should you axiomatize? \u00b6 Ideally, everything in the definition should be axiomatized when possible. For example, if we consider the cell type oxytocin receptor sst GABAergic cortical interneuron , which has the textual definition: \"An interneuron located in the cerebral cortex that expresses the oxytocin receptor. These interneurons also express somatostatin.\" The logical axioms should then follow accordingly: SubClassOf: interneuron 'has soma location' some 'cerebral cortex' expresses some 'oxytocin receptor' expresses some somatostatin 'capable of' some 'gamma-aminobutyric acid secretion, neurotransmission' These logical axioms allow a reasoner to automatically classify the term. For example, through the logical axioms, we can infer that oxytocin receptor sst GABAergic cortical interneuron is a cerebral cortex GABAergic interneuron . Axiomatizing definitions well will also allow for accurate querying. For example, if I wanted to find a neuron that expresses oxytocin receptor, having the SubClassOf axioms of interneuron and expresses some 'oxytocin receptor' will allow me to do so on DL query (see tutorial on DL query for more information about DL queries). What should you NOT axiomatize? \u00b6 Everything in the logical axioms must be true , (do not axiomatize things that are true to only part of the entity) For example, the cell type chandelier pvalb GABAergic cortical interneuron is found in upper L2/3 and deep L5 of the cerebral cortex. We do not make logical axioms for has soma location some layer 2/3 and layer 5. Axioms with both layers would mean that a cell of that type must be in both layer 2/3 and layer 5, which is an impossibility (a cell cannot be in two seperate locations at once!). Instead we axiomatize a more general location: 'has soma location' some 'cerebral cortex' Equivalent class logical definitions \u00b6 An equivalent class axiom is an axiom that defines the class; it is a necessary and sufficient logical axiom that defines the cell type. It means that if a class B fulfils all the criteria/restrictions in the equivalent axiom of class A, class B is by definition a subclass of class A. Equivalent classes allow the reasoner to automatically classify entities. For example: chandelier cell has the equivalent class axiom interneuron and ('has characteristic' some 'chandelier cell morphology') chandelier pvalb GABAergic cortical interneuron has the subclass axioms 'has characteristic' some 'chandelier cell morphology' and interneuron chandelier pvalb GABAergic cortical interneuron is therefore a subclass of chandelier cell Equivalent class axioms classification can be very powerful as it takes into consideration complex layers of axioms. For example: primary motor cortex pyramidal cell has the equivalent class axiom 'pyramidal neuron' and ('has soma location' some 'primary motor cortex') . Betz cell has the axioms 'has characteristic' some 'standard pyramidal morphology' and 'has soma location' some 'primary motor cortex layer 5' Betz cell are inferred to be primary motor cortex pyramidal cell through the following chain (you can see this in Prot\u00e9g\u00e9 by pressing the ? button on inferred class): The ability of the reasoner to infer complex classes helps identify classifications that might have been missed if done manually. However, when creating an equivalent class axiom, you must be sure that it is not overly constrictive (in which case, classes that should be classified under it gets missed) nor too loose (in which case, classes will get wrongly classified under it). Example of both overly constrictive and overly loose equivalent class axiom: neuron equivalent to cell and (part_of some 'central nervous system') This is overly constrictive as there are neurons outside the central nervous system (e.g. peripheral neurons). This is also too loose as there are cells in the central nervous system that are not neurons (e.g. glial cells). In such cases, sometimes not having an equivalent class axioms is better (like in the case of neuron), and asserting is the best way to classify a child. Style guide \u00b6 Each ontology has certain styles and conventions in how they axiomatize. This style guide is specific to OBO ontologies. We will also give reasons as to why we choose to axiomatize in the way we do. However, be aware of your local ontology's practices. Respect the ontology style \u00b6 It is important to note that ontologies have specific axiomatization styles and may apply to, for example, selecting a preferred relation. This usually reflects their use cases. For example, the Cell Ontology has a guide for what relations to use . An example of an agreement in the community is that while anatomical locations of cells are recorded using part of , neurons should be recorded with has soma location . This is to accommodate for the fact that many neurons have long reaching axons that cover multiple anatomical locations making them difficult to axiomatize using part of . For example, Betz cell , a well known cell type which defines layer V of the primary motor cortex, synapses lower motor neurons or spinal interneurons (cell types that reside outside the brain). Having the axiom 'Betz cell' part_of 'cortical layer V' is wrong. In this case has soma location is used. Because of cases like these that are common in neurons, all neurons in CL should use has soma location . Avoid redundant axioms \u00b6 Do not add axioms that are not required. If a parent class already has the axiom, it should not be added to the child class too. For example: retinal bipolar neuron is a child of bipolar neuron bipolar neuron has the axiom 'has characteristic' some 'cortical bipolar morphology' Therefore we do not add 'has characteristic' some 'cortical bipolar morphology' to retinal bipolar neuron Axioms add lines to the ontology, resulting in larger ontologies that are harder to use. They also add redundancy, making the ontology hard to maintain as a single change in classification might require multiple edits. Let the reasoner do the work \u00b6 Asserted is_a parents do not need to be retained as entries in the 'SubClass of' section of the Description window in Prot\u00e9g\u00e9 if the logical definition for a term results in their inference. For example, cerebral cortex GABAergic interneuron has the following logical axioms: Equivalent_To 'GABAergic interneuron' and ('has soma location' some 'cerebral cortex') We do not need to assert that it is a cerebral cortex neuron , CNS interneuron , or neuron of the forebrain as the reasoner automatically does that. We avoid having asserted subclass axioms as these are redundant lines in the ontology which can result in a larger ontology, making them harder to use. Good practice to let the reasoner do the work: 1) If you create a logical definition for your term, you should delete all redundant, asserted is_a parent relations by clicking on the X to the right of the term. 2) If an existing term contains a logical definition and still shows an asserted is_a parent in the 'SubClass of' section, you may delete that asserted parent. Just make sure to run the Reasoner to check that the asserted parent is now replaced with the correct reasoned parent(s). 3) Once you synchronize the Reasoner, you will see the reasoned classification of your new term, including the inferred is_a parent(s). 4) If the inferred classification does not contain the correct parentage, or doesn't make sense, then you will need to modify the logical definition.","title":"Logical axiomatization of classes & use of reasoning"},{"location":"explanation/logical-axiomatization/#logical-axiomatization-of-classes-use-of-reasoning","text":"This explainer requires understanding of ontology classifications. Please see \"an ontology as a classification\" section of the introduction to ontologies documentation if you are unfamiliar with these concepts.","title":"Logical axiomatization of classes &amp; use of reasoning"},{"location":"explanation/logical-axiomatization/#what-are-logical-axioms","text":"Logical axioms are relational information about classes that are primarily aimed at machines. This is opposed to annotations like textual definitions which are primarily aimed at humans. These logical axioms allow reasoners to assist in and verify classification, lessening the development burden and enabling expressive queries.","title":"What are logical axioms"},{"location":"explanation/logical-axiomatization/#what-should-you-axiomatize","text":"Ideally, everything in the definition should be axiomatized when possible. For example, if we consider the cell type oxytocin receptor sst GABAergic cortical interneuron , which has the textual definition: \"An interneuron located in the cerebral cortex that expresses the oxytocin receptor. These interneurons also express somatostatin.\" The logical axioms should then follow accordingly: SubClassOf: interneuron 'has soma location' some 'cerebral cortex' expresses some 'oxytocin receptor' expresses some somatostatin 'capable of' some 'gamma-aminobutyric acid secretion, neurotransmission' These logical axioms allow a reasoner to automatically classify the term. For example, through the logical axioms, we can infer that oxytocin receptor sst GABAergic cortical interneuron is a cerebral cortex GABAergic interneuron . Axiomatizing definitions well will also allow for accurate querying. For example, if I wanted to find a neuron that expresses oxytocin receptor, having the SubClassOf axioms of interneuron and expresses some 'oxytocin receptor' will allow me to do so on DL query (see tutorial on DL query for more information about DL queries).","title":"What should you axiomatize?"},{"location":"explanation/logical-axiomatization/#what-should-you-not-axiomatize","text":"Everything in the logical axioms must be true , (do not axiomatize things that are true to only part of the entity) For example, the cell type chandelier pvalb GABAergic cortical interneuron is found in upper L2/3 and deep L5 of the cerebral cortex. We do not make logical axioms for has soma location some layer 2/3 and layer 5. Axioms with both layers would mean that a cell of that type must be in both layer 2/3 and layer 5, which is an impossibility (a cell cannot be in two seperate locations at once!). Instead we axiomatize a more general location: 'has soma location' some 'cerebral cortex'","title":"What should you NOT axiomatize?"},{"location":"explanation/logical-axiomatization/#equivalent-class-logical-definitions","text":"An equivalent class axiom is an axiom that defines the class; it is a necessary and sufficient logical axiom that defines the cell type. It means that if a class B fulfils all the criteria/restrictions in the equivalent axiom of class A, class B is by definition a subclass of class A. Equivalent classes allow the reasoner to automatically classify entities. For example: chandelier cell has the equivalent class axiom interneuron and ('has characteristic' some 'chandelier cell morphology') chandelier pvalb GABAergic cortical interneuron has the subclass axioms 'has characteristic' some 'chandelier cell morphology' and interneuron chandelier pvalb GABAergic cortical interneuron is therefore a subclass of chandelier cell Equivalent class axioms classification can be very powerful as it takes into consideration complex layers of axioms. For example: primary motor cortex pyramidal cell has the equivalent class axiom 'pyramidal neuron' and ('has soma location' some 'primary motor cortex') . Betz cell has the axioms 'has characteristic' some 'standard pyramidal morphology' and 'has soma location' some 'primary motor cortex layer 5' Betz cell are inferred to be primary motor cortex pyramidal cell through the following chain (you can see this in Prot\u00e9g\u00e9 by pressing the ? button on inferred class): The ability of the reasoner to infer complex classes helps identify classifications that might have been missed if done manually. However, when creating an equivalent class axiom, you must be sure that it is not overly constrictive (in which case, classes that should be classified under it gets missed) nor too loose (in which case, classes will get wrongly classified under it). Example of both overly constrictive and overly loose equivalent class axiom: neuron equivalent to cell and (part_of some 'central nervous system') This is overly constrictive as there are neurons outside the central nervous system (e.g. peripheral neurons). This is also too loose as there are cells in the central nervous system that are not neurons (e.g. glial cells). In such cases, sometimes not having an equivalent class axioms is better (like in the case of neuron), and asserting is the best way to classify a child.","title":"Equivalent class logical definitions"},{"location":"explanation/logical-axiomatization/#style-guide","text":"Each ontology has certain styles and conventions in how they axiomatize. This style guide is specific to OBO ontologies. We will also give reasons as to why we choose to axiomatize in the way we do. However, be aware of your local ontology's practices.","title":"Style guide"},{"location":"explanation/logical-axiomatization/#respect-the-ontology-style","text":"It is important to note that ontologies have specific axiomatization styles and may apply to, for example, selecting a preferred relation. This usually reflects their use cases. For example, the Cell Ontology has a guide for what relations to use . An example of an agreement in the community is that while anatomical locations of cells are recorded using part of , neurons should be recorded with has soma location . This is to accommodate for the fact that many neurons have long reaching axons that cover multiple anatomical locations making them difficult to axiomatize using part of . For example, Betz cell , a well known cell type which defines layer V of the primary motor cortex, synapses lower motor neurons or spinal interneurons (cell types that reside outside the brain). Having the axiom 'Betz cell' part_of 'cortical layer V' is wrong. In this case has soma location is used. Because of cases like these that are common in neurons, all neurons in CL should use has soma location .","title":"Respect the ontology style"},{"location":"explanation/logical-axiomatization/#avoid-redundant-axioms","text":"Do not add axioms that are not required. If a parent class already has the axiom, it should not be added to the child class too. For example: retinal bipolar neuron is a child of bipolar neuron bipolar neuron has the axiom 'has characteristic' some 'cortical bipolar morphology' Therefore we do not add 'has characteristic' some 'cortical bipolar morphology' to retinal bipolar neuron Axioms add lines to the ontology, resulting in larger ontologies that are harder to use. They also add redundancy, making the ontology hard to maintain as a single change in classification might require multiple edits.","title":"Avoid redundant axioms"},{"location":"explanation/logical-axiomatization/#let-the-reasoner-do-the-work","text":"Asserted is_a parents do not need to be retained as entries in the 'SubClass of' section of the Description window in Prot\u00e9g\u00e9 if the logical definition for a term results in their inference. For example, cerebral cortex GABAergic interneuron has the following logical axioms: Equivalent_To 'GABAergic interneuron' and ('has soma location' some 'cerebral cortex') We do not need to assert that it is a cerebral cortex neuron , CNS interneuron , or neuron of the forebrain as the reasoner automatically does that. We avoid having asserted subclass axioms as these are redundant lines in the ontology which can result in a larger ontology, making them harder to use. Good practice to let the reasoner do the work: 1) If you create a logical definition for your term, you should delete all redundant, asserted is_a parent relations by clicking on the X to the right of the term. 2) If an existing term contains a logical definition and still shows an asserted is_a parent in the 'SubClass of' section, you may delete that asserted parent. Just make sure to run the Reasoner to check that the asserted parent is now replaced with the correct reasoned parent(s). 3) Once you synchronize the Reasoner, you will see the reasoned classification of your new term, including the inferred is_a parent(s). 4) If the inferred classification does not contain the correct parentage, or doesn't make sense, then you will need to modify the logical definition.","title":"Let the reasoner do the work"},{"location":"explanation/owl-format-variants/","text":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why? \u00b6 For reference of the more technical aspects of release artefacts, please see documentation on Release Artefacts Ontologies come in different serialisations, formalisms, and variants For example, their are a full 9 (!) different release files associated with an ontology released using the default settings of the Ontology Development Kit , which causes a lot of confusion for current and prospective users. Note: In the OBO Foundry pages, \"variant\" is currently referred to as \"products\", i.e. the way we use \"variant\" here is synonymous with with notion of \"product\". Overview of the relevant concepts \u00b6 A formalism or formal language can be used to describe entities and their relationships in an ontology. The most important formalisms we have are: Web Ontology Language (OWL): OWL is by far the dominant formalism in the biomedical domain due to its inference capabilities. RDF(S): Is a generally weaker language than OWL, but widely used by triple stores and other SPARQL engines. RDF(S) is lacking some of the strong logical guarantees that come with OWL and should only be used in scenarios where scalability (computation time) is the primary concern. OBO : OBO used to be the dominant language in the biomedical domain before the advent of OWL. I also used to have its own specific semantics associated with it. OBO semantics have since been mapped into OWL semantics, so that for all practical purposes, we consider \"OBO\" now a dialect of OWL, which means that when you hear 'OBO format' today, we are generally referring to the serialisation (see below), NOT the formalism . Note that when we say OBO ontologies we mean literally Open Biomedical and Biological Ontologies , and NOT Ontology in OBO format . Some people like to also list SHACL and Shex as ontology languages and formalism. Formalisms define syntax (e.g. grammar rules) and semantics (what does what expression mean?). The analogue in the real world would be natural languages, like English or Greek. A format , or serialisation of a language is used to write down statements of a formal language in some way. Formats are not formalisms - they simply enable statements in a formalism to be expressed in some (usually textual) way. The most common formats in our domains are: RDF/XML . This is the default serialisation language of the OWL flavours of OBO ontologies. It is a pretty ugly format, really hard to understand by most users but it has one advantage - it can be understood widely by RDF-focused tools like rdflib , OWL-focused tools like those based on the OWL API OWL Functional Syntax : This is very common syntax for editing ontologies in OWL, because they look nice in diff tools such as git diff , i.e changes to ontologies in functional syntax are much easier to be reviewed. RDF/XML is not suitable for manual review, due to its verbosity and complexity. OWL Manchester Syntax : This is the default language for OWL tutorials and for writing class expressions in editors such as Protege OBO Format : The most easy to read of all the serialisations. In many ontologies such as Mondo and Uberon, we still use OBO as the editors format (as opposed to OWL Functional Syntax, which is more wide-spread). OBO format looks clear and beautiful in diffs such as git diffs, and therefore still continues to be wide-spread. OBO Format does not cover all of owl, and should only be used in conjunction with ontologies that stay within the limit of the OBO format specification . OBO Graphs JSON : A simple JSON serialisation of ontologies. This format roughly reflects the capabilities of the OBO format , but is intended for consumption by tools. Again, it does not cover all of OWL, but it does cover the parts that are relevant in 99% of the use cases. The real-world analogue of serialisation or format is the script, i.e. Latin or Cyrillic script (not quite clean analogue). A variant is a version of the ontology exported for a specific purpose. The most important variants are: Edit : The variant of the ontology that is edited by ontology curators. Its sole purpose is to be used by ontology editors, and should not be used by any other application . In a ODK-style repository, the edit file is typically located hidden from view, e.g. src/ontology/cl-edit.owl . Full : The ontology with all its imports merged in, and classified using a reasoner, see docs . The Full variant should be used by users that require the use of reasoners and a guarantee that all the inferences work as intended by the ontology developers. This is the default variant of most OBO ontologies. Base : The axioms belonging to the ontology, excluding any axioms from imported ontologies, see docs . Base variants are used by ontology repository developers to combine the latest versions of all ontologies in a way that avoids problems due to conflicting versions. Base files should not be used by users that want to use the ontology in downstream tools, such as annotation tools or scientific databases, as they are incomplete, i.e. not fully classified. Simple : A version of the ontology that only contains only a subset of the ontology (only the direct relations, see docs ). The simple variant should be used by most users that build tools that use the ontology, especially when serialised as OBO graphs json. This variant should probably be avoided by power-users working with reasoners, as many of the axioms that drive reasoning are missing. Basic : A variant of Simple , in that it is reduced to only a specific set of relations, such as subClassOf and partOf . Some users that require the ontology to correspond to acyclic graphs, or deliberately want to focus only on a set of core relations, will want to use this variant, see docs ). The formal definition of the basic variant can be found here . Other variants : Some variants are still used, like \"non-classified\", see docs ), but should be avoided. Others like base-plus , a variant that corresponds to base + the inferred axioms, are still under development, and will be explained here when they are fully developed. Best practices \u00b6 Tool developers developing tools that use the ontology (and do not need reasoners), such as database curation tools, web-browsers and similar, should typically use OBO graphs JSON and avoid using OBO format or any of the OWL focussed serialisations (Functional, Manchester or RDF/XML). OWL-focussed serialisations contain a huge deal of axiomatic content that make no sense to most users, and can lead to a variety of mistakes. We have seen it many times that software developers try to interpret OWL axioms somehow to extract relations . Do not do that! Work with the ontologies to ensure they provide the relationships you need in the appropriate form. Tool developers building tools to work with ontologies should typically ensure that they can read and write RDF/XML - this is the most widely understood serialisation. Work with ontologies means here 'enable operations that change the content of the ontology'. Tool developers building infrastructure to query across ontologies should consider using base variants - these ensure that you can always use the latest version of each ontology and avoid most of the common version clashes. It is important that such users are keenly aware of the role of OWL reasoning in such a process. Many users of ontologies think they need the reasoner actually don't . Make sure you consult with an expert before building a system that relies on OWL reasoners to deliver user facing services. As an ontology developer, it is great practice to provide the above variants in the common serialisations. The Ontology Development Kit provides defaults for all of these. As an ontology developer, you should avoid publishing your ontology with owl:imports statements - these are easily ignored by your users and make the intended \"content\" of the ontology quite none-transparent.","title":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why?"},{"location":"explanation/owl-format-variants/#owl-obo-json-base-simple-full-basic-what-should-you-use-and-why","text":"For reference of the more technical aspects of release artefacts, please see documentation on Release Artefacts Ontologies come in different serialisations, formalisms, and variants For example, their are a full 9 (!) different release files associated with an ontology released using the default settings of the Ontology Development Kit , which causes a lot of confusion for current and prospective users. Note: In the OBO Foundry pages, \"variant\" is currently referred to as \"products\", i.e. the way we use \"variant\" here is synonymous with with notion of \"product\".","title":"OWL, OBO, JSON? Base, simple, full, basic? What should you use, and why?"},{"location":"explanation/owl-format-variants/#overview-of-the-relevant-concepts","text":"A formalism or formal language can be used to describe entities and their relationships in an ontology. The most important formalisms we have are: Web Ontology Language (OWL): OWL is by far the dominant formalism in the biomedical domain due to its inference capabilities. RDF(S): Is a generally weaker language than OWL, but widely used by triple stores and other SPARQL engines. RDF(S) is lacking some of the strong logical guarantees that come with OWL and should only be used in scenarios where scalability (computation time) is the primary concern. OBO : OBO used to be the dominant language in the biomedical domain before the advent of OWL. I also used to have its own specific semantics associated with it. OBO semantics have since been mapped into OWL semantics, so that for all practical purposes, we consider \"OBO\" now a dialect of OWL, which means that when you hear 'OBO format' today, we are generally referring to the serialisation (see below), NOT the formalism . Note that when we say OBO ontologies we mean literally Open Biomedical and Biological Ontologies , and NOT Ontology in OBO format . Some people like to also list SHACL and Shex as ontology languages and formalism. Formalisms define syntax (e.g. grammar rules) and semantics (what does what expression mean?). The analogue in the real world would be natural languages, like English or Greek. A format , or serialisation of a language is used to write down statements of a formal language in some way. Formats are not formalisms - they simply enable statements in a formalism to be expressed in some (usually textual) way. The most common formats in our domains are: RDF/XML . This is the default serialisation language of the OWL flavours of OBO ontologies. It is a pretty ugly format, really hard to understand by most users but it has one advantage - it can be understood widely by RDF-focused tools like rdflib , OWL-focused tools like those based on the OWL API OWL Functional Syntax : This is very common syntax for editing ontologies in OWL, because they look nice in diff tools such as git diff , i.e changes to ontologies in functional syntax are much easier to be reviewed. RDF/XML is not suitable for manual review, due to its verbosity and complexity. OWL Manchester Syntax : This is the default language for OWL tutorials and for writing class expressions in editors such as Protege OBO Format : The most easy to read of all the serialisations. In many ontologies such as Mondo and Uberon, we still use OBO as the editors format (as opposed to OWL Functional Syntax, which is more wide-spread). OBO format looks clear and beautiful in diffs such as git diffs, and therefore still continues to be wide-spread. OBO Format does not cover all of owl, and should only be used in conjunction with ontologies that stay within the limit of the OBO format specification . OBO Graphs JSON : A simple JSON serialisation of ontologies. This format roughly reflects the capabilities of the OBO format , but is intended for consumption by tools. Again, it does not cover all of OWL, but it does cover the parts that are relevant in 99% of the use cases. The real-world analogue of serialisation or format is the script, i.e. Latin or Cyrillic script (not quite clean analogue). A variant is a version of the ontology exported for a specific purpose. The most important variants are: Edit : The variant of the ontology that is edited by ontology curators. Its sole purpose is to be used by ontology editors, and should not be used by any other application . In a ODK-style repository, the edit file is typically located hidden from view, e.g. src/ontology/cl-edit.owl . Full : The ontology with all its imports merged in, and classified using a reasoner, see docs . The Full variant should be used by users that require the use of reasoners and a guarantee that all the inferences work as intended by the ontology developers. This is the default variant of most OBO ontologies. Base : The axioms belonging to the ontology, excluding any axioms from imported ontologies, see docs . Base variants are used by ontology repository developers to combine the latest versions of all ontologies in a way that avoids problems due to conflicting versions. Base files should not be used by users that want to use the ontology in downstream tools, such as annotation tools or scientific databases, as they are incomplete, i.e. not fully classified. Simple : A version of the ontology that only contains only a subset of the ontology (only the direct relations, see docs ). The simple variant should be used by most users that build tools that use the ontology, especially when serialised as OBO graphs json. This variant should probably be avoided by power-users working with reasoners, as many of the axioms that drive reasoning are missing. Basic : A variant of Simple , in that it is reduced to only a specific set of relations, such as subClassOf and partOf . Some users that require the ontology to correspond to acyclic graphs, or deliberately want to focus only on a set of core relations, will want to use this variant, see docs ). The formal definition of the basic variant can be found here . Other variants : Some variants are still used, like \"non-classified\", see docs ), but should be avoided. Others like base-plus , a variant that corresponds to base + the inferred axioms, are still under development, and will be explained here when they are fully developed.","title":"Overview of the relevant concepts"},{"location":"explanation/owl-format-variants/#best-practices","text":"Tool developers developing tools that use the ontology (and do not need reasoners), such as database curation tools, web-browsers and similar, should typically use OBO graphs JSON and avoid using OBO format or any of the OWL focussed serialisations (Functional, Manchester or RDF/XML). OWL-focussed serialisations contain a huge deal of axiomatic content that make no sense to most users, and can lead to a variety of mistakes. We have seen it many times that software developers try to interpret OWL axioms somehow to extract relations . Do not do that! Work with the ontologies to ensure they provide the relationships you need in the appropriate form. Tool developers building tools to work with ontologies should typically ensure that they can read and write RDF/XML - this is the most widely understood serialisation. Work with ontologies means here 'enable operations that change the content of the ontology'. Tool developers building infrastructure to query across ontologies should consider using base variants - these ensure that you can always use the latest version of each ontology and avoid most of the common version clashes. It is important that such users are keenly aware of the role of OWL reasoning in such a process. Many users of ontologies think they need the reasoner actually don't . Make sure you consult with an expert before building a system that relies on OWL reasoners to deliver user facing services. As an ontology developer, it is great practice to provide the above variants in the common serialisations. The Ontology Development Kit provides defaults for all of these. As an ontology developer, you should avoid publishing your ontology with owl:imports statements - these are easily ignored by your users and make the intended \"content\" of the ontology quite none-transparent.","title":"Best practices"},{"location":"explanation/taxon-constraints-explainer/","text":"Guide to Taxon Restrictions \u00b6 What are taxon restrictions? \u00b6 Taxon restrictions are a formalised way of to record what species a term applies to - something crucial in multi-species ontologies. Even species neutral ontologies (eg GO) have classes that have implicit taxon restriction. Eg GO:0007595 ! Lactation - defined as \u201cThe secretion of milk by the mammary gland.\u201d Why restrict taxon? \u00b6 Finding inconsistencies Taxon restriction use NCBITaxon which have pairwise disjointness between species (eg Nothing is both an insect and a rodent and a primate) When addint taxon constraints, a reasoner can check for inconsistencies. E.g. when GO implemented taxon restrictions, they found 5874 errors PMID:20973947 Creating SLIMs Allows for use of reasoner to generate taxon related SLIMs Querying E.g. in Brain Data Standards, in_taxon axioms that allow faceting cell types by species. (note: there are limitations on this and may be incomplete) How to add taxon restrictions: \u00b6 Please see how-to guide on adding taxon restrictions Why annotation for some taxon restrictions? \u00b6 Annotations for taxon restrictions are used as a shortcut. These are used to more simply represent complex description. Shortcuts work as a macro that is expanded out (see this document for technical details): Eg C never_in_taxon T -> C disjointWith in-taxon some T","title":"What are taxon constraints?"},{"location":"explanation/taxon-constraints-explainer/#guide-to-taxon-restrictions","text":"","title":"Guide to Taxon Restrictions"},{"location":"explanation/taxon-constraints-explainer/#what-are-taxon-restrictions","text":"Taxon restrictions are a formalised way of to record what species a term applies to - something crucial in multi-species ontologies. Even species neutral ontologies (eg GO) have classes that have implicit taxon restriction. Eg GO:0007595 ! Lactation - defined as \u201cThe secretion of milk by the mammary gland.\u201d","title":"What are taxon restrictions?"},{"location":"explanation/taxon-constraints-explainer/#why-restrict-taxon","text":"Finding inconsistencies Taxon restriction use NCBITaxon which have pairwise disjointness between species (eg Nothing is both an insect and a rodent and a primate) When addint taxon constraints, a reasoner can check for inconsistencies. E.g. when GO implemented taxon restrictions, they found 5874 errors PMID:20973947 Creating SLIMs Allows for use of reasoner to generate taxon related SLIMs Querying E.g. in Brain Data Standards, in_taxon axioms that allow faceting cell types by species. (note: there are limitations on this and may be incomplete)","title":"Why restrict taxon?"},{"location":"explanation/taxon-constraints-explainer/#how-to-add-taxon-restrictions","text":"Please see how-to guide on adding taxon restrictions","title":"How to add taxon restrictions:"},{"location":"explanation/taxon-constraints-explainer/#why-annotation-for-some-taxon-restrictions","text":"Annotations for taxon restrictions are used as a shortcut. These are used to more simply represent complex description. Shortcuts work as a macro that is expanded out (see this document for technical details): Eg C never_in_taxon T -> C disjointWith in-taxon some T","title":"Why annotation for some taxon restrictions?"},{"location":"explanation/term-comments/","text":"Term Comments \u00b6 What are comments? \u00b6 Comments are annotations that may be added to ontology terms to further explain their intended usage, or include information that is useful but does not fit in areas like definition. Some examples of comments, and possible standard language for their usage, are: WARNING: THESE EXAMPLES ARE NOT UNIVERSALLY USED AND CAN BE CONTROVERSIAL IN SOME ONTOLOGIES! PLEASE CHECK WITH THE CONVENTIONS OF YOUR ONTOLOGY BEFORE DOING THIS! Do Not Annotate \u00b6 This term should not be used for direct annotation. It should be possible to make a more specific annotation to one of the children of this term. Example: GO:0006810 transport Note that this term should not be used for direct annotation. It should be possible to make a more specific annotation to one of the children of this term, for e.g. transmembrane transport, microtubule-based transport, vesicle-mediated transport, etc. Do Not Manually Annotate \u00b6 This term should not be used for direct manual annotation. It should be possible to make a more specific manual annotation to one of the children of this term. Example: GO:0000910 cytokinesis Note that this term should not be used for direct annotation. When annotating eukaryotic species, mitotic or meiotic cytokinesis should always be specified for manual annotation and for prokaryotic species use 'FtsZ-dependent cytokinesis; GO:0043093' or 'Cdv-dependent cytokinesis; GO:0061639'. Also, note that cytokinesis does not necessarily result in physical separation and detachment of the two daughter cells from each other. Additional Information \u00b6 Information about the term that do not belong belong in the definition or gloss, but are useful for users or editors. This might include information that is adjacent to the class but pertinent to its usage, extended information about the class (eg extended notes about a characteristic of a cell type) that might be useful but does not belong in the definition, important notes on why certain choices were made in the curation of this terms (eg why certain logical axioms were excluded/included in the way they are) (Note: dependent on ontology, some of these might belong in editors_notes, etc.). Standard language for these are not given as they vary dependent on usage.","title":"Term Comments"},{"location":"explanation/term-comments/#term-comments","text":"","title":"Term Comments"},{"location":"explanation/term-comments/#what-are-comments","text":"Comments are annotations that may be added to ontology terms to further explain their intended usage, or include information that is useful but does not fit in areas like definition. Some examples of comments, and possible standard language for their usage, are: WARNING: THESE EXAMPLES ARE NOT UNIVERSALLY USED AND CAN BE CONTROVERSIAL IN SOME ONTOLOGIES! PLEASE CHECK WITH THE CONVENTIONS OF YOUR ONTOLOGY BEFORE DOING THIS!","title":"What are comments?"},{"location":"explanation/term-comments/#do-not-annotate","text":"This term should not be used for direct annotation. It should be possible to make a more specific annotation to one of the children of this term. Example: GO:0006810 transport Note that this term should not be used for direct annotation. It should be possible to make a more specific annotation to one of the children of this term, for e.g. transmembrane transport, microtubule-based transport, vesicle-mediated transport, etc.","title":"Do Not Annotate"},{"location":"explanation/term-comments/#do-not-manually-annotate","text":"This term should not be used for direct manual annotation. It should be possible to make a more specific manual annotation to one of the children of this term. Example: GO:0000910 cytokinesis Note that this term should not be used for direct annotation. When annotating eukaryotic species, mitotic or meiotic cytokinesis should always be specified for manual annotation and for prokaryotic species use 'FtsZ-dependent cytokinesis; GO:0043093' or 'Cdv-dependent cytokinesis; GO:0061639'. Also, note that cytokinesis does not necessarily result in physical separation and detachment of the two daughter cells from each other.","title":"Do Not Manually Annotate"},{"location":"explanation/term-comments/#additional-information","text":"Information about the term that do not belong belong in the definition or gloss, but are useful for users or editors. This might include information that is adjacent to the class but pertinent to its usage, extended information about the class (eg extended notes about a characteristic of a cell type) that might be useful but does not belong in the definition, important notes on why certain choices were made in the curation of this terms (eg why certain logical axioms were excluded/included in the way they are) (Note: dependent on ontology, some of these might belong in editors_notes, etc.). Standard language for these are not given as they vary dependent on usage.","title":"Additional Information"},{"location":"explanation/which-ontology-to-use/","text":"Which biomedical ontologies should we use? \u00b6 As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \" 10 simple rules for selecting a Bio-ontology \" by Malone et al, but I would further recommend to ask yourself the following: Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation). Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently poly-hierarchical (such as Mondo ), while others strive to be single inheritance ( DO , ICD). Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data. Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being paid every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI ) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS !) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them.","title":"Which biomedical ontologies should we use?"},{"location":"explanation/which-ontology-to-use/#which-biomedical-ontologies-should-we-use","text":"As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \" 10 simple rules for selecting a Bio-ontology \" by Malone et al, but I would further recommend to ask yourself the following: Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation). Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently poly-hierarchical (such as Mondo ), while others strive to be single inheritance ( DO , ICD). Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data. Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being paid every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI ) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS !) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them.","title":"Which biomedical ontologies should we use?"},{"location":"explanation/writing-good-issues/","text":"Writing Good Issues \u00b6 Based on Intro to GitHub (GO-Centric) with credit to Nomi Harris and Chris Mungall Writing a good ticket (or issue) is crucial to good management of a repo. In this explainer, we will discuss some good practices in writing a ticket and show examples of what not to do. Best Practices \u00b6 Search existing issues before creating a new one -- maybe someone else already reported your problem Give your issue a short but descriptive and actionable title Describe the problem and the context and include a repeatable example. Clearly state what needs to be done to close the ticket Tickets should ideally be actionable units that can be closed via a PR Fag relevant people with @ (e.g., @nlharris) Mention related issues with # (e.g., #123) use a complete URL to link to tickets in other repos Make issue titles actionable eg \"Ontology download page on GO website\" is non-actionable, whereas \"Fix URLs on ontology download page on GO website\" is actionable and hence better a title Example of a good ticket \u00b6 Example of a bad ticket \u00b6","title":"Writing good issues"},{"location":"explanation/writing-good-issues/#writing-good-issues","text":"Based on Intro to GitHub (GO-Centric) with credit to Nomi Harris and Chris Mungall Writing a good ticket (or issue) is crucial to good management of a repo. In this explainer, we will discuss some good practices in writing a ticket and show examples of what not to do.","title":"Writing Good Issues"},{"location":"explanation/writing-good-issues/#best-practices","text":"Search existing issues before creating a new one -- maybe someone else already reported your problem Give your issue a short but descriptive and actionable title Describe the problem and the context and include a repeatable example. Clearly state what needs to be done to close the ticket Tickets should ideally be actionable units that can be closed via a PR Fag relevant people with @ (e.g., @nlharris) Mention related issues with # (e.g., #123) use a complete URL to link to tickets in other repos Make issue titles actionable eg \"Ontology download page on GO website\" is non-actionable, whereas \"Fix URLs on ontology download page on GO website\" is actionable and hence better a title","title":"Best Practices"},{"location":"explanation/writing-good-issues/#example-of-a-good-ticket","text":"","title":"Example of a good ticket"},{"location":"explanation/writing-good-issues/#example-of-a-bad-ticket","text":"","title":"Example of a bad ticket"},{"location":"howto/add-new-slim/","text":"NOTE This documentation is incomplete, for now you may be better consulting the GO Editor Docs Adding a new Subset (Slim) \u00b6 See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. In the main Protege window, click on the Annotation Properties tab. Navigate to the subset_property and select it. Click on the top left-hand button of the window to add a new subset property. In the pop-up window add the name of the new slim. The identifier will fill in according to your preferences and will be the next identifier in your set. Click on Refactor in the menu. Select rename entities. Replace the IDSPACE_ identifier with the name of your new slim. It is standard to use the same string as when you created the term. In the annotations tab, click on the + . In the pop up window, select rdfs:comment . In the right hand window, type a small descriptor statement for the slim. Select xsd:string as the type. Click OK to save the changes. You should now see the comment field in the annotations tab. See Daily Curator Workflow section for commit, push and merge instructions. Adding a term to a Subset (Slim) \u00b6 See Daily Curator Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. In Protege, navigate to the term you wish to add to a subset (slim). With the term selected, click on the Entities tab. In the Annotation window on the right, click on the + to the right of 'Annotations' at the very top of the window. In the pop-up window that appears, click on in_subset on the left-hand panel. In the right-hand panel click on the \u2018Entity IRI\u2019 tab. Navigate to the 'Annotation Properties' sub-tab. Navigate to the subtypes of subset_property and select the appropriate slim. Click on OK to save your edits.","title":"Creating a new Slim (under construction)"},{"location":"howto/add-new-slim/#adding-a-new-subset-slim","text":"See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. In the main Protege window, click on the Annotation Properties tab. Navigate to the subset_property and select it. Click on the top left-hand button of the window to add a new subset property. In the pop-up window add the name of the new slim. The identifier will fill in according to your preferences and will be the next identifier in your set. Click on Refactor in the menu. Select rename entities. Replace the IDSPACE_ identifier with the name of your new slim. It is standard to use the same string as when you created the term. In the annotations tab, click on the + . In the pop up window, select rdfs:comment . In the right hand window, type a small descriptor statement for the slim. Select xsd:string as the type. Click OK to save the changes. You should now see the comment field in the annotations tab. See Daily Curator Workflow section for commit, push and merge instructions.","title":"Adding a new Subset (Slim)"},{"location":"howto/add-new-slim/#adding-a-term-to-a-subset-slim","text":"See Daily Curator Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. In Protege, navigate to the term you wish to add to a subset (slim). With the term selected, click on the Entities tab. In the Annotation window on the right, click on the + to the right of 'Annotations' at the very top of the window. In the pop-up window that appears, click on in_subset on the left-hand panel. In the right-hand panel click on the \u2018Entity IRI\u2019 tab. Navigate to the 'Annotation Properties' sub-tab. Navigate to the subtypes of subset_property and select the appropriate slim. Click on OK to save your edits.","title":"Adding a term to a Subset (Slim)"},{"location":"howto/add-taxon-restrictions/","text":"Adding taxon restrictions \u00b6 See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. in taxon relations are added as Subclasses . Navigate to the term for which you want to add the only in taxon restriction. In the Description window click on the + . In the pop-up window type a new relationship (e.g. 'in taxon' some Viridiplantae ). The taxa available are imported ontology terms and can be browsed just like any other ontology term. never in taxon or present in taxon relations added as Annotations . Navigate to the taxon term you want to add a restriction on. Copy the IRI (you can use command U to display this on a Mac) Navigate to the term for which you want to add the never in taxon restriction. In the class annotations window, click on the + . In the left-hand panel, select never_in_taxon or present_in_taxon as appropriate. In the right-hand panel, in the IRI editor panel, paste in the IRI. Click OK to save your changes. (Note - you can achieve the same thing using the Entity IRI tab + navigating to the correct taxon but this is slow and not very practical) See Daily Workflow section for commit, push and merge instructions.","title":"Adding Taxon Restrictions"},{"location":"howto/add-taxon-restrictions/#adding-taxon-restrictions","text":"See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. in taxon relations are added as Subclasses . Navigate to the term for which you want to add the only in taxon restriction. In the Description window click on the + . In the pop-up window type a new relationship (e.g. 'in taxon' some Viridiplantae ). The taxa available are imported ontology terms and can be browsed just like any other ontology term. never in taxon or present in taxon relations added as Annotations . Navigate to the taxon term you want to add a restriction on. Copy the IRI (you can use command U to display this on a Mac) Navigate to the term for which you want to add the never in taxon restriction. In the class annotations window, click on the + . In the left-hand panel, select never_in_taxon or present_in_taxon as appropriate. In the right-hand panel, in the IRI editor panel, paste in the IRI. Click OK to save your changes. (Note - you can achieve the same thing using the Entity IRI tab + navigating to the correct taxon but this is slow and not very practical) See Daily Workflow section for commit, push and merge instructions.","title":"Adding taxon restrictions"},{"location":"howto/change-files-pull-request/","text":"How to change files in an existing pull request \u00b6 Using GitHub \u00b6 Warning: You should only use this method if the files you are editing are reasonably small (less than 1 MB). This method only works if the file you want to edit has already been editing as part of the pull request. Go to the pull request on GitHub, and click on the \"Files Changed\" tab up top Find the file you want to edit in the pull request. On the right, click on on the three ... , and then \"Edit file\". If this option is greyed out, it means that - you don't have edit rights on the repository - the edit was made from a different fork, and the person that created the pull request did not activate the \"Allow maintainers to make edits\" option when submitting the PR - the pull request has already been merged Do the edits, and then commit changes, usually to the same branch","title":"Change a pull request"},{"location":"howto/change-files-pull-request/#how-to-change-files-in-an-existing-pull-request","text":"","title":"How to change files in an existing pull request"},{"location":"howto/change-files-pull-request/#using-github","text":"Warning: You should only use this method if the files you are editing are reasonably small (less than 1 MB). This method only works if the file you want to edit has already been editing as part of the pull request. Go to the pull request on GitHub, and click on the \"Files Changed\" tab up top Find the file you want to edit in the pull request. On the right, click on on the three ... , and then \"Edit file\". If this option is greyed out, it means that - you don't have edit rights on the repository - the edit was made from a different fork, and the person that created the pull request did not activate the \"Allow maintainers to make edits\" option when submitting the PR - the pull request has already been merged Do the edits, and then commit changes, usually to the same branch","title":"Using GitHub"},{"location":"howto/clone-mondo-repo/","text":"Cloning a repo \u00b6 Prerequisite : Install Github Desktop Github Desktop can be downloaded here For the purpose of going through this how-to guide, we will use Mondo as an example. However, all obo onotlogies can be cloned in a similar way. Open the GitHub repository where the ontology you want to clone lives, in this case, Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time depending on the size of the repository. Open the Ontology in Protege \u00b6 Open Protege Go to: File -> Open Navigate to mondo/src/ontology/mondo-edit.obo and open this file in Protege Note: mondo can be replaced with any ontology that is setup using the ODK as their architecture should be the same. If this all works okay, you are all set to start editing!","title":"Clone a repository"},{"location":"howto/clone-mondo-repo/#cloning-a-repo","text":"Prerequisite : Install Github Desktop Github Desktop can be downloaded here For the purpose of going through this how-to guide, we will use Mondo as an example. However, all obo onotlogies can be cloned in a similar way. Open the GitHub repository where the ontology you want to clone lives, in this case, Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time depending on the size of the repository.","title":"Cloning a repo"},{"location":"howto/clone-mondo-repo/#open-the-ontology-in-protege","text":"Open Protege Go to: File -> Open Navigate to mondo/src/ontology/mondo-edit.obo and open this file in Protege Note: mondo can be replaced with any ontology that is setup using the ODK as their architecture should be the same. If this all works okay, you are all set to start editing!","title":"Open the Ontology in Protege"},{"location":"howto/create-new-term/","text":"Creating a New Ontology Term in Protege \u00b6 To create a new term, the 'Asserted view' must be active (not the 'Inferred view'). In the Class hierarchy window, click on the 'Add subclass' button at the upper left of the window. A pop-up window will appear asking you to enter the Name of the new term. When you enter the term name, you will see your ID automatically populate the IRI box. Once you have entered the term, click 'OK' to save the new term. You will see it appear in the class hierarchy. In the annotation window add: Definition Click on the + next to Annotations Select defintion (if there are multiple, you should use IAO:0000115) Add the textual definition in the pop-up box. Click OK. 2. Add Definition References 1. Click on the circle with the \u2018@\u2019 in it next to definition and in the resulting pop-up click on the ```+``` to add a new ref, making sure they are properly formatted with a database abbreviation followed by a colon, followed by the text string or ID. Examples: ```PMID:27450630```. 2. Click OK. 3. Add each definition reference separately by clicking on the ```+``` sign. 3. Add synonyms and dbxrefs following the same procedure if they are required for the term. Add appropriate logical axioms in the Description by clicking the + sign in the appropriate section (usually SubClass Of) and typing it in, using Tab to autocomplete terms. Converting to Equivalent To axioms: If you want to convert your SubClassOf axioms to EquivalentTo axioms, you can select the appropriate rows and right click, selecting \"Convert selected rows to defined class\" In some cases, logical axioms reuiqre external ontologies (eg in the above example, the newly added CL term has_soma_location in the cerebellar cortex which is an uberon term), it might be necessary to import the term in. For instructions on how to do this, please see the import managment section of your local ontology documentation (an example of that in CL can be found here: https://obophenotype.github.io/cell-ontology/odk-workflows/UpdateImports/) When you have finished adding the term, run the reasoner to ensure that nothing is problematic with the axioms you have added (if there is an issue, you will see it being asserted under owl:Nothing) Save the file on protege and review the changes you have made in your Github Desktop (or use git diff in your terminal if you do not use Github Desktop) See Daily Workflow section for commit, push and merge instructions.","title":"Creating New Terms in Protege"},{"location":"howto/create-new-term/#creating-a-new-ontology-term-in-protege","text":"To create a new term, the 'Asserted view' must be active (not the 'Inferred view'). In the Class hierarchy window, click on the 'Add subclass' button at the upper left of the window. A pop-up window will appear asking you to enter the Name of the new term. When you enter the term name, you will see your ID automatically populate the IRI box. Once you have entered the term, click 'OK' to save the new term. You will see it appear in the class hierarchy. In the annotation window add: Definition Click on the + next to Annotations Select defintion (if there are multiple, you should use IAO:0000115) Add the textual definition in the pop-up box. Click OK. 2. Add Definition References 1. Click on the circle with the \u2018@\u2019 in it next to definition and in the resulting pop-up click on the ```+``` to add a new ref, making sure they are properly formatted with a database abbreviation followed by a colon, followed by the text string or ID. Examples: ```PMID:27450630```. 2. Click OK. 3. Add each definition reference separately by clicking on the ```+``` sign. 3. Add synonyms and dbxrefs following the same procedure if they are required for the term. Add appropriate logical axioms in the Description by clicking the + sign in the appropriate section (usually SubClass Of) and typing it in, using Tab to autocomplete terms. Converting to Equivalent To axioms: If you want to convert your SubClassOf axioms to EquivalentTo axioms, you can select the appropriate rows and right click, selecting \"Convert selected rows to defined class\" In some cases, logical axioms reuiqre external ontologies (eg in the above example, the newly added CL term has_soma_location in the cerebellar cortex which is an uberon term), it might be necessary to import the term in. For instructions on how to do this, please see the import managment section of your local ontology documentation (an example of that in CL can be found here: https://obophenotype.github.io/cell-ontology/odk-workflows/UpdateImports/) When you have finished adding the term, run the reasoner to ensure that nothing is problematic with the axioms you have added (if there is an issue, you will see it being asserted under owl:Nothing) Save the file on protege and review the changes you have made in your Github Desktop (or use git diff in your terminal if you do not use Github Desktop) See Daily Workflow section for commit, push and merge instructions.","title":"Creating a New Ontology Term in Protege"},{"location":"howto/create-ontology-from-scratch/","text":"How to create an OBO ontology from scratch \u00b6 Editors : Nicolas Matentzoglu (@matentzn) Sabrina Toro (@sabrinatoro) Summary : This is a guide to build an OBO ontology from scratch. We will focus on the kind of thought processes you want to go through, and providing the following: Reasons for NOT building an ontology A basic recipe for getting started An overview of different starting points on your journey to building an ontology A guide for deciding what kind of ontology you want to build An example walk-through of the process. Minimal conditions for building an ontology \u00b6 Before reading on, there are three simple rules for when NOT to build an ontology everyone interested in ontologies should master, like a mantra: Do not build a new ontology if: one in scope already exists (none-in-scope condition). something simpler than a full-fledged OWL ontology can do the job (something-simpler-works condition). there is not at least one glass-clear use case written down which could be addressed by the existence of the ontology (killer-use-case condition). None-in-scope condition \u00b6 Scope is one of the hardest and most debated subjects in the OBO Foundry operation calls. There are essentially two aspects to scope: The entities you intended to model belong to some specific biological categories. For example phenotype , disease , anatomical entity , assay , environmental exposure , biological process , chemical entity . Before setting out to build an ontology, you should get a rough sense of what kind of entities you need to describe your domain. However, this is an iterative process and more entities will be revealed later on. The subject domain you intend to model. For example, you may want to provide an ontology to describe the domain of Alzheimer's Disease , which will need many different kinds of biological entities (like anatomical entity and disease classes). As a rule of thumb, you should NOT create a term if another OBO ontology has a branch of for entities of the same kind . For example, if you have to add terms for assays, you should work with the Ontology for Biomedical Investigations to add these to their assay branch . Remember, the vision of OBO is to build a semantically coherent ontology for all of biology, and the individual ontologies in the OBO Foundry should be considered \"modules\" of this super ontology. You will find that while collaboration is always hard the only way for our community to be sustainable and compete with commercial solutions is to take that hard route and work together. Something-simpler-works condition \u00b6 There are many kinds of semantic artefacts that can work for your use case: Controlled vocabularies: Creating identifiers for concepts in your domain and without too much concern for logical reasoning. Some examples can be are Linked Open Data Vocabularies (LOV) or schema.org vocabularies . Sometimes a table of identifiers in an SQL database is enough. Thesauri: Describe the synonyms used in your domain in a standardised fashion. Taxonomies: Create a hierarchical categorisations for concepts in your domain, without any specific regards for semantics. You just create a hierarchy that \"makes some sense\" for your use case. Examples: ICD10 , United Nations Standard Products and Services Code (UNSPSC) . Semantic data models: If you need to define how terms in your database should be constrained in a semantic way (similar to a database schema), then Shape languages like SHEX or SHACL may be much more suitable for your use case. See LinkML tutorials to get a sense of this: you will build a semantic data model in Yaml which can then be exported to SHACL, OWL or JSON Schema (great tutorial, useful to do no matter what). Ontologies: Sets of logical axioms. If you require formal reasoning (and only then!) does it make sense to jump in the deep pit of ontology engineering. This is, by far, the hardest to build of the bunch. You will have to wrestle with Logic, Open World Assumption and many more arcane subjects. Think of it in terms of cost. Building a simple vocabulary with minimal axiomatisation is 10x cheaper than building a full fledged domain model in OWL, and helps solving your use case just the same. Do not start building an ontology unless you have some understanding of these alternatives first. Killer-use-case condition \u00b6 Do not build an ontology because someone tells you to or because you \"think it might be useful\". Write out a proper use case description, for example in the form of an agile user story , convince two or three colleagues this is worthwhile and only then get to work. Many ontologies are created for very vague use cases, and not only do they cost you time to build, they also cost the rest of the community time - time it takes them to figure out that they do not want to use your ontology. Often, someone you trust tells you to build one and you believe they know what they are doing - do not do that. Question the use of building the ontology until you are convinced it is the right thing to do. If you do not care about reasoning (either for validation or for your application), do not build an ontology. Basic recipe to start building an ontology \u00b6 Depending on your specific starting points, the way you start will be slightly different, but some common principles apply. Write down the use cases for the ontology (see above). This will determine certain design decisions later on. These should be concrete, like: controlled vocabulary for named entity recognition, logical model of a domain, auto-classification of data. Make a table of all similar ontologies that exist, within and outside OBO (this requires research , and is an essential part of the process). Document exactly in what way they are different from your use case, and why you need to build a new one (see none-in-scope condition above). Determine whether you have something to start from. Often, you will have a database with entities you may wish to turn into classes in your ontology. See starting points below . Gather your tools . You need to think about tools for at least two kinds of workflows to start with: Curation workflows: How will you edit your ontology? Some simple ontologies are edited using tables that link to logical templates . Others are edited primarily with Protege . Continuous integration and release workflows: How will you import terms from other ontologies? How will you ensure the quality of you ontology moving forward? Decide on the Ontology ID (important, do not skip). Changing this later can be extremely costly. Refer to the OBO ID policy for details. An ID should be short and unique. Create a basic set-up for managing your workflows. This comprises (usually) three aspects (you may wish to try and use the Ontology Development Kit - it does exactly that): Make a GitHub repository. Add your editors files (owl, tsv, whatever you decided to use) to that repository. Implement some workflow system, i.e. some way to run commands like release or test , as you will run these repeatedly. A typical system to achieve this is make , and many projects choose to encode their workflows as make targets ( ODK , OBI Makfile ). Determine the metadata and logical patterns you wish to employ for your curation. Here it is important that you determine what kind of an ontology you want to build. Note : Later in the process, you also want to think about the following: Think about how to manage your ontology project : Which roles you need, and how you manage community contributions. Starting points \u00b6 There are many different starting points for building an ontology: We have a database or a dataset and want to build an ontology that covers entities in that database. As a variation, you have two or more databases that you need to integrate. We already have a basic ontology in our domain (a cell ontology, an anatomy ontology), but need to build an extension (e.g. a species specific extension to an existing cross-species ontology). We have controlled vocabulary or a list of standard or commonly used terms for a domain and want to formalise them in an ontology for interoperability and machine-readability, with versioning support to manage evolution. Sometimes, we may even wish to simply using ontology infrastructure (tools and best practices) to maintain a quite informal vocabulary structure. There are existing ontologies that are, however, not quite fit for purpose (even if they should be) and there's no way to make any of them right, so I have to create Yet Another Variant. We have a large, hierarchical enumeration in a datamodel that pulls terms from many ontologies. We need to build a completely new ontology for a domain that currently does not even have a controlled vocabulary. This case almost never happens nowadays. In this case, all domain knowledge (concepts and their relations) is somewhere in the heads of the experts. What kind of ontology do you need? \u00b6 There are two fundamentally different kinds of ontologies which need to be distinguished: Project ontologies (sometimes referred to as application ontologies) are ontologies that are developed to fulfil a specific use case, like: Grouping data in your project Indexing search engines or your organisation Informing Natural Language Processing applications Populating the biocuration interface your organisation provides to enable curators to annotate data Domain ontologies are ontologies which seek to model a domain of discourse. In particular they: Reflect scientific consensus and are therefore social and collaborative enterprises subject to change Are build with re-use in mind: They re-use terms from other domain ontologies They provide terms intended for re-use by other ontologies They work with other ontologies on implementing consistent logical patterns that apply across all domain ontologies in the community. Are logically consistent with all ontologies they depend on, refer to, import. Some things to consider: It is extremely hard to build domain ontologies. Do not try to do that without a proper sustainability plan (i.e. considerable resources over multiple years). Project ontologies are not bad domain ontologies . Project ontologies can be build according to the same standards as domain ontologies. While controversial, the OBO Foundry is currently (March 2022) debating whether project ontologies are admissible to the OBO ontology library. Project ontologies can have huge impact . One of the most impactful ontologies in the biomedical world is the Experimental Factor Ontology (EFO) - a massive project ontology used for many applications from knowledge graph integration to biocuration. Project ontologies are allowed to change the semantics of imported ontologies, for example by adding additional axioms or even removing some - anything necessary to achieve the use case! Domain ontologies (in the OBO world) are not allowed to change semantics of imported ontologies. Project ontologies can import terms from domain ontologies, and coin their own terms where necessary. This can be a good option if resources are scarce, and there is not enough time for consensus building with the community or the often lengthy contribution workflows. \"I just need some terms\" usually points to \"I need a project ontology\". Domain ontologies seek to model a domain exhaustively: any concept that \"belongs\" to that domain is a strong candidate for a term. It is imperative that it is clear which of the two you are building. Project ontologies sold as domain ontologies are a very common practice and they cause a lot of harm for open biomedical data integration. Example: Building Vertebrate Breed Ontology \u00b6 We will re-iterate some of the steps taken to develop the Vertebrate Breed Ontology . At the time of this writing, the VBO is still in early stages, but it nicely illustrates all the points above. Use case \u00b6 See here . Initial interactions with the OMIA team further determined more long term goals such as phenotypic similarity and reasoning. Similar ontologies \u00b6 Similar ontologies. While there is no ontology OBO ontology related to breeds, the Livestock Breed Ontology (LBO) served as an inspiration (much different scale). NCBI taxonomy is a more general ontology about existing taxa as they occur in the wild. Starting point \u00b6 Our starting point was the raw OMIA data. We got a list of breeds from DAD-IS, which includes name of the breed, transboundary name, species, country, and more We first had to understand the data and how the different pieces of data relate to each other. Some breed names are the same, but refer to either different species and/or different countries Several breeds share a common \"transboundary name\", which represent the original breed from which they come from We needed to determine what a single concept / an indentifiable term would be In order to define a single breed, we needed to include the name of the breed, the transboundary name (when applicable), the species, and the country We needed to understand the metadata and how each concepts relate to each other 'breed' is an instance of 'species', therefore 'species' should be the parent term of 'breeds' (using a is_a relation) when applicable, 'transboundary' should be the parent term of 'breeds' Note about the concept of \"species\": is \"species\" equivalent to the NCBI Taxon representing \"species\"? Design decision: Since species represents the same concept as \u2018species\u2019 in NCBI, the ontology should be built \u2018on top of\u2019 NCBI terms to avoid confusion of concepts and to avoid conflation of terms with the same concept Warnings based on our experience: Always retain links to original source ids (encoding problems, update problems) Always add provenance to as much information as you can (where do labels come from?) Gather your tools \u00b6 For us this was using Google Sheets, ROBOT & ODK. The Ontology ID \u00b6 At first, we chose to name the ontology \"Unified Breed Ontology\" (UBO). Which meant that for everything from ODK setup to creating identifiers for our terms, we used the UBO prefix. Later in the process, we decided to change the name to \"Vertebrate Breed Ontology\". Migrating all the terms and the ODK setup from ubo to vbo required some expert knowledge on the workings of the ODK, and created an unnecessary cost. We should have finalised the choice of name first. Create a basic set up \u00b6 Making a Repo with ODK Develop a workflow that turns a Google Sheet into a component . Determine the metadata and logical patterns you wish to employ. \u00b6 We decided to build a domain ontology, for the representation of vertebrate breeds. As our initial data is relatively flat, we decided to use ROBOT templates and Google Sheets to manage them. Some notes, need to be cleaned up (ignore) \u00b6 Creation of components: for basic information: each \u201clayer\u201d is built in a google sheet for example: Transboundary: are children of species Breeds: are children of either species or transboundary (therefore we need transboundary and species in order to be able to add breeds) Addition of new information as we have them E.g. adding xref and synonym from OMIA Upcoming: xref and synonym form another database. Future: Continue to add to the original document and/or create new components Acknowledgements \u00b6 Thank you to Melanie Courtot, Sierra Moxon, John Graybeal, Chris Stoeckert, Lars Vogt and Nomi Harris for their helpful comments on this how-to.","title":"Creating an ontology from scratch"},{"location":"howto/create-ontology-from-scratch/#how-to-create-an-obo-ontology-from-scratch","text":"Editors : Nicolas Matentzoglu (@matentzn) Sabrina Toro (@sabrinatoro) Summary : This is a guide to build an OBO ontology from scratch. We will focus on the kind of thought processes you want to go through, and providing the following: Reasons for NOT building an ontology A basic recipe for getting started An overview of different starting points on your journey to building an ontology A guide for deciding what kind of ontology you want to build An example walk-through of the process.","title":"How to create an OBO ontology from scratch"},{"location":"howto/create-ontology-from-scratch/#minimal-conditions-for-building-an-ontology","text":"Before reading on, there are three simple rules for when NOT to build an ontology everyone interested in ontologies should master, like a mantra: Do not build a new ontology if: one in scope already exists (none-in-scope condition). something simpler than a full-fledged OWL ontology can do the job (something-simpler-works condition). there is not at least one glass-clear use case written down which could be addressed by the existence of the ontology (killer-use-case condition).","title":"Minimal conditions for building an ontology"},{"location":"howto/create-ontology-from-scratch/#none-in-scope-condition","text":"Scope is one of the hardest and most debated subjects in the OBO Foundry operation calls. There are essentially two aspects to scope: The entities you intended to model belong to some specific biological categories. For example phenotype , disease , anatomical entity , assay , environmental exposure , biological process , chemical entity . Before setting out to build an ontology, you should get a rough sense of what kind of entities you need to describe your domain. However, this is an iterative process and more entities will be revealed later on. The subject domain you intend to model. For example, you may want to provide an ontology to describe the domain of Alzheimer's Disease , which will need many different kinds of biological entities (like anatomical entity and disease classes). As a rule of thumb, you should NOT create a term if another OBO ontology has a branch of for entities of the same kind . For example, if you have to add terms for assays, you should work with the Ontology for Biomedical Investigations to add these to their assay branch . Remember, the vision of OBO is to build a semantically coherent ontology for all of biology, and the individual ontologies in the OBO Foundry should be considered \"modules\" of this super ontology. You will find that while collaboration is always hard the only way for our community to be sustainable and compete with commercial solutions is to take that hard route and work together.","title":"None-in-scope condition"},{"location":"howto/create-ontology-from-scratch/#something-simpler-works-condition","text":"There are many kinds of semantic artefacts that can work for your use case: Controlled vocabularies: Creating identifiers for concepts in your domain and without too much concern for logical reasoning. Some examples can be are Linked Open Data Vocabularies (LOV) or schema.org vocabularies . Sometimes a table of identifiers in an SQL database is enough. Thesauri: Describe the synonyms used in your domain in a standardised fashion. Taxonomies: Create a hierarchical categorisations for concepts in your domain, without any specific regards for semantics. You just create a hierarchy that \"makes some sense\" for your use case. Examples: ICD10 , United Nations Standard Products and Services Code (UNSPSC) . Semantic data models: If you need to define how terms in your database should be constrained in a semantic way (similar to a database schema), then Shape languages like SHEX or SHACL may be much more suitable for your use case. See LinkML tutorials to get a sense of this: you will build a semantic data model in Yaml which can then be exported to SHACL, OWL or JSON Schema (great tutorial, useful to do no matter what). Ontologies: Sets of logical axioms. If you require formal reasoning (and only then!) does it make sense to jump in the deep pit of ontology engineering. This is, by far, the hardest to build of the bunch. You will have to wrestle with Logic, Open World Assumption and many more arcane subjects. Think of it in terms of cost. Building a simple vocabulary with minimal axiomatisation is 10x cheaper than building a full fledged domain model in OWL, and helps solving your use case just the same. Do not start building an ontology unless you have some understanding of these alternatives first.","title":"Something-simpler-works condition"},{"location":"howto/create-ontology-from-scratch/#killer-use-case-condition","text":"Do not build an ontology because someone tells you to or because you \"think it might be useful\". Write out a proper use case description, for example in the form of an agile user story , convince two or three colleagues this is worthwhile and only then get to work. Many ontologies are created for very vague use cases, and not only do they cost you time to build, they also cost the rest of the community time - time it takes them to figure out that they do not want to use your ontology. Often, someone you trust tells you to build one and you believe they know what they are doing - do not do that. Question the use of building the ontology until you are convinced it is the right thing to do. If you do not care about reasoning (either for validation or for your application), do not build an ontology.","title":"Killer-use-case condition"},{"location":"howto/create-ontology-from-scratch/#basic-recipe-to-start-building-an-ontology","text":"Depending on your specific starting points, the way you start will be slightly different, but some common principles apply. Write down the use cases for the ontology (see above). This will determine certain design decisions later on. These should be concrete, like: controlled vocabulary for named entity recognition, logical model of a domain, auto-classification of data. Make a table of all similar ontologies that exist, within and outside OBO (this requires research , and is an essential part of the process). Document exactly in what way they are different from your use case, and why you need to build a new one (see none-in-scope condition above). Determine whether you have something to start from. Often, you will have a database with entities you may wish to turn into classes in your ontology. See starting points below . Gather your tools . You need to think about tools for at least two kinds of workflows to start with: Curation workflows: How will you edit your ontology? Some simple ontologies are edited using tables that link to logical templates . Others are edited primarily with Protege . Continuous integration and release workflows: How will you import terms from other ontologies? How will you ensure the quality of you ontology moving forward? Decide on the Ontology ID (important, do not skip). Changing this later can be extremely costly. Refer to the OBO ID policy for details. An ID should be short and unique. Create a basic set-up for managing your workflows. This comprises (usually) three aspects (you may wish to try and use the Ontology Development Kit - it does exactly that): Make a GitHub repository. Add your editors files (owl, tsv, whatever you decided to use) to that repository. Implement some workflow system, i.e. some way to run commands like release or test , as you will run these repeatedly. A typical system to achieve this is make , and many projects choose to encode their workflows as make targets ( ODK , OBI Makfile ). Determine the metadata and logical patterns you wish to employ for your curation. Here it is important that you determine what kind of an ontology you want to build. Note : Later in the process, you also want to think about the following: Think about how to manage your ontology project : Which roles you need, and how you manage community contributions.","title":"Basic recipe to start building an ontology"},{"location":"howto/create-ontology-from-scratch/#starting-points","text":"There are many different starting points for building an ontology: We have a database or a dataset and want to build an ontology that covers entities in that database. As a variation, you have two or more databases that you need to integrate. We already have a basic ontology in our domain (a cell ontology, an anatomy ontology), but need to build an extension (e.g. a species specific extension to an existing cross-species ontology). We have controlled vocabulary or a list of standard or commonly used terms for a domain and want to formalise them in an ontology for interoperability and machine-readability, with versioning support to manage evolution. Sometimes, we may even wish to simply using ontology infrastructure (tools and best practices) to maintain a quite informal vocabulary structure. There are existing ontologies that are, however, not quite fit for purpose (even if they should be) and there's no way to make any of them right, so I have to create Yet Another Variant. We have a large, hierarchical enumeration in a datamodel that pulls terms from many ontologies. We need to build a completely new ontology for a domain that currently does not even have a controlled vocabulary. This case almost never happens nowadays. In this case, all domain knowledge (concepts and their relations) is somewhere in the heads of the experts.","title":"Starting points"},{"location":"howto/create-ontology-from-scratch/#what-kind-of-ontology-do-you-need","text":"There are two fundamentally different kinds of ontologies which need to be distinguished: Project ontologies (sometimes referred to as application ontologies) are ontologies that are developed to fulfil a specific use case, like: Grouping data in your project Indexing search engines or your organisation Informing Natural Language Processing applications Populating the biocuration interface your organisation provides to enable curators to annotate data Domain ontologies are ontologies which seek to model a domain of discourse. In particular they: Reflect scientific consensus and are therefore social and collaborative enterprises subject to change Are build with re-use in mind: They re-use terms from other domain ontologies They provide terms intended for re-use by other ontologies They work with other ontologies on implementing consistent logical patterns that apply across all domain ontologies in the community. Are logically consistent with all ontologies they depend on, refer to, import. Some things to consider: It is extremely hard to build domain ontologies. Do not try to do that without a proper sustainability plan (i.e. considerable resources over multiple years). Project ontologies are not bad domain ontologies . Project ontologies can be build according to the same standards as domain ontologies. While controversial, the OBO Foundry is currently (March 2022) debating whether project ontologies are admissible to the OBO ontology library. Project ontologies can have huge impact . One of the most impactful ontologies in the biomedical world is the Experimental Factor Ontology (EFO) - a massive project ontology used for many applications from knowledge graph integration to biocuration. Project ontologies are allowed to change the semantics of imported ontologies, for example by adding additional axioms or even removing some - anything necessary to achieve the use case! Domain ontologies (in the OBO world) are not allowed to change semantics of imported ontologies. Project ontologies can import terms from domain ontologies, and coin their own terms where necessary. This can be a good option if resources are scarce, and there is not enough time for consensus building with the community or the often lengthy contribution workflows. \"I just need some terms\" usually points to \"I need a project ontology\". Domain ontologies seek to model a domain exhaustively: any concept that \"belongs\" to that domain is a strong candidate for a term. It is imperative that it is clear which of the two you are building. Project ontologies sold as domain ontologies are a very common practice and they cause a lot of harm for open biomedical data integration.","title":"What kind of ontology do you need?"},{"location":"howto/create-ontology-from-scratch/#example-building-vertebrate-breed-ontology","text":"We will re-iterate some of the steps taken to develop the Vertebrate Breed Ontology . At the time of this writing, the VBO is still in early stages, but it nicely illustrates all the points above.","title":"Example: Building Vertebrate Breed Ontology"},{"location":"howto/create-ontology-from-scratch/#use-case","text":"See here . Initial interactions with the OMIA team further determined more long term goals such as phenotypic similarity and reasoning.","title":"Use case"},{"location":"howto/create-ontology-from-scratch/#similar-ontologies","text":"Similar ontologies. While there is no ontology OBO ontology related to breeds, the Livestock Breed Ontology (LBO) served as an inspiration (much different scale). NCBI taxonomy is a more general ontology about existing taxa as they occur in the wild.","title":"Similar ontologies"},{"location":"howto/create-ontology-from-scratch/#starting-point","text":"Our starting point was the raw OMIA data. We got a list of breeds from DAD-IS, which includes name of the breed, transboundary name, species, country, and more We first had to understand the data and how the different pieces of data relate to each other. Some breed names are the same, but refer to either different species and/or different countries Several breeds share a common \"transboundary name\", which represent the original breed from which they come from We needed to determine what a single concept / an indentifiable term would be In order to define a single breed, we needed to include the name of the breed, the transboundary name (when applicable), the species, and the country We needed to understand the metadata and how each concepts relate to each other 'breed' is an instance of 'species', therefore 'species' should be the parent term of 'breeds' (using a is_a relation) when applicable, 'transboundary' should be the parent term of 'breeds' Note about the concept of \"species\": is \"species\" equivalent to the NCBI Taxon representing \"species\"? Design decision: Since species represents the same concept as \u2018species\u2019 in NCBI, the ontology should be built \u2018on top of\u2019 NCBI terms to avoid confusion of concepts and to avoid conflation of terms with the same concept Warnings based on our experience: Always retain links to original source ids (encoding problems, update problems) Always add provenance to as much information as you can (where do labels come from?)","title":"Starting point"},{"location":"howto/create-ontology-from-scratch/#gather-your-tools","text":"For us this was using Google Sheets, ROBOT & ODK.","title":"Gather your tools"},{"location":"howto/create-ontology-from-scratch/#the-ontology-id","text":"At first, we chose to name the ontology \"Unified Breed Ontology\" (UBO). Which meant that for everything from ODK setup to creating identifiers for our terms, we used the UBO prefix. Later in the process, we decided to change the name to \"Vertebrate Breed Ontology\". Migrating all the terms and the ODK setup from ubo to vbo required some expert knowledge on the workings of the ODK, and created an unnecessary cost. We should have finalised the choice of name first.","title":"The Ontology ID"},{"location":"howto/create-ontology-from-scratch/#create-a-basic-set-up","text":"Making a Repo with ODK Develop a workflow that turns a Google Sheet into a component .","title":"Create a basic set up"},{"location":"howto/create-ontology-from-scratch/#determine-the-metadata-and-logical-patterns-you-wish-to-employ","text":"We decided to build a domain ontology, for the representation of vertebrate breeds. As our initial data is relatively flat, we decided to use ROBOT templates and Google Sheets to manage them.","title":"Determine the metadata and logical patterns you wish to employ."},{"location":"howto/create-ontology-from-scratch/#some-notes-need-to-be-cleaned-up-ignore","text":"Creation of components: for basic information: each \u201clayer\u201d is built in a google sheet for example: Transboundary: are children of species Breeds: are children of either species or transboundary (therefore we need transboundary and species in order to be able to add breeds) Addition of new information as we have them E.g. adding xref and synonym from OMIA Upcoming: xref and synonym form another database. Future: Continue to add to the original document and/or create new components","title":"Some notes, need to be cleaned up (ignore)"},{"location":"howto/create-ontology-from-scratch/#acknowledgements","text":"Thank you to Melanie Courtot, Sierra Moxon, John Graybeal, Chris Stoeckert, Lars Vogt and Nomi Harris for their helpful comments on this how-to.","title":"Acknowledgements"},{"location":"howto/daily-curator-workflow/","text":"Daily Ontology Curator Workflow with GitHub \u00b6 Updating the local copy of the ontology with 'git pull' \u00b6 Navigate to the ontology directory of go-ontology: cd repos/MY-ONTOLOGY/src/ontology . If the terminal window is not configured to display the branch name, type: git status . You will see: On branch [master] [or the name of the branch you are on] Your branch is up-to-date with 'origin/master'. If you\u2019re not in the master branch, type: git checkout master . From the master branch, type: git pull . This will update your master branch, and all working branches, with the files that are most current on GitHub, bringing in and merging any changes that were made since you last pulled the repository using the command git pull . You will see something like this: ~/repos/MY-ONTOLOGY(master) $ git pull remote: Counting objects: 26, done. remote: Compressing objects: 100% (26/26), done. remote: Total 26 (delta 12), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (26/26), done. From https://github.com/geneontology/go-ontology 580c01d..7225e89 master -> origin/master * [new branch] issue#13029 -> origin/issue#13029 Updating 580c01d..7225e89 Fast-forward src/ontology/go-edit.obo | 39 ++++++++++++++++++++++++--------------- 1 file changed, 24 insertions(+), 15 deletions(-) ~/repos/MY-ONTOLOGY(master) $ Creating a New Working Branch with 'git checkout' \u00b6 When starting to work on a ticket, you should create a new branch of the repository to edit the ontology file. Make sure you are on the master branch before creating a new branch . If the terminal window is not configured to display the branch name, type: git status to check which is the active branch. If necessary, go to master by typing git checkout master . To create a new branch, type: git checkout -b issue-NNNNN in the terminal window. For naming branches , we recommend using the string 'issue-' followed by the issue number. For instance, for this issue in the tracker: https://github.com/geneontology/go-ontology/issues/13390, you would create this branch: git checkout -b issue-13390 . Typing this command will automatically put you in the new branch. You will see this message in your terminal window: ~/repos/MY-ONTOLOGY/src/ontology(master) $ git checkout -b issue-13390 Switched to a new branch 'issue-13390' ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $ Continuing work on an existing Working Branch \u00b6 If you are continuing to do work on an existing branch, in addition to updating master, go to your branch by typing git checkout [branch name] . Note that you can view the existing local branches by typing git branch -l . OPTIONAL : To update the working branch with respect to the current version of the ontology, type git pull origin master . This step is optional because it is not necessary to work on the current version of the ontology; all changes will be synchronized when git merge is performed. Loading, navigating and saving the Ontology in Prot\u00e9g\u00e9 \u00b6 Before launching Prot\u00e9g\u00e9, make sure you are in the correct branch. To check the active branch, type git status . Click on the 'File' pulldown. Open the file: go-edit.obo. The first time, you will have to navigate to repos/MY-ONTOLOGY/src/ontology . Once you have worked on the file, it will show up in the menu under 'Open'/'Recent'. Click on the 'Classes' tab. Searching: Use the search box on the upper right to search for a term in the ontology. Wait for autocomplete to work in the pop-up window. Viewing a term: Double-click on the term. This will reveal the term in the 'Class hierarchy' window after a few seconds. Launching the reasoner: To see the term in the 'Class hierarchy' (inferred) window, you will need to run the 'ELK reasoner'. 'Reasoner' > select ELK 0.4.3, then click 'Start reasoner'. Close the various pop-up warnings about the ELK reasoner. You will now see the terms in the inferred hierarchy. After modification of the ontology, synchronize the reasoner. Go to menu: 'Reasoner' > ' Synchronize reasoner'. NOTE : The only changes that the reasoner will detect are those impacting the ontology structure: changes in equivalence axioms, subclasses, merges, obsoletions, new terms. TIP : When adding new relations/axioms, 'Synchronize' the reasoner. When deleting relations/axioms, it is more reliable to 'Stop' and 'Start' the reasoner again. Use File > Save to save your changes. Committing, pushing and merging your changes to the repository \u00b6 Review : Changes made to the ontology can be viewed by typing git diff in the terminal window. If there are changes that have already been committed, the changes in the active branch relative to master can be viewed by typing git diff master . Commit : Changes can be committed by typing: git commit -m \u2018Meaningful message Fixes #ticketnumber\u2019 go-edit.obo . For example: git commit -m \u2018hepatic stellate cell migration and contraction and regulation terms. Fixes #13390\u2019 go-edit.obo This will save the changes to the go-edit.obo file. The terminal window will show something like: ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $ git commit -m 'Added hepatic stellate cell migration and contraction and regulation terms. Fixes #13390' go-edit.obo [issue-13390 dec9df0] Added hepatic stellate cell migration and contraction and regulation terms. Fixes #13390 1 file changed, 79 insertions(+) ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $ NOTE : The word 'fixes' is a magic word in GitHub; when used in combination with the ticket number, it will automatically close the ticket. In the above example, when the file is merged in GitHub, it will close issue number 13390. Learn more on this GitHub Help Documentation page about 'Closing issues via commit messages' . 'Fixes' is case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE : It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. TIP : Git needs to know who is committing changes to the repository, so the first time you commit, you may see the following message: Committer: Kimberly Van Auken vanauken@kimberlukensmbp.dhcp.lbnl.us Your name and email address were configured automatically based on your username and hostname. Please check that they are accurate. See Configuration instructions to specify your name and email address. Push : To incorporate the changes into the remote repository, type: git push origin mynewbranch . Example: git push origin issue-13390 TIP : Once you have pushed your changes to the repository, they are available for everyone to see, so at this stage you can ask for feedback. Pull In your browser, return to the GO Ontology repository on GitHub. Navigate to the tab labeled as 'Code' geneontology/go-ontology/code . You will see your commit listed at the top of the page in a light yellow box. If you don\u2019t see it, click on the 'Branches' link to reveal it in the list, and click on it. Click the green button 'Compare & pull request' on the right. You may now add comments and ask a colleague to review your pull request. If you want to have the ticket reviewed before closing it, you can select a reviewer for the ticket before you make the pull request by clicking on the 'Reviewers' list and entering a GitHub identifier (e.g. @superuser1). The reviewer will be notified when the pull request is submitted. Since the Pull Request is also a GitHub issue, the reviewer\u2019s comments will show up in the dialog tab of the pull request, similarly to any other issue filed on the tracker. The diff for your file is at the bottom of the page. Examine it as a sanity check. Click on the green box 'Pull request' to generate a pull request. Wait for the Travis checks to complete (this can take a few minutes). If the Travis checks failed, go back to your working copy and correct the reported errrors. Merge If the Travis checks are succesful and if you are done working on that branch , merge the pull request. Confirming the merge will close the ticket if you have used the word 'fixes' in your commit comment. NOTE : Merge the branches only when the work is completed. If there is related work to be done as a follow up to the original request, create a new GitHub ticket and start the process from the beginning. Delete your branch on the repository using the button on the right of the successful merge message. You may also delete the working branch on your local copy. Note that this step is optional. However, if you wish to delete branches on your local machine, in your terminal window: Go back to the master branch by typing git checkout master . Update your local copy of the repository by typing git pull origin master Delete the branch by typing git branch -d workingbranchname . Example: git branch -d issue-13390","title":"Daily Workflow"},{"location":"howto/daily-curator-workflow/#daily-ontology-curator-workflow-with-github","text":"","title":"Daily Ontology Curator Workflow with GitHub"},{"location":"howto/daily-curator-workflow/#updating-the-local-copy-of-the-ontology-with-git-pull","text":"Navigate to the ontology directory of go-ontology: cd repos/MY-ONTOLOGY/src/ontology . If the terminal window is not configured to display the branch name, type: git status . You will see: On branch [master] [or the name of the branch you are on] Your branch is up-to-date with 'origin/master'. If you\u2019re not in the master branch, type: git checkout master . From the master branch, type: git pull . This will update your master branch, and all working branches, with the files that are most current on GitHub, bringing in and merging any changes that were made since you last pulled the repository using the command git pull . You will see something like this: ~/repos/MY-ONTOLOGY(master) $ git pull remote: Counting objects: 26, done. remote: Compressing objects: 100% (26/26), done. remote: Total 26 (delta 12), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (26/26), done. From https://github.com/geneontology/go-ontology 580c01d..7225e89 master -> origin/master * [new branch] issue#13029 -> origin/issue#13029 Updating 580c01d..7225e89 Fast-forward src/ontology/go-edit.obo | 39 ++++++++++++++++++++++++--------------- 1 file changed, 24 insertions(+), 15 deletions(-) ~/repos/MY-ONTOLOGY(master) $","title":"Updating the local copy of the ontology with 'git pull'"},{"location":"howto/daily-curator-workflow/#creating-a-new-working-branch-with-git-checkout","text":"When starting to work on a ticket, you should create a new branch of the repository to edit the ontology file. Make sure you are on the master branch before creating a new branch . If the terminal window is not configured to display the branch name, type: git status to check which is the active branch. If necessary, go to master by typing git checkout master . To create a new branch, type: git checkout -b issue-NNNNN in the terminal window. For naming branches , we recommend using the string 'issue-' followed by the issue number. For instance, for this issue in the tracker: https://github.com/geneontology/go-ontology/issues/13390, you would create this branch: git checkout -b issue-13390 . Typing this command will automatically put you in the new branch. You will see this message in your terminal window: ~/repos/MY-ONTOLOGY/src/ontology(master) $ git checkout -b issue-13390 Switched to a new branch 'issue-13390' ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $","title":"Creating a New Working Branch with 'git checkout'"},{"location":"howto/daily-curator-workflow/#continuing-work-on-an-existing-working-branch","text":"If you are continuing to do work on an existing branch, in addition to updating master, go to your branch by typing git checkout [branch name] . Note that you can view the existing local branches by typing git branch -l . OPTIONAL : To update the working branch with respect to the current version of the ontology, type git pull origin master . This step is optional because it is not necessary to work on the current version of the ontology; all changes will be synchronized when git merge is performed.","title":"Continuing work on an existing Working Branch"},{"location":"howto/daily-curator-workflow/#loading-navigating-and-saving-the-ontology-in-protege","text":"Before launching Prot\u00e9g\u00e9, make sure you are in the correct branch. To check the active branch, type git status . Click on the 'File' pulldown. Open the file: go-edit.obo. The first time, you will have to navigate to repos/MY-ONTOLOGY/src/ontology . Once you have worked on the file, it will show up in the menu under 'Open'/'Recent'. Click on the 'Classes' tab. Searching: Use the search box on the upper right to search for a term in the ontology. Wait for autocomplete to work in the pop-up window. Viewing a term: Double-click on the term. This will reveal the term in the 'Class hierarchy' window after a few seconds. Launching the reasoner: To see the term in the 'Class hierarchy' (inferred) window, you will need to run the 'ELK reasoner'. 'Reasoner' > select ELK 0.4.3, then click 'Start reasoner'. Close the various pop-up warnings about the ELK reasoner. You will now see the terms in the inferred hierarchy. After modification of the ontology, synchronize the reasoner. Go to menu: 'Reasoner' > ' Synchronize reasoner'. NOTE : The only changes that the reasoner will detect are those impacting the ontology structure: changes in equivalence axioms, subclasses, merges, obsoletions, new terms. TIP : When adding new relations/axioms, 'Synchronize' the reasoner. When deleting relations/axioms, it is more reliable to 'Stop' and 'Start' the reasoner again. Use File > Save to save your changes.","title":"Loading, navigating and saving the Ontology in Prot\u00e9g\u00e9"},{"location":"howto/daily-curator-workflow/#committing-pushing-and-merging-your-changes-to-the-repository","text":"Review : Changes made to the ontology can be viewed by typing git diff in the terminal window. If there are changes that have already been committed, the changes in the active branch relative to master can be viewed by typing git diff master . Commit : Changes can be committed by typing: git commit -m \u2018Meaningful message Fixes #ticketnumber\u2019 go-edit.obo . For example: git commit -m \u2018hepatic stellate cell migration and contraction and regulation terms. Fixes #13390\u2019 go-edit.obo This will save the changes to the go-edit.obo file. The terminal window will show something like: ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $ git commit -m 'Added hepatic stellate cell migration and contraction and regulation terms. Fixes #13390' go-edit.obo [issue-13390 dec9df0] Added hepatic stellate cell migration and contraction and regulation terms. Fixes #13390 1 file changed, 79 insertions(+) ~/repos/MY-ONTOLOGY/src/ontology(issue-13390) $ NOTE : The word 'fixes' is a magic word in GitHub; when used in combination with the ticket number, it will automatically close the ticket. In the above example, when the file is merged in GitHub, it will close issue number 13390. Learn more on this GitHub Help Documentation page about 'Closing issues via commit messages' . 'Fixes' is case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE : It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. TIP : Git needs to know who is committing changes to the repository, so the first time you commit, you may see the following message: Committer: Kimberly Van Auken vanauken@kimberlukensmbp.dhcp.lbnl.us Your name and email address were configured automatically based on your username and hostname. Please check that they are accurate. See Configuration instructions to specify your name and email address. Push : To incorporate the changes into the remote repository, type: git push origin mynewbranch . Example: git push origin issue-13390 TIP : Once you have pushed your changes to the repository, they are available for everyone to see, so at this stage you can ask for feedback. Pull In your browser, return to the GO Ontology repository on GitHub. Navigate to the tab labeled as 'Code' geneontology/go-ontology/code . You will see your commit listed at the top of the page in a light yellow box. If you don\u2019t see it, click on the 'Branches' link to reveal it in the list, and click on it. Click the green button 'Compare & pull request' on the right. You may now add comments and ask a colleague to review your pull request. If you want to have the ticket reviewed before closing it, you can select a reviewer for the ticket before you make the pull request by clicking on the 'Reviewers' list and entering a GitHub identifier (e.g. @superuser1). The reviewer will be notified when the pull request is submitted. Since the Pull Request is also a GitHub issue, the reviewer\u2019s comments will show up in the dialog tab of the pull request, similarly to any other issue filed on the tracker. The diff for your file is at the bottom of the page. Examine it as a sanity check. Click on the green box 'Pull request' to generate a pull request. Wait for the Travis checks to complete (this can take a few minutes). If the Travis checks failed, go back to your working copy and correct the reported errrors. Merge If the Travis checks are succesful and if you are done working on that branch , merge the pull request. Confirming the merge will close the ticket if you have used the word 'fixes' in your commit comment. NOTE : Merge the branches only when the work is completed. If there is related work to be done as a follow up to the original request, create a new GitHub ticket and start the process from the beginning. Delete your branch on the repository using the button on the right of the successful merge message. You may also delete the working branch on your local copy. Note that this step is optional. However, if you wish to delete branches on your local machine, in your terminal window: Go back to the master branch by typing git checkout master . Update your local copy of the repository by typing git pull origin master Delete the branch by typing git branch -d workingbranchname . Example: git branch -d issue-13390","title":"Committing, pushing and merging your changes to the repository"},{"location":"howto/deal-with-large-ontologies/","text":"Dealing with huge ontologies in your import chain \u00b6 Dealing with very large ontologies, such as the Protein Ontology (PR), NCBI Taxonomy (NCBITaxon), Gene Ontology (GO) and the CHEBI Ontology is a big challenge when developing ontologies, especially if we want to import and re-use terms from them. There are two major problems: It currently takes about 12\u201316 GB of memory to process PR and NCBITaxon \u2013 memory that many of us do not have available. The files are so large, pulling them over the internet can lead to failures, timeouts and other problems. There are a few strategies we can employ to deal with the problem of memory consumption: We try to reduce the memory footprint of the import as much as possible. In other words: we try to not do the fancy stuff ODK does by default when extracting a module, and keep it simple. We manage the import manually ourselves (no import) To deal with file size, we: Instead of importing the whole thing, we import curated subsets. If available, we use gzipped (compressed) versions. All four strategies will be discussed in the following. We will then look a bit Overwrite ODK default: less fancy, custom modules \u00b6 The default recipe for creating a module looks something like this: imports/%_import.owl: mirror/%.owl imports/%_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/$*_terms_combined.txt --force true --copy-ontology-annotations true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/%_import.owl (Note: This snippet was copied here on 10 February 2021 and may be out of date by the time you read this.) As you can see, a lot of stuff is going on here: first we run some preprocessing (which is really costly in ROBOT, as we need to load the ontology into Jena, and then back into the OWL API \u2013 so basically the ontology is loaded three times in total), then extract a module, then run more SPARQL queries etc, etc. Costly. For small ontologies, this is fine. All of these processes are important to mitigate some of the shortcomings of module extraction techniques, but even if they could be sorted in ROBOT, it may still not be enough. So what we can do now is this. In your ont.Makefile (for example, go.Makefile , NOT Makefile ), located in src/ontology , you can add a snippet like this: imports/pr_import.owl: mirror/pr.owl imports/pr_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/pr_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/pr_import.owl Note that all the % variables and uses of $* are replaced by the ontology ID in question. Adding this to your ont.Makefile will overwrite the default ODK behaviour in favour of this new recipe. The ODK supports this reduced module out of the box. To activate it, do this: import_group: products: - id: pr use_gzipped: TRUE is_large: TRUE This will (a) ensure that PR is pulled from a gzipped location (you have to check whether it exists though. It must correspond to the PURL, followed by the extension .gz , for example http://purl.obolibrary.org/obo/pr.owl.gz ) and (b) that it is considered large, so the default handling of large imports is activated for pr , and you don't need to paste anything into ont.Makefile . If you prefer to do it yourself, in the following sections you can find a few snippets that work for three large ontologies. Just copy and paste them into ont.Makefile , and adjust them however you wish. Protein Ontology (PR) \u00b6 imports/pr_import.owl: mirror/pr.owl imports/pr_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/pr_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/pr_import.owl NCBI Taxonomy (NCBITaxon) \u00b6 imports/ncbitaxon_import.owl: mirror/ncbitaxon.owl imports/ncbitaxon_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/ncbitaxon_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/ncbitaxon_import.owl CHEBI \u00b6 imports/chebi_import.owl: mirror/chebi.owl imports/chebi_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/chebi_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/chebi_import.owl Feel free to use an even cheaper approach, even one that does not use ROBOT, as long as it produces the target of the goal (e.g. imports/chebi_import.owl ). Use slims when they are available \u00b6 For some ontologies, you can find slims that are much smaller than full ontology. For example, NCBITaxon maintains a slim for OBO here: http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.owl, which smaller than the 1 or 2 GB of the full version. Many ontologies maintain such slims, and if not, probably should. (I would really like to see an OBO slim for Protein Ontology!) (note the .obo file is even smaller but currently robot has issues getting obo files from the web) You can also add your favourite taxa to the NCBITaxon slim by simply making a pull request on here: https://github.com/obophenotype/ncbitaxon/blob/master/subsets/taxon-subset-ids.txt You can use those slims simply like this: import_group: products: - id: ncbitaxon mirror_from: http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.obo Manage imports manually \u00b6 This is a real hack \u2013 and we want to strongly discourage it \u2013 but sometimes, importing an ontology just to import a single term is total overkill. What we do in these cases is to maintain a simple template to \"import\" minimal information. I can't stress enough that we want to avoid this, as such information will necessarily go out of date, but here is a pattern you can use to handle it in a sensible way: Add this to your src/ontology/ont-odk.yaml : import_group: products: - id: my_ncbitaxon Then add this to src/ontology/ont.Makefile : mirror/my_ncbitaxon.owl: echo \"No mirror for $@\" imports/my_ncbitaxon_import.owl: imports/my_ncbitaxon_import.tsv if [ $(IMP) = true ]; then $(ROBOT) template --template $< \\ --ontology-iri \"$(ONTBASE)/$@\" --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/my_ncbitaxon_import.owl Now you can manage your import manually in the template, and the ODK will not include your manually-curated import in your base release. But again, avoid this pattern for anything except the most trivial case (e.g. you need one term from a huge ontology). File is too large: Network timeouts and long runtimes \u00b6 Remember that ontologies are text files. While this makes them easy to read in your browser, it also makes them huge: from 500 MB (CHEBI) to 2 GB (NCBITaxon), which is an enormous amount. Thankfully, ROBOT can automatically read gzipped ontologies without the need of unpacking. To avoid long runtimes and network timeouts, we can do the following two things (with the new ODK 1.2.26): import_group: products: - id: pr use_gzipped: TRUE This will try to append .gz to the default download location (http://purl.obolibrary.org/obo/pr.owl \u2192 http://purl.obolibrary.org/obo/pr.owl.gz). Note that you must make sure that this file actually exists. It does for CHEBI and the Protein Ontology, but not for many others. If the file exists, but is located elsewhere, you can do this: import_group: products: - id: pr mirror_from: http://purl.obolibrary.org/obo/pr.owl.gz You can put any URL in mirror_from (including non-OBO ones!)","title":"Dealing with Large Ontologies"},{"location":"howto/deal-with-large-ontologies/#dealing-with-huge-ontologies-in-your-import-chain","text":"Dealing with very large ontologies, such as the Protein Ontology (PR), NCBI Taxonomy (NCBITaxon), Gene Ontology (GO) and the CHEBI Ontology is a big challenge when developing ontologies, especially if we want to import and re-use terms from them. There are two major problems: It currently takes about 12\u201316 GB of memory to process PR and NCBITaxon \u2013 memory that many of us do not have available. The files are so large, pulling them over the internet can lead to failures, timeouts and other problems. There are a few strategies we can employ to deal with the problem of memory consumption: We try to reduce the memory footprint of the import as much as possible. In other words: we try to not do the fancy stuff ODK does by default when extracting a module, and keep it simple. We manage the import manually ourselves (no import) To deal with file size, we: Instead of importing the whole thing, we import curated subsets. If available, we use gzipped (compressed) versions. All four strategies will be discussed in the following. We will then look a bit","title":"Dealing with huge ontologies in your import chain"},{"location":"howto/deal-with-large-ontologies/#overwrite-odk-default-less-fancy-custom-modules","text":"The default recipe for creating a module looks something like this: imports/%_import.owl: mirror/%.owl imports/%_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/$*_terms_combined.txt --force true --copy-ontology-annotations true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/%_import.owl (Note: This snippet was copied here on 10 February 2021 and may be out of date by the time you read this.) As you can see, a lot of stuff is going on here: first we run some preprocessing (which is really costly in ROBOT, as we need to load the ontology into Jena, and then back into the OWL API \u2013 so basically the ontology is loaded three times in total), then extract a module, then run more SPARQL queries etc, etc. Costly. For small ontologies, this is fine. All of these processes are important to mitigate some of the shortcomings of module extraction techniques, but even if they could be sorted in ROBOT, it may still not be enough. So what we can do now is this. In your ont.Makefile (for example, go.Makefile , NOT Makefile ), located in src/ontology , you can add a snippet like this: imports/pr_import.owl: mirror/pr.owl imports/pr_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/pr_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/pr_import.owl Note that all the % variables and uses of $* are replaced by the ontology ID in question. Adding this to your ont.Makefile will overwrite the default ODK behaviour in favour of this new recipe. The ODK supports this reduced module out of the box. To activate it, do this: import_group: products: - id: pr use_gzipped: TRUE is_large: TRUE This will (a) ensure that PR is pulled from a gzipped location (you have to check whether it exists though. It must correspond to the PURL, followed by the extension .gz , for example http://purl.obolibrary.org/obo/pr.owl.gz ) and (b) that it is considered large, so the default handling of large imports is activated for pr , and you don't need to paste anything into ont.Makefile . If you prefer to do it yourself, in the following sections you can find a few snippets that work for three large ontologies. Just copy and paste them into ont.Makefile , and adjust them however you wish.","title":"Overwrite ODK default: less fancy, custom modules"},{"location":"howto/deal-with-large-ontologies/#protein-ontology-pr","text":"imports/pr_import.owl: mirror/pr.owl imports/pr_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/pr_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/pr_import.owl","title":"Protein Ontology (PR)"},{"location":"howto/deal-with-large-ontologies/#ncbi-taxonomy-ncbitaxon","text":"imports/ncbitaxon_import.owl: mirror/ncbitaxon.owl imports/ncbitaxon_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/ncbitaxon_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/ncbitaxon_import.owl","title":"NCBI Taxonomy (NCBITaxon)"},{"location":"howto/deal-with-large-ontologies/#chebi","text":"imports/chebi_import.owl: mirror/chebi.owl imports/chebi_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) extract -i $< -T imports/chebi_terms_combined.txt --force true --method BOT \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/chebi_import.owl Feel free to use an even cheaper approach, even one that does not use ROBOT, as long as it produces the target of the goal (e.g. imports/chebi_import.owl ).","title":"CHEBI"},{"location":"howto/deal-with-large-ontologies/#use-slims-when-they-are-available","text":"For some ontologies, you can find slims that are much smaller than full ontology. For example, NCBITaxon maintains a slim for OBO here: http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.owl, which smaller than the 1 or 2 GB of the full version. Many ontologies maintain such slims, and if not, probably should. (I would really like to see an OBO slim for Protein Ontology!) (note the .obo file is even smaller but currently robot has issues getting obo files from the web) You can also add your favourite taxa to the NCBITaxon slim by simply making a pull request on here: https://github.com/obophenotype/ncbitaxon/blob/master/subsets/taxon-subset-ids.txt You can use those slims simply like this: import_group: products: - id: ncbitaxon mirror_from: http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.obo","title":"Use slims when they are available"},{"location":"howto/deal-with-large-ontologies/#manage-imports-manually","text":"This is a real hack \u2013 and we want to strongly discourage it \u2013 but sometimes, importing an ontology just to import a single term is total overkill. What we do in these cases is to maintain a simple template to \"import\" minimal information. I can't stress enough that we want to avoid this, as such information will necessarily go out of date, but here is a pattern you can use to handle it in a sensible way: Add this to your src/ontology/ont-odk.yaml : import_group: products: - id: my_ncbitaxon Then add this to src/ontology/ont.Makefile : mirror/my_ncbitaxon.owl: echo \"No mirror for $@\" imports/my_ncbitaxon_import.owl: imports/my_ncbitaxon_import.tsv if [ $(IMP) = true ]; then $(ROBOT) template --template $< \\ --ontology-iri \"$(ONTBASE)/$@\" --output $@.tmp.owl && mv $@.tmp.owl $@; fi .PRECIOUS: imports/my_ncbitaxon_import.owl Now you can manage your import manually in the template, and the ODK will not include your manually-curated import in your base release. But again, avoid this pattern for anything except the most trivial case (e.g. you need one term from a huge ontology).","title":"Manage imports manually"},{"location":"howto/deal-with-large-ontologies/#file-is-too-large-network-timeouts-and-long-runtimes","text":"Remember that ontologies are text files. While this makes them easy to read in your browser, it also makes them huge: from 500 MB (CHEBI) to 2 GB (NCBITaxon), which is an enormous amount. Thankfully, ROBOT can automatically read gzipped ontologies without the need of unpacking. To avoid long runtimes and network timeouts, we can do the following two things (with the new ODK 1.2.26): import_group: products: - id: pr use_gzipped: TRUE This will try to append .gz to the default download location (http://purl.obolibrary.org/obo/pr.owl \u2192 http://purl.obolibrary.org/obo/pr.owl.gz). Note that you must make sure that this file actually exists. It does for CHEBI and the Protein Ontology, but not for many others. If the file exists, but is located elsewhere, you can do this: import_group: products: - id: pr mirror_from: http://purl.obolibrary.org/obo/pr.owl.gz You can put any URL in mirror_from (including non-OBO ones!)","title":"File is too large: Network timeouts and long runtimes"},{"location":"howto/deploy-custom-obo-dashboard/","text":"How to deploy a custom OBO dashboard \u00b6 Contributed by @XinsongDu, edited by @matentzn Clone https://github.com/OBOFoundry/obo-nor.github.io and copy all its contents to a new GitHub repo under your account. Ensure that the .gitignore from the obo-nor.github.io repo is also copied to your new repo (it is frequently skipped or hidden from the user in Finder or when using the cp command) and push to everything to GitHub. Pull the Ontology Development Kit from Docker Hub (can take a while): docker pull obolibrary/odkfull Modify the dashboard-config.yml file, in particular the ontologies section: Important: Add your ontology ID to the ID 'id' field Add the path to your ontology to the mirror_from field. Get the \"base uri namespace\" of the ontology using the following steps: a. Open the ontology in Prot\u00e9g\u00e9 b. Select a class and press \"command + u\" (MacOS), the stem of the path would be the base URI namespace (e.g., in EDAM ontology, the base uri namespace is http://edamontology.org/, for Uberon it would be http://purl.obolibrary.org/obo/UBERON_) Add the base uri namespace to 'base_ns' field of your ontology in the dashboard-config.yml (As of October 2021 make sure there are multiple ontologies in the dashboard-config.yml, otherwise errors would be reported while running the code. There are currently some bugs in the dashboard code that require at least 2 or 3 ontologies in the list). In the Makefile uncomment the # before pip install networkx==2.6.2 to ensure the correct network x version is installed. Run sh run-dash.sh (make sure dashboard folder is empty before running, e.g. rm -rf dashboard/* ). When run successfully, push all changes to GitHub. Go to GitHub repo you just created, and go to Settings, then Pages, and select your main/master branch as \"source\", and your root directory. You will see a website URL highlighted in green, where your OBO dashboard is deployed.","title":"Deploy a custom OBO-Dashboard"},{"location":"howto/deploy-custom-obo-dashboard/#how-to-deploy-a-custom-obo-dashboard","text":"Contributed by @XinsongDu, edited by @matentzn Clone https://github.com/OBOFoundry/obo-nor.github.io and copy all its contents to a new GitHub repo under your account. Ensure that the .gitignore from the obo-nor.github.io repo is also copied to your new repo (it is frequently skipped or hidden from the user in Finder or when using the cp command) and push to everything to GitHub. Pull the Ontology Development Kit from Docker Hub (can take a while): docker pull obolibrary/odkfull Modify the dashboard-config.yml file, in particular the ontologies section: Important: Add your ontology ID to the ID 'id' field Add the path to your ontology to the mirror_from field. Get the \"base uri namespace\" of the ontology using the following steps: a. Open the ontology in Prot\u00e9g\u00e9 b. Select a class and press \"command + u\" (MacOS), the stem of the path would be the base URI namespace (e.g., in EDAM ontology, the base uri namespace is http://edamontology.org/, for Uberon it would be http://purl.obolibrary.org/obo/UBERON_) Add the base uri namespace to 'base_ns' field of your ontology in the dashboard-config.yml (As of October 2021 make sure there are multiple ontologies in the dashboard-config.yml, otherwise errors would be reported while running the code. There are currently some bugs in the dashboard code that require at least 2 or 3 ontologies in the list). In the Makefile uncomment the # before pip install networkx==2.6.2 to ensure the correct network x version is installed. Run sh run-dash.sh (make sure dashboard folder is empty before running, e.g. rm -rf dashboard/* ). When run successfully, push all changes to GitHub. Go to GitHub repo you just created, and go to Settings, then Pages, and select your main/master branch as \"source\", and your root directory. You will see a website URL highlighted in green, where your OBO dashboard is deployed.","title":"How to deploy a custom OBO dashboard"},{"location":"howto/edit-in-protege/","text":"Editing a term in protege \u00b6 Before you start: make sure you are working on a branch here . make sure you have the editor's file open in Protege (in ODK ontologies, located in: ./src/ontology/ONT-edit.owl) where ONT is the name of your ontology (eg mondo-edit.owl for MONDO) familiarise yourself with the user interface of protege Adding, editing, annotating and deleting axioms \u00b6 Adding annotations \u00b6 Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. OBO Foundry ontologies includes a pre-declared set of annotation properties. The most commonly used annotations are below. rdfs:label definition has_exact_synonym has_broad_synonym has_narrow_synonym has_related synonym database_cross_reference rdfs:comment Note: OBO ontologies allow only one rdfs:label, definition, and comment. Note, most of these are bold in the annotation property list: Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK. Example (based on MONDO): Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns. Definitions in OBO ontologies should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space) To add a dbxref to the definition: Click the @ symbol next to the definition Click the + button next in the pop-up window Scroll up on the left hand side until you find 'database_cross_reference', and click it Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces) Click OK Add the additional dbxref (e.g., adding GARD:0013029) The dbxrefs should appear as below. Add Synonyms and Database cross reference \u00b6 To add a synonym: Select the + button to add an annotation to the selected entity Add the synonyms as 'has_exact_synonym' (note: use appropriate synonym annotation ) Synonyms should have a reference to it Click the @ symbol next to the synonym Click the + button Select database_cross_reference on the left panel and add your reference to the Literal tab on the right hand side The Class description view \u00b6 We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class. Note the Anonymous Ancestors. These are superclasses that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the superclass from. When you press the '+' button to add a SubClass of axiom, you will notice a few ways you can add a term. The easiest of this is to use the Class expression editor. This allows you to type in the expression utilizing autocomplete. As you start typing, you can press the 'TAB' or '->|' button on your keyboard, and protege will suggest terms. You will also note that the term you enter is not in the ontology, protege will not allow you add it, with the box being highlighted red, and the term underlined red. Make a Pull Request \u00b6 Make a pull request as usual ( instructions here )","title":"Edit in Protege"},{"location":"howto/edit-in-protege/#editing-a-term-in-protege","text":"Before you start: make sure you are working on a branch here . make sure you have the editor's file open in Protege (in ODK ontologies, located in: ./src/ontology/ONT-edit.owl) where ONT is the name of your ontology (eg mondo-edit.owl for MONDO) familiarise yourself with the user interface of protege","title":"Editing a term in protege"},{"location":"howto/edit-in-protege/#adding-editing-annotating-and-deleting-axioms","text":"","title":"Adding, editing, annotating and deleting axioms"},{"location":"howto/edit-in-protege/#adding-annotations","text":"Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. OBO Foundry ontologies includes a pre-declared set of annotation properties. The most commonly used annotations are below. rdfs:label definition has_exact_synonym has_broad_synonym has_narrow_synonym has_related synonym database_cross_reference rdfs:comment Note: OBO ontologies allow only one rdfs:label, definition, and comment. Note, most of these are bold in the annotation property list: Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK. Example (based on MONDO): Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns. Definitions in OBO ontologies should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space) To add a dbxref to the definition: Click the @ symbol next to the definition Click the + button next in the pop-up window Scroll up on the left hand side until you find 'database_cross_reference', and click it Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces) Click OK Add the additional dbxref (e.g., adding GARD:0013029) The dbxrefs should appear as below.","title":"Adding annotations"},{"location":"howto/edit-in-protege/#add-synonyms-and-database-cross-reference","text":"To add a synonym: Select the + button to add an annotation to the selected entity Add the synonyms as 'has_exact_synonym' (note: use appropriate synonym annotation ) Synonyms should have a reference to it Click the @ symbol next to the synonym Click the + button Select database_cross_reference on the left panel and add your reference to the Literal tab on the right hand side","title":"Add Synonyms and Database cross reference"},{"location":"howto/edit-in-protege/#the-class-description-view","text":"We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class. Note the Anonymous Ancestors. These are superclasses that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the superclass from. When you press the '+' button to add a SubClass of axiom, you will notice a few ways you can add a term. The easiest of this is to use the Class expression editor. This allows you to type in the expression utilizing autocomplete. As you start typing, you can press the 'TAB' or '->|' button on your keyboard, and protege will suggest terms. You will also note that the term you enter is not in the ontology, protege will not allow you add it, with the box being highlighted red, and the term underlined red.","title":"The Class description view"},{"location":"howto/edit-in-protege/#make-a-pull-request","text":"Make a pull request as usual ( instructions here )","title":"Make a Pull Request"},{"location":"howto/filter-text-file/","text":"Command Line Trick: Filter text files based on a list of strings \u00b6 Let's say you want to remove some lines from a large text file programmatically. For example, you want to remove every line that contains certain IDs, but you want to keep the rest of the lines intact. You can use the command line utility grep with option -v to find all the lines in the file that do NOT contain your search term(s). You can make a file with a list of several search terms and use that file with grep using the -f option as follows: grep -v -f your_list.txt target_file.tsv | tee out_file.tsv Explanation \u00b6 The target file is your text file from which you wish to remove lines. The text file can be of type csv , tsv , obo etc. For example, you wish to filter a file with these lines: keep this 1 this line is undesired 2, so you do not wish to keep it keep this 3 keep this 4 keep this 5 keep this 6 something undesired 2 this line is undesired 1 keep this 7 The file your_list.txt is a text file with your list of search terms. Format: one search term per line. For example: undesired 1 undesired 2 The utility tee will redirect the standard output to both the terminal and write it out to a file. You expect the out_file.tsv to contain lines: keep this 1 keep this 3 keep this 4 keep this 5 keep this 6 keep this 7 Do the filtering and updating of your target file in one step \u00b6 You can also do a one-step filter-update when you are confident that your filtering works as expected, or if you have a backup copy of your target_file.tsv . Use cat and pipe the contents of your text file as the input for grep . Redirect the results to both your terminal and overwrite your original file so it will contain only the filtered lines. cat target_file.tsv | grep -v -f your_list.txt | tee target_file.tsv","title":"How to filter a file based on another file"},{"location":"howto/filter-text-file/#command-line-trick-filter-text-files-based-on-a-list-of-strings","text":"Let's say you want to remove some lines from a large text file programmatically. For example, you want to remove every line that contains certain IDs, but you want to keep the rest of the lines intact. You can use the command line utility grep with option -v to find all the lines in the file that do NOT contain your search term(s). You can make a file with a list of several search terms and use that file with grep using the -f option as follows: grep -v -f your_list.txt target_file.tsv | tee out_file.tsv","title":"Command Line Trick: Filter text files based on a list of strings"},{"location":"howto/filter-text-file/#explanation","text":"The target file is your text file from which you wish to remove lines. The text file can be of type csv , tsv , obo etc. For example, you wish to filter a file with these lines: keep this 1 this line is undesired 2, so you do not wish to keep it keep this 3 keep this 4 keep this 5 keep this 6 something undesired 2 this line is undesired 1 keep this 7 The file your_list.txt is a text file with your list of search terms. Format: one search term per line. For example: undesired 1 undesired 2 The utility tee will redirect the standard output to both the terminal and write it out to a file. You expect the out_file.tsv to contain lines: keep this 1 keep this 3 keep this 4 keep this 5 keep this 6 keep this 7","title":"Explanation"},{"location":"howto/filter-text-file/#do-the-filtering-and-updating-of-your-target-file-in-one-step","text":"You can also do a one-step filter-update when you are confident that your filtering works as expected, or if you have a backup copy of your target_file.tsv . Use cat and pipe the contents of your text file as the input for grep . Redirect the results to both your terminal and overwrite your original file so it will contain only the filtered lines. cat target_file.tsv | grep -v -f your_list.txt | tee target_file.tsv","title":"Do the filtering and updating of your target file in one step"},{"location":"howto/fixing-conflicts/","text":"Fixing merge conflicts \u00b6 This video illustrates an example of fixing a merge conflict in the Mondo Disease Ontology. Instructions: If a merge conflict error appears in your Github.com pull request after committing a change, open GitHub Desktop and select the corresponding repository from the \"Current Repository\" button. If the conflict emerged after editing the ontology outside of Prot\u00e9g\u00e9 5.5.0, see Ad hoc Reserialisation below. With the repository selected, click the \"Fetch origin\" button to fetch the most up-to-date version of the repository. Click the \"Current Branch\" button and select the branch with the merge conflict. From the menu bar, select Branch > \"Update from master\". A message indicating the file with a conflict should appear along with the option to open the file (owl or obo file) in a text/code editor, such as Sublime Text. Click the button to open the file. Search the file for conflict markings ( <<<<<<< ======= >>>>>>> ). Make edits to resolve the conflict, e.g., arrange terms in the correct order. Remove the conflict markings. Save the file. Open the file in Prot\u00e9g\u00e9. If prompted, do not reload any previously opened file. Open as a new file. Check that the terms involved in the conflict appear OK, i.e., have no obvious errors. Save the file in Prot\u00e9g\u00e9 using File > 'Save as...' from the menu bar and replace the ontology edit file, e.g., mondo-edit.obo Return to GitHub Desktop and confirm the conflicts are now resolved. Click the \"Continue Merge\" button and then the \"Push origin\" button. Return to Github.com and allow the QC queries to rerun. The conflict should be resolved and the branch allowed to be merged. Ad hoc Reserialisation If the owl or obo file involved in the merge conflict was edited using Prot\u00e9g\u00e9 5.5.0, the above instructions should be sufficient. If edited in any other way, such as fixing a conflict in a text editor, the serialisation order may need to be fixed. This can be done as follows: Reserialise the master file using the Ontology Development Kit (ODK). This requires setting up Docker and ODK. If not already set up, follow the instructions here . Open Docker. At the line command (PC) or Terminal (Mac), use the cd (change directory) command to navigate to the repository's src/ontology/ directory. For example, cd PATH_TO_ONTOLOGY/src/ontology/ Replace \"PATH_TO_ONTOLOGY\" with the actual file path to the ontology. If you need to orient yourself, use the pwd (present working directory) or ls (list) line commands. If you are resolving a conflict in an .owl file, run: sh run.sh make normalize_src If you are resolving a conflict in an .obo file, run: sh run.sh make normalize_obo_src In some ontologies (such as the Cell ontology (CL)), edits may result in creating a large amount of unintended differences involving ^^xsd:string. If you see these differences after running the command above, they can be resolved by following the instructions here . Continue by going to step 1 under the main Instructions above.","title":"Fixing conflicts"},{"location":"howto/fixing-conflicts/#fixing-merge-conflicts","text":"This video illustrates an example of fixing a merge conflict in the Mondo Disease Ontology. Instructions: If a merge conflict error appears in your Github.com pull request after committing a change, open GitHub Desktop and select the corresponding repository from the \"Current Repository\" button. If the conflict emerged after editing the ontology outside of Prot\u00e9g\u00e9 5.5.0, see Ad hoc Reserialisation below. With the repository selected, click the \"Fetch origin\" button to fetch the most up-to-date version of the repository. Click the \"Current Branch\" button and select the branch with the merge conflict. From the menu bar, select Branch > \"Update from master\". A message indicating the file with a conflict should appear along with the option to open the file (owl or obo file) in a text/code editor, such as Sublime Text. Click the button to open the file. Search the file for conflict markings ( <<<<<<< ======= >>>>>>> ). Make edits to resolve the conflict, e.g., arrange terms in the correct order. Remove the conflict markings. Save the file. Open the file in Prot\u00e9g\u00e9. If prompted, do not reload any previously opened file. Open as a new file. Check that the terms involved in the conflict appear OK, i.e., have no obvious errors. Save the file in Prot\u00e9g\u00e9 using File > 'Save as...' from the menu bar and replace the ontology edit file, e.g., mondo-edit.obo Return to GitHub Desktop and confirm the conflicts are now resolved. Click the \"Continue Merge\" button and then the \"Push origin\" button. Return to Github.com and allow the QC queries to rerun. The conflict should be resolved and the branch allowed to be merged. Ad hoc Reserialisation If the owl or obo file involved in the merge conflict was edited using Prot\u00e9g\u00e9 5.5.0, the above instructions should be sufficient. If edited in any other way, such as fixing a conflict in a text editor, the serialisation order may need to be fixed. This can be done as follows: Reserialise the master file using the Ontology Development Kit (ODK). This requires setting up Docker and ODK. If not already set up, follow the instructions here . Open Docker. At the line command (PC) or Terminal (Mac), use the cd (change directory) command to navigate to the repository's src/ontology/ directory. For example, cd PATH_TO_ONTOLOGY/src/ontology/ Replace \"PATH_TO_ONTOLOGY\" with the actual file path to the ontology. If you need to orient yourself, use the pwd (present working directory) or ls (list) line commands. If you are resolving a conflict in an .owl file, run: sh run.sh make normalize_src If you are resolving a conflict in an .obo file, run: sh run.sh make normalize_obo_src In some ontologies (such as the Cell ontology (CL)), edits may result in creating a large amount of unintended differences involving ^^xsd:string. If you see these differences after running the command above, they can be resolved by following the instructions here . Continue by going to step 1 under the main Instructions above.","title":"Fixing merge conflicts"},{"location":"howto/github-actions/","text":"Using Github actions to automate tasks \u00b6 Post a comment with ontology differences on pull request \u00b6 The command line tool Robot has a diff tool that compares two ontology files and can print the differences between them in multiple formats, among them markdown. We can use this tool and GitHub actions to automatically post a comment when a Pull Request to master is created, with the differences between the two ontologies. To create a new GitHub action, create a folder in your ontology project root folder called .github . Then create a yaml file in a subfolder called workflows , e.g. .github/workflows/diff.yml . This file contains code that will be executed in GitHub when certain conditions are meant, in this case, when a PR to master is submitted. The comments in this file from FYPO will help you write an action for your own repository. The comment will look something like this .","title":"Github actions to automate tasks"},{"location":"howto/github-actions/#using-github-actions-to-automate-tasks","text":"","title":"Using Github actions to automate tasks"},{"location":"howto/github-actions/#post-a-comment-with-ontology-differences-on-pull-request","text":"The command line tool Robot has a diff tool that compares two ontology files and can print the differences between them in multiple formats, among them markdown. We can use this tool and GitHub actions to automatically post a comment when a Pull Request to master is created, with the differences between the two ontologies. To create a new GitHub action, create a folder in your ontology project root folder called .github . Then create a yaml file in a subfolder called workflows , e.g. .github/workflows/diff.yml . This file contains code that will be executed in GitHub when certain conditions are meant, in this case, when a PR to master is submitted. The comments in this file from FYPO will help you write an action for your own repository. The comment will look something like this .","title":"Post a comment with ontology differences on pull request"},{"location":"howto/github-create-fork/","text":"Fork an ontology for editing \u00b6 Note: Creating a fork allows you to create your copy GitHub repository. This example provides instructions on forking the Mondo GitHub reposiitory. You can't break any of the Mondo files by editing your forked copy. On GitHub, navigate to https://github.com/monarch-initiative/mondo In the top-right corner of the page, click Fork. When prompted 'Where should we fork mondo', choose your own repo (eg Nicole Vasilevsky). Be careful if you have multiple forks (i.e. the original and your own personal fork, as this can cause confusion). Clone your forked repo: If you have GitHub Desktop installed - click Code -> Open with GitHub Desktop How are you planning to use this fork? To contribute to parent project In GitHub Desktop, create a new branch: Click Current Branch - > New Branch Give your branch a name, like c-path-training-1 You will make changes to the Mondo on the branch of your local copy. Further instructions on forking a repo","title":"Fork an ontology for editing"},{"location":"howto/github-create-fork/#fork-an-ontology-for-editing","text":"Note: Creating a fork allows you to create your copy GitHub repository. This example provides instructions on forking the Mondo GitHub reposiitory. You can't break any of the Mondo files by editing your forked copy. On GitHub, navigate to https://github.com/monarch-initiative/mondo In the top-right corner of the page, click Fork. When prompted 'Where should we fork mondo', choose your own repo (eg Nicole Vasilevsky). Be careful if you have multiple forks (i.e. the original and your own personal fork, as this can cause confusion). Clone your forked repo: If you have GitHub Desktop installed - click Code -> Open with GitHub Desktop How are you planning to use this fork? To contribute to parent project In GitHub Desktop, create a new branch: Click Current Branch - > New Branch Give your branch a name, like c-path-training-1 You will make changes to the Mondo on the branch of your local copy. Further instructions on forking a repo","title":"Fork an ontology for editing"},{"location":"howto/github-create-pull-request/","text":"Create a Pull Request in GitHub \u00b6 Overview \u00b6 GitHub workflows \u00b6 A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub. A Git repo typically has a master or main branch that is not directly edited. Changes are made by creating a branch from Master (complete copy of the Master + its history) (either a direct branch or via a fork). Branch vs Fork \u00b6 You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners. If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. See the Appendix for instructions on how to make a fork. If you have permission from the owners, you can instead make a new branch. What is a Pull Request? \u00b6 A Pull Request (PR) is an event in Git where a contributor (you!) asks a maintainer of a Git repository to review changes (e.g. edits to an ontology file) they want to merge into a project (e.g. the owl file) (see reference ). Create a pull request to propose and collaborate on changes to a repository. These changes are proposed in a branch, which ensures that the default branch only contains finished and approved work. See more details here . Committing, pushing and making pull requests \u00b6 See these instructions on cloning an ontology repo and creating a branch using GitHub Dekstop. Review: Once changes are made to the ontology file, they can be viewed in GitHub Desktop. Before committing, check the diff. An example diff from the Cell Ontology (CL) is pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and consider asking the ontology editor team for help instead. Example 1 (Cell Ontology): Example 2 (Mondo): Commit message: Before Committing, you must add a commit message. In GitHub Desktop in the Commit field in the lower left, there is a subject line and a description. Give a very descriptive title: Add a descriptive title in the subject line. For example: add new class ONTOLOGY:ID [term name] (e.g. add new class MONDO:0000006 heart disease) Write a great summary of what the change is in the Description box, referring to the issue. The sentence should clearly state how the issue is addressed. To link the issue, you can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Addresses'. The commit will be associated with the correct ticket but the ticket will remain open. 7.NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the ontology edit file. Push: To incorporate the changes into the remote repository, click Publish branch. Make a Pull Request \u00b6 Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release.","title":"Creating an GitHub Pull Request"},{"location":"howto/github-create-pull-request/#create-a-pull-request-in-github","text":"","title":"Create a Pull Request in GitHub"},{"location":"howto/github-create-pull-request/#overview","text":"","title":"Overview"},{"location":"howto/github-create-pull-request/#github-workflows","text":"A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub. A Git repo typically has a master or main branch that is not directly edited. Changes are made by creating a branch from Master (complete copy of the Master + its history) (either a direct branch or via a fork).","title":"GitHub workflows"},{"location":"howto/github-create-pull-request/#branch-vs-fork","text":"You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners. If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. See the Appendix for instructions on how to make a fork. If you have permission from the owners, you can instead make a new branch.","title":"Branch vs Fork"},{"location":"howto/github-create-pull-request/#what-is-a-pull-request","text":"A Pull Request (PR) is an event in Git where a contributor (you!) asks a maintainer of a Git repository to review changes (e.g. edits to an ontology file) they want to merge into a project (e.g. the owl file) (see reference ). Create a pull request to propose and collaborate on changes to a repository. These changes are proposed in a branch, which ensures that the default branch only contains finished and approved work. See more details here .","title":"What is a Pull Request?"},{"location":"howto/github-create-pull-request/#committing-pushing-and-making-pull-requests","text":"See these instructions on cloning an ontology repo and creating a branch using GitHub Dekstop. Review: Once changes are made to the ontology file, they can be viewed in GitHub Desktop. Before committing, check the diff. An example diff from the Cell Ontology (CL) is pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and consider asking the ontology editor team for help instead. Example 1 (Cell Ontology): Example 2 (Mondo): Commit message: Before Committing, you must add a commit message. In GitHub Desktop in the Commit field in the lower left, there is a subject line and a description. Give a very descriptive title: Add a descriptive title in the subject line. For example: add new class ONTOLOGY:ID [term name] (e.g. add new class MONDO:0000006 heart disease) Write a great summary of what the change is in the Description box, referring to the issue. The sentence should clearly state how the issue is addressed. To link the issue, you can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Addresses'. The commit will be associated with the correct ticket but the ticket will remain open. 7.NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the ontology edit file. Push: To incorporate the changes into the remote repository, click Publish branch.","title":"Committing, pushing and making pull requests"},{"location":"howto/github-create-pull-request/#make-a-pull-request","text":"Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release.","title":"Make a Pull Request"},{"location":"howto/idrange/","text":"Setting up your ID Ranges \u00b6 Setting ID ranges in your ontology \u00b6 Curators and projects are assigned specific ID ranges within the prefix for your ontology. See the README-editors.md for your ontology An example: go-idranges.owl NOTE: You should only use IDs within your range. If you have only just set up this repository, modify the idranges file and add yourself or other editors. Setting ID ranges in Protege \u00b6 Once you have your assigned ID range, you need to configure Protege so that your ID range is recorded in the Preferences menu. Protege does not read the idranges file. In the Protege menu, select Preferences. In the resulting pop-up window, click on the New Entities tab and set the values as follows. In the Entity IRI box: Start with: Specified IRI: http://purl.obolibrary.org/obo Followed by: / End with: Auto-generated ID In the Entity Label section: Same as label renderer: IRI: http://www.w3.org/2000/01/rdf-schema#label In the Auto-generated ID section: Numeric Prefix GO_ Suffix: leave this blank Digit Count 7 Start: see go-idranges.owl . Only paste the number after the GO: prefix. Also, note that when you paste in your GO ID range, the number will automatically be converted to a standard number, e.g. pasting 0110001 will be converted to 110,001.) End: see go-idranges.owl Remember last ID between Protege sessions: ALWAYS CHECK THIS ( Note: You want the ID to be remembered to prevent clashes when working in parallel on branches.)","title":"Setting up your ID range"},{"location":"howto/idrange/#setting-up-your-id-ranges","text":"","title":"Setting up your ID Ranges"},{"location":"howto/idrange/#setting-id-ranges-in-your-ontology","text":"Curators and projects are assigned specific ID ranges within the prefix for your ontology. See the README-editors.md for your ontology An example: go-idranges.owl NOTE: You should only use IDs within your range. If you have only just set up this repository, modify the idranges file and add yourself or other editors.","title":"Setting ID ranges in your ontology"},{"location":"howto/idrange/#setting-id-ranges-in-protege","text":"Once you have your assigned ID range, you need to configure Protege so that your ID range is recorded in the Preferences menu. Protege does not read the idranges file. In the Protege menu, select Preferences. In the resulting pop-up window, click on the New Entities tab and set the values as follows. In the Entity IRI box: Start with: Specified IRI: http://purl.obolibrary.org/obo Followed by: / End with: Auto-generated ID In the Entity Label section: Same as label renderer: IRI: http://www.w3.org/2000/01/rdf-schema#label In the Auto-generated ID section: Numeric Prefix GO_ Suffix: leave this blank Digit Count 7 Start: see go-idranges.owl . Only paste the number after the GO: prefix. Also, note that when you paste in your GO ID range, the number will automatically be converted to a standard number, e.g. pasting 0110001 will be converted to 110,001.) End: see go-idranges.owl Remember last ID between Protege sessions: ALWAYS CHECK THIS ( Note: You want the ID to be remembered to prevent clashes when working in parallel on branches.)","title":"Setting ID ranges in Protege"},{"location":"howto/installing-elk-in-protege/","text":"Install Elk 0.5 in Protege \u00b6 Click here to get the latest Protege Plugin latest build (this is available on the bottom of ELK pages . This will download a zipped file.) When downloaded, unzip and copy puli and elk jars (two .jar files) in the unpacked directory. Paste these files in your Protege plugin directory. Remove old org.semanticweb.elk.jar Install ELK plugin on Mac: This can be done via one of two ways: Approach 1 In Terminal: open ~/.Protege, then click on plugins Click on plugins Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. Approach 2 Paste these files in your Protege plugin directory. This is in one of two locations: ~/.Protege/plugins (note this is usually hidden from finder, but you can see it in the terminal) or Go to Protege in Applications (Finder), right click, 'Show package contents' -> Java -> plugins If you go to ~/.Protege and a directory called plugins does not exist in this folder, you can create it. Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. Important: it seems Elk 0.5. Does not work with all versions of Protege, in particular, 5.2 and below. These instructions were only tested with Protege 5.5. Video Explanation \u00b6","title":"Installing ELK in Protege"},{"location":"howto/installing-elk-in-protege/#install-elk-05-in-protege","text":"Click here to get the latest Protege Plugin latest build (this is available on the bottom of ELK pages . This will download a zipped file.) When downloaded, unzip and copy puli and elk jars (two .jar files) in the unpacked directory. Paste these files in your Protege plugin directory. Remove old org.semanticweb.elk.jar Install ELK plugin on Mac: This can be done via one of two ways: Approach 1 In Terminal: open ~/.Protege, then click on plugins Click on plugins Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. Approach 2 Paste these files in your Protege plugin directory. This is in one of two locations: ~/.Protege/plugins (note this is usually hidden from finder, but you can see it in the terminal) or Go to Protege in Applications (Finder), right click, 'Show package contents' -> Java -> plugins If you go to ~/.Protege and a directory called plugins does not exist in this folder, you can create it. Copy and paste the two files into the plugins directory Remove old elk.jar (Ex. org.semanticweb.elk.jar) Restart Protege. You should see ELK 0.5 installed in your Reasoner menu. Important: it seems Elk 0.5. Does not work with all versions of Protege, in particular, 5.2 and below. These instructions were only tested with Protege 5.5.","title":"Install Elk 0.5 in Protege"},{"location":"howto/installing-elk-in-protege/#video-explanation","text":"","title":"Video Explanation"},{"location":"howto/merge-terms/","text":"NOTE This documentation is incomplete, for now you may be better consulting the GO Editor Docs For instructions on obsoleting terms (without merging/replacing with a new term, see obsoletion how to guide. ) Merging Ontology Terms \u00b6 See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. Note Before performing a merge, make sure that you know all of the consequences that the merge will cause. In particular, be sure to look at child terms and any other terms that refer to the \u2018obsoleted\u2019 term. In many cases a simple merge of two terms is not sufficient because it will result in equivalent classes for child terms. For example if obsoleted term X is going to be merged into target term Y and \u2018regulation of X\u2019 and \u2018regulation of Y\u2019 terms exist, then you will need to merge the regulation terms in addition to the primary terms. You will also need to edit any terms that refer to the obsoleted term to be sure that the names and definitions are consistent. Manual Workflow \u00b6 Find the ID of the term in which the obsoleted term will be merged Navigate to 'winning' term using the Search box. Copy the ID of the winning term somewhere. Duplicate annotations from the obsoleted terms Navigate to the term to be obsoleted. Right click on it and select Duplicate class then OK in the pop up window. This should create a class with the exact same name. On the duplicated class (you can see this by (CL:XXXX) within your range added), right click and select Change IRI (Rename) Copy the ID of the winning term (obtained in Step 1). Be sure to use the underscore _ in the identifier instead of the colon : , for example: GO_1234567 . Make sure that the 'change all entities with this URI' box is checked. Navigate to the winning term IRI, all annotations should be merged. Change obsoleted term label to a synonym In the annotations box of the winning term there are now two terms with labels 'rdfs:label'. Click the o to change the label of the obsoleted term. In the resulting pop-up window, select the appropriate synonym label from the list on the left: has_broad_synonym has_exact_synonym has_narrow_synonym has_related_synonym (if unsure, this is the safest choice) Remove duplicated or inappropriate annotations Check the definition, if there are multiple entries, remove the obsoleted one by clicking on the x on the right. Check the subclasses and remove inappropriate/duplciated ones by clicking on the x on the right. Check list of synonyms and remove inappropriate/duplciated ones by clicking on the x on the right. Note down the created_by and created_date (there can only be one value per term for each of these fields; this will be useful if you need to pick one after the merge is done). Obsolete old term Oobsolete the old term by following instructions found in obsoletion how to guide. . Ensure that you add a rdfs:comment that states that term was duplicated and to refer to the new new. Ensure that you add a term replaced by annotations as per the instructions and add the winning merged term. Synchronize the reasoner and make sure there are no terms that have identical definitions as a result of the merge. These are displayed with an 'equivalent' sign \u2261 in the class hierarchy view on the left hand panel. Add alt_id to the winning term . Navigate to the winning term and add the annotation hasAlternativeId with the ID (eg CL:0000000) of the losing term in the Literal tab. Note : any id annotation with value = the OBO ID of the losing term should be deleted. Failure to do this breaks OBO<->OWL conversion. Save changes. See Daily Workflow section for commit, push and merge instructions. Merge using owltools \u00b6 To use owltools will need to have Docker installed and running (see instructions here ). This is the workflow that is used in Mondo . Create a branch and name it issue-### (for example issue-2864) Open Protege Prepre the owltools command: owltools --use-catalog mondo-edit.obo --obsolete-replace [CURIE 1] [CURIE 2] -o -f obo mondo-edit.obo CURIE 1 = term to be obsoleted CURIE 2 = replacement term (ie term to be merged with) For example: If to merge MONDO:0023052 ectrodactyly polydactyly with MONDO:0009156 ectrodactyly-polydactyly syndrome, the command is: owltools --use-catalog mondo-edit.obo --obsolete-replace MONDO:0023052 MONDO:0009156 -o -f obo mondo-edit.obo In Terminal, navigate to your ontology directory: src/ontology Run your owltools command Check the output in GitHub desktop Open a new version of your ontology edit file in Protege Search for the term that was obsoleted Add 'term tracker item' (type xsd:anyURI) with a link to the GitHub issue that requested the obsoletion. Add an obsoletion reason: use the annotation property 'has obsolescence reason' and write 'terms merged' in the literal field. Search for the 'term replaced by' term Delete the old ID Review the annotations to ensure there are no duplicate annotations. If there are, they should be merged. Review the subClassOf assertions, and make sure there are no duplicates. If there are, they should be merged. When reviewing the diff, make sure there is not an Alt ID. The diff should only show additions to the merged term and the obsoletion TROUBLESHOOTING: Travis/Jenkins errors Merging a term that is used as 'replaced by' for an obsolete term :: ERROR: ID-mentioned-twice:: GO:0030722 :: ERROR: ID-mentioned-twice:: GO:0048126 GO:0030722 :: ERROR: has-definition: missing definition for id The cause of this error is that Term A (GO:0048126) was obsoleted and had replace by Term B (GO:0030722). The GO editor tried to merge Term B into a third term term C (GO:0007312). The Jenkins checkk failed because 'Term A replaced by' was an alternative_id rather than by a main_id. Solution: In the ontology, go to the obsolete term A and replace the Term B by term C to have a primary ID as the replace_by.","title":"Merging Terms"},{"location":"howto/merge-terms/#merging-ontology-terms","text":"See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. Note Before performing a merge, make sure that you know all of the consequences that the merge will cause. In particular, be sure to look at child terms and any other terms that refer to the \u2018obsoleted\u2019 term. In many cases a simple merge of two terms is not sufficient because it will result in equivalent classes for child terms. For example if obsoleted term X is going to be merged into target term Y and \u2018regulation of X\u2019 and \u2018regulation of Y\u2019 terms exist, then you will need to merge the regulation terms in addition to the primary terms. You will also need to edit any terms that refer to the obsoleted term to be sure that the names and definitions are consistent.","title":"Merging Ontology Terms"},{"location":"howto/merge-terms/#manual-workflow","text":"Find the ID of the term in which the obsoleted term will be merged Navigate to 'winning' term using the Search box. Copy the ID of the winning term somewhere. Duplicate annotations from the obsoleted terms Navigate to the term to be obsoleted. Right click on it and select Duplicate class then OK in the pop up window. This should create a class with the exact same name. On the duplicated class (you can see this by (CL:XXXX) within your range added), right click and select Change IRI (Rename) Copy the ID of the winning term (obtained in Step 1). Be sure to use the underscore _ in the identifier instead of the colon : , for example: GO_1234567 . Make sure that the 'change all entities with this URI' box is checked. Navigate to the winning term IRI, all annotations should be merged. Change obsoleted term label to a synonym In the annotations box of the winning term there are now two terms with labels 'rdfs:label'. Click the o to change the label of the obsoleted term. In the resulting pop-up window, select the appropriate synonym label from the list on the left: has_broad_synonym has_exact_synonym has_narrow_synonym has_related_synonym (if unsure, this is the safest choice) Remove duplicated or inappropriate annotations Check the definition, if there are multiple entries, remove the obsoleted one by clicking on the x on the right. Check the subclasses and remove inappropriate/duplciated ones by clicking on the x on the right. Check list of synonyms and remove inappropriate/duplciated ones by clicking on the x on the right. Note down the created_by and created_date (there can only be one value per term for each of these fields; this will be useful if you need to pick one after the merge is done). Obsolete old term Oobsolete the old term by following instructions found in obsoletion how to guide. . Ensure that you add a rdfs:comment that states that term was duplicated and to refer to the new new. Ensure that you add a term replaced by annotations as per the instructions and add the winning merged term. Synchronize the reasoner and make sure there are no terms that have identical definitions as a result of the merge. These are displayed with an 'equivalent' sign \u2261 in the class hierarchy view on the left hand panel. Add alt_id to the winning term . Navigate to the winning term and add the annotation hasAlternativeId with the ID (eg CL:0000000) of the losing term in the Literal tab. Note : any id annotation with value = the OBO ID of the losing term should be deleted. Failure to do this breaks OBO<->OWL conversion. Save changes. See Daily Workflow section for commit, push and merge instructions.","title":"Manual Workflow"},{"location":"howto/merge-terms/#merge-using-owltools","text":"To use owltools will need to have Docker installed and running (see instructions here ). This is the workflow that is used in Mondo . Create a branch and name it issue-### (for example issue-2864) Open Protege Prepre the owltools command: owltools --use-catalog mondo-edit.obo --obsolete-replace [CURIE 1] [CURIE 2] -o -f obo mondo-edit.obo CURIE 1 = term to be obsoleted CURIE 2 = replacement term (ie term to be merged with) For example: If to merge MONDO:0023052 ectrodactyly polydactyly with MONDO:0009156 ectrodactyly-polydactyly syndrome, the command is: owltools --use-catalog mondo-edit.obo --obsolete-replace MONDO:0023052 MONDO:0009156 -o -f obo mondo-edit.obo In Terminal, navigate to your ontology directory: src/ontology Run your owltools command Check the output in GitHub desktop Open a new version of your ontology edit file in Protege Search for the term that was obsoleted Add 'term tracker item' (type xsd:anyURI) with a link to the GitHub issue that requested the obsoletion. Add an obsoletion reason: use the annotation property 'has obsolescence reason' and write 'terms merged' in the literal field. Search for the 'term replaced by' term Delete the old ID Review the annotations to ensure there are no duplicate annotations. If there are, they should be merged. Review the subClassOf assertions, and make sure there are no duplicates. If there are, they should be merged. When reviewing the diff, make sure there is not an Alt ID. The diff should only show additions to the merged term and the obsoletion TROUBLESHOOTING: Travis/Jenkins errors Merging a term that is used as 'replaced by' for an obsolete term :: ERROR: ID-mentioned-twice:: GO:0030722 :: ERROR: ID-mentioned-twice:: GO:0048126 GO:0030722 :: ERROR: has-definition: missing definition for id The cause of this error is that Term A (GO:0048126) was obsoleted and had replace by Term B (GO:0030722). The GO editor tried to merge Term B into a third term term C (GO:0007312). The Jenkins checkk failed because 'Term A replaced by' was an alternative_id rather than by a main_id. Solution: In the ontology, go to the obsolete term A and replace the Term B by term C to have a primary ID as the replace_by.","title":"Merge using owltools"},{"location":"howto/obsolete-term/","text":"Obsoleting an Existing Ontology Term \u00b6 See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. Warning: Every ontology has their procedures on how they obsolete terms (eg notice periods, notification emails, to_be_obsolete tags, etc.), this how-to guide only serves as a guide on how obsolete a term directly on protege. For instructions on how to merge terms (i.e., replace a term with another term in the ontology), see instructions here . PRE OBSOLETION PROCESS (or basic obsoletion etiquette) \u00b6 Check if the term (or any of its children) is being used for annotation: Go to your ontology browser of choice, search for the term, either by label or ID See which other ontologies use the to be obsolete term Notify affected groups (usually by adding an issue in their tracker) Check if the term is used elsewhere in the ontology In Prot\u00e9g\u00e9, navigate to the term to be obsolete and go to the 'Usage' tab to see if that ID is used elsewhere. If the term is a parent to other terms or is used in logical definitions, make sure that another term replaces the obsolete term OBSOLETION PROCESS (Manual) \u00b6 Warning: some ontologies give advance notice on terms that will be obsoleted through the annotation 'scheduled for obsoletion on or after' instead of directly obsoleting the term. Please check with the conventions of your ontology before obsoleting a term. Navigate to the term to be obsoleted. Select Edit > Deprecate entity... A deprecation wizard will pop up, in here, select GO style, and select continue (note this is specifc to GO style ontologies, if you are working with an OBI style ontology, there is an option for that too, if not use basic. For this how to, we will follow GO style) Next, enter your reason for deprecation. For this, we advice for you to enter the github issue. (eg https://github.com/obophenotype/cell-ontology/issues/####) This will appear as a rdfs:comment Next enter a replacement entity if there is one. This will automatically replace axioms in the ontology with the term, and add an 'item replaced by' axiom on the obsolete term. Your obsolete term should now be stripped of its logical axioms and should look similar to the figure below. Add any additional annotations needed - this is specific to ontologies and you should consult the conventions of the ontology you are working on. Examples of additional annotations to add: IAO:0000233 term tracker item (type xsd:anyURI) - link to GitHub issue has_obsolence_reason add \u2018OBSOLETE.\u2019 to the term definition: In the 'Description' window, click on the o on the right-hand side of the definition entry. In the resulting window, in the Literal tab, at the beginning of the definition, type: OBSOLETE. if the obsoleted term was not replaced by another term in the ontology, but there are existing terms that might be appropriate for annotation, add those term IDs in the 'consider' tag: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select consider and enter the ID of the replacement term. NOTE: Here you have to add the ID of the entity as an xsd:string , e.g. GO:0005819, not the term label. Obsolete a class (using Protege 'Make entity obsolete' function) \u00b6 Navigate to the term to be obsoleted. In the Protege edit menu-> Make entity obsolete Prepend the definition with OBSOLETE. For example, OBSOLETE. Chronic form of myeloproliferative neoplasm. Add a statement about why the term was made obsolete: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select rdfs:comment and select Type: Xsd:string . Consult the wiki documentation for suggestions on standard comments: - [http://wiki.geneontology.org/index.php/Curator_Guide:_Obsoletion](http://wiki.geneontology.org/index.php/Curator_Guide:_Obsoletion) - [http://wiki.geneontology.org/index.php/Obsoleting_GO_Terms](http://wiki.geneontology.org/index.php/Obsoleting_GO_Terms) - [http://wiki.geneontology.org/index.php/Editor_Guide](http://wiki.geneontology.org/index.php/Editor_Guide) If the obsoleted term was replaced by another term in the ontology: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select term replaced by and enter the ID of the replacement term. If the obsoleted term was not replaced by another term in the ontology, but there are existing terms that might be appropriate for annotation, add those term IDs in the 'consider' tag: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select consider and enter the ID of the replacement term. NOTE: Here you have to add the ID of the entity as an xsd:string , e.g. GO:0005819, not the term label. Add any additional annotations needed - this is specific to ontologies and you should consult the conventions of the ontology you are working on. Examples of additional annotations to add: IAO:0000233 term tracker item (type xsd:anyURI) - link to GitHub issue has_obsolence_reason See Daily Workflow section for commit, push and merge instructions.","title":"Obsoleting a Term"},{"location":"howto/obsolete-term/#obsoleting-an-existing-ontology-term","text":"See Daily Workflow for creating branches and basic Prot\u00e9g\u00e9 instructions. Warning: Every ontology has their procedures on how they obsolete terms (eg notice periods, notification emails, to_be_obsolete tags, etc.), this how-to guide only serves as a guide on how obsolete a term directly on protege. For instructions on how to merge terms (i.e., replace a term with another term in the ontology), see instructions here .","title":"Obsoleting an Existing Ontology Term"},{"location":"howto/obsolete-term/#pre-obsoletion-process-or-basic-obsoletion-etiquette","text":"Check if the term (or any of its children) is being used for annotation: Go to your ontology browser of choice, search for the term, either by label or ID See which other ontologies use the to be obsolete term Notify affected groups (usually by adding an issue in their tracker) Check if the term is used elsewhere in the ontology In Prot\u00e9g\u00e9, navigate to the term to be obsolete and go to the 'Usage' tab to see if that ID is used elsewhere. If the term is a parent to other terms or is used in logical definitions, make sure that another term replaces the obsolete term","title":"PRE OBSOLETION PROCESS (or basic obsoletion etiquette)"},{"location":"howto/obsolete-term/#obsoletion-process-manual","text":"Warning: some ontologies give advance notice on terms that will be obsoleted through the annotation 'scheduled for obsoletion on or after' instead of directly obsoleting the term. Please check with the conventions of your ontology before obsoleting a term. Navigate to the term to be obsoleted. Select Edit > Deprecate entity... A deprecation wizard will pop up, in here, select GO style, and select continue (note this is specifc to GO style ontologies, if you are working with an OBI style ontology, there is an option for that too, if not use basic. For this how to, we will follow GO style) Next, enter your reason for deprecation. For this, we advice for you to enter the github issue. (eg https://github.com/obophenotype/cell-ontology/issues/####) This will appear as a rdfs:comment Next enter a replacement entity if there is one. This will automatically replace axioms in the ontology with the term, and add an 'item replaced by' axiom on the obsolete term. Your obsolete term should now be stripped of its logical axioms and should look similar to the figure below. Add any additional annotations needed - this is specific to ontologies and you should consult the conventions of the ontology you are working on. Examples of additional annotations to add: IAO:0000233 term tracker item (type xsd:anyURI) - link to GitHub issue has_obsolence_reason add \u2018OBSOLETE.\u2019 to the term definition: In the 'Description' window, click on the o on the right-hand side of the definition entry. In the resulting window, in the Literal tab, at the beginning of the definition, type: OBSOLETE. if the obsoleted term was not replaced by another term in the ontology, but there are existing terms that might be appropriate for annotation, add those term IDs in the 'consider' tag: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select consider and enter the ID of the replacement term. NOTE: Here you have to add the ID of the entity as an xsd:string , e.g. GO:0005819, not the term label.","title":"OBSOLETION PROCESS (Manual)"},{"location":"howto/obsolete-term/#obsolete-a-class-using-protege-make-entity-obsolete-function","text":"Navigate to the term to be obsoleted. In the Protege edit menu-> Make entity obsolete Prepend the definition with OBSOLETE. For example, OBSOLETE. Chronic form of myeloproliferative neoplasm. Add a statement about why the term was made obsolete: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select rdfs:comment and select Type: Xsd:string . Consult the wiki documentation for suggestions on standard comments: - [http://wiki.geneontology.org/index.php/Curator_Guide:_Obsoletion](http://wiki.geneontology.org/index.php/Curator_Guide:_Obsoletion) - [http://wiki.geneontology.org/index.php/Obsoleting_GO_Terms](http://wiki.geneontology.org/index.php/Obsoleting_GO_Terms) - [http://wiki.geneontology.org/index.php/Editor_Guide](http://wiki.geneontology.org/index.php/Editor_Guide) If the obsoleted term was replaced by another term in the ontology: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select term replaced by and enter the ID of the replacement term. If the obsoleted term was not replaced by another term in the ontology, but there are existing terms that might be appropriate for annotation, add those term IDs in the 'consider' tag: In the 'Annotations' window, select + to add an annotation. In the resulting menu, select consider and enter the ID of the replacement term. NOTE: Here you have to add the ID of the entity as an xsd:string , e.g. GO:0005819, not the term label. Add any additional annotations needed - this is specific to ontologies and you should consult the conventions of the ontology you are working on. Examples of additional annotations to add: IAO:0000233 term tracker item (type xsd:anyURI) - link to GitHub issue has_obsolence_reason See Daily Workflow section for commit, push and merge instructions.","title":"Obsolete a class (using Protege 'Make entity obsolete' function)"},{"location":"howto/odk-create-repo/","text":"Creating a new Repository with the Ontology Development Kit \u00b6 This is instructions on how to create an ontology repository in GitHub. This will only need to be done once per project. You may need assistance from someone with basic unix knowledge in following instructions here. We will walk you though the steps to make a new ontology project 1. Install requirements \u00b6 docker : Install Docker and make sure its runnning properly, for example by typing docker ps in your terminal or command line (CMD). If all is ok, you should be seeing something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES git, for example bundled with GitHub Desktop 2. Download the wrapper script and pull latest ODK version \u00b6 Linux/Mac: seed-via-docker.sh PC: seed-via-docker.bat Make sure to save the wrapper script in your working directory and that the filetype is correct. You should have git installed - for the repo command to work perfectly, it requires a .gitconfig file in your user directory! First, make sure you have Docker running (you will see the Docker whale in your toolbar on a Mac) To make sure you have the latest version of the ODK installed, run in the command line docker pull obolibrary/odkfull NOTE The very first time you run this it may be slow, while docker downloads necessary images. Don't worry, subsequent runs should be much faster! NOTE Windows users, occasionally it has been reported that files downloaded on a Windows machine get a wrong file ending, for example seed-via-docker.bat.txt instead of seed-via-docker.bat , or, as we will see later, project.yaml.txt instead of project.yaml . If you have problems, double check your files are named correctly after the download! 3. Run the wrapper script \u00b6 You can either pass in a configuration file in YAML format that specifies your ontology project setup, or you can pass arguments on the command line. You can use dir in your command line on PC to ensure that your wrapper script, .gitconfig, and project.yaml (if you so choose) are all in the correct directory before running the wrapper script. Unix (Max, Linux) \u00b6 Passing arguments on the command line: ./seed-via-docker.sh -d po -d ro -d pato -u cmungall -t \"Triffid Behavior ontology\" triffo Using a the predefined project.yaml file: ./seed-via-docker.sh -C examples/triffo/project.yaml Windows \u00b6 Passing arguments on the command line: seed-via-docker.bat -d po -d ro -d pato -u cmungall -t \"Triffid Behavior ontology\" triffo Using a the predefined project.yaml config file: seed-via-docker.bat -C project.yaml General instructions for both Linux and Windows \u00b6 Instead of -u cmungall you should be using your own username (i.e. -u nico ), for example for your GitHub or GitLab hosting sites. You can add a -c (lowercase) just before the -C (capital c) in the command to first delete any previous attempt to generate your ontology with the ODK, and then replaces it with a completely new one. So, -c stands for clean or \"clean up previous attempts before running again\" and -C stands for \"the next parameter is the relative path to my config file\". In general, we now always recommend the use of config files. The ODK has a rich set of configuration options, most of which can only be set through the config file, but in general the config also serves as documentation and will help with updating your ontology at later stages. To create a config file, you can download for example project.yaml by clicking on the link and then typing command+s on Mac or ctrl+s on Windows to save it in the same directory as your seed-via-docker script. Then you can open the file with a text editor like Notepad++, Atom, Sublime or even nano, and adapt it to your project. Other more comprehensive examples can be found here . This will create your starter files in target/triffid-behavior-ontology . It will also prepare an initial release and initialize a local repository (not yet pushed to your Git host site such as GitHub or GitLab). Problems? \u00b6 There are three frequently encountered problems at this stage: No .gitconfig in user directory Spaces is user path During download, your filenames got changed (Windows) No .gitconfig in user directory \u00b6 The seed-via-docker script requires a .gitconfig file in your user directory. If your .gitconfig is in a different directory, you need to change the path in the downloaded seed-via-docker script. For example on Windows (look at seed-via-docker.bat ): docker run -v %userprofile%/.gitconfig:/root/.gitconfig -v %cd%:/work -w /work --rm -ti obolibrary/odkfull /tools/odk.py seed %* %userprofile%/.gitconfig should be changed to the correct path of your local .gitconfig file. Spaces is user path \u00b6 We have had reports of users having trouble if there paths (say, D:\\data ) contain a space symbol, like D:/Dropbox (Personal) or similar. In this case, we recommend to find a directory you can work in that does not contain a space symbol. You can customize at this stage, but we recommend to first push the changes to you Git hosting site (see next steps). During download, your filenames got changed (Windows) \u00b6 Windows users, occasionally it has been reported that files downloaded on a Windows machine get a wrong file ending, for example seed-via-docker.bat.txt instead of seed-via-docker.bat , or, as we will see later, project.yaml.txt instead of project.yaml . If you have problems, double check your files are named correctly after the download! 4. Push to Git hosting website \u00b6 The development kit will automatically initialize a git project, add all files and commit. You will need to create a project on you Git hosting site. For GitHub: Go to: https://github.com/new The owner MUST be the org you selected with the -u option. The name MUST be the one you set with -t , just with lower case letters and dashes instead of spaces. In our example above, the name \"Triffid Behavior Ontology\" translates to triffid-behavior-ontology . Do not initialize with a README (you already have one) Click Create See the section under \"\u2026or push an existing repository from the command line\" For GitLab: Go to: https://gitlab.com/projects/new The owner MUST be the org you selected with the -u option. The name MUST be the one you set with -t . Do not initialize with a README (you already have one) Click 'Create project' See the section under \"Push an existing Git repository\" Follow the instructions there. E.g. (make sure the location of your remote is exactly correct!). cd target/triffo git remote add origin https://github.com/matentzn/triffid-behavior-ontology.git git branch -M main git push -u origin main Note: you can now mv target/triffid-behavior-ontology to anywhere you like in your home directory. Or you can do a fresh checkout from github. Alternative recommendation for GitHub by @matentzn \u00b6 I generally feel its easier and less error prone to deviate from the standard instructions above. I keep having problems with git, passwords, typose etc, so I tend to do it, inofficially, as follows: When my repo is created I go to my GitHub Desktop I then do File > Add local repository, and select the directory which contains my newly created repo (e.g. target/triffo ). I then Click on \"Publish repository\". If I want the code to be public, I deselect \"Keep this code private\". By default, the repo will be uploaded to my own user profile on GitHub, but I can also select another Organization I have access to in the respective Dropdown menu. NOTE: there seem to be some issues with pushing a GitHub Workflow file recently - you may be asked by GitHub Desktop to provide an additional permission to push the Workflow file. Next Steps: Edit and release cycle \u00b6 In your repo you will see a README-editors.md file that has been customized for your project. Follow these instructions. OBO Library metadata \u00b6 The assumption here is that you are adhering to OBO principles and want to eventually submit to OBO. Your repo will contain stub metadata files to help you do this. You can create pull requests for your ontology on the OBO Foundry. See the src/metadata file for more details. For more documentation, see http://obofoundry.org Additional \u00b6 You will want to also: enable GitHub actions See the README-editors.md file that has been generated for your project.","title":"Creating a Repo with ODK"},{"location":"howto/odk-create-repo/#creating-a-new-repository-with-the-ontology-development-kit","text":"This is instructions on how to create an ontology repository in GitHub. This will only need to be done once per project. You may need assistance from someone with basic unix knowledge in following instructions here. We will walk you though the steps to make a new ontology project","title":"Creating a new Repository with the Ontology Development Kit"},{"location":"howto/odk-create-repo/#1-install-requirements","text":"docker : Install Docker and make sure its runnning properly, for example by typing docker ps in your terminal or command line (CMD). If all is ok, you should be seeing something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES git, for example bundled with GitHub Desktop","title":"1. Install requirements"},{"location":"howto/odk-create-repo/#2-download-the-wrapper-script-and-pull-latest-odk-version","text":"Linux/Mac: seed-via-docker.sh PC: seed-via-docker.bat Make sure to save the wrapper script in your working directory and that the filetype is correct. You should have git installed - for the repo command to work perfectly, it requires a .gitconfig file in your user directory! First, make sure you have Docker running (you will see the Docker whale in your toolbar on a Mac) To make sure you have the latest version of the ODK installed, run in the command line docker pull obolibrary/odkfull NOTE The very first time you run this it may be slow, while docker downloads necessary images. Don't worry, subsequent runs should be much faster! NOTE Windows users, occasionally it has been reported that files downloaded on a Windows machine get a wrong file ending, for example seed-via-docker.bat.txt instead of seed-via-docker.bat , or, as we will see later, project.yaml.txt instead of project.yaml . If you have problems, double check your files are named correctly after the download!","title":"2. Download the wrapper script and pull latest ODK version"},{"location":"howto/odk-create-repo/#3-run-the-wrapper-script","text":"You can either pass in a configuration file in YAML format that specifies your ontology project setup, or you can pass arguments on the command line. You can use dir in your command line on PC to ensure that your wrapper script, .gitconfig, and project.yaml (if you so choose) are all in the correct directory before running the wrapper script.","title":"3. Run the wrapper script"},{"location":"howto/odk-create-repo/#unix-max-linux","text":"Passing arguments on the command line: ./seed-via-docker.sh -d po -d ro -d pato -u cmungall -t \"Triffid Behavior ontology\" triffo Using a the predefined project.yaml file: ./seed-via-docker.sh -C examples/triffo/project.yaml","title":"Unix (Max, Linux)"},{"location":"howto/odk-create-repo/#windows","text":"Passing arguments on the command line: seed-via-docker.bat -d po -d ro -d pato -u cmungall -t \"Triffid Behavior ontology\" triffo Using a the predefined project.yaml config file: seed-via-docker.bat -C project.yaml","title":"Windows"},{"location":"howto/odk-create-repo/#general-instructions-for-both-linux-and-windows","text":"Instead of -u cmungall you should be using your own username (i.e. -u nico ), for example for your GitHub or GitLab hosting sites. You can add a -c (lowercase) just before the -C (capital c) in the command to first delete any previous attempt to generate your ontology with the ODK, and then replaces it with a completely new one. So, -c stands for clean or \"clean up previous attempts before running again\" and -C stands for \"the next parameter is the relative path to my config file\". In general, we now always recommend the use of config files. The ODK has a rich set of configuration options, most of which can only be set through the config file, but in general the config also serves as documentation and will help with updating your ontology at later stages. To create a config file, you can download for example project.yaml by clicking on the link and then typing command+s on Mac or ctrl+s on Windows to save it in the same directory as your seed-via-docker script. Then you can open the file with a text editor like Notepad++, Atom, Sublime or even nano, and adapt it to your project. Other more comprehensive examples can be found here . This will create your starter files in target/triffid-behavior-ontology . It will also prepare an initial release and initialize a local repository (not yet pushed to your Git host site such as GitHub or GitLab).","title":"General instructions for both Linux and Windows"},{"location":"howto/odk-create-repo/#problems","text":"There are three frequently encountered problems at this stage: No .gitconfig in user directory Spaces is user path During download, your filenames got changed (Windows)","title":"Problems?"},{"location":"howto/odk-create-repo/#no-gitconfig-in-user-directory","text":"The seed-via-docker script requires a .gitconfig file in your user directory. If your .gitconfig is in a different directory, you need to change the path in the downloaded seed-via-docker script. For example on Windows (look at seed-via-docker.bat ): docker run -v %userprofile%/.gitconfig:/root/.gitconfig -v %cd%:/work -w /work --rm -ti obolibrary/odkfull /tools/odk.py seed %* %userprofile%/.gitconfig should be changed to the correct path of your local .gitconfig file.","title":"No .gitconfig in user directory"},{"location":"howto/odk-create-repo/#spaces-is-user-path","text":"We have had reports of users having trouble if there paths (say, D:\\data ) contain a space symbol, like D:/Dropbox (Personal) or similar. In this case, we recommend to find a directory you can work in that does not contain a space symbol. You can customize at this stage, but we recommend to first push the changes to you Git hosting site (see next steps).","title":"Spaces is user path"},{"location":"howto/odk-create-repo/#during-download-your-filenames-got-changed-windows","text":"Windows users, occasionally it has been reported that files downloaded on a Windows machine get a wrong file ending, for example seed-via-docker.bat.txt instead of seed-via-docker.bat , or, as we will see later, project.yaml.txt instead of project.yaml . If you have problems, double check your files are named correctly after the download!","title":"During download, your filenames got changed (Windows)"},{"location":"howto/odk-create-repo/#4-push-to-git-hosting-website","text":"The development kit will automatically initialize a git project, add all files and commit. You will need to create a project on you Git hosting site. For GitHub: Go to: https://github.com/new The owner MUST be the org you selected with the -u option. The name MUST be the one you set with -t , just with lower case letters and dashes instead of spaces. In our example above, the name \"Triffid Behavior Ontology\" translates to triffid-behavior-ontology . Do not initialize with a README (you already have one) Click Create See the section under \"\u2026or push an existing repository from the command line\" For GitLab: Go to: https://gitlab.com/projects/new The owner MUST be the org you selected with the -u option. The name MUST be the one you set with -t . Do not initialize with a README (you already have one) Click 'Create project' See the section under \"Push an existing Git repository\" Follow the instructions there. E.g. (make sure the location of your remote is exactly correct!). cd target/triffo git remote add origin https://github.com/matentzn/triffid-behavior-ontology.git git branch -M main git push -u origin main Note: you can now mv target/triffid-behavior-ontology to anywhere you like in your home directory. Or you can do a fresh checkout from github.","title":"4. Push to Git hosting website"},{"location":"howto/odk-create-repo/#alternative-recommendation-for-github-by-matentzn","text":"I generally feel its easier and less error prone to deviate from the standard instructions above. I keep having problems with git, passwords, typose etc, so I tend to do it, inofficially, as follows: When my repo is created I go to my GitHub Desktop I then do File > Add local repository, and select the directory which contains my newly created repo (e.g. target/triffo ). I then Click on \"Publish repository\". If I want the code to be public, I deselect \"Keep this code private\". By default, the repo will be uploaded to my own user profile on GitHub, but I can also select another Organization I have access to in the respective Dropdown menu. NOTE: there seem to be some issues with pushing a GitHub Workflow file recently - you may be asked by GitHub Desktop to provide an additional permission to push the Workflow file.","title":"Alternative recommendation for GitHub by @matentzn"},{"location":"howto/odk-create-repo/#next-steps-edit-and-release-cycle","text":"In your repo you will see a README-editors.md file that has been customized for your project. Follow these instructions.","title":"Next Steps: Edit and release cycle"},{"location":"howto/odk-create-repo/#obo-library-metadata","text":"The assumption here is that you are adhering to OBO principles and want to eventually submit to OBO. Your repo will contain stub metadata files to help you do this. You can create pull requests for your ontology on the OBO Foundry. See the src/metadata file for more details. For more documentation, see http://obofoundry.org","title":"OBO Library metadata"},{"location":"howto/odk-create-repo/#additional","text":"You will want to also: enable GitHub actions See the README-editors.md file that has been generated for your project.","title":"Additional"},{"location":"howto/odk-setup/","text":"Getting set up with Docker and the Ontology Development Kit \u00b6 Installation \u00b6 For Windows \u00b6 Follow the instructions here . Note that you should have Windows 10 Professional installed for this to work. We are not sure Docker Desktop works at all with Windows 10 Home, but we have not tried in a while. If you know what you are doing, you could try to configure Docker toolbox, but we have had many issues with it, and do not recommend it unless absolutely necessary. Once installed, you should be able to open your command line and download the ODK. Click on your Windows symbol (usually in bottom left corner of screen), type \"cmd\" and you should be able to see and open the Command Line tool. in the command line type, type docker pull obolibrary/odkfull . This will download the ODK (will take a few minutes, depending on you internet connection). Executing something in a Docker container can be \"wordy\", because the docker container requires quite a few parameters to be run. To make this easier, we prepared a wrapper script here . You can download this file by clicking on Raw , and then, when the file is open in your browser, CTRL+S to save it. Ideally, you save this file in your project directory, the directory you will be using for your exercises, as it will only allow you to edit files in that very same directory (or one of its sub-directories). Setting the memory: Typical issues (WSL 1 vs 2) For Mac/Linux \u00b6 Install docker : Install Docker following the official instructions. Make sure its running properly, for example by typing docker ps in your terminal or command line (CMD). If all is ok, you should be seeing something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run docker pull obolibrary/odkfull on your command line to install the ODK. This will take while. Download an ODK wrapper script . The odk.sh has further instruction on how to best use it. Now you are ready to go to a directory containing the odk.sh wrapper script and running sh odk.sh robot --version to see whether it works. The ODK wrapper script is generally useful to have: you can for example enter a ODK container, similar to a virtual machine, by simply running sh odk.sh bash (to leave the ODK container again, simply run exit from within the container). On Windows, use run.bat bash instead. However, for many of the ontologies we develop, we already ship an ODK wrapper script in the ontology repo, so we dont need the odk.sh or odk.bat file. That file is usually called run.sh or run.bat and can be found in your ontology repo in the src/ontology directory and can be used in the exact same way. Problems with memory (important) \u00b6 One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. There are two potential causes for out-of-memory errors: The application (for example, the ODK release run) needs more memory than assigned to JAVA inside the ODK docker container . This memory is set as part of the ODK wrapper files, i.e. src/ontology/run.bat or src/ontology/run.sh , usually with ODK_JAVA_OPTS . The application needs more memory than is assigned to your docker installation. On most systems (apart from a handful fo Windows ones based on WSL), you have to set docker memory in the docker preferences. That happens here is that the Java memory above may be set to something like 10GB, while the maximum docker memory is set to 8GB. If the application needs, say, 9GB to run, you have assigned enough Java memory, but docker does not permit more than 8 to be used. Out-of-memory errors can take many forms, like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . Solving memory issues \u00b6 Setting memory limits: \u00b6 There are two places you need to consider to set your memory: Your ODK wrapper script (see above), i.e. odk.bat, odk.sh or src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/cl-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below. More intelligent pipeline design \u00b6 If your problem is that you do not have enough memory on your machine, the only solution is to try to engineer the pipelines a bit more intelligently, but even that has limits: large ontologies require a lot of memory to process when using ROBOT. For example, handling ncbitaxon as an import in any meaningful way easily consumes up to 12GB alone. Here are some tricks you may want to contemplate to reduce memory: robot query uses an entirely different framework for representing the ontology, which means that whenever you use ROBOT query, for at least a short moment, you will have the entire ontology in memory twice . Sometimes you can optimse memory by seperating query and other robot commands into seperate commands (i.e. not chained in the same robot command). The robot reason command consumes a lot of memory. reduce and materialise potentially even more. Use these only ever in the last possible moment in a pipeline. `","title":"Setting up the ODK"},{"location":"howto/odk-setup/#getting-set-up-with-docker-and-the-ontology-development-kit","text":"","title":"Getting set up with Docker and the Ontology Development Kit"},{"location":"howto/odk-setup/#installation","text":"","title":"Installation"},{"location":"howto/odk-setup/#for-windows","text":"Follow the instructions here . Note that you should have Windows 10 Professional installed for this to work. We are not sure Docker Desktop works at all with Windows 10 Home, but we have not tried in a while. If you know what you are doing, you could try to configure Docker toolbox, but we have had many issues with it, and do not recommend it unless absolutely necessary. Once installed, you should be able to open your command line and download the ODK. Click on your Windows symbol (usually in bottom left corner of screen), type \"cmd\" and you should be able to see and open the Command Line tool. in the command line type, type docker pull obolibrary/odkfull . This will download the ODK (will take a few minutes, depending on you internet connection). Executing something in a Docker container can be \"wordy\", because the docker container requires quite a few parameters to be run. To make this easier, we prepared a wrapper script here . You can download this file by clicking on Raw , and then, when the file is open in your browser, CTRL+S to save it. Ideally, you save this file in your project directory, the directory you will be using for your exercises, as it will only allow you to edit files in that very same directory (or one of its sub-directories). Setting the memory: Typical issues (WSL 1 vs 2)","title":"For Windows"},{"location":"howto/odk-setup/#for-maclinux","text":"Install docker : Install Docker following the official instructions. Make sure its running properly, for example by typing docker ps in your terminal or command line (CMD). If all is ok, you should be seeing something like: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run docker pull obolibrary/odkfull on your command line to install the ODK. This will take while. Download an ODK wrapper script . The odk.sh has further instruction on how to best use it. Now you are ready to go to a directory containing the odk.sh wrapper script and running sh odk.sh robot --version to see whether it works. The ODK wrapper script is generally useful to have: you can for example enter a ODK container, similar to a virtual machine, by simply running sh odk.sh bash (to leave the ODK container again, simply run exit from within the container). On Windows, use run.bat bash instead. However, for many of the ontologies we develop, we already ship an ODK wrapper script in the ontology repo, so we dont need the odk.sh or odk.bat file. That file is usually called run.sh or run.bat and can be found in your ontology repo in the src/ontology directory and can be used in the exact same way.","title":"For Mac/Linux"},{"location":"howto/odk-setup/#problems-with-memory-important","text":"One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. There are two potential causes for out-of-memory errors: The application (for example, the ODK release run) needs more memory than assigned to JAVA inside the ODK docker container . This memory is set as part of the ODK wrapper files, i.e. src/ontology/run.bat or src/ontology/run.sh , usually with ODK_JAVA_OPTS . The application needs more memory than is assigned to your docker installation. On most systems (apart from a handful fo Windows ones based on WSL), you have to set docker memory in the docker preferences. That happens here is that the Java memory above may be set to something like 10GB, while the maximum docker memory is set to 8GB. If the application needs, say, 9GB to run, you have assigned enough Java memory, but docker does not permit more than 8 to be used. Out-of-memory errors can take many forms, like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 .","title":"Problems with memory (important)"},{"location":"howto/odk-setup/#solving-memory-issues","text":"","title":"Solving memory issues"},{"location":"howto/odk-setup/#setting-memory-limits","text":"There are two places you need to consider to set your memory: Your ODK wrapper script (see above), i.e. odk.bat, odk.sh or src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/cl-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting memory limits:"},{"location":"howto/odk-setup/#more-intelligent-pipeline-design","text":"If your problem is that you do not have enough memory on your machine, the only solution is to try to engineer the pipelines a bit more intelligently, but even that has limits: large ontologies require a lot of memory to process when using ROBOT. For example, handling ncbitaxon as an import in any meaningful way easily consumes up to 12GB alone. Here are some tricks you may want to contemplate to reduce memory: robot query uses an entirely different framework for representing the ontology, which means that whenever you use ROBOT query, for at least a short moment, you will have the entire ontology in memory twice . Sometimes you can optimse memory by seperating query and other robot commands into seperate commands (i.e. not chained in the same robot command). The robot reason command consumes a lot of memory. reduce and materialise potentially even more. Use these only ever in the last possible moment in a pipeline. `","title":"More intelligent pipeline design"},{"location":"howto/odk-update/","text":"Updating ODK \u00b6 A new version of the Ontology Development Kit (ODK) is out? This is what you should be doing: Install the latest version of ODK by pulling the ODK docker images. In your terminal, run: docker pull obolibrary/odkfull To update your repository, go to your src/ontology directory. cd myrepo/src/ontology Create a new git branch in your usual way (optional) Now run the update command TWICE (the first time it may fail, as the update command needs to update itself). sh run.sh make update_repo sh run.sh make update_repo Edit the following file: .github/workflows/qc.yml (from the top level of your repository) and make sure that it is using the latest version of the ODK. For example, container: obolibrary/odkfull:v1.3.0 , if v1.3.0 . Is the latest version. If you are unsure what the latest version is, you can find that information here: https://hub.docker.com/r/obolibrary/odkfull/tags OPTIONAL: if you have any other GitHub actions you would like to update to the latest ODK, now is the time! All of your GitHub actions can be found in the .github/workflows/ directory from the top level of your repo. Review all the changes and commit them, and make a PR the usual way. 100% wait for the PR to pass QC - ODK updates can be significant! Send a reminder to all other ontology developers of your repo and tell them to install the latest version of ODK (step 1 only).","title":"Update to a newer ODK version"},{"location":"howto/odk-update/#updating-odk","text":"A new version of the Ontology Development Kit (ODK) is out? This is what you should be doing: Install the latest version of ODK by pulling the ODK docker images. In your terminal, run: docker pull obolibrary/odkfull To update your repository, go to your src/ontology directory. cd myrepo/src/ontology Create a new git branch in your usual way (optional) Now run the update command TWICE (the first time it may fail, as the update command needs to update itself). sh run.sh make update_repo sh run.sh make update_repo Edit the following file: .github/workflows/qc.yml (from the top level of your repository) and make sure that it is using the latest version of the ODK. For example, container: obolibrary/odkfull:v1.3.0 , if v1.3.0 . Is the latest version. If you are unsure what the latest version is, you can find that information here: https://hub.docker.com/r/obolibrary/odkfull/tags OPTIONAL: if you have any other GitHub actions you would like to update to the latest ODK, now is the time! All of your GitHub actions can be found in the .github/workflows/ directory from the top level of your repo. Review all the changes and commit them, and make a PR the usual way. 100% wait for the PR to pass QC - ODK updates can be significant! Send a reminder to all other ontology developers of your repo and tell them to install the latest version of ODK (step 1 only).","title":"Updating ODK"},{"location":"howto/ontology-overview/","text":"How to prepare OBO Academy ontology overview \u00b6 Overview \u00b6 This 'how to' guide provides a template for an Ontology Overview for your ontology. Please create a markdown file using this template and share it in your ontology repository, either as part of your ReadMe file or as a separate document in your documentation. The Ontology Overview should include the following three sections: Scope Curation and governance workflows How the ontology is used in practice Scope \u00b6 Describe the domain and scope of ontology. For example, the Mondo ontology covers concepts in the area of diseases across species and integrates disease terminologies from several underlying sources. Include a figure of upper level terms (critical: give a list of all the high level terms that the ontology covers (1-2 levels). Eg Mondo: disease or disorder, disease susceptibility, disease characteristic). Include a figure with exemplary term (using OBO graph) Curation and governance workflows \u00b6 Ontology Curation \u00b6 Describe the ontology level curation, ie how to add terms. For example, terms are added to the ontology via: Manual additions via Protege ROBOT templates DOSDP templates Governance \u00b6 How do people request new terms or changes How do people contribute terms directly (ie ROBOT templates, etc) (if applicable) Note: There is no need for details about QC, ODK unless it is related to curation (ie pipeline that automatically generates mappings, include that) How the ontology used in practice \u00b6 Include 1-3 actual use cases. Please provide concrete examples. For example: this group uses the ontology to annotate this data for this purpose this group uses the ontology to compute phenotypic similarity for prediction of related diseases The ontology is used for named entity recognition (NER) as a dictionary as a synonym source","title":"Create an ontology overview"},{"location":"howto/ontology-overview/#how-to-prepare-obo-academy-ontology-overview","text":"","title":"How to prepare OBO Academy ontology overview"},{"location":"howto/ontology-overview/#overview","text":"This 'how to' guide provides a template for an Ontology Overview for your ontology. Please create a markdown file using this template and share it in your ontology repository, either as part of your ReadMe file or as a separate document in your documentation. The Ontology Overview should include the following three sections: Scope Curation and governance workflows How the ontology is used in practice","title":"Overview"},{"location":"howto/ontology-overview/#scope","text":"Describe the domain and scope of ontology. For example, the Mondo ontology covers concepts in the area of diseases across species and integrates disease terminologies from several underlying sources. Include a figure of upper level terms (critical: give a list of all the high level terms that the ontology covers (1-2 levels). Eg Mondo: disease or disorder, disease susceptibility, disease characteristic). Include a figure with exemplary term (using OBO graph)","title":"Scope"},{"location":"howto/ontology-overview/#curation-and-governance-workflows","text":"","title":"Curation and governance workflows"},{"location":"howto/ontology-overview/#ontology-curation","text":"Describe the ontology level curation, ie how to add terms. For example, terms are added to the ontology via: Manual additions via Protege ROBOT templates DOSDP templates","title":"Ontology Curation"},{"location":"howto/ontology-overview/#governance","text":"How do people request new terms or changes How do people contribute terms directly (ie ROBOT templates, etc) (if applicable) Note: There is no need for details about QC, ODK unless it is related to curation (ie pipeline that automatically generates mappings, include that)","title":"Governance"},{"location":"howto/ontology-overview/#how-the-ontology-used-in-practice","text":"Include 1-3 actual use cases. Please provide concrete examples. For example: this group uses the ontology to annotate this data for this purpose this group uses the ontology to compute phenotypic similarity for prediction of related diseases The ontology is used for named entity recognition (NER) as a dictionary as a synonym source","title":"How the ontology used in practice"},{"location":"howto/open-science-engineer/","text":"How to be an Open Science Engineer - maximising impact for a better world \u00b6 Contributors : Nicole Vasilevsky Nicolas Matentzoglu Bradley Varner Status : This is a working document! Feel free to add more content! The Open Science Engineer contributes to the collection and standardisation of publicly available scientific knowledge through curation, community-building and data, ontology and software engineering. Open Science and all its sub-divisions, including Open Data and Open Ontologies, are central to tackling global challenges from rare disease to climate change. Open licenses are only part of the answer - the really tough part is the standardisation of data (including the unification of ontologies, the FAIRification of data and adoption of common semantic data models) and the organisation of a global, fully decentralised community of Open Science engineers. Here, we will discuss some basic principles on how we can maximise our impact as members of a global community combating the issues of our time: Principle of Collaboration : How do we create a welcoming and inclusive environment for implementing social workflows and deepen our ties across project boundaries? Principle of Upstream Fixing : How can we maximise benefits to the global community by pushing fixes as far upstream as possible? Principle of No-ownership : How do we develop a sense of co-, or -no, ownership for community driven ontologies and ontology tools? We discuss how to best utilise social workflows to achieve positive impact. We will try to convince you that building a close collaborative international community by spending time on submitting and answering issues on GitHub, helping on Stack Overflow and other online platforms, or just reaching out and donating small amounts of time to other open science efforts can make a huge difference. Table of contents \u00b6 TLDR - Summary Principle of Collaboration Principle of Upstream fixing Principle of No-ownership Principle of Collaboration \u00b6 The heart and soul of a successful Open Science culture is collaboration. The relative isolation into which many projects are forced due to limitations imposed by certain kinds of funding makes it even more important to develop effective social, collaborative workflows. This involve effective online communication, vocal appreciation (likes, upvotes, comments), documentation and open-ness. Question answering and documentation \u00b6 When you find an answer on Stack Overflow , GitHub issues (or even Hacker News ) that helps you, upvote it . It cannot be stressed enough how important this is to let good answers float up to the top (become more visible), and recognise the time and energy people spend formulating answers. If you do not have a Stack Overflow account, make one now . If you cannot find an answer to a question on GitHub or Stack Overflow, and you figure out a solution consider asking the question and answering it yourself! Consider this: if you provide an answer that will result in only 6 open science people spending one hour less solving a problem (a conservative estimate), you saved the taxpayer 6 hours of salary (not to speak of improving the quality of the solutions)! Answer questions on GitHub issues even when on trackers not in your purview. People often falsely assume that they should not try to answer queries in an open source project that they are not directly involved with. This is wrong! Open source developers highly (!) appreciate it when you chip in on answering queries, from dealing with errors and exceptions to \"How do I?\" kind of queries. Get involved on other peoples issue trackers ! Before asking questions on slack or issue trackers, always do a basic search first . Consider (1) the project's documentation, (2) open and closed issues on the issue tracker of the project your query is related to. If you do not find the information you need try and craft questions that are concise yet give sufficient context. To enable future users to find these answers, consider using the issue tracker instead of slack! Also it is important to remember that the people that answer your questions have to spend time they could have spend on other Open Science work! Make yourself responsible for continuously improving open science documentation . If a question was answered on slack or in an issue, consider a 10 minute detour to update the documentation of the project to reflect the solution if you think it could be helpful. This is the only way to scale Open Science projects: as the user base increases, providing support on a 1:1 basis will soon be infeasible . Good documentation is as important as good quality code, and everyone can help out, even if it is \"only\" about formatting, typos and adding additional links! Social Conduct and review \u00b6 Be overly generous with likes . Being a Open Science Engineer can be quite a lonely affair: hundreds of unanswered issues and questions, pull requests that remain un-reviewed for months, projects for which you never really get credit (want to be a QC engineer, anyone?). Even if you do not have time to respond to all issues you scroll through in a day, add a like if you find an answer useful . This goes for slack comments, Tweets and random Pull Requests you have nothing to do with as well. And, perhaps most importantly star all GitHub repos that are useful to you (you could, for example, scroll through https://github.com/topics/obofoundry and star all ontologies and ontology projects useful in your work)! This is a huge deal as it significantly motivates other OSEs and therefore provides fuel to the Open Science movement. Reduce work for others as much as possible by communicating clearly. Take the time to write clear responses with just the right amount of detail. The goal of your communication is to get the point across as swiftly as possible, and misunderstandings are a huge time killer. Again: Sloppy, quick responses can cause more work than not responding at all ! Use bullet lists to structure your response, and checklists for action items. Hide unnecessary details with <details> tag: <details><summary>[click arrow to expand]</summary> . See example here Be generous linking external issues to provide context. Example: Be positive and generous with gratitude and attribution If a member of the community opens an issue, always thank them for the issue right away. We need to encourage people to open issues. Don't let issues linger without any response. It is better to respond with a thank you and some instructions for the issuer on what to do to fix the issue themselves than to let it linger. We need our stakeholders to be more involved - first time issuers are especially vulnerable and may not bother to come back if they are ignored. When giving feedback, be positive, friendly and constructive. Show appreciation : thank users for issues and thank a PR reviewer for their review. Always. Attribute, attribute, attribute . If someone helped you sort something out emphasise this publicly! Openness \u00b6 Promote truly open communication: Contribute your thoughts openly so other people can benefit from it. Don't put issues on Slack where the public can't see it. Consider moving interesting discussions on Slack into a more public space, like GitHub discussions. Create public tickets which can be searched and referred to later. Principle of Upstream Fixing \u00b6 Maximising impact of your changes is by far the best way you can benefit society as an Open Science Engineer. Open Science projects are a web of mutually dependent efforts, for example: Ontologies re-use terms and axioms from other ontologies Software packages provide functionality that help building Knowledge Graphs, ensuring the quality of ontology releases and extract new insights from existing knowledge and scientific facts. Projects use ontologies for indexing data and making them discoverable. The key to maximising your impact is to push any fixes as far upstream as possible . Consider the following projects and the way they depend on each other (note that this is a gross simplification for illustration; in reality the number of dependencies is much higher): Let's think of the following (entirely fabricated) scenario based on the image above. Open Targets provides evidence for a gene association with some disease . This association is only supported by the IMPC data source which associates mouse and human phenotypes using the phenodigm algorithm which is based on semantic similarity. The semantic similarity scores are computed using cross-species axioms provided by the Monarch Initiative , especially uPheno . The cross-species logical axioms provided by Monarch depend on a range of lexical and logical approaches. This means the structure of uPheno is directly influenced by the naming and synonyms provided by species specific phenotype ontologies (SSPOs), such as the Human Phenotype Ontology (HPO) . It is, therefore, possible that: A faulty synonym is accidentally added to the HPO... ...which causes a faulty logical axiom candidate in uPheno (again missed by QC measures)... ...which causes a faulty similarity value in Monarch... ...which causes a wrong disease-gene association in IMPC... ...which ultimately leads to a faulty piece of evidence in Open Targets. Imagine a user of Open Targets that sees this evidence, and reports it to Open Targets as a bug. Open Targets could take the easy way out: remove the erroneous record from the database permanently. This means that the IMPC (itself with hundreds of dependent users and tools), Monarch (again with many dependents), uPheno and HPO (with probably thousands of dependents) would still carry forward that (tiny) mistake. This is the basic idea of maximising impact through Upstream Fixing : The higher up-stream (up the dependency graph) an error is fixed, the more cumulative benefit there is to a huge ecosystem of tools and services . An even better fix would be to have each fix to the ontology result in a new, shared quality control test. For example, some errors (duplicate labels, missing definition, etc) can be caught by automated testing. Here is a cool story. Case Study: External contribution and upstream fixing \u00b6 Over time, we have developed QC checks that ensure that the same exact synonym cannot be shared between two classes. However the checks where not perfect.. @vasvir (GitHub name), a member of the global community reached out to us on Uberon: https://github.com/obophenotype/uberon/issues/2424. https://github.com/obophenotype/uberon/pull/2640 Instead of fixing the discovered issue by ourselves, we invited @vasvir to fix the issues himself. We gave him some instructions on how to proceed, leading not only to eight new pull requests , but also an entirely new Quality Control check that augments the existing checks with case-insensitivity ( Gasserian ganglion and gasserian ganglion where previously considered distinct). Note: before the PRs, @vasvir did not speak any SPARQL . Members of our team helped @vasvir to see his first pull requests through by instructing them how to use the technology (robot, ODK, docker, SPARQL), and follow our pull request conventions. Instead of simply deleting the synonyms for his NLP projects, @vasvir instead decided to report the issues straight to the source. This way, hundreds, if not thousands of projects will directly or indirectly benefit from him! Other examples of upstream fixing \u00b6 Example 1 : While curating Mondo, Nicole identified issues relevant to Orphanet and created this issue . Example 2 : There is overlap between Mondo and Human Phenotype Ontology and the Mondo and HPO curators tag each other on relevant tickets. Example 3 : In Mondo, if new classifications are made, Mondo curators report this back to the source ontology to see if they would like to follow our classification. Conclusions: Upstream Fixing \u00b6 Have you ever wondered how much impact changing a synonym from exact to related could have? Or the addition of a precise mapping? The fixing of a typo in a label? It can be huge. And this does not only relate to ontologies, this goes for tool development as well. We tend to work around bugs when we are building software. Instead, or at least in addition to, we should always report the bug at the source to make sure it gets fixed eventually. Principle of No-ownership \u00b6 Many of the resources we develop are financed by grants. Grants are financed in the end by the taxpayer. While it is occasionally appropriate to protect open work with creative licenses, it rarely makes sense to restrict access to Open Ontologies work - neither to commercial nor research exploitation (we may want to insist on appropriate attribution to satisfy our grant developers). On the other side there is always the risk of well-funded commercial endeavours simply \"absorbing\" our work - and then tying stakeholders into their closed, commercial ecosystem. However, this is not our concern. We cannot really call it stealing if it is not really ours to begin with! Instead of trying to prevent unwanted commercialisation and closing, it is better to work with corporations in pre-competitive schemes such as Pistoia Alliance or Allotrope Foundation and lobby for more openness. (Also, grant authorities should probably not allow linking scientific data to less than totally open controlled vocabularies.) Here, we invite you to embrace the idea that ontologies and many of the tools we develop are actually community-driven , with no particular \"owners\" and \"decision makers\". While we are not yet there (we don't have sufficiently mature governance workflows for full fledged onto-communism), and most ontologies are still \"owned\" by an organisation that provides a major source of funding, we invite you to think of this as a preliminary state. It is better to embrace the idea of \"No-ownership\" and figure out social workflows and governance processes that can handle the problems of decision making. Take responsibility for your community (ontologies) \u00b6 Ensure that you see your issues and pull requests through to the end . No one will do this for you. Remember - contributors to open source projects, especially ontologies, have their own agendas, and do not automatically care about other peoples work. Feel empowered to nudge reviewers or experts to help. Get that issue answered and PR merged whatever it takes! Example : After waiting for the PR to be reviewed, Meghan kindly asked Nicole if she should find a different reviewer. 1. Find review buddies . For every ontology you seek to contribute to pair up with someone who will review your pull requests and you will review their pull requests. Sometimes, it is very difficult to get anyone to review your pull request. Reach out to people directly, and form an alliance for review. It is fun, and you learn new things (and get to know new people!). 1. Be proactive Problem of decentralization and lack of hierarchial organisation needs proactive and brave decision makers. No one will do your work for you. See your pull requests and issues through all the way to the release! Learn the tools necessary to make basic fixes - just try it/do it. Always have your index finger on the Edit button when reading documentation. There is always something to fix, including typos and content. Reduce your fear of \"breaking the ontology\". \u00b6 Most of our ontologies have many checks in place, and GitHub has version control. Nothing has ever been broken to the point where it can't be fixed. (Remember to work on a branch!) Example: The QC checks on this PR failed 6 times before it passed. Perfect is the enemy of good enough. It's okay if your PR is rejected . Mentally, prepare yourself for having the PR rejected. This is fine - the community always looks to the best possible way to change the ontology. The next PR will be accepted! If you make a mistake, and it is pointed out during pull request review, consider adding an appropriate QC check to prevent the issue from happening again. And who knows - maybe you have an opportunity to fix past mistakes! TLDR - Summary \u00b6 Principle of Collaboration Upvote answers, on Stack Overflow , GitHub and any other open communication platform. Get involved on other peoples issue trackers. Always do a basic search before asking. Continuously improve Open Science documentation. Be overly generous with likes. Always strive to reduce work for other members of the community. Be positive and generous with gratitude and attribution. Promote open communication (less slack, more GitHub). Principle of Upstream fixing The key to maximising your impact is to push any fixes as far upstream as possible. When you experience a problem, always report it to the immediate source. If you can report it as high upstream as possible. In a perfect world, provide a fix in the form of a pull request. Principle of No-ownership See your issues and pull requests through to the end (dont drop the ball, no one will do your work for you) Feel empowered to nudge reviewers until they tell you not to. Find review buddies (this is really helpful to organise community work). Be proactive... and brave. Reduce your fear of breaking the ontology. Reduce your fear of getting a pull request rejected. Reduce other peoples fear of breaking the ontology by adding additional QC checks.","title":"Maximising impact as an open science engineer"},{"location":"howto/open-science-engineer/#how-to-be-an-open-science-engineer-maximising-impact-for-a-better-world","text":"Contributors : Nicole Vasilevsky Nicolas Matentzoglu Bradley Varner Status : This is a working document! Feel free to add more content! The Open Science Engineer contributes to the collection and standardisation of publicly available scientific knowledge through curation, community-building and data, ontology and software engineering. Open Science and all its sub-divisions, including Open Data and Open Ontologies, are central to tackling global challenges from rare disease to climate change. Open licenses are only part of the answer - the really tough part is the standardisation of data (including the unification of ontologies, the FAIRification of data and adoption of common semantic data models) and the organisation of a global, fully decentralised community of Open Science engineers. Here, we will discuss some basic principles on how we can maximise our impact as members of a global community combating the issues of our time: Principle of Collaboration : How do we create a welcoming and inclusive environment for implementing social workflows and deepen our ties across project boundaries? Principle of Upstream Fixing : How can we maximise benefits to the global community by pushing fixes as far upstream as possible? Principle of No-ownership : How do we develop a sense of co-, or -no, ownership for community driven ontologies and ontology tools? We discuss how to best utilise social workflows to achieve positive impact. We will try to convince you that building a close collaborative international community by spending time on submitting and answering issues on GitHub, helping on Stack Overflow and other online platforms, or just reaching out and donating small amounts of time to other open science efforts can make a huge difference.","title":"How to be an Open Science Engineer - maximising impact for a better world"},{"location":"howto/open-science-engineer/#table-of-contents","text":"TLDR - Summary Principle of Collaboration Principle of Upstream fixing Principle of No-ownership","title":"Table of contents"},{"location":"howto/open-science-engineer/#principle-of-collaboration","text":"The heart and soul of a successful Open Science culture is collaboration. The relative isolation into which many projects are forced due to limitations imposed by certain kinds of funding makes it even more important to develop effective social, collaborative workflows. This involve effective online communication, vocal appreciation (likes, upvotes, comments), documentation and open-ness.","title":"Principle of Collaboration"},{"location":"howto/open-science-engineer/#question-answering-and-documentation","text":"When you find an answer on Stack Overflow , GitHub issues (or even Hacker News ) that helps you, upvote it . It cannot be stressed enough how important this is to let good answers float up to the top (become more visible), and recognise the time and energy people spend formulating answers. If you do not have a Stack Overflow account, make one now . If you cannot find an answer to a question on GitHub or Stack Overflow, and you figure out a solution consider asking the question and answering it yourself! Consider this: if you provide an answer that will result in only 6 open science people spending one hour less solving a problem (a conservative estimate), you saved the taxpayer 6 hours of salary (not to speak of improving the quality of the solutions)! Answer questions on GitHub issues even when on trackers not in your purview. People often falsely assume that they should not try to answer queries in an open source project that they are not directly involved with. This is wrong! Open source developers highly (!) appreciate it when you chip in on answering queries, from dealing with errors and exceptions to \"How do I?\" kind of queries. Get involved on other peoples issue trackers ! Before asking questions on slack or issue trackers, always do a basic search first . Consider (1) the project's documentation, (2) open and closed issues on the issue tracker of the project your query is related to. If you do not find the information you need try and craft questions that are concise yet give sufficient context. To enable future users to find these answers, consider using the issue tracker instead of slack! Also it is important to remember that the people that answer your questions have to spend time they could have spend on other Open Science work! Make yourself responsible for continuously improving open science documentation . If a question was answered on slack or in an issue, consider a 10 minute detour to update the documentation of the project to reflect the solution if you think it could be helpful. This is the only way to scale Open Science projects: as the user base increases, providing support on a 1:1 basis will soon be infeasible . Good documentation is as important as good quality code, and everyone can help out, even if it is \"only\" about formatting, typos and adding additional links!","title":"Question answering and documentation"},{"location":"howto/open-science-engineer/#social-conduct-and-review","text":"Be overly generous with likes . Being a Open Science Engineer can be quite a lonely affair: hundreds of unanswered issues and questions, pull requests that remain un-reviewed for months, projects for which you never really get credit (want to be a QC engineer, anyone?). Even if you do not have time to respond to all issues you scroll through in a day, add a like if you find an answer useful . This goes for slack comments, Tweets and random Pull Requests you have nothing to do with as well. And, perhaps most importantly star all GitHub repos that are useful to you (you could, for example, scroll through https://github.com/topics/obofoundry and star all ontologies and ontology projects useful in your work)! This is a huge deal as it significantly motivates other OSEs and therefore provides fuel to the Open Science movement. Reduce work for others as much as possible by communicating clearly. Take the time to write clear responses with just the right amount of detail. The goal of your communication is to get the point across as swiftly as possible, and misunderstandings are a huge time killer. Again: Sloppy, quick responses can cause more work than not responding at all ! Use bullet lists to structure your response, and checklists for action items. Hide unnecessary details with <details> tag: <details><summary>[click arrow to expand]</summary> . See example here Be generous linking external issues to provide context. Example: Be positive and generous with gratitude and attribution If a member of the community opens an issue, always thank them for the issue right away. We need to encourage people to open issues. Don't let issues linger without any response. It is better to respond with a thank you and some instructions for the issuer on what to do to fix the issue themselves than to let it linger. We need our stakeholders to be more involved - first time issuers are especially vulnerable and may not bother to come back if they are ignored. When giving feedback, be positive, friendly and constructive. Show appreciation : thank users for issues and thank a PR reviewer for their review. Always. Attribute, attribute, attribute . If someone helped you sort something out emphasise this publicly!","title":"Social Conduct and review"},{"location":"howto/open-science-engineer/#openness","text":"Promote truly open communication: Contribute your thoughts openly so other people can benefit from it. Don't put issues on Slack where the public can't see it. Consider moving interesting discussions on Slack into a more public space, like GitHub discussions. Create public tickets which can be searched and referred to later.","title":"Openness"},{"location":"howto/open-science-engineer/#principle-of-upstream-fixing","text":"Maximising impact of your changes is by far the best way you can benefit society as an Open Science Engineer. Open Science projects are a web of mutually dependent efforts, for example: Ontologies re-use terms and axioms from other ontologies Software packages provide functionality that help building Knowledge Graphs, ensuring the quality of ontology releases and extract new insights from existing knowledge and scientific facts. Projects use ontologies for indexing data and making them discoverable. The key to maximising your impact is to push any fixes as far upstream as possible . Consider the following projects and the way they depend on each other (note that this is a gross simplification for illustration; in reality the number of dependencies is much higher): Let's think of the following (entirely fabricated) scenario based on the image above. Open Targets provides evidence for a gene association with some disease . This association is only supported by the IMPC data source which associates mouse and human phenotypes using the phenodigm algorithm which is based on semantic similarity. The semantic similarity scores are computed using cross-species axioms provided by the Monarch Initiative , especially uPheno . The cross-species logical axioms provided by Monarch depend on a range of lexical and logical approaches. This means the structure of uPheno is directly influenced by the naming and synonyms provided by species specific phenotype ontologies (SSPOs), such as the Human Phenotype Ontology (HPO) . It is, therefore, possible that: A faulty synonym is accidentally added to the HPO... ...which causes a faulty logical axiom candidate in uPheno (again missed by QC measures)... ...which causes a faulty similarity value in Monarch... ...which causes a wrong disease-gene association in IMPC... ...which ultimately leads to a faulty piece of evidence in Open Targets. Imagine a user of Open Targets that sees this evidence, and reports it to Open Targets as a bug. Open Targets could take the easy way out: remove the erroneous record from the database permanently. This means that the IMPC (itself with hundreds of dependent users and tools), Monarch (again with many dependents), uPheno and HPO (with probably thousands of dependents) would still carry forward that (tiny) mistake. This is the basic idea of maximising impact through Upstream Fixing : The higher up-stream (up the dependency graph) an error is fixed, the more cumulative benefit there is to a huge ecosystem of tools and services . An even better fix would be to have each fix to the ontology result in a new, shared quality control test. For example, some errors (duplicate labels, missing definition, etc) can be caught by automated testing. Here is a cool story.","title":"Principle of Upstream Fixing"},{"location":"howto/open-science-engineer/#case-study-external-contribution-and-upstream-fixing","text":"Over time, we have developed QC checks that ensure that the same exact synonym cannot be shared between two classes. However the checks where not perfect.. @vasvir (GitHub name), a member of the global community reached out to us on Uberon: https://github.com/obophenotype/uberon/issues/2424. https://github.com/obophenotype/uberon/pull/2640 Instead of fixing the discovered issue by ourselves, we invited @vasvir to fix the issues himself. We gave him some instructions on how to proceed, leading not only to eight new pull requests , but also an entirely new Quality Control check that augments the existing checks with case-insensitivity ( Gasserian ganglion and gasserian ganglion where previously considered distinct). Note: before the PRs, @vasvir did not speak any SPARQL . Members of our team helped @vasvir to see his first pull requests through by instructing them how to use the technology (robot, ODK, docker, SPARQL), and follow our pull request conventions. Instead of simply deleting the synonyms for his NLP projects, @vasvir instead decided to report the issues straight to the source. This way, hundreds, if not thousands of projects will directly or indirectly benefit from him!","title":"Case Study: External contribution and upstream fixing"},{"location":"howto/open-science-engineer/#other-examples-of-upstream-fixing","text":"Example 1 : While curating Mondo, Nicole identified issues relevant to Orphanet and created this issue . Example 2 : There is overlap between Mondo and Human Phenotype Ontology and the Mondo and HPO curators tag each other on relevant tickets. Example 3 : In Mondo, if new classifications are made, Mondo curators report this back to the source ontology to see if they would like to follow our classification.","title":"Other examples of upstream fixing"},{"location":"howto/open-science-engineer/#conclusions-upstream-fixing","text":"Have you ever wondered how much impact changing a synonym from exact to related could have? Or the addition of a precise mapping? The fixing of a typo in a label? It can be huge. And this does not only relate to ontologies, this goes for tool development as well. We tend to work around bugs when we are building software. Instead, or at least in addition to, we should always report the bug at the source to make sure it gets fixed eventually.","title":"Conclusions: Upstream Fixing"},{"location":"howto/open-science-engineer/#principle-of-no-ownership","text":"Many of the resources we develop are financed by grants. Grants are financed in the end by the taxpayer. While it is occasionally appropriate to protect open work with creative licenses, it rarely makes sense to restrict access to Open Ontologies work - neither to commercial nor research exploitation (we may want to insist on appropriate attribution to satisfy our grant developers). On the other side there is always the risk of well-funded commercial endeavours simply \"absorbing\" our work - and then tying stakeholders into their closed, commercial ecosystem. However, this is not our concern. We cannot really call it stealing if it is not really ours to begin with! Instead of trying to prevent unwanted commercialisation and closing, it is better to work with corporations in pre-competitive schemes such as Pistoia Alliance or Allotrope Foundation and lobby for more openness. (Also, grant authorities should probably not allow linking scientific data to less than totally open controlled vocabularies.) Here, we invite you to embrace the idea that ontologies and many of the tools we develop are actually community-driven , with no particular \"owners\" and \"decision makers\". While we are not yet there (we don't have sufficiently mature governance workflows for full fledged onto-communism), and most ontologies are still \"owned\" by an organisation that provides a major source of funding, we invite you to think of this as a preliminary state. It is better to embrace the idea of \"No-ownership\" and figure out social workflows and governance processes that can handle the problems of decision making.","title":"Principle of No-ownership"},{"location":"howto/open-science-engineer/#take-responsibility-for-your-community-ontologies","text":"Ensure that you see your issues and pull requests through to the end . No one will do this for you. Remember - contributors to open source projects, especially ontologies, have their own agendas, and do not automatically care about other peoples work. Feel empowered to nudge reviewers or experts to help. Get that issue answered and PR merged whatever it takes! Example : After waiting for the PR to be reviewed, Meghan kindly asked Nicole if she should find a different reviewer. 1. Find review buddies . For every ontology you seek to contribute to pair up with someone who will review your pull requests and you will review their pull requests. Sometimes, it is very difficult to get anyone to review your pull request. Reach out to people directly, and form an alliance for review. It is fun, and you learn new things (and get to know new people!). 1. Be proactive Problem of decentralization and lack of hierarchial organisation needs proactive and brave decision makers. No one will do your work for you. See your pull requests and issues through all the way to the release! Learn the tools necessary to make basic fixes - just try it/do it. Always have your index finger on the Edit button when reading documentation. There is always something to fix, including typos and content.","title":"Take responsibility for your community (ontologies)"},{"location":"howto/open-science-engineer/#reduce-your-fear-of-breaking-the-ontology","text":"Most of our ontologies have many checks in place, and GitHub has version control. Nothing has ever been broken to the point where it can't be fixed. (Remember to work on a branch!) Example: The QC checks on this PR failed 6 times before it passed. Perfect is the enemy of good enough. It's okay if your PR is rejected . Mentally, prepare yourself for having the PR rejected. This is fine - the community always looks to the best possible way to change the ontology. The next PR will be accepted! If you make a mistake, and it is pointed out during pull request review, consider adding an appropriate QC check to prevent the issue from happening again. And who knows - maybe you have an opportunity to fix past mistakes!","title":"Reduce your fear of \"breaking the ontology\"."},{"location":"howto/open-science-engineer/#tldr-summary","text":"Principle of Collaboration Upvote answers, on Stack Overflow , GitHub and any other open communication platform. Get involved on other peoples issue trackers. Always do a basic search before asking. Continuously improve Open Science documentation. Be overly generous with likes. Always strive to reduce work for other members of the community. Be positive and generous with gratitude and attribution. Promote open communication (less slack, more GitHub). Principle of Upstream fixing The key to maximising your impact is to push any fixes as far upstream as possible. When you experience a problem, always report it to the immediate source. If you can report it as high upstream as possible. In a perfect world, provide a fix in the form of a pull request. Principle of No-ownership See your issues and pull requests through to the end (dont drop the ball, no one will do your work for you) Feel empowered to nudge reviewers until they tell you not to. Find review buddies (this is really helpful to organise community work). Be proactive... and brave. Reduce your fear of breaking the ontology. Reduce your fear of getting a pull request rejected. Reduce other peoples fear of breaking the ontology by adding additional QC checks.","title":"TLDR - Summary"},{"location":"howto/prettify/","text":"Prettify markdown files \u00b6 Description \u00b6 Prettier standardizes the representation and formatting of Markdown. More information is available at https://prettier.io/ . Note, these instructions are for a Mac. Install npm \u00b6 If you do not have npm installed, this can be installed using homebrew (if you have homebrew installed). brew install node Install Prettier locally \u00b6 Run npm install --save-dev --save-exact prettier Prettify your files \u00b6 Create a new branch Navigate to your root directory containing obook Run npx prettier --write . Commit to your branch and create a pull request","title":"Prettify Markdown Files"},{"location":"howto/prettify/#prettify-markdown-files","text":"","title":"Prettify markdown files"},{"location":"howto/prettify/#description","text":"Prettier standardizes the representation and formatting of Markdown. More information is available at https://prettier.io/ . Note, these instructions are for a Mac.","title":"Description"},{"location":"howto/prettify/#install-npm","text":"If you do not have npm installed, this can be installed using homebrew (if you have homebrew installed). brew install node","title":"Install npm"},{"location":"howto/prettify/#install-prettier-locally","text":"Run npm install --save-dev --save-exact prettier","title":"Install Prettier locally"},{"location":"howto/prettify/#prettify-your-files","text":"Create a new branch Navigate to your root directory containing obook Run npx prettier --write . Commit to your branch and create a pull request","title":"Prettify your files"},{"location":"howto/protege-browse-search/","text":"Browsing and Searching \u00b6 Open the ontology in Prot\u00e9g\u00e9 \u00b6 Note: Windows users should open Protege using run.bat Note: For the purpose of this how-to, we will be using MONDO as the ontology Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web. The Prot\u00e9g\u00e9 UI \u00b6 The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information. Running the reasoner \u00b6 Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner: Entities tab \u00b6 You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query. Searching in Protege \u00b6 You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: 'congenital heart disease' 'Kindler syndrome' 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.","title":"Browsing and Searching"},{"location":"howto/protege-browse-search/#browsing-and-searching","text":"","title":"Browsing and Searching"},{"location":"howto/protege-browse-search/#open-the-ontology-in-protege","text":"Note: Windows users should open Protege using run.bat Note: For the purpose of this how-to, we will be using MONDO as the ontology Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web.","title":"Open the ontology in Prot\u00e9g\u00e9"},{"location":"howto/protege-browse-search/#the-protege-ui","text":"The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information.","title":"The Prot\u00e9g\u00e9 UI"},{"location":"howto/protege-browse-search/#running-the-reasoner","text":"Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner:","title":"Running the reasoner"},{"location":"howto/protege-browse-search/#entities-tab","text":"You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query.","title":"Entities tab"},{"location":"howto/protege-browse-search/#searching-in-protege","text":"You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: 'congenital heart disease' 'Kindler syndrome' 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.","title":"Searching in Protege"},{"location":"howto/screenshot/","text":"Create Screenshot and paste into an issue \u00b6 Description \u00b6 The instructions below describe how to capture a screenshot of your screen, either your entire screen or a partial screenshot. These can be pasted into GitHub issues, pull requests or any markdown file. Screenshot Instructions (Mac) \u00b6 Full screen : Hit the Command, Shift and 3 keys together to take a screenshot of the entire screen Partial screen : Hit the Shift, Control, Command and 4 keys together to take a screenshot of a portion of the screen. Crosshairs will appear and select the portion you would like to capture. Paste the partial screenshot into comment box in GitHub. Video explanation: Partial screenshot on a Mac \u00b6 Screenshot Instructions (PC) \u00b6 Use the Snipping Tool (available with Windows 10) \u00b6 The easiest way to elicit the tool is to press Windows logo key + Shift + S to open a dialog window to select the type of screen capture you would like to do. 1. Options exist for free form, rectangular, window or full screen snips. After snipping, a notification dialog pops up so the user can select the location to save or annotate the screenshot. Otherwise, the default setting is to save to the clipboard. Use your keyboard \u00b6 Different keyboards have different keys. One of the following options should work: Fn + Print Screen is an option on some keyboards PrtScn or PrtSc Hit Shift, Window, S keys together to take a screenshot. You will be able to resize the screenshot as needed. Reference \u00b6 Wikihow Take a Screenshot on a Mac Using snipping tools to capture screenshots is described here .","title":"Create Screenshot and paste into an issue"},{"location":"howto/screenshot/#create-screenshot-and-paste-into-an-issue","text":"","title":"Create Screenshot and paste into an issue"},{"location":"howto/screenshot/#description","text":"The instructions below describe how to capture a screenshot of your screen, either your entire screen or a partial screenshot. These can be pasted into GitHub issues, pull requests or any markdown file.","title":"Description"},{"location":"howto/screenshot/#screenshot-instructions-mac","text":"Full screen : Hit the Command, Shift and 3 keys together to take a screenshot of the entire screen Partial screen : Hit the Shift, Control, Command and 4 keys together to take a screenshot of a portion of the screen. Crosshairs will appear and select the portion you would like to capture. Paste the partial screenshot into comment box in GitHub.","title":"Screenshot Instructions (Mac)"},{"location":"howto/screenshot/#video-explanation-partial-screenshot-on-a-mac","text":"","title":"Video explanation: Partial screenshot on a Mac"},{"location":"howto/screenshot/#screenshot-instructions-pc","text":"","title":"Screenshot Instructions (PC)"},{"location":"howto/screenshot/#use-the-snipping-tool-available-with-windows-10","text":"The easiest way to elicit the tool is to press Windows logo key + Shift + S to open a dialog window to select the type of screen capture you would like to do. 1. Options exist for free form, rectangular, window or full screen snips. After snipping, a notification dialog pops up so the user can select the location to save or annotate the screenshot. Otherwise, the default setting is to save to the clipboard.","title":"Use the Snipping Tool (available with Windows 10)"},{"location":"howto/screenshot/#use-your-keyboard","text":"Different keyboards have different keys. One of the following options should work: Fn + Print Screen is an option on some keyboards PrtScn or PrtSc Hit Shift, Window, S keys together to take a screenshot. You will be able to resize the screenshot as needed.","title":"Use your keyboard"},{"location":"howto/screenshot/#reference","text":"Wikihow Take a Screenshot on a Mac Using snipping tools to capture screenshots is described here .","title":"Reference"},{"location":"howto/set-up-protege/","text":"Setup Protege 5.5 \u00b6 (This was adopted from the Gene Ontology editors guide and Mondo documentation ) Operating System \u00b6 These instructions are for Mac OS Protege version \u00b6 As of July 2022, OBO ontology editors are using Protege version 5.5.0 Download and install Protege \u00b6 Get Protege from protege.stanford.edu Unzip and move the Protege app to your Applications folder. See Install_Protege5_Mac for more instructions and troubleshooting common problems. Increase memory in Protege \u00b6 Protege needs at least 4G of RAM to cope with Mondo, ideally use 12G or 16G if your machine can handle it. Mac Instructions \u00b6 If running from Protege.app on a Mac, open the /Applications/Protege-5.5.0/Prot\u00e9g\u00e9.app/Contents/info.plist file Below the line: -Xss16M Insert another line: -Xmx12G PC Instructions \u00b6 adapted from Memory Management with Prot\u00e9g\u00e9 by Michael DeBellis Navigate to folder where Protege is installed (likely c:/programfiles/protege or open explorer and search protege.exe and find the location of file) Open Protege.l4j.ini. This file is what Protege looks at when it starts up to determine how much memory Java can allocate. If you edit that file in Notepad or some other lightweight text processor it should currently look like this: -Xms200M -Xmx500M -Xss16M Xmx is the one you want to edit (it is recommended that you make a copy of the original file first). That specifies the maximum amount of memory Protege can use. Xms is the initial memory that will be allocated and Xss defines the increments used when allocating more memory. Change -Xmx500M to Xmx12G Save and reopen Protege. Note on increasing memory \u00b6 If you have issues opening Protege, then reduce the memory, try 10G (or lower) instead. Add ELK reasoner \u00b6 See instructions here . Instructions for new Protege users \u00b6 Setting your ID range \u00b6 See instructions here . User details \u00b6 User name Click Use supplied user name: add your name (ie nicolevasilevsky) Check Use Git user name when available Add ORCID . Add the ID number only, do not include https://, ie 0000-0001-5208-3432 Setting username and auto-adding creation date \u00b6 In the Protege menu, go to Preferences > New Entities Metadata tab Check Annotate new entities with creator (user) box Creator property Add http://www.geneontology.org/formats/oboInOwl#created_by Creator value Select Use ORCID Date property http://purl.org/dc/elements/1.1/date Date value format Select ISO-8601 Install Protege OBO plugin \u00b6 This plugin enables some extra functionality, such as the option to obsolete entities from the menu. To install it: Go to File > Check for plugins... . Click on OBO Annotations Editor and click on Install . Restart Protege for the plugin to be active. You should now have the option to obsolete entities in Edit > Make entity obsolete . You can see a list of all installed plugins in Preferences > Plugins .","title":"Setup Protege 5.5"},{"location":"howto/set-up-protege/#setup-protege-55","text":"(This was adopted from the Gene Ontology editors guide and Mondo documentation )","title":"Setup Protege 5.5"},{"location":"howto/set-up-protege/#operating-system","text":"These instructions are for Mac OS","title":"Operating System"},{"location":"howto/set-up-protege/#protege-version","text":"As of July 2022, OBO ontology editors are using Protege version 5.5.0","title":"Protege version"},{"location":"howto/set-up-protege/#download-and-install-protege","text":"Get Protege from protege.stanford.edu Unzip and move the Protege app to your Applications folder. See Install_Protege5_Mac for more instructions and troubleshooting common problems.","title":"Download and install Protege"},{"location":"howto/set-up-protege/#increase-memory-in-protege","text":"Protege needs at least 4G of RAM to cope with Mondo, ideally use 12G or 16G if your machine can handle it.","title":"Increase memory in Protege"},{"location":"howto/set-up-protege/#mac-instructions","text":"If running from Protege.app on a Mac, open the /Applications/Protege-5.5.0/Prot\u00e9g\u00e9.app/Contents/info.plist file Below the line: -Xss16M Insert another line: -Xmx12G","title":"Mac Instructions"},{"location":"howto/set-up-protege/#pc-instructions","text":"adapted from Memory Management with Prot\u00e9g\u00e9 by Michael DeBellis Navigate to folder where Protege is installed (likely c:/programfiles/protege or open explorer and search protege.exe and find the location of file) Open Protege.l4j.ini. This file is what Protege looks at when it starts up to determine how much memory Java can allocate. If you edit that file in Notepad or some other lightweight text processor it should currently look like this: -Xms200M -Xmx500M -Xss16M Xmx is the one you want to edit (it is recommended that you make a copy of the original file first). That specifies the maximum amount of memory Protege can use. Xms is the initial memory that will be allocated and Xss defines the increments used when allocating more memory. Change -Xmx500M to Xmx12G Save and reopen Protege.","title":"PC Instructions"},{"location":"howto/set-up-protege/#note-on-increasing-memory","text":"If you have issues opening Protege, then reduce the memory, try 10G (or lower) instead.","title":"Note on increasing memory"},{"location":"howto/set-up-protege/#add-elk-reasoner","text":"See instructions here .","title":"Add ELK reasoner"},{"location":"howto/set-up-protege/#instructions-for-new-protege-users","text":"","title":"Instructions for new Protege users"},{"location":"howto/set-up-protege/#setting-your-id-range","text":"See instructions here .","title":"Setting your ID range"},{"location":"howto/set-up-protege/#user-details","text":"User name Click Use supplied user name: add your name (ie nicolevasilevsky) Check Use Git user name when available Add ORCID . Add the ID number only, do not include https://, ie 0000-0001-5208-3432","title":"User details"},{"location":"howto/set-up-protege/#setting-username-and-auto-adding-creation-date","text":"In the Protege menu, go to Preferences > New Entities Metadata tab Check Annotate new entities with creator (user) box Creator property Add http://www.geneontology.org/formats/oboInOwl#created_by Creator value Select Use ORCID Date property http://purl.org/dc/elements/1.1/date Date value format Select ISO-8601","title":"Setting username and auto-adding creation date"},{"location":"howto/set-up-protege/#install-protege-obo-plugin","text":"This plugin enables some extra functionality, such as the option to obsolete entities from the menu. To install it: Go to File > Check for plugins... . Click on OBO Annotations Editor and click on Install . Restart Protege for the plugin to be active. You should now have the option to obsolete entities in Edit > Make entity obsolete . You can see a list of all installed plugins in Preferences > Plugins .","title":"Install Protege OBO plugin"},{"location":"howto/setup-docker/","text":"Set up docker \u00b6 Follow the instructions here (note these instructions are for a Mac). Once installed, you should be able to open your command line and download the ODK. Open Terminal in the command line type, type docker pull obolibrary/odkfull . This will download the ODK (will take a few minutes, depending on you internet connection). Setting the memory: Set memory ~60% of your system memory, for example, if you have 16GB of RAM, then you should assign 10-11.","title":"Set-up docker"},{"location":"howto/setup-docker/#set-up-docker","text":"Follow the instructions here (note these instructions are for a Mac). Once installed, you should be able to open your command line and download the ODK. Open Terminal in the command line type, type docker pull obolibrary/odkfull . This will download the ODK (will take a few minutes, depending on you internet connection). Setting the memory: Set memory ~60% of your system memory, for example, if you have 16GB of RAM, then you should assign 10-11.","title":"Set up docker"},{"location":"howto/setup-ontology-development-odk/","text":"Getting set up to manage ontology pipelines with the ODK \u00b6 Set up docker and install the ODK ( howto ) To warm up with ODK development, follow the ODK Tutorial here","title":"Get set up for ODK-based ontology development"},{"location":"howto/setup-ontology-development-odk/#getting-set-up-to-manage-ontology-pipelines-with-the-odk","text":"Set up docker and install the ODK ( howto ) To warm up with ODK development, follow the ODK Tutorial here","title":"Getting set up to manage ontology pipelines with the ODK"},{"location":"howto/switching-ontologies/","text":"Switching Ontologies in Protege \u00b6 By: Nicole Vasilevsky Description \u00b6 When you edit an ontology, you need to make sure you are using the correct prefix and your assigned ID range for that on ontology. Protege (unfortunately) does not remember the last prefix or ID range that you used when you switch between ontologies. Therefore we need to manually update this each time we switch ontologies. Instructions \u00b6 When you switch to a new ontology file, open your preferences in Protege (File -> Preferences). Be sure you are on the New entities tab. Add the Prefix for the ontology you are working on. If you don't know your ID range, go to the ID ranges file for that ontology (it should be in src/ontology/[ontology-name]-idranges.owl . (For example, src/ontology/mondo-idranges.owl.) Copy and paste in the start and end values for your ID range. Tips \u00b6 I work on many ontologies, so I keep a note in OneNote (or Evernote) that keeps track of all my ID ranges for quick reference. You don't need to track the last ID that was used, Protege will know to pick the next ID in your range. For example, if your ID range is 8000000 to 8999999, you can enter that as your range, even if you have already added 10 terms within your range. Protege will know to assign the next ID as 8000011. Video Explanation \u00b6","title":"Switching Ontologies in Protege"},{"location":"howto/switching-ontologies/#switching-ontologies-in-protege","text":"By: Nicole Vasilevsky","title":"Switching Ontologies in Protege"},{"location":"howto/switching-ontologies/#description","text":"When you edit an ontology, you need to make sure you are using the correct prefix and your assigned ID range for that on ontology. Protege (unfortunately) does not remember the last prefix or ID range that you used when you switch between ontologies. Therefore we need to manually update this each time we switch ontologies.","title":"Description"},{"location":"howto/switching-ontologies/#instructions","text":"When you switch to a new ontology file, open your preferences in Protege (File -> Preferences). Be sure you are on the New entities tab. Add the Prefix for the ontology you are working on. If you don't know your ID range, go to the ID ranges file for that ontology (it should be in src/ontology/[ontology-name]-idranges.owl . (For example, src/ontology/mondo-idranges.owl.) Copy and paste in the start and end values for your ID range.","title":"Instructions"},{"location":"howto/switching-ontologies/#tips","text":"I work on many ontologies, so I keep a note in OneNote (or Evernote) that keeps track of all my ID ranges for quick reference. You don't need to track the last ID that was used, Protege will know to pick the next ID in your range. For example, if your ID range is 8000000 to 8999999, you can enter that as your range, even if you have already added 10 terms within your range. Protege will know to assign the next ID as 8000011.","title":"Tips"},{"location":"howto/switching-ontologies/#video-explanation","text":"","title":"Video Explanation"},{"location":"howto/term-request/","text":"Make term requests to existing ontologies \u00b6 Prerequisites \u00b6 You need to have a GitHub account to make term requests. Sign up for a free GitHub account. Background \u00b6 Recommended reading \u00b6 This guide on How to select and request terms from ontologies by Chris Mungall provides some helpful background and tips for making term requests. Why make a new term request? \u00b6 Onologies are under constant development and are continuously expanded and iterated upon. You may discover that a term you need is not available in your preferred ontology. In this case, please make a new term request to the ontology. Making term requests to existing ontologies \u00b6 In the following text below, we describe best practices for making a term request to an ontology. In general, requests for new terms are make on the ontology GitHub issue tracker. For example, this is the GitHub issue tracker for the Uberon Anatomy onology . Note : These are suggestions and not strict rules. We appreciate your contributions to extending and improving ontologies. Following best guidelines is appreciated by the curators and developers, and assists them in addressing your issue more quickly. However, we understand if you are not always able to follow these best practices. Please add as much information as possible, and if there are any questions, the ontology developer may follow up with you for further clarification. Making a new term request \u00b6 Go to the ontology issue tracker in GitHub Select New issue Pick appropriate template (if applicable) If there is a template, fill in the information that is requested on the template below each header. General information that should be included in a new term request: Preferred label : Your preferred name or label for the new term. Note- new term request should not match existing terms or synonyms. Parent : The parent or superclass for that term. Remember that ontologies use subsumption reasoning, meaning that a subclass/child will inherit all the properties of the parent. In most ontologies, terms can have multiple classification, means terms can be classified under more than one parent. Note : You can use a ontology search enginge like OLS to double check your class does not already exist and to search for parent terms in your respective ontology. Definition : Please write a concise definition of your term (see this guide on writing good ontology definitions ). Definition database cross-reference(s) : Indicate the source or database cross-reference(s) or source for the definition, such as a PubMed ID (PMID) or reference to a website. Synonym(s) : an alternative term that has the same or closely related meaning for your new term. Please indicate the synonym scope (see more details below). Synonym database cross-reference(s) : Provide a database cross-reference or source for the synonym, if applicable. Your ORCID or the URL for your working group, if applicable. If you do not have an ORCID, you can sign up for one for free here . Note : You can link your ORCID in your GitHub profile. Comments : You can add any additional comments at the end. Please indicate if the comment should be included as a 'comment' annotation on the ontology term. Click Submit New Issue An ontology curator will review your issue and follow up with you if more information is needed. Synonym scopes: \u00b6 Exact - an exact match Narrow - more specific term Broad - more general term Related - a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct Formatting: \u00b6 For most ontologies, the preferred term labels should be lowercase (unless it is a proper name or abbreviation) Write the request below the prompts on the template so the Markdown formatting displays properly Synonyms should be lowercase (with exceptions above) Definition source - if from PubMed, please use the format PMID:XXXXXX (no space) Include the ID and label for the parent term Submitting other issues \u00b6 Users may want to request other types of changes to an ontology such as Mondo beyond just adding a new term. Other types of requests may include changes to the classification, typos, bugs, etc. Some ontologies have other templates available on their issue tracker. Select the appropriate template. If there is not an appropriate template available, scroll to the bottom and select 'open a blank issue'. Contributors \u00b6 Nicole Vasilevsky","title":"Make term requests to existing ontologies"},{"location":"howto/term-request/#make-term-requests-to-existing-ontologies","text":"","title":"Make term requests to existing ontologies"},{"location":"howto/term-request/#prerequisites","text":"You need to have a GitHub account to make term requests. Sign up for a free GitHub account.","title":"Prerequisites"},{"location":"howto/term-request/#background","text":"","title":"Background"},{"location":"howto/term-request/#recommended-reading","text":"This guide on How to select and request terms from ontologies by Chris Mungall provides some helpful background and tips for making term requests.","title":"Recommended reading"},{"location":"howto/term-request/#why-make-a-new-term-request","text":"Onologies are under constant development and are continuously expanded and iterated upon. You may discover that a term you need is not available in your preferred ontology. In this case, please make a new term request to the ontology.","title":"Why make a new term request?"},{"location":"howto/term-request/#making-term-requests-to-existing-ontologies","text":"In the following text below, we describe best practices for making a term request to an ontology. In general, requests for new terms are make on the ontology GitHub issue tracker. For example, this is the GitHub issue tracker for the Uberon Anatomy onology . Note : These are suggestions and not strict rules. We appreciate your contributions to extending and improving ontologies. Following best guidelines is appreciated by the curators and developers, and assists them in addressing your issue more quickly. However, we understand if you are not always able to follow these best practices. Please add as much information as possible, and if there are any questions, the ontology developer may follow up with you for further clarification.","title":"Making term requests to existing ontologies"},{"location":"howto/term-request/#making-a-new-term-request","text":"Go to the ontology issue tracker in GitHub Select New issue Pick appropriate template (if applicable) If there is a template, fill in the information that is requested on the template below each header. General information that should be included in a new term request: Preferred label : Your preferred name or label for the new term. Note- new term request should not match existing terms or synonyms. Parent : The parent or superclass for that term. Remember that ontologies use subsumption reasoning, meaning that a subclass/child will inherit all the properties of the parent. In most ontologies, terms can have multiple classification, means terms can be classified under more than one parent. Note : You can use a ontology search enginge like OLS to double check your class does not already exist and to search for parent terms in your respective ontology. Definition : Please write a concise definition of your term (see this guide on writing good ontology definitions ). Definition database cross-reference(s) : Indicate the source or database cross-reference(s) or source for the definition, such as a PubMed ID (PMID) or reference to a website. Synonym(s) : an alternative term that has the same or closely related meaning for your new term. Please indicate the synonym scope (see more details below). Synonym database cross-reference(s) : Provide a database cross-reference or source for the synonym, if applicable. Your ORCID or the URL for your working group, if applicable. If you do not have an ORCID, you can sign up for one for free here . Note : You can link your ORCID in your GitHub profile. Comments : You can add any additional comments at the end. Please indicate if the comment should be included as a 'comment' annotation on the ontology term. Click Submit New Issue An ontology curator will review your issue and follow up with you if more information is needed.","title":"Making a new term request"},{"location":"howto/term-request/#synonym-scopes","text":"Exact - an exact match Narrow - more specific term Broad - more general term Related - a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct","title":"Synonym scopes:"},{"location":"howto/term-request/#formatting","text":"For most ontologies, the preferred term labels should be lowercase (unless it is a proper name or abbreviation) Write the request below the prompts on the template so the Markdown formatting displays properly Synonyms should be lowercase (with exceptions above) Definition source - if from PubMed, please use the format PMID:XXXXXX (no space) Include the ID and label for the parent term","title":"Formatting:"},{"location":"howto/term-request/#submitting-other-issues","text":"Users may want to request other types of changes to an ontology such as Mondo beyond just adding a new term. Other types of requests may include changes to the classification, typos, bugs, etc. Some ontologies have other templates available on their issue tracker. Select the appropriate template. If there is not an appropriate template available, scroll to the bottom and select 'open a blank issue'.","title":"Submitting other issues"},{"location":"howto/term-request/#contributors","text":"Nicole Vasilevsky","title":"Contributors"},{"location":"howto/update-import/","text":"Update Imports Workflow \u00b6 This page discusses how to update the contents of your imports using the ODK, like adding or removing terms. Note: This is a specialised how-to for ODK managed ontologies and is replicated from ODK docs to consolidate workflows in the obook. Not all ontologies use ODKs and many ontologies have their own workflows for imports, please also check with your local ontology documents and/or developers. Note: The extract function in ROBOT can also be used to extract subsets from onotlogies for modular imports without the use of the ODK. For details on that, please refer to the ROBOT documentation Importing a new term \u00b6 Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically Declaring terms to be imported \u00b6 There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Protege-based declaration Using term files Using the custom import template Protege-based declaration \u00b6 This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Protege (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc) for this term are imported from the respective external source ontology and becomes visible in your ontology. Using term files \u00b6 Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported. Using the custom import template \u00b6 This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/cl-odk.yaml ), and update the repository (using sh run.sh make update_repo ): use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say current import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extent the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file (see format variant documentation for explanation on what base file is) Refresh imports \u00b6 If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B Using the Base Module approach \u00b6 Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex then the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we First obtain the base (ususally by simply downloading it, but there is also an option now to create it with ROBOT) We merge all base files into one big pile Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/cl-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and your one of the local devs should pick this up and run the import for you. Lastly, restart Protege, and the term should be imported in ready to be used.","title":"Updating Imports with ODK"},{"location":"howto/update-import/#update-imports-workflow","text":"This page discusses how to update the contents of your imports using the ODK, like adding or removing terms. Note: This is a specialised how-to for ODK managed ontologies and is replicated from ODK docs to consolidate workflows in the obook. Not all ontologies use ODKs and many ontologies have their own workflows for imports, please also check with your local ontology documents and/or developers. Note: The extract function in ROBOT can also be used to extract subsets from onotlogies for modular imports without the use of the ODK. For details on that, please refer to the ROBOT documentation","title":"Update Imports Workflow"},{"location":"howto/update-import/#importing-a-new-term","text":"Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically","title":"Importing a new term"},{"location":"howto/update-import/#declaring-terms-to-be-imported","text":"There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Protege-based declaration Using term files Using the custom import template","title":"Declaring terms to be imported"},{"location":"howto/update-import/#protege-based-declaration","text":"This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Protege (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc) for this term are imported from the respective external source ontology and becomes visible in your ontology.","title":"Protege-based declaration"},{"location":"howto/update-import/#using-term-files","text":"Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported.","title":"Using term files"},{"location":"howto/update-import/#using-the-custom-import-template","text":"This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/cl-odk.yaml ), and update the repository (using sh run.sh make update_repo ): use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say current import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extent the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file (see format variant documentation for explanation on what base file is)","title":"Using the custom import template"},{"location":"howto/update-import/#refresh-imports","text":"If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B","title":"Refresh imports"},{"location":"howto/update-import/#using-the-base-module-approach","text":"Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex then the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we First obtain the base (ususally by simply downloading it, but there is also an option now to create it with ROBOT) We merge all base files into one big pile Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/cl-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and your one of the local devs should pick this up and run the import for you. Lastly, restart Protege, and the term should be imported in ready to be used.","title":"Using the Base Module approach"},{"location":"images/","text":"About using images in Git/GitHub \u00b6 There are two places you'll probaby want to use images in GitHub, in issue tracker and in markdown files, html etc. The way you handle images in these contexts is quite different, but easy once you get the hang of it. In markdown files (and html etc) \u00b6 All images referenced in static files such as html and markdown need to be referenced using a URL; dragging and dropping is not supported and could actually cause problems. Keeping images in a single directory enables them to be referenced more readily. Sensible file names are highly recommended, preferably without spaces as these are hard to read when encoded. An identical file, named in two different ways is shown as an example below. They render in the same way, but the source \"code\" looks ugly when spaces are used in file names. Eg. encoding needed no encoding needed ![](github%20organizations%20teams%20repos.png ![](github-organizations-teams-repos.png) In this example, the filename is enough of a 'url' because this file (https://ohsu-library.github.io/github-tutorial/howto/images/index.md) and the images are in the same directory https://ohsu-library.github.io/github-tutorial/howto/images/. To reference/embed an image that is not in the same directory, a more careful approach is needed. Referencing images in your repository and elsewhere \u00b6 Absolute path referencing Relative path referencing ![](https://github.com/OHSU-Library/github-tutorial/raw/master/docs/other-images/owl.jpg) ![](other-images/owl.jpg) Each instance of ../ means 'go up one level' in the file tree. It is also possible to reference an image using an external URL outside your control, in another github organization, or anywhere on the web, however this method can be fragile if the URL changes or could lead to unintended changes. Therefore make your own copies and reference those unless: You're sure that referencing the originals will not end in broken links or surprising content. Copying the image is prohibited The images are too large to make copying worth the hassle/expense. For example, it is not clear for how long the image below will manage to persist at this EPA link, or sadly, for how long the image will even be an accurate reflection of the current situation in the arctic. https://www.epa.gov/sites/production/files/styles/microsite_banner/public/2016-12/epa-banner-images/science_banner_arctic.png In GitHub issue tracker \u00b6 Images that are embedded into issues can be dragged and dropped in the GitHub issues interface. Once you've done so, it will look something like this with GitHub assigning an arbitrary URL (githubuserassets) for the image. ![](screenshot-of-images-in-issues.png) Sizing images \u00b6 Ideally, a Markdown document is renderable in a variety of output formats and devices. In some cases, it may be desirable to create non-portable Markdown that uses HTML syntax to position images. This limits the longevity of the artifact, but may be necessary sometimes. We describe how to manage this below. In order to size images, use the native html syntax: width = with the <img src=, as per below. <img src=\"https://github.com/monarch-initiative/monarch-app/raw/master/image/Phenogrid3Compare.png\" width=\"53\"> Back to Home \u00b6","title":"How to add images"},{"location":"images/#about-using-images-in-gitgithub","text":"There are two places you'll probaby want to use images in GitHub, in issue tracker and in markdown files, html etc. The way you handle images in these contexts is quite different, but easy once you get the hang of it.","title":"About using images in Git/GitHub"},{"location":"images/#in-markdown-files-and-html-etc","text":"All images referenced in static files such as html and markdown need to be referenced using a URL; dragging and dropping is not supported and could actually cause problems. Keeping images in a single directory enables them to be referenced more readily. Sensible file names are highly recommended, preferably without spaces as these are hard to read when encoded. An identical file, named in two different ways is shown as an example below. They render in the same way, but the source \"code\" looks ugly when spaces are used in file names. Eg. encoding needed no encoding needed ![](github%20organizations%20teams%20repos.png ![](github-organizations-teams-repos.png) In this example, the filename is enough of a 'url' because this file (https://ohsu-library.github.io/github-tutorial/howto/images/index.md) and the images are in the same directory https://ohsu-library.github.io/github-tutorial/howto/images/. To reference/embed an image that is not in the same directory, a more careful approach is needed.","title":"In markdown files (and html etc)"},{"location":"images/#referencing-images-in-your-repository-and-elsewhere","text":"Absolute path referencing Relative path referencing ![](https://github.com/OHSU-Library/github-tutorial/raw/master/docs/other-images/owl.jpg) ![](other-images/owl.jpg) Each instance of ../ means 'go up one level' in the file tree. It is also possible to reference an image using an external URL outside your control, in another github organization, or anywhere on the web, however this method can be fragile if the URL changes or could lead to unintended changes. Therefore make your own copies and reference those unless: You're sure that referencing the originals will not end in broken links or surprising content. Copying the image is prohibited The images are too large to make copying worth the hassle/expense. For example, it is not clear for how long the image below will manage to persist at this EPA link, or sadly, for how long the image will even be an accurate reflection of the current situation in the arctic. https://www.epa.gov/sites/production/files/styles/microsite_banner/public/2016-12/epa-banner-images/science_banner_arctic.png","title":"Referencing images in your repository and elsewhere"},{"location":"images/#in-github-issue-tracker","text":"Images that are embedded into issues can be dragged and dropped in the GitHub issues interface. Once you've done so, it will look something like this with GitHub assigning an arbitrary URL (githubuserassets) for the image. ![](screenshot-of-images-in-issues.png)","title":"In GitHub issue tracker"},{"location":"images/#sizing-images","text":"Ideally, a Markdown document is renderable in a variety of output formats and devices. In some cases, it may be desirable to create non-portable Markdown that uses HTML syntax to position images. This limits the longevity of the artifact, but may be necessary sometimes. We describe how to manage this below. In order to size images, use the native html syntax: width = with the <img src=, as per below. <img src=\"https://github.com/monarch-initiative/monarch-app/raw/master/image/Phenogrid3Compare.png\" width=\"53\">","title":"Sizing images"},{"location":"images/#back-to-home","text":"","title":"Back to Home"},{"location":"lesson/analysing-linked-data/","text":"Analysing Linked Data (Fundamentals) \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Ontology Theory Preparation \u00b6 Essential Linked Data Engineering : Week 1 Support Programming Historian Linked Data tutorial Original Whitepaper (Tim Berners Lee et al) Educational curriculum for Linked Data Tools: Browse through the tools and standards listed in the Semantic Engineer Toolbox . Learning objectives \u00b6 Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition Tutorials \u00b6 The Linked Data landscape from an OBO perspective: Standards, Services and Tools \u00b6 In the following we will look a bit at the general Linked Data landscape, and name some of its flagship projects and standards. It is important to be clear that the Semantic Web field is a very heterogenous one: Flagship projects of the wider Semantic Web community \u00b6 Linked Open Data (LOD) cloud : The flagship project of the Semantic Web. An attempt to make all, or anyways a lot, of Linked Data accessible in one giant knowledge graph. A good overview can be found in this medium article. Note that some people seem to think that the Semantic Web is (or should be) the Linked Open Data cloud. I would question this view, but I am not yet decided what my position is. Schema.org : General purpose vocabulary for entities on the web, founded by Google, Microsoft, Yahoo and Yandex. To get a better sense of the types of entities and relationships covered see here . DBpedia : Project that extracts structured data from Wikipedia and makes it available as a giant knowledge graph. The associated ontology , similar to schema.org, covers entities encountered in common sense knowledge. Wikidata : Free and open knowledge base that can be edited in much the same way as Wikipedia is edited. While these Semantic Web flagship projects are doubtlessly useful, it is sometimes hard to see how they can help for your biomedical research. We rarely make use of them in our day to day work as ontologists, but there are some notable exceptions: Where our work involves modelling environmental factors, we sometimes use wikidata as a standard way to refer for example to countries. For some more common sense knowledge use cases, such as nutrition, consider augmenting your knowledge graph with data from wikidata or dbpedia. While they may be a bit more messy and not directly useful for exploration by humans, it is quite possible that Machine Learning approaches can use the additional context provided by these knowledge graphs to improve embeddings and deliver more meaningful link predictions. Some OBO ontologies are already on Wikidata - perhaps you can find additional synonyms and labels which help with your data mapping problems! Where the OBO and Semantic Web communities are slightly at odds \u00b6 The OBO format is a very popular syntax for representing biomedical ontologies. A lot of tools have been built over the years to hack OBO ontologies on the basis of that format - I still work with it on a daily basis. Although it has semantically been proven to be a subset of OWL (i.e. there is a lossless mapping of OBO into OWL) and can be viewed as just another syntax, it is in many ways idiosyncratic. For starters, you wont find many, if any, IRIs in OBO ontologies. The format itself uses CURIEs which are mapped to the general OBO PURL namespace during transformation to OWL. For example, if you see MONDO:0003847 in an OBO file, and were to translate it to OWL, you will see this term being translated to http://purl.obolibrary.org/obo/MONDO_0003847. Secondly, you have a bunch of built-in properties like BROAD or ABBREVIATION that mapped to a vocabulary called oboInOwl (oio). These are pretty non-standard on the general Semantic Web, and often have to be manually mapped to the more popular counterparts in the Dublin Core or SKOS namespaces. Having URIs as identifiers is not generally popular in the life sciences. As discussed elsewhere, it is much more likely to encounter CURIEs such as MONDO:0003847 than URIs such as http://purl.obolibrary.org/obo/MONDO_0003847 in biomedical databases. Useful tools for biomedical research \u00b6 Why does the biomedical research, and clinical, community care about the Semantic Web and Linked Data? There are endless lists of applications that try to apply semantic technologies to biomedical problems, but for this week, we only want to look at the broader picture. In our experience, the use cases where Semantic Web standards are applied successfully are: Where to find ontologies: Ontology repositories OBO Foundry Ontology Library BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Where to find terms: Term browsers OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. A key difference between Ontobee and OLS/Bioportal is that Ontobee limits hierarchical relationships to is_a. This means if you are browsing ontologies such as GO, Uberon, CL, ENVO, you will not see part-of links in the hierarchy, and these links are crucial for understanding these ontologies. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. Curate biomedical data. There are a lot of different tools in this space - which we will discuss in a bespoke unit later in the course. Examples: isatools : The open source ISA framework and tools help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies. RightField : System for curating ontology terms in Excel spreadsheets. CEDAR Templates : Basically a templating system that allows to create templates to record metadata, for example in a lab setting, of course with ontology integration. Other examples of tabular data to RDF converters, but new ones coming up every year. Building ontologies Populous/Webulous : A system to maintain/generate ontologies from spreadsheets. The idea was to basically to define patterns in a (now mostly dead) language called OPPL, and then apply them to spreadsheets to generate OWL axioms. EBI recently discontinued the service, as there is a general exodus to Google Sheets + ROBOT templates instead. ROBOT templates + Google Sheets and Cogs : A lightweight approach based on a set of tools that allows curating ontologies in spreadsheets (e.g. Google Sheets) which are converted into OWL using ROBOT. DOSDP tools + Dead Simple Design Patterns (DOSDP) : Similar to ROBOT templates, DOSDPs (which really should be called DOSDTs, because they are not really design patterns ; they are ontology templates), another system that allows the generation of OWL axioms based on spreadsheet data. Cleaning messy data OpenRefine : I have not myself used this ever, but some of my colleagues have. OpenRefine allows you to upload (spreadsheet) data, explore it and clean it (going as far as reconciling terms using Wikidata concepts). Which biomedical ontologies should we use? \u00b6 As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \" 10 simple rules for selecting a Bio-ontology \" by Malone et al, but I would further recommend to ask yourself the following: Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation). Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently ploy-hierarchical (such as Mondo ), while others strive to be single inheritance ( DO , ICD). Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data. Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being payed every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI ) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS !) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them. We will discuss ontologies in the medical, phenomics and genomics space in more detail in a later session of the course. Other interesting links \u00b6 Linked Data in e-Government Industrial Ontologies Foundry : Something like the OBO Foundry for Industrial Ontologies OntoCommons : An H2020 CSA project dedicated to the standardisation of data documentation across all domains related to materials and manufacturing. Basic Linked data and Semantic Web Concepts for the Semantic Data Engineer in the Biomedical Domain \u00b6 In this section we will discuss the following: Introductory remarks The advantages of globally unique identifiers Some success stories of the Semantic Web in the biomedical domain Some basic concepts you should probably have heard about The ecosystem of the Semantic Web: Standards, Technologies and Research Areas Typical tasks of Semantic Data Engineers in the biomedical domain Introduction \u00b6 Note of caution : No two Semantic Web overviews will be equivalent to each other. Some people claim the Semantic Web as an idea is an utter failure, while others praise it as a great success (in the making) - in the end you will have to make up your own mind. In this section I focus on parts of the Semantic Web step particularly valuable to the biomedical domain, and I will omit many relevant topics in the wider Semantic Web area, such as Enterprise Knowledge Graphs, decentralisation and personalisation, and many more. Also, the reader is expected to be familiar with the basic notions of the Semantic Web, and should use this overview mainly to tie some of the ideas together. The goal of this section is to give the aspiring Semantic Data Engineer in the biomedical domain a rough idea of key concepts around Linked Data and the Semantic Web insofar as they relate to their data science and and data engineering problems. Even after 20 years of Semantic Web research (the seminal paper , conveniently and somewhat ironically behind a paywall, was published in May 2001), the area is still dominated by \"academic types\", although the advent of the Knowledge Graph is already changing that. As I already mentioned above, no two stories of what the Semantic Web is will sound the same. However, there are a few stories that are often told to illustrate why we need semantics. The OpenHPI course names a few: \"From the web of documents to the web of data\" tells the story of how the original web is essentially a huge heap of (interlinked) natural language text documents which are very hard to understand for search engines: Does the word \"Jaguar\" on this site refer to the car or the cat? Clarifying in your web page that the word Jaguar refers to the concept of \"Jaguar the cat\", for example like this: <span about=\"dbpedia:Jaguar\">Jaguar</span> , will make it easier for the search engine to understand what your site is about and link it to other relevant content. From this kind of mark-up, structured data can be extracted and integrate into a giant, worldwide database, and exposed through SPARQL endpoints, that can then be queried using a suitable query language. \"From human to machine understandable\": as a Human, we know that a Jaguar is a kind of cat, and all cats have four legs. If you ask a normal search engine: \"Does a Jaguar have four legs?\" it will have a tough time to answer this question correctly (if it cannot find that exact statement anywhere). That is why we need proper semantics , some kind of formalism such that a \"machine\" can deduce from the statements \"Jaguar is a cat; Cat has four legs\" that \"Jaguar has four legs\". The \"Semantic Layer Cake\": a box/brick diagram showing how Semantic Web Technologies are stacked on top of each other. An engineering centric view that has been used countless times to introduce the Semantic Web, but rarely helped anyone to understand what it is about. I am not entirely sure anymore that any of these ways (web of data, machine understanding, layered stack of matching standards) to motivate the Semantic Web are particularly effective for the average data scientists or engineer. If I had to explain the Semantic Web stack to my junior self, just having finished my undergraduate, I would explain it as follows (no guarantee though it will help you). The Semantic Web / Linked Data stack comprises roughly four components that are useful for the aspiring Semantic (Biomedical) Data Engineer/Scientist to distinguish: A way to refer to things (including entities and relations) in a global namespace. \u00b6 You, as a scientist, might be using the term \"gene\" to refer to basic physical and functional unit of heredity, but me, as a German, prefer the term \"Gen\". In the Semantic Web, instead of natural language words, we prefer to use URIs to refer to things such as https://www.wikidata.org/wiki/Q7187: if you say something using the name https://www.wikidata.org/wiki/Q7187, both your German and Japanese colleagues will \"understand\" what you are referring to. More about that in the next chapter. Lots (loaaaads!) of ways to make statements about things . \u00b6 For example, to express \"a mutation of SHH in humans causes isolated microphthalmia with coloboma-5\" you could say something like (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://purl.obolibrary.org/obo/RO_0004020 | \"has basis in dysfunction of\"]-->(https://identifiers.org/HGNC:10848 | \"SSH (gene)\"). Or you could say: (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://www.w3.org/2000/01/rdf-schema#subClassOf | \"is a\"]-->(http://purl.obolibrary.org/obo/MONDO_0003847 | \"Mendelian Disease\"). If we use the analogy of \"language\", then the URIs (above) are the words, and the statements are sentences in a language . Unfortunately, there are many languages in the Semantic Web, such as OWL, RDFS, SKOS, SWRL, SHACL, SHEX, and dialects (OWL 2 EL, OWL 2 RL) and a plethora of formats, or serialisations (you can store the exact same sentence in the same language such as RDF, or OWL, in many different ways)- more about that later. In here lies also one of the largest problems of the Semantic Web - lots of overlapping standards means, lots of incompatible data - which raises the bar for actually being able to seamlessly integrate \"statements about things\" across resources. Collections of statements about things that somehow belong together and provide some meaning, or context, for those things . \u00b6 Examples include: controlled vocabularies , that define, for example, how to refer to a disease (e.g., we use http://purl.obolibrary.org/obo/MONDO_0012709 to refer to \"isolated microphthalmia with coloboma 5\"), terminologies which describe how we humans refer to a disease (How is it called in German? Which other synonyms are used in the literature? How is the term defined in the medical literature?), taxonomies which define how diseases are related hierarchically (\"microphthalmia, isolated, with coloboma 5 is a kind of Mendelian disease\"), ontologies which further define how diseases are defined in terms of other entities, for example \"microphthalmia, isolated, with coloboma 5 is a Mendelian disease that has its basis in the dysfunction of SSH\". Note : In practice, when we say \"ontology\", we mean all of the above together - it is, however, good to know that they are somewhat distinct, and that there are different \"languages\" that can be used for each of these distinctions. Tools that do something useful with these collections of statements. \u00b6 For example (as always, non exhaustive): Efficient storage (triple stores, in-memory ontology APIs, other databases). Similar to traditional SQL databases, the Semantic Web comes with a number of database solutions that are optimised to deliver \"semantic content\". Semantically aware querying : Very similar to traditional SQL (which, incidentally is often great to query semantic data), there are various ways to \"interrogate\", or query, your Linked Data, such as SPARQL, DL Querying , Ontology-based data access (OBDA). Subsetting (module/subset extraction): Often, ontologies (or other collections of Linked Data statements) are very large and cover a lot of entities and knowledge that is not important to your work. There are a number of techniques that allow you to extract meaningful subsets for your use case; for example, you may be interested to get all the information you can about Mendelian diseases, but you don't care about common diseases (e.g. see ROBOT extract ). Visualisation : As a Data Scientist, you are used to looking at your data in tabular form - while a lot of information stored in ontologies can still be inspected this way, in general semantic data is best perceived as a graph - which are notoriously hard to visualise. Fortunately, a lot of Linked Data, in particular ontologies in the biomedical domain are predominantly tree-shaped (you have a disease, and underneath sub-diseases). Term browsers like OLS typically render ontologies as trees. Data linking/matching : This is key in particular in the biomedical sciences, as there is almost never just one way to refer to the same thing. In my experience, a good rule of thumb is that there are 3-6, e.g. 3-6 URIs that refer to \"Mendelian Disease\", all of which need to be matched together to integrate data across resources. There are many approaches to ontology matching - none of which are anywhere near perfect. Automated error checking and validation (Syntax, Structure (SHEX, SHACL), logical Consistency (DL Reasoner)): Naturally, writing sentences in any language is hard in the beginning, but this is even more true for highly complex languages such as OWL. In my experience, no-one can write flawless OWL without the help of automatic syntax and semantics checking, at least not consistently. Validation tools are a crucial part of your Semantic Engineering toolbox. Translate between languages: Often we need to translate from one language, for example OWL, to another, for example SKOS to integrate divergent resources. Translations in the Semantic Web context are nearly always lossy (there are always things you can say in one language, but not in another), but they may be necessary nevertheless. Discovery of terms ( OLS , BioPortal ): If you are curating terms, you need to know what ID (URI) to use for \"isolated microphthalmia with coloboma 5\". For that, term browsers such as OLS are perfect. Just type in your natural language search term, and you will find a series of suggestions for URIs you can use for your curation. Discovery of vocabularies OBO Foundry ontology library , BioPortal ): we will have a section later on on how to select appropriate ontologies for your use case, but the general problem of finding vocabularies, or ontologies, is answered by ontology repositories or libraries. Naturally, our favourite ontology library is the OBO Foundry ontology library which contain a lot of high quality ontologies for the biomedical domain. Make implicit knowledge explicit, aka reasoning : Deductive (DL Reasoning, Rule-based reasoning such as Datalog, SWRL). One of the major selling points for OWL, for example, in the biomedical domain is the ability to use logical reasoning in a way that is sound (only gives you correct inferences, at all times) and complete (all hidden implications are found, at all times, by the reasoner) - this is particularly great for medical knowledge where mistakes in computer algorithms can have devastating effects. However, I am slowly coming to the conviction that sound and complete reasoning is not the only form of deductive reasoning that is useful - many rule languages can offer value to your work by unveiling hidden relationships in the data without giving such strong \"logical guarantees\". Inductive (Machine Learning approaches, Knowledge Graph Representation Learning). The new frontier - we will discuss later in the course how our ontology-powered Knowledge Graphs can be leveraged to identify drug targets, novel gene to phenotype associations and more, using a diverse set of Machine Learning-based approaches. This week will focus on 1 (identifiers) and 4 (applications) - 2 (languages and standards) and 3 (controlled vocabularies and ontologies) will be covered in depth in the following weeks. Note on the side : Its not always 100% clear what is meant by Linked Data in regular discourse. There are some supposedly \"clear\" definitions (\" method for publishing structured data \", \" collection of interrelated datasets on the Web \"), but when it comes down to the details, there is plenty of confusion (does an OWL ontology constitute Linked Data when it is published on the Web? Is it Linked Data if it does not use RDF? Is it Linked Data if it is less than 5-star - see below). In practice all these debates are academic and won't mean much to you and your daily work. There are entities , statements (context) being said about these entities using some standard (associated with the Semantic Web, such as OWL or RDFS) and tools that do something useful with the stuff being said. When I say \"Mendelian Disease\" I mean http://purl.obolibrary.org/obo/MONDO_0003847 \u00b6 One of the top 5 features of the Semantic Web (at least in the context of biomedical sciences) is the fact that we can use URIs as a global identifier scheme that is unambiguous, independent of database implementations, independent of language concerns to refer to the entities in our domain. For example, if I want to refer to the concept of \"Mendelian Disease\", I simply refer to http://purl.obolibrary.org/obo/MONDO_0003847 - and everyone, in Japan, Germany, China or South Africa, will be able to \"understand\" or look up what I mean. I don't quite like the word \"understanding\" in this context as it is not actually trivial to explain to a human how a particular ID relates to a thing in the real world (semiotics). In my experience, this process is a bit rough in practice - it requires that there is a concept like \"Mendelian Disease\" in the mental model of the person, and it requires some way to link the ID http://purl.obolibrary.org/obo/MONDO_0003847 to that \"mental\" concept - not always as trivial as in this case (where there are standard textbook definitions). The latter is usually achieved (philosophers and linguists please stop reading) by using an annotation that somehow explains the term - either a label or some kind of formal definition - that a person can understand. In any case, not trivial, but thankfully not the worst problem in the biomedical domain where we do have quite a wide range of shared \"mental models\" (more so in Biology than Medical Science..). Using URIs allows us to facilitate this \"understanding\" process by leaving behind some kind of information at the location that is dereferenced by the URI (basically you click on the URI and see what comes up). Note that there is a huge deal of compromise already happening across communities. In the original Semantic Web community, the hope was somehow that dereferencing the URI (clicking on it, navigating to it) would reveal structured information about the entity in question that could used by machines to understand what the entity is all about. In my experience, this was rarely ever realised in the biomedical domain. Some services like Ontobee expose such machine readable data on request (using a technique called content negotiation), but most URIs simply refer to some website that allow humans to understand what it means - which is already a huge deal. For more on names and identifiers I refer the interested reader to James Overton's OBO tutorial here . Personal note: Some of my experienced friends in the bioinformatics world say that \"IRI have been more pain than benefit\". It is clear that there is no single thing in the Semantic Web is entirely uncontested - everything has its critics and proponents. The advent of the CURIE and the bane of the CURIE map \u00b6 In reality, few biological resources will contain a reference to http://purl.obolibrary.org/obo/MONDO_0003847. More often, you will find something like MONDO:0003847 , which is called a CURIE . You will find CURIEs in many contexts, to make Semantic Web languages easier to read and manage. The premise is basically that your document contains a prefix declaration that says something like this: PREFIX MONDO: <http://purl.obolibrary.org/obo/MONDO_> which allows allows the interpreter to unfold the CURIE into the IRI: MONDO:0003847 -> http://purl.obolibrary.org/obo/MONDO_0003847 In reality, the proliferation of CURIEs has become a big problem for data engineers and data scientists when analysing data. Databases rarely, if ever, ship the CURIE maps with their data required to understand what a prefix effectively stands for, leading to a lot of guess-work in the daily practice of the Semantic Data Engineer (if you ever had to distinguish ICD: ICD10: ICD9: UMLS:, UMLSCUI: without a prefix map, etc you will know what I am talking about). Efforts to bring order to this chaos, essentially globally agreed CURIE maps (e.g. prefixcommons ), or ID management services such as identifiers.org exist, but right now there is no one solution - prepare yourself to having to deal with this issue when dealing with data integration efforts in the biomedical sciences. More likely than not, your organisation will build its own curie map and maintain it for the duration of your project. Semantic Web in the biomedical domain: Success stories \u00b6 There are probably quite a few divergent opinions on this, but I would like to humbly list the following four use cases as among the most impactful applications of Semantic Web Technology in the biomedical domain. Light Semantics for data aggregation . \u00b6 We can use hierarchical relations in ontology to group data. For example, if I know that http://purl.obolibrary.org/obo/MONDO_0012709 (\"microphthalmia, isolated, with coloboma 5\") http://www.w3.org/2000/01/rdf-schema#subClassOf (\"is a\") http://purl.obolibrary.org/obo/MONDO_0003847 (\"Mendelian Disease\"), then a specialised Semantic Web tool called a reasoner will know that, if I ask for all genes associated with Mendelian diseases, you also want to get those associated with \"microphthalmia, isolated, with coloboma 5\" specifically (note that many query engines such as SPARQL with RDFS entailment regime have simple reasoners embedded in them, but we would not call them \"reasoner\" - just query engine). Heavy Semantics for ontology management . \u00b6 Ontologies are extremely hard to manage and profit from the sound logical foundation provided by the Web Ontology Language (OWL). We can logically define our classes in terms of other ontologies, and then use a reasoner to classify our ontology automatically. For example, we can define abnormal biological process phenotypes in terms of biological processes (Gene Ontology) and classify our phenotypes entirely using the classification of biological processes in the Gene Ontology (don't worry if you don't understand a thing - we will get to that in a later week). Globally unique identifiers for data integration . \u00b6 Refer to the same thing the same way. While this goal was never reached in total perfection, we have gotten quite close. In my experience, there are roughly 3-6 ways to refer to entities in the biomedical domain (like say, ENSEMBL, HGNC, Entrez for genes; or SNOMED, NCIT, DO, MONDO, UMLS for diseases). So while the \"refer to the same thing the same way\" did not truly happen, a combination of standard identifiers with terminological mappings , i.e. links between terms, can be used to integrate data across resources (more about Ontology Matching later). Again, many of my colleagues disagree - they don't like IRIs, and unfortunately, you will have to build your own position on that. Personal note : From an evolutionary perspective, I sometimes think that having 2 or 3 competing terminological systems is better than 1, as the competition also drives the improvements in quality, but there is a lot of disagreement on this. Coordinated development of mutually compatible ontologies across the biomedical domain : The Open Biological and Biomedical Ontologies (OBO) Foundry. \u00b6 The OBO Foundry is a community-driven effort to coordinate the development of vocabularies and ontologies across the biomedical domain. It develops standards for the representation of terminological content (like standard properties), and ontological knowledge (shared design patterns) as well as shared systems for quality control. Flagship projects include: The Relation Ontology (RO) for the standardisation of relationships that connect entities in biomedical ontologies. The Core Ontology for Biology and Biomedicine (COB) : upper ontology to align key entities used throughout biomedical ontologies. The OBO Metadata Ontology for aligning ontology metadata properties across OBO ontologies. The OBO Persistent Identifier System : an Identifier scheme for persistent URIs used by many ontologies on the web. The system is used to refer to terms as well as ontologies and their versions. OBO Dashboard : a system for the monitoring and continued improvement of OBO ontologies with automated Quality Control checks. Semantic Web and Linked Data: Things you should have heard about \u00b6 The Semantic Web Layer Cake : A iconic, colourful graphic that describes the layered design of the semantic web, from URIs to Logic. Its not particularly useful, but as a Semantic Web Explorer, you should have seen it. Linked Data is mostly referred to as a \"method for publishing data\", a key concept in the Semantic Web domain, coined by Tim Berners Lee in 2006. Related concepts: Linked Data Principles : Use URIs as names for things Use HTTP URIs so that people can look up those names. When someone looks up a URI, provide useful information. Include links to other URIs. so that they can discover more things. 5-Star system make your stuff available on the Web (whatever format) under an open license make it available as structured data (e.g., Excel instead of image scan of a table) use non-proprietary formats (e.g., CSV instead of Excel) use URIs to denote things, so that people can point at your stuff link your data to other data to provide context FAIR data : Principles defined in 2016, somewhat orthogonal to Linked Data Principles. A nice tutorial, also going a bit more in depth into identifiers than what we did in this section, can be found here . The idea of FAIR data is probably more impactful in the biomedical and pharmaceutical world then the idea of Linked Data. While there are some (slighltly irritating) voices on the sidelines that say that \"It can't be FAIR if its not RDF\", it is probably true that a nicely formatted CSV file on the Web is at least as useful as a (hard to understand) RDF dump containing the same data. Worldwide collaborations between major pharmaceutical corporations promoting FAIR data, such as the Pistoia Alliance do mention Semantic Web Technologies in their White papers , but keep the jargon a bit more hidden from the general public. Data, according to the FAIR principles, should be: Findable (machine readable metadata, etc) Accessible (open authentication, authorisation) Interoperable (integrated with other data, closely related to controlled vocabularies and linked data) Reusable (metadata, license, provenance) World Wide Web Consortium (W3C) : The World Wide Web Consortium (W3C) is an international community that develops open standards, in particular many of those (but not all!) pertaining to the Semantic Web. The Ecosystem of Linked Data and Semantic Web: Standards, Technologies and Research Areas \u00b6 In the following, we will list some of the technologies you may find useful, or will be forced to use, as a Semantic Data Engineer. Most of these standards will be covered in the subsequent weeks of this course. Standard Purpose Use case Web Ontology Language (OWL) Representing Knowledge in Biomedical Ontologies All OBO ontologies must be provided in OWL as well. Resource Description Framework (RDF) Model for data interchange. Triples, the fundamental unit of RDF, are ubiquitous on the Semantic Web SPARQL Query Language for RDF A standard query language for RDF and RDFS. Primary query language to interrogate RDF/RDFS/Linked Data on the Web. Simple Knowledge Organization System (SKOS) Another, more lightweight, knowledge organisation system in many ways competing with OWL. Not as widely used in the biomedical domain as OWL, but increasing uptake of \"matching\" vocabulary (skos:exactMatch, etc). RDF-star A key shortcoming of RDF is that, while I can in principle say everything about everything, I cannot directly talk about edges, for example to attribute provenance: \"microphthalmia, isolated, with coloboma 5 is kind of Mendelian disease\"--source: Wikipedia Use cases here . JSON-LD A method to encoding linked data in JSON format. (Very useful to at least know about). RDFa W3C Recommendation to embed rich semantic metadata in HTML (and XML). I have to admit - in 11 years Semantic Web Work I have not come across much use of RDFa in the biomedical domain. But @jamesaoverton is using it in his tools! A thorough overview of all the key standards and tools can be found on the Awesome Semantic Web repo. For a rough sense of current research trends it is always good to look at the accepted papers at one of the major conferences in the area. I like ISWC ( 2020 papers ), but for the aspiring Semantic Data Engineering in the biomedical sphere, it is probably a bit broad and theoretical. Other interesting specialised venues are the Journal of Biomedical Semantics and the International Conference on Biomedical Ontologies , but with the shift of the focus in the whole community towards Knowledge Graphs, other journals and conferences are becoming relevant. Here are a few key research areas, which are, by no means (!), exhaustive. How can we combine data and knowledge from different ontologies/knowledge graphs? Ontology/Knowledge graph alignment : How can we effectively link to ontologies, or knowledge graphs, together? Ontology merging : combine two ontologies by corresponding concepts and relations. Ontology matching: A sub-problem of ontology alignment, namely the problem of determining whether two terms (for example two diseases) from different ontologies should be linked together or not. How can we integrate data from unstructured and semistructured sources such as documents or spreadsheets? Named Entity Recognition (NER) : the process of identifying a named \"thing\" in a text. Entity linking : The task of associating a named entity, for example the result of a Named Entity Recognition algorithm, or the column of a spreadsheet, to a concept in an ontology. For example, the value \"Mendelian Disease\" is linked to the concept http://purl.obolibrary.org/obo/MONDO_0003847. Relationship extraction : Once you have identified the genes and diseases in your Pubmed abstracts, you will want to understand how they related to each other. Is the gene the \"basis in dysfunction of\" the disease? Or just randomly co-occurs in the sentence? Note: Many of the problems in this category are typically associated with the domain of Natural Language Processing rather than Semantic Web. How can we generate insight from semantically integrated data? Knowledge Graphs and Machine Learning Knowledge Graph Embeddings . The number one hype topic in recent years: How do you get from a graph of interrelated entities to a faithful representation in a vector space (basically numbers), so that Machine Learning algorithms can do their magic? Link predication : Based on what we know, which are the best drug targets for my rare disease of interest? Logical reasoning: While the research on deductive reasoning, at least the more \"hard-core\" Description Logic kind, seems to be a bit more quiet in recent years (maybe I am wrong here, I just see much less papers coming through my Google Scholar alerts now then I used to), there is still a lot going on in this domain: more efficient SPARQL engines, rule-based reasoning such as the recently commercialised RDFox reasoner and many more. Other research areas (not in any way exhaustive): Web decentralisation and privacy: Solid : Solid (Social Linked Data) is a web decentralization project led by Tim Berners-Lee, with the aim of true ownership of personal data and improved privacy. \"Pods\" are like secure personal web servers for data from which application can request data. Shape validation : It is very difficult to validate huge Knowledge Graphs of interrelated data efficiently (by validate we can mean a lot of things, such as making sure that your cat does not accidentally end up as someone's \"Mendelian Disease\"). Shape languages such as Shex and SHACL are poised to solve this problem, but the research is ongoing. New standards and tools: There is always someone proposing a new semantic standard for something or building a new kind of triple store, SPARQL extension or similar. Typical Jobs of Semantic Data Engineers in the biomedical domain \u00b6 It is useful to get a picture of the typical tasks a Semantic Data Engineer faces when building ontologies are Knowledge Graphs. In my experience, it is unlikely that any particular set of tools will work in all cases - most likely you will have to try and assemble the right toolchain for your use case and refine it over the lifetime of your project. The following are just a few points for consideration of tasks I regularly encountered - which may or may not overlap with the specific problems you will face. Finding the right ontologies \u00b6 There are no simple answers here and it very heavily depends on your use cases. We are discussing some places to look for ontologies here , but it may also be useful to simply upload the terms you are interested in to a service like Zooma and see what the terms map to at a major database provider like EBI. Finding the right data sources \u00b6 This is much harder still than it should have to be. Scientific databases are scattered across institutions that often do not talk to each other. Prepare for some significant work in researching the appropriate databases that could benefit your work, using Google and the scientific literature. Extending existing ontologies \u00b6 It is rare nowadays that you will have to develop an ontology entirely from scratch - most biomedical sub-domains will have some kind of reasonable ontology to build upon. However, there is often a great need to extend existing ontologies - usually because you have the need of representing certain concepts in much more detail, or your specific problem has not been modelled yet - think for example when how disease ontologies needed to be extended during the Coronavirus Crisis. Extending ontologies usually have two major facets: If at all possible you should seek to contribute new terms, synonyms and relationships to the ontologies you seek to extend directly. Here, you can use GitHub to make issues requesting new terms, but more boldly, you can also add new terms yourself. We will teach you how to do that in one of the next weeks. If the knowledge is considered out of scope for the ontology to be extended (for example because the terms are considered too detailed), then you will maintain your own \"branch\" of the ontology. Many tools such as the Ontology Development Kit and ROBOT can help you maintain such a branch but the general instinct should be: Make a public GitHub repo. Reach out to the developers of the main ontology Stay in touch and coordinate releases Mapping data into ontologies/knowledge graphs \u00b6 Also sometimes more broadly referred to as \"data integration\", this problem involves a variety of tasks, such as: Named Entity Recognition . If you have a set of documents, such as PubMed abstracts or clinical notes, you may have to first identify the parts of speech that refer to clinical entities. Entity Linking : Once you have identified the biomedical entities of interest, you may want to link them to your existing knowledge graph. This process is sometimes called entity mapping or data mapping as well. Very often, this task is not fully automated. We have worked on projects where we used approaches to Entity Linking to suggest good mappings to users, which then had to confirm or reject them. It is also good to understand that not all entity linking must be vertical (i.e. between \"equivalent\" entities). Very often, there is no equivalent entity in your knowledge graph to map to, and here you need to decide whether to (a) create a new entity in the knowledge graph to map to or (b) map to a broader entity (for example \"microphthalmia, isolated, with coloboma 5\" to \"Mendelian Disease\"). What is more efficient / useful solely depends on your use case! Build application ontologies \u00b6 To make your data discoverable, it is often useful to extract a view from the ontologies you are using (for example, Gene Ontology, Disease Ontology) that only contains the terms and relationships of relevance to your data. We usually refer to this kind of ontology as an application ontology, or an ontology specific to your application, which will integrate subsets of other ontologies. This process will typically involve the following: Define a seed, or a set of terms you want to import from your external ontologies of interest. Extract relevant subsets from ontologies using this seed (for example using ROBOT extract ). Combine and potentially link these subsets together. Frameworks such as the Ontology Development Kit can help with this task, see for example the Coronavirus Vocabulary maintained by EBI. Leverage ontologies and knowledge graphs for you data analysis problems \u00b6 There are many ways your semantic data can be leveraged for data analysis, but in my experience, two are particularly central: Data grouping and search : make data about \"microphthalmia, isolated, with coloboma 5\" available when searching for data about \"Mendelian Disease\". Link prediction : Figure out what additional knowledge is hidden in your data that can drive your research (e.g. possible new therapies or drug targets). Additional materials and resources \u00b6 The open courses of the Hasso Plattner Institute (HPI) offer introductions into the concepts around Linked Data, Semantic Web and Knowledge Engineering. There are three courses of relevance to this weeks topics, all of which overlap significantly. Knowledge Engineering and the Web of Data : The oldest (2015), of the courses, but the most thorough when it comes to logical foundations, semantics and OWL. We will come back to this course in Weeks 4 and 5. Linked Data Engineering : Overlaps a lot with the Knowledge Engineering and the Web of Data course, with a bit more RDF/Linked Data focus. Knowledge Graphs : The most up-to-date of the three courses (2020), and will be referred to again in Week 12 of our course here. Contributors \u00b6 add name/ORCID here","title":"Analysing Linked Data"},{"location":"lesson/analysing-linked-data/#analysing-linked-data-fundamentals","text":"","title":"Analysing Linked Data (Fundamentals)"},{"location":"lesson/analysing-linked-data/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/analysing-linked-data/#prerequisites","text":"Review tutorial on Ontology Theory","title":"Prerequisites"},{"location":"lesson/analysing-linked-data/#preparation","text":"Essential Linked Data Engineering : Week 1 Support Programming Historian Linked Data tutorial Original Whitepaper (Tim Berners Lee et al) Educational curriculum for Linked Data Tools: Browse through the tools and standards listed in the Semantic Engineer Toolbox .","title":"Preparation"},{"location":"lesson/analysing-linked-data/#learning-objectives","text":"Advanced SPARQL Term enrichment Semantic similarity Named Entity Recognition","title":"Learning objectives"},{"location":"lesson/analysing-linked-data/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/analysing-linked-data/#the-linked-data-landscape-from-an-obo-perspective-standards-services-and-tools","text":"In the following we will look a bit at the general Linked Data landscape, and name some of its flagship projects and standards. It is important to be clear that the Semantic Web field is a very heterogenous one:","title":"The Linked Data landscape from an OBO perspective: Standards, Services and Tools"},{"location":"lesson/analysing-linked-data/#flagship-projects-of-the-wider-semantic-web-community","text":"Linked Open Data (LOD) cloud : The flagship project of the Semantic Web. An attempt to make all, or anyways a lot, of Linked Data accessible in one giant knowledge graph. A good overview can be found in this medium article. Note that some people seem to think that the Semantic Web is (or should be) the Linked Open Data cloud. I would question this view, but I am not yet decided what my position is. Schema.org : General purpose vocabulary for entities on the web, founded by Google, Microsoft, Yahoo and Yandex. To get a better sense of the types of entities and relationships covered see here . DBpedia : Project that extracts structured data from Wikipedia and makes it available as a giant knowledge graph. The associated ontology , similar to schema.org, covers entities encountered in common sense knowledge. Wikidata : Free and open knowledge base that can be edited in much the same way as Wikipedia is edited. While these Semantic Web flagship projects are doubtlessly useful, it is sometimes hard to see how they can help for your biomedical research. We rarely make use of them in our day to day work as ontologists, but there are some notable exceptions: Where our work involves modelling environmental factors, we sometimes use wikidata as a standard way to refer for example to countries. For some more common sense knowledge use cases, such as nutrition, consider augmenting your knowledge graph with data from wikidata or dbpedia. While they may be a bit more messy and not directly useful for exploration by humans, it is quite possible that Machine Learning approaches can use the additional context provided by these knowledge graphs to improve embeddings and deliver more meaningful link predictions. Some OBO ontologies are already on Wikidata - perhaps you can find additional synonyms and labels which help with your data mapping problems!","title":"Flagship projects of the wider Semantic Web community"},{"location":"lesson/analysing-linked-data/#where-the-obo-and-semantic-web-communities-are-slightly-at-odds","text":"The OBO format is a very popular syntax for representing biomedical ontologies. A lot of tools have been built over the years to hack OBO ontologies on the basis of that format - I still work with it on a daily basis. Although it has semantically been proven to be a subset of OWL (i.e. there is a lossless mapping of OBO into OWL) and can be viewed as just another syntax, it is in many ways idiosyncratic. For starters, you wont find many, if any, IRIs in OBO ontologies. The format itself uses CURIEs which are mapped to the general OBO PURL namespace during transformation to OWL. For example, if you see MONDO:0003847 in an OBO file, and were to translate it to OWL, you will see this term being translated to http://purl.obolibrary.org/obo/MONDO_0003847. Secondly, you have a bunch of built-in properties like BROAD or ABBREVIATION that mapped to a vocabulary called oboInOwl (oio). These are pretty non-standard on the general Semantic Web, and often have to be manually mapped to the more popular counterparts in the Dublin Core or SKOS namespaces. Having URIs as identifiers is not generally popular in the life sciences. As discussed elsewhere, it is much more likely to encounter CURIEs such as MONDO:0003847 than URIs such as http://purl.obolibrary.org/obo/MONDO_0003847 in biomedical databases.","title":"Where the OBO and Semantic Web communities are slightly at odds"},{"location":"lesson/analysing-linked-data/#useful-tools-for-biomedical-research","text":"Why does the biomedical research, and clinical, community care about the Semantic Web and Linked Data? There are endless lists of applications that try to apply semantic technologies to biomedical problems, but for this week, we only want to look at the broader picture. In our experience, the use cases where Semantic Web standards are applied successfully are: Where to find ontologies: Ontology repositories OBO Foundry Ontology Library BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Where to find terms: Term browsers OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. A key difference between Ontobee and OLS/Bioportal is that Ontobee limits hierarchical relationships to is_a. This means if you are browsing ontologies such as GO, Uberon, CL, ENVO, you will not see part-of links in the hierarchy, and these links are crucial for understanding these ontologies. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. Curate biomedical data. There are a lot of different tools in this space - which we will discuss in a bespoke unit later in the course. Examples: isatools : The open source ISA framework and tools help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies. RightField : System for curating ontology terms in Excel spreadsheets. CEDAR Templates : Basically a templating system that allows to create templates to record metadata, for example in a lab setting, of course with ontology integration. Other examples of tabular data to RDF converters, but new ones coming up every year. Building ontologies Populous/Webulous : A system to maintain/generate ontologies from spreadsheets. The idea was to basically to define patterns in a (now mostly dead) language called OPPL, and then apply them to spreadsheets to generate OWL axioms. EBI recently discontinued the service, as there is a general exodus to Google Sheets + ROBOT templates instead. ROBOT templates + Google Sheets and Cogs : A lightweight approach based on a set of tools that allows curating ontologies in spreadsheets (e.g. Google Sheets) which are converted into OWL using ROBOT. DOSDP tools + Dead Simple Design Patterns (DOSDP) : Similar to ROBOT templates, DOSDPs (which really should be called DOSDTs, because they are not really design patterns ; they are ontology templates), another system that allows the generation of OWL axioms based on spreadsheet data. Cleaning messy data OpenRefine : I have not myself used this ever, but some of my colleagues have. OpenRefine allows you to upload (spreadsheet) data, explore it and clean it (going as far as reconciling terms using Wikidata concepts).","title":"Useful tools for biomedical research"},{"location":"lesson/analysing-linked-data/#which-biomedical-ontologies-should-we-use","text":"As a rule of thumb, for every single problem/term/use case, you will have 3-6 options to choose from, in some cases even more. The criteria for selecting a good ontology are very much dependent on your particular use case, but some concerns are generally relevant. A good first pass is to apply to \" 10 simple rules for selecting a Bio-ontology \" by Malone et al, but I would further recommend to ask yourself the following: Do I need the ontology for grouping and semantic analysis? In this case a high quality hierarchy reflecting biological subsumption is imperative. We will explain later what this means, but in essence, you should be able to ask the following question: \"All instances/occurrences of this concept in the ontology are also instances of all its parent classes. Everything that is true about the parent class is always also true about instances of the children.\" It is important for you to understand that, while OWL semantics imply the above, OWL is difficult and many ontologies \"pretend\" that the subclass link means something else (like a rule of thumb grouping relation). Can I handle multiple inheritance in my analysis? While I personally recommend to always consider multiple inheritance (i.e, allow a term to have more than one parent class), there are some analysis frameworks, in particular in the clinical domain, that make this hard. Some ontologies are inherently ploy-hierarchical (such as Mondo ), while others strive to be single inheritance ( DO , ICD). Are key resources I am interested in using the ontology? Maybe the most important question that will drastically reduce the amount of data mapping work you will have to do: Does the resource you wish to integrate already annotate to a particular ontology? For example, EBI resources will be annotating phenotype data using EFO, which in turn used HPO identifiers. If your use case demands to integrate EBI databases, it is likely a good idea to consider using HPO as the reference ontology for your phenotype data. Aside from aspects of your analysis, there is one more thing you should consider carefully: the open-ness of your ontology in question. As a user, you have quite a bit of power on the future trajectory of the domain, and therefore should seek to endorse and promote open standards as much as possible (for egotistic reasons as well: you don't want to have to suddenly pay for the ontologies that drive your semantic analyses). It is true that ontologies such as SNOMED have some great content, and, even more compellingly, some really great coverage. In fact, I would probably compare SNOMED not with any particular disease ontology, but with the OBO Foundry as a whole, and if you do that, it is a) cleaner, b) better integrated. But this comes at a cost. SNOMED is a commercial product - millions are being payed every year in license fees, and the more millions come, the better SNOMED will become - and the more drastic consequences will the lock-in have if one day you are forced to use SNOMED because OBO has fallen too far behind. Right now, the sum of all OBO ontologies is probably still richer and more valuable, given their use in many of the central biological databases (such as the ones hosted by the EBI ) - but as SNOMED is seeping into the all aspects of genomics now (for example, it will soon be featured on OLS !) it will become increasingly important to actively promote the use of open biomedical ontologies - by contributing to them as well as by using them. We will discuss ontologies in the medical, phenomics and genomics space in more detail in a later session of the course.","title":"Which biomedical ontologies should we use?"},{"location":"lesson/analysing-linked-data/#other-interesting-links","text":"Linked Data in e-Government Industrial Ontologies Foundry : Something like the OBO Foundry for Industrial Ontologies OntoCommons : An H2020 CSA project dedicated to the standardisation of data documentation across all domains related to materials and manufacturing.","title":"Other interesting links"},{"location":"lesson/analysing-linked-data/#basic-linked-data-and-semantic-web-concepts-for-the-semantic-data-engineer-in-the-biomedical-domain","text":"In this section we will discuss the following: Introductory remarks The advantages of globally unique identifiers Some success stories of the Semantic Web in the biomedical domain Some basic concepts you should probably have heard about The ecosystem of the Semantic Web: Standards, Technologies and Research Areas Typical tasks of Semantic Data Engineers in the biomedical domain","title":"Basic Linked data and Semantic Web Concepts for the Semantic Data Engineer in the Biomedical Domain"},{"location":"lesson/analysing-linked-data/#introduction","text":"Note of caution : No two Semantic Web overviews will be equivalent to each other. Some people claim the Semantic Web as an idea is an utter failure, while others praise it as a great success (in the making) - in the end you will have to make up your own mind. In this section I focus on parts of the Semantic Web step particularly valuable to the biomedical domain, and I will omit many relevant topics in the wider Semantic Web area, such as Enterprise Knowledge Graphs, decentralisation and personalisation, and many more. Also, the reader is expected to be familiar with the basic notions of the Semantic Web, and should use this overview mainly to tie some of the ideas together. The goal of this section is to give the aspiring Semantic Data Engineer in the biomedical domain a rough idea of key concepts around Linked Data and the Semantic Web insofar as they relate to their data science and and data engineering problems. Even after 20 years of Semantic Web research (the seminal paper , conveniently and somewhat ironically behind a paywall, was published in May 2001), the area is still dominated by \"academic types\", although the advent of the Knowledge Graph is already changing that. As I already mentioned above, no two stories of what the Semantic Web is will sound the same. However, there are a few stories that are often told to illustrate why we need semantics. The OpenHPI course names a few: \"From the web of documents to the web of data\" tells the story of how the original web is essentially a huge heap of (interlinked) natural language text documents which are very hard to understand for search engines: Does the word \"Jaguar\" on this site refer to the car or the cat? Clarifying in your web page that the word Jaguar refers to the concept of \"Jaguar the cat\", for example like this: <span about=\"dbpedia:Jaguar\">Jaguar</span> , will make it easier for the search engine to understand what your site is about and link it to other relevant content. From this kind of mark-up, structured data can be extracted and integrate into a giant, worldwide database, and exposed through SPARQL endpoints, that can then be queried using a suitable query language. \"From human to machine understandable\": as a Human, we know that a Jaguar is a kind of cat, and all cats have four legs. If you ask a normal search engine: \"Does a Jaguar have four legs?\" it will have a tough time to answer this question correctly (if it cannot find that exact statement anywhere). That is why we need proper semantics , some kind of formalism such that a \"machine\" can deduce from the statements \"Jaguar is a cat; Cat has four legs\" that \"Jaguar has four legs\". The \"Semantic Layer Cake\": a box/brick diagram showing how Semantic Web Technologies are stacked on top of each other. An engineering centric view that has been used countless times to introduce the Semantic Web, but rarely helped anyone to understand what it is about. I am not entirely sure anymore that any of these ways (web of data, machine understanding, layered stack of matching standards) to motivate the Semantic Web are particularly effective for the average data scientists or engineer. If I had to explain the Semantic Web stack to my junior self, just having finished my undergraduate, I would explain it as follows (no guarantee though it will help you). The Semantic Web / Linked Data stack comprises roughly four components that are useful for the aspiring Semantic (Biomedical) Data Engineer/Scientist to distinguish:","title":"Introduction"},{"location":"lesson/analysing-linked-data/#a-way-to-refer-to-things-including-entities-and-relations-in-a-global-namespace","text":"You, as a scientist, might be using the term \"gene\" to refer to basic physical and functional unit of heredity, but me, as a German, prefer the term \"Gen\". In the Semantic Web, instead of natural language words, we prefer to use URIs to refer to things such as https://www.wikidata.org/wiki/Q7187: if you say something using the name https://www.wikidata.org/wiki/Q7187, both your German and Japanese colleagues will \"understand\" what you are referring to. More about that in the next chapter.","title":"A way to refer to things (including entities and relations) in a global namespace."},{"location":"lesson/analysing-linked-data/#lots-loaaaads-of-ways-to-make-statements-about-things","text":"For example, to express \"a mutation of SHH in humans causes isolated microphthalmia with coloboma-5\" you could say something like (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://purl.obolibrary.org/obo/RO_0004020 | \"has basis in dysfunction of\"]-->(https://identifiers.org/HGNC:10848 | \"SSH (gene)\"). Or you could say: (http://purl.obolibrary.org/obo/MONDO_0012709 | \"microphthalmia, isolated, with coloboma 5\")--[http://www.w3.org/2000/01/rdf-schema#subClassOf | \"is a\"]-->(http://purl.obolibrary.org/obo/MONDO_0003847 | \"Mendelian Disease\"). If we use the analogy of \"language\", then the URIs (above) are the words, and the statements are sentences in a language . Unfortunately, there are many languages in the Semantic Web, such as OWL, RDFS, SKOS, SWRL, SHACL, SHEX, and dialects (OWL 2 EL, OWL 2 RL) and a plethora of formats, or serialisations (you can store the exact same sentence in the same language such as RDF, or OWL, in many different ways)- more about that later. In here lies also one of the largest problems of the Semantic Web - lots of overlapping standards means, lots of incompatible data - which raises the bar for actually being able to seamlessly integrate \"statements about things\" across resources.","title":"Lots (loaaaads!) of ways to make statements about things."},{"location":"lesson/analysing-linked-data/#collections-of-statements-about-things-that-somehow-belong-together-and-provide-some-meaning-or-context-for-those-things","text":"Examples include: controlled vocabularies , that define, for example, how to refer to a disease (e.g., we use http://purl.obolibrary.org/obo/MONDO_0012709 to refer to \"isolated microphthalmia with coloboma 5\"), terminologies which describe how we humans refer to a disease (How is it called in German? Which other synonyms are used in the literature? How is the term defined in the medical literature?), taxonomies which define how diseases are related hierarchically (\"microphthalmia, isolated, with coloboma 5 is a kind of Mendelian disease\"), ontologies which further define how diseases are defined in terms of other entities, for example \"microphthalmia, isolated, with coloboma 5 is a Mendelian disease that has its basis in the dysfunction of SSH\". Note : In practice, when we say \"ontology\", we mean all of the above together - it is, however, good to know that they are somewhat distinct, and that there are different \"languages\" that can be used for each of these distinctions.","title":"Collections of statements about things that somehow belong together and provide some meaning, or context, for those things."},{"location":"lesson/analysing-linked-data/#tools-that-do-something-useful-with-these-collections-of-statements","text":"For example (as always, non exhaustive): Efficient storage (triple stores, in-memory ontology APIs, other databases). Similar to traditional SQL databases, the Semantic Web comes with a number of database solutions that are optimised to deliver \"semantic content\". Semantically aware querying : Very similar to traditional SQL (which, incidentally is often great to query semantic data), there are various ways to \"interrogate\", or query, your Linked Data, such as SPARQL, DL Querying , Ontology-based data access (OBDA). Subsetting (module/subset extraction): Often, ontologies (or other collections of Linked Data statements) are very large and cover a lot of entities and knowledge that is not important to your work. There are a number of techniques that allow you to extract meaningful subsets for your use case; for example, you may be interested to get all the information you can about Mendelian diseases, but you don't care about common diseases (e.g. see ROBOT extract ). Visualisation : As a Data Scientist, you are used to looking at your data in tabular form - while a lot of information stored in ontologies can still be inspected this way, in general semantic data is best perceived as a graph - which are notoriously hard to visualise. Fortunately, a lot of Linked Data, in particular ontologies in the biomedical domain are predominantly tree-shaped (you have a disease, and underneath sub-diseases). Term browsers like OLS typically render ontologies as trees. Data linking/matching : This is key in particular in the biomedical sciences, as there is almost never just one way to refer to the same thing. In my experience, a good rule of thumb is that there are 3-6, e.g. 3-6 URIs that refer to \"Mendelian Disease\", all of which need to be matched together to integrate data across resources. There are many approaches to ontology matching - none of which are anywhere near perfect. Automated error checking and validation (Syntax, Structure (SHEX, SHACL), logical Consistency (DL Reasoner)): Naturally, writing sentences in any language is hard in the beginning, but this is even more true for highly complex languages such as OWL. In my experience, no-one can write flawless OWL without the help of automatic syntax and semantics checking, at least not consistently. Validation tools are a crucial part of your Semantic Engineering toolbox. Translate between languages: Often we need to translate from one language, for example OWL, to another, for example SKOS to integrate divergent resources. Translations in the Semantic Web context are nearly always lossy (there are always things you can say in one language, but not in another), but they may be necessary nevertheless. Discovery of terms ( OLS , BioPortal ): If you are curating terms, you need to know what ID (URI) to use for \"isolated microphthalmia with coloboma 5\". For that, term browsers such as OLS are perfect. Just type in your natural language search term, and you will find a series of suggestions for URIs you can use for your curation. Discovery of vocabularies OBO Foundry ontology library , BioPortal ): we will have a section later on on how to select appropriate ontologies for your use case, but the general problem of finding vocabularies, or ontologies, is answered by ontology repositories or libraries. Naturally, our favourite ontology library is the OBO Foundry ontology library which contain a lot of high quality ontologies for the biomedical domain. Make implicit knowledge explicit, aka reasoning : Deductive (DL Reasoning, Rule-based reasoning such as Datalog, SWRL). One of the major selling points for OWL, for example, in the biomedical domain is the ability to use logical reasoning in a way that is sound (only gives you correct inferences, at all times) and complete (all hidden implications are found, at all times, by the reasoner) - this is particularly great for medical knowledge where mistakes in computer algorithms can have devastating effects. However, I am slowly coming to the conviction that sound and complete reasoning is not the only form of deductive reasoning that is useful - many rule languages can offer value to your work by unveiling hidden relationships in the data without giving such strong \"logical guarantees\". Inductive (Machine Learning approaches, Knowledge Graph Representation Learning). The new frontier - we will discuss later in the course how our ontology-powered Knowledge Graphs can be leveraged to identify drug targets, novel gene to phenotype associations and more, using a diverse set of Machine Learning-based approaches. This week will focus on 1 (identifiers) and 4 (applications) - 2 (languages and standards) and 3 (controlled vocabularies and ontologies) will be covered in depth in the following weeks. Note on the side : Its not always 100% clear what is meant by Linked Data in regular discourse. There are some supposedly \"clear\" definitions (\" method for publishing structured data \", \" collection of interrelated datasets on the Web \"), but when it comes down to the details, there is plenty of confusion (does an OWL ontology constitute Linked Data when it is published on the Web? Is it Linked Data if it does not use RDF? Is it Linked Data if it is less than 5-star - see below). In practice all these debates are academic and won't mean much to you and your daily work. There are entities , statements (context) being said about these entities using some standard (associated with the Semantic Web, such as OWL or RDFS) and tools that do something useful with the stuff being said.","title":"Tools that do something useful with these collections of statements."},{"location":"lesson/analysing-linked-data/#when-i-say-mendelian-disease-i-mean-httppurlobolibraryorgobomondo_0003847","text":"One of the top 5 features of the Semantic Web (at least in the context of biomedical sciences) is the fact that we can use URIs as a global identifier scheme that is unambiguous, independent of database implementations, independent of language concerns to refer to the entities in our domain. For example, if I want to refer to the concept of \"Mendelian Disease\", I simply refer to http://purl.obolibrary.org/obo/MONDO_0003847 - and everyone, in Japan, Germany, China or South Africa, will be able to \"understand\" or look up what I mean. I don't quite like the word \"understanding\" in this context as it is not actually trivial to explain to a human how a particular ID relates to a thing in the real world (semiotics). In my experience, this process is a bit rough in practice - it requires that there is a concept like \"Mendelian Disease\" in the mental model of the person, and it requires some way to link the ID http://purl.obolibrary.org/obo/MONDO_0003847 to that \"mental\" concept - not always as trivial as in this case (where there are standard textbook definitions). The latter is usually achieved (philosophers and linguists please stop reading) by using an annotation that somehow explains the term - either a label or some kind of formal definition - that a person can understand. In any case, not trivial, but thankfully not the worst problem in the biomedical domain where we do have quite a wide range of shared \"mental models\" (more so in Biology than Medical Science..). Using URIs allows us to facilitate this \"understanding\" process by leaving behind some kind of information at the location that is dereferenced by the URI (basically you click on the URI and see what comes up). Note that there is a huge deal of compromise already happening across communities. In the original Semantic Web community, the hope was somehow that dereferencing the URI (clicking on it, navigating to it) would reveal structured information about the entity in question that could used by machines to understand what the entity is all about. In my experience, this was rarely ever realised in the biomedical domain. Some services like Ontobee expose such machine readable data on request (using a technique called content negotiation), but most URIs simply refer to some website that allow humans to understand what it means - which is already a huge deal. For more on names and identifiers I refer the interested reader to James Overton's OBO tutorial here . Personal note: Some of my experienced friends in the bioinformatics world say that \"IRI have been more pain than benefit\". It is clear that there is no single thing in the Semantic Web is entirely uncontested - everything has its critics and proponents.","title":"When I say \"Mendelian Disease\" I mean http://purl.obolibrary.org/obo/MONDO_0003847"},{"location":"lesson/analysing-linked-data/#the-advent-of-the-curie-and-the-bane-of-the-curie-map","text":"In reality, few biological resources will contain a reference to http://purl.obolibrary.org/obo/MONDO_0003847. More often, you will find something like MONDO:0003847 , which is called a CURIE . You will find CURIEs in many contexts, to make Semantic Web languages easier to read and manage. The premise is basically that your document contains a prefix declaration that says something like this: PREFIX MONDO: <http://purl.obolibrary.org/obo/MONDO_> which allows allows the interpreter to unfold the CURIE into the IRI: MONDO:0003847 -> http://purl.obolibrary.org/obo/MONDO_0003847 In reality, the proliferation of CURIEs has become a big problem for data engineers and data scientists when analysing data. Databases rarely, if ever, ship the CURIE maps with their data required to understand what a prefix effectively stands for, leading to a lot of guess-work in the daily practice of the Semantic Data Engineer (if you ever had to distinguish ICD: ICD10: ICD9: UMLS:, UMLSCUI: without a prefix map, etc you will know what I am talking about). Efforts to bring order to this chaos, essentially globally agreed CURIE maps (e.g. prefixcommons ), or ID management services such as identifiers.org exist, but right now there is no one solution - prepare yourself to having to deal with this issue when dealing with data integration efforts in the biomedical sciences. More likely than not, your organisation will build its own curie map and maintain it for the duration of your project.","title":"The advent of the CURIE and the bane of the CURIE map"},{"location":"lesson/analysing-linked-data/#semantic-web-in-the-biomedical-domain-success-stories","text":"There are probably quite a few divergent opinions on this, but I would like to humbly list the following four use cases as among the most impactful applications of Semantic Web Technology in the biomedical domain.","title":"Semantic Web in the biomedical domain: Success stories"},{"location":"lesson/analysing-linked-data/#light-semantics-for-data-aggregation","text":"We can use hierarchical relations in ontology to group data. For example, if I know that http://purl.obolibrary.org/obo/MONDO_0012709 (\"microphthalmia, isolated, with coloboma 5\") http://www.w3.org/2000/01/rdf-schema#subClassOf (\"is a\") http://purl.obolibrary.org/obo/MONDO_0003847 (\"Mendelian Disease\"), then a specialised Semantic Web tool called a reasoner will know that, if I ask for all genes associated with Mendelian diseases, you also want to get those associated with \"microphthalmia, isolated, with coloboma 5\" specifically (note that many query engines such as SPARQL with RDFS entailment regime have simple reasoners embedded in them, but we would not call them \"reasoner\" - just query engine).","title":"Light Semantics for data aggregation."},{"location":"lesson/analysing-linked-data/#heavy-semantics-for-ontology-management","text":"Ontologies are extremely hard to manage and profit from the sound logical foundation provided by the Web Ontology Language (OWL). We can logically define our classes in terms of other ontologies, and then use a reasoner to classify our ontology automatically. For example, we can define abnormal biological process phenotypes in terms of biological processes (Gene Ontology) and classify our phenotypes entirely using the classification of biological processes in the Gene Ontology (don't worry if you don't understand a thing - we will get to that in a later week).","title":"Heavy Semantics for ontology management."},{"location":"lesson/analysing-linked-data/#globally-unique-identifiers-for-data-integration","text":"Refer to the same thing the same way. While this goal was never reached in total perfection, we have gotten quite close. In my experience, there are roughly 3-6 ways to refer to entities in the biomedical domain (like say, ENSEMBL, HGNC, Entrez for genes; or SNOMED, NCIT, DO, MONDO, UMLS for diseases). So while the \"refer to the same thing the same way\" did not truly happen, a combination of standard identifiers with terminological mappings , i.e. links between terms, can be used to integrate data across resources (more about Ontology Matching later). Again, many of my colleagues disagree - they don't like IRIs, and unfortunately, you will have to build your own position on that. Personal note : From an evolutionary perspective, I sometimes think that having 2 or 3 competing terminological systems is better than 1, as the competition also drives the improvements in quality, but there is a lot of disagreement on this.","title":"Globally unique identifiers for data integration."},{"location":"lesson/analysing-linked-data/#coordinated-development-of-mutually-compatible-ontologies-across-the-biomedical-domain-the-open-biological-and-biomedical-ontologies-obo-foundry","text":"The OBO Foundry is a community-driven effort to coordinate the development of vocabularies and ontologies across the biomedical domain. It develops standards for the representation of terminological content (like standard properties), and ontological knowledge (shared design patterns) as well as shared systems for quality control. Flagship projects include: The Relation Ontology (RO) for the standardisation of relationships that connect entities in biomedical ontologies. The Core Ontology for Biology and Biomedicine (COB) : upper ontology to align key entities used throughout biomedical ontologies. The OBO Metadata Ontology for aligning ontology metadata properties across OBO ontologies. The OBO Persistent Identifier System : an Identifier scheme for persistent URIs used by many ontologies on the web. The system is used to refer to terms as well as ontologies and their versions. OBO Dashboard : a system for the monitoring and continued improvement of OBO ontologies with automated Quality Control checks.","title":"Coordinated development of mutually compatible ontologies across the biomedical domain: The Open Biological and Biomedical Ontologies (OBO) Foundry."},{"location":"lesson/analysing-linked-data/#semantic-web-and-linked-data-things-you-should-have-heard-about","text":"The Semantic Web Layer Cake : A iconic, colourful graphic that describes the layered design of the semantic web, from URIs to Logic. Its not particularly useful, but as a Semantic Web Explorer, you should have seen it. Linked Data is mostly referred to as a \"method for publishing data\", a key concept in the Semantic Web domain, coined by Tim Berners Lee in 2006. Related concepts: Linked Data Principles : Use URIs as names for things Use HTTP URIs so that people can look up those names. When someone looks up a URI, provide useful information. Include links to other URIs. so that they can discover more things. 5-Star system make your stuff available on the Web (whatever format) under an open license make it available as structured data (e.g., Excel instead of image scan of a table) use non-proprietary formats (e.g., CSV instead of Excel) use URIs to denote things, so that people can point at your stuff link your data to other data to provide context FAIR data : Principles defined in 2016, somewhat orthogonal to Linked Data Principles. A nice tutorial, also going a bit more in depth into identifiers than what we did in this section, can be found here . The idea of FAIR data is probably more impactful in the biomedical and pharmaceutical world then the idea of Linked Data. While there are some (slighltly irritating) voices on the sidelines that say that \"It can't be FAIR if its not RDF\", it is probably true that a nicely formatted CSV file on the Web is at least as useful as a (hard to understand) RDF dump containing the same data. Worldwide collaborations between major pharmaceutical corporations promoting FAIR data, such as the Pistoia Alliance do mention Semantic Web Technologies in their White papers , but keep the jargon a bit more hidden from the general public. Data, according to the FAIR principles, should be: Findable (machine readable metadata, etc) Accessible (open authentication, authorisation) Interoperable (integrated with other data, closely related to controlled vocabularies and linked data) Reusable (metadata, license, provenance) World Wide Web Consortium (W3C) : The World Wide Web Consortium (W3C) is an international community that develops open standards, in particular many of those (but not all!) pertaining to the Semantic Web.","title":"Semantic Web and Linked Data: Things you should have heard about"},{"location":"lesson/analysing-linked-data/#the-ecosystem-of-linked-data-and-semantic-web-standards-technologies-and-research-areas","text":"In the following, we will list some of the technologies you may find useful, or will be forced to use, as a Semantic Data Engineer. Most of these standards will be covered in the subsequent weeks of this course. Standard Purpose Use case Web Ontology Language (OWL) Representing Knowledge in Biomedical Ontologies All OBO ontologies must be provided in OWL as well. Resource Description Framework (RDF) Model for data interchange. Triples, the fundamental unit of RDF, are ubiquitous on the Semantic Web SPARQL Query Language for RDF A standard query language for RDF and RDFS. Primary query language to interrogate RDF/RDFS/Linked Data on the Web. Simple Knowledge Organization System (SKOS) Another, more lightweight, knowledge organisation system in many ways competing with OWL. Not as widely used in the biomedical domain as OWL, but increasing uptake of \"matching\" vocabulary (skos:exactMatch, etc). RDF-star A key shortcoming of RDF is that, while I can in principle say everything about everything, I cannot directly talk about edges, for example to attribute provenance: \"microphthalmia, isolated, with coloboma 5 is kind of Mendelian disease\"--source: Wikipedia Use cases here . JSON-LD A method to encoding linked data in JSON format. (Very useful to at least know about). RDFa W3C Recommendation to embed rich semantic metadata in HTML (and XML). I have to admit - in 11 years Semantic Web Work I have not come across much use of RDFa in the biomedical domain. But @jamesaoverton is using it in his tools! A thorough overview of all the key standards and tools can be found on the Awesome Semantic Web repo. For a rough sense of current research trends it is always good to look at the accepted papers at one of the major conferences in the area. I like ISWC ( 2020 papers ), but for the aspiring Semantic Data Engineering in the biomedical sphere, it is probably a bit broad and theoretical. Other interesting specialised venues are the Journal of Biomedical Semantics and the International Conference on Biomedical Ontologies , but with the shift of the focus in the whole community towards Knowledge Graphs, other journals and conferences are becoming relevant. Here are a few key research areas, which are, by no means (!), exhaustive. How can we combine data and knowledge from different ontologies/knowledge graphs? Ontology/Knowledge graph alignment : How can we effectively link to ontologies, or knowledge graphs, together? Ontology merging : combine two ontologies by corresponding concepts and relations. Ontology matching: A sub-problem of ontology alignment, namely the problem of determining whether two terms (for example two diseases) from different ontologies should be linked together or not. How can we integrate data from unstructured and semistructured sources such as documents or spreadsheets? Named Entity Recognition (NER) : the process of identifying a named \"thing\" in a text. Entity linking : The task of associating a named entity, for example the result of a Named Entity Recognition algorithm, or the column of a spreadsheet, to a concept in an ontology. For example, the value \"Mendelian Disease\" is linked to the concept http://purl.obolibrary.org/obo/MONDO_0003847. Relationship extraction : Once you have identified the genes and diseases in your Pubmed abstracts, you will want to understand how they related to each other. Is the gene the \"basis in dysfunction of\" the disease? Or just randomly co-occurs in the sentence? Note: Many of the problems in this category are typically associated with the domain of Natural Language Processing rather than Semantic Web. How can we generate insight from semantically integrated data? Knowledge Graphs and Machine Learning Knowledge Graph Embeddings . The number one hype topic in recent years: How do you get from a graph of interrelated entities to a faithful representation in a vector space (basically numbers), so that Machine Learning algorithms can do their magic? Link predication : Based on what we know, which are the best drug targets for my rare disease of interest? Logical reasoning: While the research on deductive reasoning, at least the more \"hard-core\" Description Logic kind, seems to be a bit more quiet in recent years (maybe I am wrong here, I just see much less papers coming through my Google Scholar alerts now then I used to), there is still a lot going on in this domain: more efficient SPARQL engines, rule-based reasoning such as the recently commercialised RDFox reasoner and many more. Other research areas (not in any way exhaustive): Web decentralisation and privacy: Solid : Solid (Social Linked Data) is a web decentralization project led by Tim Berners-Lee, with the aim of true ownership of personal data and improved privacy. \"Pods\" are like secure personal web servers for data from which application can request data. Shape validation : It is very difficult to validate huge Knowledge Graphs of interrelated data efficiently (by validate we can mean a lot of things, such as making sure that your cat does not accidentally end up as someone's \"Mendelian Disease\"). Shape languages such as Shex and SHACL are poised to solve this problem, but the research is ongoing. New standards and tools: There is always someone proposing a new semantic standard for something or building a new kind of triple store, SPARQL extension or similar.","title":"The Ecosystem of Linked Data and Semantic Web: Standards, Technologies and Research Areas"},{"location":"lesson/analysing-linked-data/#typical-jobs-of-semantic-data-engineers-in-the-biomedical-domain","text":"It is useful to get a picture of the typical tasks a Semantic Data Engineer faces when building ontologies are Knowledge Graphs. In my experience, it is unlikely that any particular set of tools will work in all cases - most likely you will have to try and assemble the right toolchain for your use case and refine it over the lifetime of your project. The following are just a few points for consideration of tasks I regularly encountered - which may or may not overlap with the specific problems you will face.","title":"Typical Jobs of Semantic Data Engineers in the biomedical domain"},{"location":"lesson/analysing-linked-data/#finding-the-right-ontologies","text":"There are no simple answers here and it very heavily depends on your use cases. We are discussing some places to look for ontologies here , but it may also be useful to simply upload the terms you are interested in to a service like Zooma and see what the terms map to at a major database provider like EBI.","title":"Finding the right ontologies"},{"location":"lesson/analysing-linked-data/#finding-the-right-data-sources","text":"This is much harder still than it should have to be. Scientific databases are scattered across institutions that often do not talk to each other. Prepare for some significant work in researching the appropriate databases that could benefit your work, using Google and the scientific literature.","title":"Finding the right data sources"},{"location":"lesson/analysing-linked-data/#extending-existing-ontologies","text":"It is rare nowadays that you will have to develop an ontology entirely from scratch - most biomedical sub-domains will have some kind of reasonable ontology to build upon. However, there is often a great need to extend existing ontologies - usually because you have the need of representing certain concepts in much more detail, or your specific problem has not been modelled yet - think for example when how disease ontologies needed to be extended during the Coronavirus Crisis. Extending ontologies usually have two major facets: If at all possible you should seek to contribute new terms, synonyms and relationships to the ontologies you seek to extend directly. Here, you can use GitHub to make issues requesting new terms, but more boldly, you can also add new terms yourself. We will teach you how to do that in one of the next weeks. If the knowledge is considered out of scope for the ontology to be extended (for example because the terms are considered too detailed), then you will maintain your own \"branch\" of the ontology. Many tools such as the Ontology Development Kit and ROBOT can help you maintain such a branch but the general instinct should be: Make a public GitHub repo. Reach out to the developers of the main ontology Stay in touch and coordinate releases","title":"Extending existing ontologies"},{"location":"lesson/analysing-linked-data/#mapping-data-into-ontologiesknowledge-graphs","text":"Also sometimes more broadly referred to as \"data integration\", this problem involves a variety of tasks, such as: Named Entity Recognition . If you have a set of documents, such as PubMed abstracts or clinical notes, you may have to first identify the parts of speech that refer to clinical entities. Entity Linking : Once you have identified the biomedical entities of interest, you may want to link them to your existing knowledge graph. This process is sometimes called entity mapping or data mapping as well. Very often, this task is not fully automated. We have worked on projects where we used approaches to Entity Linking to suggest good mappings to users, which then had to confirm or reject them. It is also good to understand that not all entity linking must be vertical (i.e. between \"equivalent\" entities). Very often, there is no equivalent entity in your knowledge graph to map to, and here you need to decide whether to (a) create a new entity in the knowledge graph to map to or (b) map to a broader entity (for example \"microphthalmia, isolated, with coloboma 5\" to \"Mendelian Disease\"). What is more efficient / useful solely depends on your use case!","title":"Mapping data into ontologies/knowledge graphs"},{"location":"lesson/analysing-linked-data/#build-application-ontologies","text":"To make your data discoverable, it is often useful to extract a view from the ontologies you are using (for example, Gene Ontology, Disease Ontology) that only contains the terms and relationships of relevance to your data. We usually refer to this kind of ontology as an application ontology, or an ontology specific to your application, which will integrate subsets of other ontologies. This process will typically involve the following: Define a seed, or a set of terms you want to import from your external ontologies of interest. Extract relevant subsets from ontologies using this seed (for example using ROBOT extract ). Combine and potentially link these subsets together. Frameworks such as the Ontology Development Kit can help with this task, see for example the Coronavirus Vocabulary maintained by EBI.","title":"Build application ontologies"},{"location":"lesson/analysing-linked-data/#leverage-ontologies-and-knowledge-graphs-for-you-data-analysis-problems","text":"There are many ways your semantic data can be leveraged for data analysis, but in my experience, two are particularly central: Data grouping and search : make data about \"microphthalmia, isolated, with coloboma 5\" available when searching for data about \"Mendelian Disease\". Link prediction : Figure out what additional knowledge is hidden in your data that can drive your research (e.g. possible new therapies or drug targets).","title":"Leverage ontologies and knowledge graphs for you data analysis problems"},{"location":"lesson/analysing-linked-data/#additional-materials-and-resources","text":"The open courses of the Hasso Plattner Institute (HPI) offer introductions into the concepts around Linked Data, Semantic Web and Knowledge Engineering. There are three courses of relevance to this weeks topics, all of which overlap significantly. Knowledge Engineering and the Web of Data : The oldest (2015), of the courses, but the most thorough when it comes to logical foundations, semantics and OWL. We will come back to this course in Weeks 4 and 5. Linked Data Engineering : Overlaps a lot with the Knowledge Engineering and the Web of Data course, with a bit more RDF/Linked Data focus. Knowledge Graphs : The most up-to-date of the three courses (2020), and will be referred to again in Week 12 of our course here.","title":"Additional materials and resources"},{"location":"lesson/analysing-linked-data/#contributors","text":"add name/ORCID here","title":"Contributors"},{"location":"lesson/automating-ontology-workflows/","text":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Ontology Pipelines What is delivered as part of the course \u00b6 In this course, you will learn the basics of automation in and around the OBO ontology world - and beyond. The primary goal is to enable ontology pipeline developers to plan the automation of their ontology workflows and data pipelines, but some of the materials are very general and apply to scientific computing more widely. The course serves also as a prerequisite for advanced application ontology development. Learning objectives \u00b6 Unix shell make Advanced Git, GitHub Preparation \u00b6 Please complete the following tutorials. The Unix Shell (~4.5 hours) Version Control with Git (~3 hours) Introduction to GitHub Tutorials \u00b6 Thinking \"Automation\" \u00b6 By: James Overton Automation is part of the foundation of the modern world. The key to using and building automation is a certain way of thinking about processes, how they can be divided into simple steps, and how they operate on inputs and outputs that must be exactly the same in some respects but different in others. In this article I want to make some basic points about automation and how to think about it. The focus is on automation with software and data, but not on any particular software or data. Some of these points may seem too basic, especially for experienced programmers, but in 20+ years of programming I've never seen anybody lay out these basic points in quite this way. I hope it's useful. The Basics \u00b6 \"automatos\" from the Greek: \"acting of itself\" Automation has two key aspects: make the input the same process the inputs in the same way The second part is more visible, and tends to get more attention, but the first part is at least as important. While automation makes much of the modern world possible, it is not new, and there are serious pitfalls to avoid. No system is completely automatic, so it's best to think of automation on a spectrum, and starting thinking about automation at the beginning of a new project. Examples of Automation \u00b6 To my mind, the word \"automation\" brings images of car factories, with conveyor belts and robotic arms moving parts and welding them together. Soon they might be assembling self-driving (\"autonomous\") cars. Henry Ford is famous for making cars affordable by building the first assembly lines, long before there were any robots. The essential steps for Ford were standardizing the inputs and the processes to get from raw materials to a completed car. The history of the 20th century is full of examples of automation in factories of all sorts. Automation was essential to the Industrial Revolution, but it didn't start then. We can look to the printing press. We can look to clocks, which regimented lives in monasteries and villages. We can think of recipes, textiles, the logistics of armies, advances in agriculture, banking, the administration of empires, and so on. The scientific revolution was built on repeatable experiments published in letters and journal articles. I think that the humble checklist is also an important relative of automation. Automation is not new, but it's an increasingly important part of our work and our lives. Software Automation is Special \u00b6 Software is almost always written as source code in text files that are compiled and/or interpreted as machine code for a specific set of hardware. Software can drive machines of all sorts, but a lot of software automation stays inside the computer, working on data in files and databases, and across networks. We'll be focused on this kind of software automation, transforming data into data. The interesting thing about this is that source code is a kind of data, so there are software automation workflows that operate on data that defines software. The upshot is that you can have automation that modifies itself. Doing this on a large scale introduces a lot of complexity, but doing it on a small scale can be a clean solution to certain problems. Another interesting thing about software is that once we solve an automation problem once we can copy that solution and apply it again and again for almost zero cost. We don't need to build a new factory or a new threshing machine. We can just download a program and run it. Henry Ford could make an accurate estimate of how long it would take to build a car on his assembly line, but software development is not like working on the assembly line, and estimating time and budget for software development is notoriously hard. I think this is because software developers aren't just executing automation, they're building new automation for each new project. Although we talk about \"bit rot\", and software does require maintenance of a sort, software doesn't break down or wear out in the same ways that physical machines do. So while the Industrial Revolution eliminated many jobs, it also created different jobs, building and maintaining the machines. It's not clear that software automation will work the same way. Software automation is special because it can operate on itself, and once complete can be cheaply copied. Software development is largely about building automated systems of various sorts, usually out of many existing pieces. We spend most of our time building new systems, or modifying an existing system to handle new inputs, or adapting existing software to a new use case. The Dangers of Automation \u00b6 To err is human; to really foul things up requires a computer. An obvious danger of automation is that machines are faster than humans, so broken automation can often do more damage more quickly than a human can. A related problem is that humans usually have much more context and depth of experience, which we might call \"common sense\", and a wider range of sensory inputs than most automated systems. This makes humans much better at recognizing that something has gone wrong with a process and that it's time to stop. New programmers soon learn that a simple program that performs perfectly when the input is in exactly the right format, becomes a complex program once it's updated to handle a wide range of error conditions. In other words, it's almost always much harder to build automation that can gracefully handler errors and problems than it is to automate just the \"happy path\". Old programmers have learned through bitter experience that it's often practically impossible to predict all the things that can go wrong with an automated system in practise. I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. -- Abraham Maslow A less obvious danger of automation comes from the sameness requirement. When you've built a great piece of automation, perfectly suited to inputs of a certain type, it's very tempting to apply that automation more generally. You start paying too much attention to how things are the same, and not enough attention to their differences. You may begin to ignore important differences. You may surrender your common sense and good judgment, to save yourself the work of changing the automated system or making an exception. Bureaucracies are a form of automation. Everyone has had a bad experience filling out some form that ignores critical information, and with some bureaucrat who would not apply common sense and make an exception. Keep all this in mind as you build automated systems: a broken machine can do a lot of damage very quickly, and a system built around bad assumptions can do a lot of hidden damage. A Spectrum of Automation \u00b6 Let's consider a simple case of automation with software, and build from the most basic sort of automation to a full-fledged system. Say you have a bunch of text files in a directory, each containing minutes from meetings that we had together over the years. You can remember that I talked about a particular software package that might solve a problem that you just discovered, but you can't remember the name. 1. Ad Hoc \u00b6 The first thing you try is to just search the directory. On a Mac you would open the Finder, navigate to the directory, and type \"James\" into the search bar. Unfortunately that gives too many results: all the files with the minutes for a meeting where I said something. The next thing to do is double click some text files, which would open them in Text Edit program, and skim them. You might get lucky! You know that I the meeting was in 2019, so you can try and filter for files modified in that year. Unfortunately the files have been updated at different times, so the file dates aren't useful. Now if each file was named with a consistent pattern, including the meeting date, then it would be simple to filter for files with \"2019\" in the name. This isn't automation, but it's the first step in the right direction. Consistent file names are one way to make inputs the same so that you can process them in the same way. Let's say it works: you filter for files from 2019 with \"James\" in them, skim a few, and find a note where I recommended using Pandoc to convert between document formats. Mission accomplished! 2. Notes \u00b6 Next week you need to do something very similar: Becky mentioned a website where you can find an important dataset. It's basically the same problem with different inputs. If you remember exactly what you did last time, then you can get the job done quickly. As the job gets more complicated and more distant in time, and as you find yourself doing similar tasks more often, it's nice to have notes about what you did and how you did it. If I'm using a graphical user interface (GUI) then for each step I'll note the program I used, and the menu item or button I clicked, e.g. \"Preferences > General > Font Size\", or \"Search\" or \"Run\". If I'm using a command-line interface (CLI) then I'll copy-paste the commands into my notes. I often keep informal notes like this in a text file in the relevant directory. I name the file \"notes.txt\". A \"README\" file is similar. It's used to describe the contents of a directory, often saying which files are which, or what the column headers for a given table mean. Often the task is more complicated and requires one or more pieces of software that I don't use every day. If there's relevant documentation, I'll put a link to it in my notes, and then a short summmary of exactly what I did. In this example I look in the directory of minutes and see my \"notes.txt\" file. I read that and remember how I filtered on \"2019\" and searched for \"James\". This time I filter on \"2020\" and search for \"Becky\", and I find the website for the dataset quickly enough. As a rule of thumb, it might take you three times longer to find your notes file, write down the steps you took, and provide a short description, than it would to just do the job without taking notes. When you're just taking notes for yourself, this often feels like a waste of time (you'll remember, right?!), and sometimes it is a bit of a waste. If you end up using your notes to help with similar tasks in the future, then this will likely be time well spent. As a rule of thumb, it might take three times longer to write notes for a broader audience than notes for just yourself. This is because you need to take into account the background knowledge of your reader, including her skills and assumptions and context, and especially the possible misunderstandings that you can try to avoid with careful writing. I often start with notes for just myself and then expand them for a wider audience only when needed. 3. Checklist \u00b6 When tasks get more complicated or more important then informal notes are not enough. The next step on the spectrum of automation is the humble checklist. The most basic checklists are for making sure that each item has been handled. Often the order isn't important, but lists are naturally ordered from top to bottom, and in many cases that order is useful. For example, my mother lays out her shopping lists in the order of the aisles in her local grocery store, making it easier to get each item and check it off without skipping around and perhaps having to backtrack. I think of a checklist as a basic form of automation. It's like a recipe. It should lay out the things you need to start, then proceed through the required steps in enough detail that you can reproduce them. In some sense, by using the checklist you are becoming the \"machine\". You are executing an algorithm that should take you from the expected inputs to the expected output. Humble as the checklist is, there's a reason that astronauts, pilots, and surgical teams live by their checklists. Even when the stakes are not so high, it's often nice to \"put your brain on autopilot\" and just work the checklist without having to remember and reconsider the details of each step. A good checklist is more focused than a file full of notes. A checklist has a goal at the end. It has specific starting conditions. The steps have been carefully considered, so that they have the proper sequence, and none are missing. Perhaps most importantly, a checklist helps you break a complex task down into simple parts. If one of the parts is still too complex, then break it down again into a nested checklist (really a sort of tree structure). Checklists sometimes include another key element of automation: conditionals. A shopping list might say \"if there's a sale on crackers, then buy three boxes\". If-then conditions let our automated systems adapt to circumstances. The \"then\" part is just another step, but the \"if\" part is a little different. It's a test to determine whether a condition holds. We almost always want the result of the test to be a simple True or False. Given a bunch of inputs, some of which pass the test and some of which fail it, we can think of the test as determining some way in which all the things that pass are the same and all the things that fail are the same . Programmers will also be familiar with more complex conditionals such as if-then-else, if-elseif-else, and \"case\", which divide process execution across multiple \"branches\". As a rule of thumb, turning notes into a checklist will likely take at least three times as long as simply writing the notes. If the checklist is for a wider audience, expect it to take three times as long to write, for the same reasons mentioned above for notes. If a task is simple and I can hold all the steps in my head, and I can finish it in one sitting without distractions, then I won't bother with a checklist. But more and more I find myself writing myself a checklist before I begin any non-trivial tasks. I use bullet points in my favourite text editor, or sometimes the Notes app on my iPhone. I lay out the steps in the expected order, and I check them off as I go. Sometimes I start making the checklist days before I need it, so I have lots of time to think about it and improve it. If there's a job that I'm worried about, breaking it down into smaller pieces usually helps to make the job feel more manageable. Actually, I try to start every workday by skimming my (long) To Do list, picking the most important tasks, and making a checklist for what I want to get done by quitting time. 3. Checkscript \u00b6 \"Checkscript\" is a word that I think I made up, based on insights from a couple of sources, primarily this blog post on \"Do-nothing scripting: the key to gradual automation\" This is where \"real\" automation kicks in, writing \"real\" code and stuff, but hopefully you'll see that it's just one more step on the spectrum of automation that I'm describing. The notes and checklists we've been discussing are just text in your favourite text editor. A checkscript is a program. It can be written in whatever programming language you prefer. I'll give examples in Posix Shell, but that blog post uses Python, and it really doesn't matter. You start with a checklist (in your mind at least). The first version of your program should just walk you through your checklist. The program should walk you through each step of your checklist, one by one. That's it. Here's a checkscript based on the example above. It just prints the first step ( echo ), waits for you to press any key ( read ), then prints the next step, and so on. ###!/bin/sh echo \"1. Use Finder to filter for files with '2019' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for 'James'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" So far this is just a more annoying way to use a checklist. The magic happens once you break the steps down into small enough pieces and realize that you know how to tell the computer to do some of the steps instead of doing them all yourself. For example, you know that the command-line tool grep is used for searching the contents of files, and that you can use \"fileglob\"s to select just the files that you want to search, and that you can send the output of grep to another file to read in your favourite text editor. Now you know how to automate the first two steps. The computer can just do that work without waiting for you: ###!/bin/sh grep \"James\" *2019* > search_results.txt echo \"1. Open 'search_results.txt' in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" Before we were using the Finder, and it is possible to write code to tell the Finder to filter and seach for files. The key advantage of grep here is that we send the search results to another file that we can read now or save for later. This is also a good time to mention the advantage of text files over word processor files. If the minutes were stored in Word files, for example, then Finder could probably search them and you could use Word to read them, but you wouldn't be able to use grep or easily output the results to another file. Unix tools such as grep treat all text files the same, whether they're source code or meeting minutes, which means that these tools work pretty much the same on any text file. By keeping your data in Word you restrict yourself to a much smaller set of tools and make it harder to automate you work with simple scripts like this one. Even if you can't get the computer to run any of the steps for you automatically, a checkscript can still be useful by using variables instead of repeating yourself: ###!/bin/sh FILE_PATTERN=\"*2019*\" FILE_CONTENTS=\"James\" echo \"1. Use Finder to filter for files with '${FILE_PATTERN}' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"Done!\" Now if I want to search for \"Becky\" I can just change the FILE_CONTENTS variable in one place. I find this especially useful for dates and version numbers. This is pretty simple for a checkscript, with very few steps. A more realistic example would be if there were many directories containing the minutes of many meetings, maybe in different file formats and with different naming conventions. In order to be sure that we're searching all of them we might need a longer checkscript. Writing and using a checkscript instead of a checklist will likely take (you guessed it) about three times as long. But the magic of the checkscript is in the title of the blog post I mentioned: \"gradual automation\". Once you have a checkscript, you can run through it all manually, but you can also automate bits a pieces of the task, saving yourself time and effort next time. 5. Script \u00b6 A \"script\" is a kind of program that's easy to edit and run. There are technical distinctions to be made between \"compiled\" programs and \"interpreted\" programs, but they turn out to be more complicated and less helpful than they seem at first. Technically, a checkscript is just a script that waits for you to do the hard parts. In this section I want to talk about \"fully automated\" or \"standalone\" scripts that you just provide some input and execute. Most useful programs are useful because they call other programs (in the right ways). I like shell scripts because they're basically just commands that are copied and pasted from work I was doing on the command-line. It's really easy to call other programs. To continue our example, say that our minutes were stored in Word files. There are Python libraries for this, such as python-docx . You can write a little script using this library that works like grep to search for specified text in selected files, and output the results to a search results file. As you add more and more functionality to a script it can become unwieldy. Scripts work best when they have a simple \"flow\" from beginning to end. They may have some conditionals and some loops, but once you start seeing nested conditionals and loops, then your script is doing too much. There are two main options to consider: break your script into smaller, simpler scripts build a specialized tool: the next step on the spectrum of automation The key difference between a checkscript and a \"standalone\" script is handling problems. A checkscript relies on you to supervise it. A standalone script is expected to work properly without supervision. So the script has to be designed to handle a wider range of inputs and fail gracefully when it gets into trouble. This is a typical case of the \"80% rule\": the last 20% takes 80% of the time. As a rule of thumb, expect it to take three times as long to write a script that can run unsupervised than it takes you to write a checkscript that does \"almost\" the same thing. 6. Specialized Tool \u00b6 When your script needs nested conditionals and loops, then it's probably time to reach for a programming language that's designed to write code \"in the large\". Some languages such as Python can make a pretty smooth transition from a script in a single file to a set of files in a module, working together nicely. You might also choose another language that can provide better performance or efficiency. It's not just the size and the logical complexity of your script, consider its purpose . The specialized tools that I have in mind have a clear purpose that helps guide their design. This also makes them easier to reuse across multiple projects. I often divide my specialized tools into two parts: a library and a command-line interface. The library can be used in other programs, and contains the most distinctive and important functionality. But the command-line interface is essential, because it lets me use my specialized tool in the shell and in scripts, so I can build more automation on top of it. Writing a tool in Java or C++ or Rust usually takes longer than a script in shell or Python because there are more details to worry about such as types and efficient memory management. In return you usually get more reliability and efficiency. But as a rule of thumb, expect it to take three times as long to write a specialized tool than it would to \"just\" write the script. On the other hand, if you already have a script that does most of what you want, and you're already familiar with the target you are moving to, then it can be fairly straightforward to translate from the script to the specialized tool. That's why it's often most efficient to write a prototype script first, do lots of quick experiments to explore the design space, and when you're happy with the design then start on the \"production\" version. 7. Workflow \u00b6 The last step in the spectrum of automation is to bring together all your scripts into a single \"workflow\". My favourite tool for this is the venerable Make . A Makefile is essentially a bunch of small scripts with their input and output files carefully specified. When you ask Make to build a given output file, it will look at the whole tree of scripts, figure out which input files are required to build your requested output file, then which files are required to build those files, and so on until it has determined a sequence of steps. Make is also smart enough to check whether some of the dependencies are already up-to-date, and can skip those steps. Looking at a Makefile you can see everything broken down into simple steps and organized into a tree, through which you can trace various paths. You can make changes at any point, and run Make again to update your project. I've done this all so many times that now I often start with a Makefile in an empty directory and build from there. I try experiments on the command line. I make notes. I break the larger task into parts with a checklist. I automate the easy parts first, and leave some parts as manual steps with instructions. I write little scripts in the Makefile . I write larger scripts in the src/ directory. If these get too big or complex, I start thinking about building a specialized tool. (And of course, I store everything in version control.) It takes more time at the beginning, but I think that I usually save time later, because I have a nice place to put everything from the start. In other words, I start thinking about automation at the very beginning of the project, assuming from the start that it will grow, and that I'll need to go back and change things. With a mindset for automation, from the start I'm thinking about how the inputs I care about are the same and different, which similarities I can use for my tests and code, and which differences are important or unimportant. Conclusion \u00b6 In the end, my project isn't ever completely automated. It doesn't \"act of itself\". But by making everything clear and explicit I'm telling the computer how to do a lot of the work and other humans (or just my future self) how to do the rest of it. The final secret of automation, especially when it comes to software and data, is communication : expressing things clearly for humans and machines so they can see and do exactly what you did. Scientific Computing: An Overview \u00b6 By: James Overton By \"scientific computing\" we mean using computers to help with key aspect of science such as data collection, cleaning, interpretation, analysis, and visualization. Some people use \"scientific computing\" to mean something more specific, focusing on computational modelling or computationally intensive analysis. We'll be focusing on more general and day-to-day topics: how can a scientist make best use of a computer to do their work well? These three things apply to lots of fields, but are particularly important to scientists: reliability reproducibility communication It should be no surprise that automation can help with all of these. When working properly, computers make fewer mistakes than people, and the mistakes they do make are more predictable. If we're careful, our software systems can be easily reproduced, which means that an entire data analysis pipeline can be copied and run by another lab to confirm the results. And scientific publications are increasingly including data and code as part of the review and final publication process. Clear code is one of the best ways to communicate detailed steps. Automation is critical to scientific instruments and experiments, but we'll focus on the data processing and analysis side: after the data has been generated, how should you deal with it. Basic information management is always important: community standard file formats consistent file naming documentation, READMEs backups version control More advanced data management is part of this course: consistent use of versioned software reference data terminology controlled vocabularies data dictionaries ontologies Some simple rules of thumb can help reduce complexity and confusion: make space firm foundations one-way data flow plan for change test from the start documentation is also for you Make Space \u00b6 When starting a new project, make a nice clean new space for it. Try for that \"new project smell\". I always create a new directory on my computer. I almost always create a new GitHub repository. I usually create a README and a Makefile, right away. It's not always clear when a project is really \"new\" or just a new phase of an old project. But try to clear some space to make a fresh start. Firm Foundations \u00b6 A lot of data analysis starts with a reference data set. It might be a genome or a proteome. It might be a corpus. It might be a set of papers or data from those papers. Start by finding that data and selecting a particular version of it. Write that down clearly in your notes. If possible, include a unique identifier such as a ( persistent ) URL or DOI. If that's not possible, write down the steps you took. If the data isn't too big, keep a copy of it in your fresh new project directory. If the data is a bit too big, keep a compressed copy in a zip or gz file. A lot of software is perfectly happy to read directly from compressed files, and you can compress or uncompress data using piped commands in your shell or script. If the data is really too big, then be extra careful to keep notes on exactly where you can find it again. Consider storing just the hashes of the big files, so you can confirm that they have exactly the same contents. If you know from the start that you will need to compare your results with someone else's, make sure that you're using the same reference data that they are. This may require a conversation, but trust me that it's better to have this conversation now than later. One-Way Data Flow \u00b6 It's much easier to think about processes that flow in one direction. Branches are a little trickier, but usually fine. The real trouble comes with loops. Once a process loops back on itself it's much more difficult to reason about what's happening. Loops are powerful, but with great power comes great responsibility. Keep the systems you design as simple as possible (but no simpler). In practical terms: Try not to read then write to the same file. If you have to, try to append rather than overwrite. This is one reason why I prefer tables on disk to databases. Don't hesitate to write intermediate files. These are very useful for testing and debugging. When you're \"finished\" you can comment out these steps. Plan for Change \u00b6 It's very tempting: you could automate this step, or you could just do it manually. It might take three times as long to automate it, right? So you can save yourself some precious time by just opening Excel and \"fixing\" things by hand. Sometimes that bet will pay off, but I lose that bet most of the time. I tend to realize my mistake only at the last minute. The submission deadline is tomorrow but the core lab \"fixed\" something and they have a new version of the dataset that we need to use for the figures. Now I really don't have time to automate, so I'm up late clicking through Excel again and hoping that I remembered to redo all the changes that I made last time. Automating the process would have actually saved me time, but more importantly it would have avoided a lot of stress. By now I should know that the dataset will almost certainly be revised at the last minute. If I have the automation set up, then I just update the data, run the automation again, and quickly check the results. Test from the Start \u00b6 Tests are another thing that take time to implement. One of the key benefits to tests is (again) communication. When assessing or trying out some new piece of software I often look to the test files to see examples of how the code is really used, and the shape of the inputs and outputs. There's a spectrum of tests that apply to different parts of your system: unit tests: individual functions and methods regression tests: ensure that fixed bugs do not reappear integration tests: end-to-end functionality performance tests: system speed and resource usage acceptance tests: whether the overall system meets its design goals Tests should be automated. The test suite should either pass or fail, and if it fails something needs to be fixed before any more development is done. The automated test suite should run before each new version is committed to version control, and ideally more often during development. Tests come with costs: development cost of writing the tests time and resources spent running the tests maintenance costs of updating the tests The first is obvious but the other two often more important. A slow test suite is annoying to run, and so it won't get run. A test suite that's hard to update won't get updated, and then failures will be ignored, which defeats the entire purpose. Documentation is also for You \u00b6 I tend to forget how bad a memory I have. In the moment, when I'm writing brilliant code nothing could be more obvious than the perfect solution that is pouring forth from my mind all over my keyboard. But when I come back to that code weeks, months, or years later, I often wonder what the heck I was thinking. We think about the documentation we write as being for other people, but for a lot of small projects it's really for your future self. Be kind to your future self. They may be even more tired, even more stressed than you are today. There's a range of different forms of documentation, worth a whole discussion of its own. I like this four-way distinction : tutorials: getting started, basic concepts, an overview how-to guides: how to do common tasks explanation: why does it work this way? reference: looking up the details You don't need all of these for your small project, but consider a brief explanation of why it works the way it does (aimed at a colleague who knows your field well), and some brief notes on how-to do the stuff this project is for. These could both go in the README of a small project. Additional materials and resources \u00b6 A whirlwind introduction to the command line Programming with Python Oh My Git! Contributors \u00b6 Nico Matentzoglu James Overton","title":"Automating Ontology Workflows"},{"location":"lesson/automating-ontology-workflows/#automating-ontology-development-workflows-make-shell-and-automation-thinking","text":"","title":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking"},{"location":"lesson/automating-ontology-workflows/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/automating-ontology-workflows/#prerequisites","text":"Ontology Pipelines","title":"Prerequisites"},{"location":"lesson/automating-ontology-workflows/#what-is-delivered-as-part-of-the-course","text":"In this course, you will learn the basics of automation in and around the OBO ontology world - and beyond. The primary goal is to enable ontology pipeline developers to plan the automation of their ontology workflows and data pipelines, but some of the materials are very general and apply to scientific computing more widely. The course serves also as a prerequisite for advanced application ontology development.","title":"What is delivered as part of the course"},{"location":"lesson/automating-ontology-workflows/#learning-objectives","text":"Unix shell make Advanced Git, GitHub","title":"Learning objectives"},{"location":"lesson/automating-ontology-workflows/#preparation","text":"Please complete the following tutorials. The Unix Shell (~4.5 hours) Version Control with Git (~3 hours) Introduction to GitHub","title":"Preparation"},{"location":"lesson/automating-ontology-workflows/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/automating-ontology-workflows/#thinking-automation","text":"By: James Overton Automation is part of the foundation of the modern world. The key to using and building automation is a certain way of thinking about processes, how they can be divided into simple steps, and how they operate on inputs and outputs that must be exactly the same in some respects but different in others. In this article I want to make some basic points about automation and how to think about it. The focus is on automation with software and data, but not on any particular software or data. Some of these points may seem too basic, especially for experienced programmers, but in 20+ years of programming I've never seen anybody lay out these basic points in quite this way. I hope it's useful.","title":"Thinking \"Automation\""},{"location":"lesson/automating-ontology-workflows/#the-basics","text":"\"automatos\" from the Greek: \"acting of itself\" Automation has two key aspects: make the input the same process the inputs in the same way The second part is more visible, and tends to get more attention, but the first part is at least as important. While automation makes much of the modern world possible, it is not new, and there are serious pitfalls to avoid. No system is completely automatic, so it's best to think of automation on a spectrum, and starting thinking about automation at the beginning of a new project.","title":"The Basics"},{"location":"lesson/automating-ontology-workflows/#examples-of-automation","text":"To my mind, the word \"automation\" brings images of car factories, with conveyor belts and robotic arms moving parts and welding them together. Soon they might be assembling self-driving (\"autonomous\") cars. Henry Ford is famous for making cars affordable by building the first assembly lines, long before there were any robots. The essential steps for Ford were standardizing the inputs and the processes to get from raw materials to a completed car. The history of the 20th century is full of examples of automation in factories of all sorts. Automation was essential to the Industrial Revolution, but it didn't start then. We can look to the printing press. We can look to clocks, which regimented lives in monasteries and villages. We can think of recipes, textiles, the logistics of armies, advances in agriculture, banking, the administration of empires, and so on. The scientific revolution was built on repeatable experiments published in letters and journal articles. I think that the humble checklist is also an important relative of automation. Automation is not new, but it's an increasingly important part of our work and our lives.","title":"Examples of Automation"},{"location":"lesson/automating-ontology-workflows/#software-automation-is-special","text":"Software is almost always written as source code in text files that are compiled and/or interpreted as machine code for a specific set of hardware. Software can drive machines of all sorts, but a lot of software automation stays inside the computer, working on data in files and databases, and across networks. We'll be focused on this kind of software automation, transforming data into data. The interesting thing about this is that source code is a kind of data, so there are software automation workflows that operate on data that defines software. The upshot is that you can have automation that modifies itself. Doing this on a large scale introduces a lot of complexity, but doing it on a small scale can be a clean solution to certain problems. Another interesting thing about software is that once we solve an automation problem once we can copy that solution and apply it again and again for almost zero cost. We don't need to build a new factory or a new threshing machine. We can just download a program and run it. Henry Ford could make an accurate estimate of how long it would take to build a car on his assembly line, but software development is not like working on the assembly line, and estimating time and budget for software development is notoriously hard. I think this is because software developers aren't just executing automation, they're building new automation for each new project. Although we talk about \"bit rot\", and software does require maintenance of a sort, software doesn't break down or wear out in the same ways that physical machines do. So while the Industrial Revolution eliminated many jobs, it also created different jobs, building and maintaining the machines. It's not clear that software automation will work the same way. Software automation is special because it can operate on itself, and once complete can be cheaply copied. Software development is largely about building automated systems of various sorts, usually out of many existing pieces. We spend most of our time building new systems, or modifying an existing system to handle new inputs, or adapting existing software to a new use case.","title":"Software Automation is Special"},{"location":"lesson/automating-ontology-workflows/#the-dangers-of-automation","text":"To err is human; to really foul things up requires a computer. An obvious danger of automation is that machines are faster than humans, so broken automation can often do more damage more quickly than a human can. A related problem is that humans usually have much more context and depth of experience, which we might call \"common sense\", and a wider range of sensory inputs than most automated systems. This makes humans much better at recognizing that something has gone wrong with a process and that it's time to stop. New programmers soon learn that a simple program that performs perfectly when the input is in exactly the right format, becomes a complex program once it's updated to handle a wide range of error conditions. In other words, it's almost always much harder to build automation that can gracefully handler errors and problems than it is to automate just the \"happy path\". Old programmers have learned through bitter experience that it's often practically impossible to predict all the things that can go wrong with an automated system in practise. I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. -- Abraham Maslow A less obvious danger of automation comes from the sameness requirement. When you've built a great piece of automation, perfectly suited to inputs of a certain type, it's very tempting to apply that automation more generally. You start paying too much attention to how things are the same, and not enough attention to their differences. You may begin to ignore important differences. You may surrender your common sense and good judgment, to save yourself the work of changing the automated system or making an exception. Bureaucracies are a form of automation. Everyone has had a bad experience filling out some form that ignores critical information, and with some bureaucrat who would not apply common sense and make an exception. Keep all this in mind as you build automated systems: a broken machine can do a lot of damage very quickly, and a system built around bad assumptions can do a lot of hidden damage.","title":"The Dangers of Automation"},{"location":"lesson/automating-ontology-workflows/#a-spectrum-of-automation","text":"Let's consider a simple case of automation with software, and build from the most basic sort of automation to a full-fledged system. Say you have a bunch of text files in a directory, each containing minutes from meetings that we had together over the years. You can remember that I talked about a particular software package that might solve a problem that you just discovered, but you can't remember the name.","title":"A Spectrum of Automation"},{"location":"lesson/automating-ontology-workflows/#1-ad-hoc","text":"The first thing you try is to just search the directory. On a Mac you would open the Finder, navigate to the directory, and type \"James\" into the search bar. Unfortunately that gives too many results: all the files with the minutes for a meeting where I said something. The next thing to do is double click some text files, which would open them in Text Edit program, and skim them. You might get lucky! You know that I the meeting was in 2019, so you can try and filter for files modified in that year. Unfortunately the files have been updated at different times, so the file dates aren't useful. Now if each file was named with a consistent pattern, including the meeting date, then it would be simple to filter for files with \"2019\" in the name. This isn't automation, but it's the first step in the right direction. Consistent file names are one way to make inputs the same so that you can process them in the same way. Let's say it works: you filter for files from 2019 with \"James\" in them, skim a few, and find a note where I recommended using Pandoc to convert between document formats. Mission accomplished!","title":"1. Ad Hoc"},{"location":"lesson/automating-ontology-workflows/#2-notes","text":"Next week you need to do something very similar: Becky mentioned a website where you can find an important dataset. It's basically the same problem with different inputs. If you remember exactly what you did last time, then you can get the job done quickly. As the job gets more complicated and more distant in time, and as you find yourself doing similar tasks more often, it's nice to have notes about what you did and how you did it. If I'm using a graphical user interface (GUI) then for each step I'll note the program I used, and the menu item or button I clicked, e.g. \"Preferences > General > Font Size\", or \"Search\" or \"Run\". If I'm using a command-line interface (CLI) then I'll copy-paste the commands into my notes. I often keep informal notes like this in a text file in the relevant directory. I name the file \"notes.txt\". A \"README\" file is similar. It's used to describe the contents of a directory, often saying which files are which, or what the column headers for a given table mean. Often the task is more complicated and requires one or more pieces of software that I don't use every day. If there's relevant documentation, I'll put a link to it in my notes, and then a short summmary of exactly what I did. In this example I look in the directory of minutes and see my \"notes.txt\" file. I read that and remember how I filtered on \"2019\" and searched for \"James\". This time I filter on \"2020\" and search for \"Becky\", and I find the website for the dataset quickly enough. As a rule of thumb, it might take you three times longer to find your notes file, write down the steps you took, and provide a short description, than it would to just do the job without taking notes. When you're just taking notes for yourself, this often feels like a waste of time (you'll remember, right?!), and sometimes it is a bit of a waste. If you end up using your notes to help with similar tasks in the future, then this will likely be time well spent. As a rule of thumb, it might take three times longer to write notes for a broader audience than notes for just yourself. This is because you need to take into account the background knowledge of your reader, including her skills and assumptions and context, and especially the possible misunderstandings that you can try to avoid with careful writing. I often start with notes for just myself and then expand them for a wider audience only when needed.","title":"2. Notes"},{"location":"lesson/automating-ontology-workflows/#3-checklist","text":"When tasks get more complicated or more important then informal notes are not enough. The next step on the spectrum of automation is the humble checklist. The most basic checklists are for making sure that each item has been handled. Often the order isn't important, but lists are naturally ordered from top to bottom, and in many cases that order is useful. For example, my mother lays out her shopping lists in the order of the aisles in her local grocery store, making it easier to get each item and check it off without skipping around and perhaps having to backtrack. I think of a checklist as a basic form of automation. It's like a recipe. It should lay out the things you need to start, then proceed through the required steps in enough detail that you can reproduce them. In some sense, by using the checklist you are becoming the \"machine\". You are executing an algorithm that should take you from the expected inputs to the expected output. Humble as the checklist is, there's a reason that astronauts, pilots, and surgical teams live by their checklists. Even when the stakes are not so high, it's often nice to \"put your brain on autopilot\" and just work the checklist without having to remember and reconsider the details of each step. A good checklist is more focused than a file full of notes. A checklist has a goal at the end. It has specific starting conditions. The steps have been carefully considered, so that they have the proper sequence, and none are missing. Perhaps most importantly, a checklist helps you break a complex task down into simple parts. If one of the parts is still too complex, then break it down again into a nested checklist (really a sort of tree structure). Checklists sometimes include another key element of automation: conditionals. A shopping list might say \"if there's a sale on crackers, then buy three boxes\". If-then conditions let our automated systems adapt to circumstances. The \"then\" part is just another step, but the \"if\" part is a little different. It's a test to determine whether a condition holds. We almost always want the result of the test to be a simple True or False. Given a bunch of inputs, some of which pass the test and some of which fail it, we can think of the test as determining some way in which all the things that pass are the same and all the things that fail are the same . Programmers will also be familiar with more complex conditionals such as if-then-else, if-elseif-else, and \"case\", which divide process execution across multiple \"branches\". As a rule of thumb, turning notes into a checklist will likely take at least three times as long as simply writing the notes. If the checklist is for a wider audience, expect it to take three times as long to write, for the same reasons mentioned above for notes. If a task is simple and I can hold all the steps in my head, and I can finish it in one sitting without distractions, then I won't bother with a checklist. But more and more I find myself writing myself a checklist before I begin any non-trivial tasks. I use bullet points in my favourite text editor, or sometimes the Notes app on my iPhone. I lay out the steps in the expected order, and I check them off as I go. Sometimes I start making the checklist days before I need it, so I have lots of time to think about it and improve it. If there's a job that I'm worried about, breaking it down into smaller pieces usually helps to make the job feel more manageable. Actually, I try to start every workday by skimming my (long) To Do list, picking the most important tasks, and making a checklist for what I want to get done by quitting time.","title":"3. Checklist"},{"location":"lesson/automating-ontology-workflows/#3-checkscript","text":"\"Checkscript\" is a word that I think I made up, based on insights from a couple of sources, primarily this blog post on \"Do-nothing scripting: the key to gradual automation\" This is where \"real\" automation kicks in, writing \"real\" code and stuff, but hopefully you'll see that it's just one more step on the spectrum of automation that I'm describing. The notes and checklists we've been discussing are just text in your favourite text editor. A checkscript is a program. It can be written in whatever programming language you prefer. I'll give examples in Posix Shell, but that blog post uses Python, and it really doesn't matter. You start with a checklist (in your mind at least). The first version of your program should just walk you through your checklist. The program should walk you through each step of your checklist, one by one. That's it. Here's a checkscript based on the example above. It just prints the first step ( echo ), waits for you to press any key ( read ), then prints the next step, and so on. ###!/bin/sh echo \"1. Use Finder to filter for files with '2019' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for 'James'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" So far this is just a more annoying way to use a checklist. The magic happens once you break the steps down into small enough pieces and realize that you know how to tell the computer to do some of the steps instead of doing them all yourself. For example, you know that the command-line tool grep is used for searching the contents of files, and that you can use \"fileglob\"s to select just the files that you want to search, and that you can send the output of grep to another file to read in your favourite text editor. Now you know how to automate the first two steps. The computer can just do that work without waiting for you: ###!/bin/sh grep \"James\" *2019* > search_results.txt echo \"1. Open 'search_results.txt' in Text Edit and search for 'James'\" read -p \"Press enter to continue\" echo \"Done!\" Before we were using the Finder, and it is possible to write code to tell the Finder to filter and seach for files. The key advantage of grep here is that we send the search results to another file that we can read now or save for later. This is also a good time to mention the advantage of text files over word processor files. If the minutes were stored in Word files, for example, then Finder could probably search them and you could use Word to read them, but you wouldn't be able to use grep or easily output the results to another file. Unix tools such as grep treat all text files the same, whether they're source code or meeting minutes, which means that these tools work pretty much the same on any text file. By keeping your data in Word you restrict yourself to a much smaller set of tools and make it harder to automate you work with simple scripts like this one. Even if you can't get the computer to run any of the steps for you automatically, a checkscript can still be useful by using variables instead of repeating yourself: ###!/bin/sh FILE_PATTERN=\"*2019*\" FILE_CONTENTS=\"James\" echo \"1. Use Finder to filter for files with '${FILE_PATTERN}' in the name\" read -p \"Press enter to continue\" echo \"2. Use finder to search file content for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"3. Open files in Text Edit and search for '${FILE_CONTENTS}'\" read -p \"Press enter to continue\" echo \"Done!\" Now if I want to search for \"Becky\" I can just change the FILE_CONTENTS variable in one place. I find this especially useful for dates and version numbers. This is pretty simple for a checkscript, with very few steps. A more realistic example would be if there were many directories containing the minutes of many meetings, maybe in different file formats and with different naming conventions. In order to be sure that we're searching all of them we might need a longer checkscript. Writing and using a checkscript instead of a checklist will likely take (you guessed it) about three times as long. But the magic of the checkscript is in the title of the blog post I mentioned: \"gradual automation\". Once you have a checkscript, you can run through it all manually, but you can also automate bits a pieces of the task, saving yourself time and effort next time.","title":"3. Checkscript"},{"location":"lesson/automating-ontology-workflows/#5-script","text":"A \"script\" is a kind of program that's easy to edit and run. There are technical distinctions to be made between \"compiled\" programs and \"interpreted\" programs, but they turn out to be more complicated and less helpful than they seem at first. Technically, a checkscript is just a script that waits for you to do the hard parts. In this section I want to talk about \"fully automated\" or \"standalone\" scripts that you just provide some input and execute. Most useful programs are useful because they call other programs (in the right ways). I like shell scripts because they're basically just commands that are copied and pasted from work I was doing on the command-line. It's really easy to call other programs. To continue our example, say that our minutes were stored in Word files. There are Python libraries for this, such as python-docx . You can write a little script using this library that works like grep to search for specified text in selected files, and output the results to a search results file. As you add more and more functionality to a script it can become unwieldy. Scripts work best when they have a simple \"flow\" from beginning to end. They may have some conditionals and some loops, but once you start seeing nested conditionals and loops, then your script is doing too much. There are two main options to consider: break your script into smaller, simpler scripts build a specialized tool: the next step on the spectrum of automation The key difference between a checkscript and a \"standalone\" script is handling problems. A checkscript relies on you to supervise it. A standalone script is expected to work properly without supervision. So the script has to be designed to handle a wider range of inputs and fail gracefully when it gets into trouble. This is a typical case of the \"80% rule\": the last 20% takes 80% of the time. As a rule of thumb, expect it to take three times as long to write a script that can run unsupervised than it takes you to write a checkscript that does \"almost\" the same thing.","title":"5. Script"},{"location":"lesson/automating-ontology-workflows/#6-specialized-tool","text":"When your script needs nested conditionals and loops, then it's probably time to reach for a programming language that's designed to write code \"in the large\". Some languages such as Python can make a pretty smooth transition from a script in a single file to a set of files in a module, working together nicely. You might also choose another language that can provide better performance or efficiency. It's not just the size and the logical complexity of your script, consider its purpose . The specialized tools that I have in mind have a clear purpose that helps guide their design. This also makes them easier to reuse across multiple projects. I often divide my specialized tools into two parts: a library and a command-line interface. The library can be used in other programs, and contains the most distinctive and important functionality. But the command-line interface is essential, because it lets me use my specialized tool in the shell and in scripts, so I can build more automation on top of it. Writing a tool in Java or C++ or Rust usually takes longer than a script in shell or Python because there are more details to worry about such as types and efficient memory management. In return you usually get more reliability and efficiency. But as a rule of thumb, expect it to take three times as long to write a specialized tool than it would to \"just\" write the script. On the other hand, if you already have a script that does most of what you want, and you're already familiar with the target you are moving to, then it can be fairly straightforward to translate from the script to the specialized tool. That's why it's often most efficient to write a prototype script first, do lots of quick experiments to explore the design space, and when you're happy with the design then start on the \"production\" version.","title":"6. Specialized Tool"},{"location":"lesson/automating-ontology-workflows/#7-workflow","text":"The last step in the spectrum of automation is to bring together all your scripts into a single \"workflow\". My favourite tool for this is the venerable Make . A Makefile is essentially a bunch of small scripts with their input and output files carefully specified. When you ask Make to build a given output file, it will look at the whole tree of scripts, figure out which input files are required to build your requested output file, then which files are required to build those files, and so on until it has determined a sequence of steps. Make is also smart enough to check whether some of the dependencies are already up-to-date, and can skip those steps. Looking at a Makefile you can see everything broken down into simple steps and organized into a tree, through which you can trace various paths. You can make changes at any point, and run Make again to update your project. I've done this all so many times that now I often start with a Makefile in an empty directory and build from there. I try experiments on the command line. I make notes. I break the larger task into parts with a checklist. I automate the easy parts first, and leave some parts as manual steps with instructions. I write little scripts in the Makefile . I write larger scripts in the src/ directory. If these get too big or complex, I start thinking about building a specialized tool. (And of course, I store everything in version control.) It takes more time at the beginning, but I think that I usually save time later, because I have a nice place to put everything from the start. In other words, I start thinking about automation at the very beginning of the project, assuming from the start that it will grow, and that I'll need to go back and change things. With a mindset for automation, from the start I'm thinking about how the inputs I care about are the same and different, which similarities I can use for my tests and code, and which differences are important or unimportant.","title":"7. Workflow"},{"location":"lesson/automating-ontology-workflows/#conclusion","text":"In the end, my project isn't ever completely automated. It doesn't \"act of itself\". But by making everything clear and explicit I'm telling the computer how to do a lot of the work and other humans (or just my future self) how to do the rest of it. The final secret of automation, especially when it comes to software and data, is communication : expressing things clearly for humans and machines so they can see and do exactly what you did.","title":"Conclusion"},{"location":"lesson/automating-ontology-workflows/#scientific-computing-an-overview","text":"By: James Overton By \"scientific computing\" we mean using computers to help with key aspect of science such as data collection, cleaning, interpretation, analysis, and visualization. Some people use \"scientific computing\" to mean something more specific, focusing on computational modelling or computationally intensive analysis. We'll be focusing on more general and day-to-day topics: how can a scientist make best use of a computer to do their work well? These three things apply to lots of fields, but are particularly important to scientists: reliability reproducibility communication It should be no surprise that automation can help with all of these. When working properly, computers make fewer mistakes than people, and the mistakes they do make are more predictable. If we're careful, our software systems can be easily reproduced, which means that an entire data analysis pipeline can be copied and run by another lab to confirm the results. And scientific publications are increasingly including data and code as part of the review and final publication process. Clear code is one of the best ways to communicate detailed steps. Automation is critical to scientific instruments and experiments, but we'll focus on the data processing and analysis side: after the data has been generated, how should you deal with it. Basic information management is always important: community standard file formats consistent file naming documentation, READMEs backups version control More advanced data management is part of this course: consistent use of versioned software reference data terminology controlled vocabularies data dictionaries ontologies Some simple rules of thumb can help reduce complexity and confusion: make space firm foundations one-way data flow plan for change test from the start documentation is also for you","title":"Scientific Computing: An Overview"},{"location":"lesson/automating-ontology-workflows/#make-space","text":"When starting a new project, make a nice clean new space for it. Try for that \"new project smell\". I always create a new directory on my computer. I almost always create a new GitHub repository. I usually create a README and a Makefile, right away. It's not always clear when a project is really \"new\" or just a new phase of an old project. But try to clear some space to make a fresh start.","title":"Make Space"},{"location":"lesson/automating-ontology-workflows/#firm-foundations","text":"A lot of data analysis starts with a reference data set. It might be a genome or a proteome. It might be a corpus. It might be a set of papers or data from those papers. Start by finding that data and selecting a particular version of it. Write that down clearly in your notes. If possible, include a unique identifier such as a ( persistent ) URL or DOI. If that's not possible, write down the steps you took. If the data isn't too big, keep a copy of it in your fresh new project directory. If the data is a bit too big, keep a compressed copy in a zip or gz file. A lot of software is perfectly happy to read directly from compressed files, and you can compress or uncompress data using piped commands in your shell or script. If the data is really too big, then be extra careful to keep notes on exactly where you can find it again. Consider storing just the hashes of the big files, so you can confirm that they have exactly the same contents. If you know from the start that you will need to compare your results with someone else's, make sure that you're using the same reference data that they are. This may require a conversation, but trust me that it's better to have this conversation now than later.","title":"Firm Foundations"},{"location":"lesson/automating-ontology-workflows/#one-way-data-flow","text":"It's much easier to think about processes that flow in one direction. Branches are a little trickier, but usually fine. The real trouble comes with loops. Once a process loops back on itself it's much more difficult to reason about what's happening. Loops are powerful, but with great power comes great responsibility. Keep the systems you design as simple as possible (but no simpler). In practical terms: Try not to read then write to the same file. If you have to, try to append rather than overwrite. This is one reason why I prefer tables on disk to databases. Don't hesitate to write intermediate files. These are very useful for testing and debugging. When you're \"finished\" you can comment out these steps.","title":"One-Way Data Flow"},{"location":"lesson/automating-ontology-workflows/#plan-for-change","text":"It's very tempting: you could automate this step, or you could just do it manually. It might take three times as long to automate it, right? So you can save yourself some precious time by just opening Excel and \"fixing\" things by hand. Sometimes that bet will pay off, but I lose that bet most of the time. I tend to realize my mistake only at the last minute. The submission deadline is tomorrow but the core lab \"fixed\" something and they have a new version of the dataset that we need to use for the figures. Now I really don't have time to automate, so I'm up late clicking through Excel again and hoping that I remembered to redo all the changes that I made last time. Automating the process would have actually saved me time, but more importantly it would have avoided a lot of stress. By now I should know that the dataset will almost certainly be revised at the last minute. If I have the automation set up, then I just update the data, run the automation again, and quickly check the results.","title":"Plan for Change"},{"location":"lesson/automating-ontology-workflows/#test-from-the-start","text":"Tests are another thing that take time to implement. One of the key benefits to tests is (again) communication. When assessing or trying out some new piece of software I often look to the test files to see examples of how the code is really used, and the shape of the inputs and outputs. There's a spectrum of tests that apply to different parts of your system: unit tests: individual functions and methods regression tests: ensure that fixed bugs do not reappear integration tests: end-to-end functionality performance tests: system speed and resource usage acceptance tests: whether the overall system meets its design goals Tests should be automated. The test suite should either pass or fail, and if it fails something needs to be fixed before any more development is done. The automated test suite should run before each new version is committed to version control, and ideally more often during development. Tests come with costs: development cost of writing the tests time and resources spent running the tests maintenance costs of updating the tests The first is obvious but the other two often more important. A slow test suite is annoying to run, and so it won't get run. A test suite that's hard to update won't get updated, and then failures will be ignored, which defeats the entire purpose.","title":"Test from the Start"},{"location":"lesson/automating-ontology-workflows/#documentation-is-also-for-you","text":"I tend to forget how bad a memory I have. In the moment, when I'm writing brilliant code nothing could be more obvious than the perfect solution that is pouring forth from my mind all over my keyboard. But when I come back to that code weeks, months, or years later, I often wonder what the heck I was thinking. We think about the documentation we write as being for other people, but for a lot of small projects it's really for your future self. Be kind to your future self. They may be even more tired, even more stressed than you are today. There's a range of different forms of documentation, worth a whole discussion of its own. I like this four-way distinction : tutorials: getting started, basic concepts, an overview how-to guides: how to do common tasks explanation: why does it work this way? reference: looking up the details You don't need all of these for your small project, but consider a brief explanation of why it works the way it does (aimed at a colleague who knows your field well), and some brief notes on how-to do the stuff this project is for. These could both go in the README of a small project.","title":"Documentation is also for You"},{"location":"lesson/automating-ontology-workflows/#additional-materials-and-resources","text":"A whirlwind introduction to the command line Programming with Python Oh My Git!","title":"Additional materials and resources"},{"location":"lesson/automating-ontology-workflows/#contributors","text":"Nico Matentzoglu James Overton","title":"Contributors"},{"location":"lesson/contributing-to-obo-ontologies/","text":"Contributing to OBO ontologies \u00b6 Prerequisites \u00b6 Participants will need to have access to the following resources and tools prior to the training: GitHub account - register for a free GitHub account here Protege - Install Protege 5.5, download it here Install ELK 0.5 Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop Preparation \u00b6 Review tutorial on Ontology Term Use What is delivered as part of the course \u00b6 Description: How to contribute terms to existing ontologies. Learning objectives \u00b6 How to use GitHub GitHub workflows Branch vs Fork How to create GitHub Issues Understand basic Open Source etiquette Reading READMEs Understand basics of ontology development workflows Browsing and Searching in Protege Add new terms to an ontology Initial Protege setup Protege editing The Class description view Use GitHub: make pull requests Understand ontology design patterns Use templates: ROBOT, DOSDP ( under development ) Basics of OWL Logic and debugging Tutorials \u00b6 N/A Additional materials and resources \u00b6 Contributors \u00b6 Nicole Vasilevsky Rebecca Jackson Melissa Haendel Chris Mungall David Osumi-Sutherland Matt Yoder Carlo Torniai Simon Jupp Use GitHub \u00b6 GitHub workflows \u00b6 GitHub - distributed version control (Git) + social media for geeks who like to build code/documented collaboratively. A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub. A Git repo typically has a master or main branch that is not directly editing. Changes are made by creating a branch from Master (complete copy of the Master + its history). Branch vs Fork \u00b6 You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners. If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. If you have permission from the owners, you can instead make a new branch. For this training, we gave you access to the repository. See the Appendix for instructions on how to make a fork. Create GitHub Issues \u00b6 Go to GitHub tracker for the ontology where you'd like to create an issue Select New issue Pick appropriate template (if applicable) Fill in the information that is requested on the template below each header For a new term request, please include: The parent ID and label A definition in the proper format Sources/cross references for synonyms Your ORCID Add any additional comments at the end If you are requesting changes to an existing term, include as much information as possible, including the term ID and label. If you use a template, an ontology curator may automatically be assigned. Tip : you can easily obtain term metadata like OBO ID, IRI, or the term label by clicking the three lines above the Annotations box (next to the term name) in Protege, see screenshot below. You can also copy the IRI in markdown, which is really convenient for pasting into GitHub. Video Explanation \u00b6 See this example video on creating a new term request to the Mondo Disease Ontology: Basic Open Source etiquette \u00b6 Keep in mind that open source ontology repositories on GitHub are public and open to all. Be respectful in your requests and comments. Do not include any private information. GitHub sends notifications to your email, and you can respond via your email client. Keep in mind, the responses are posted publicly. Be sure to delete your email signature that includes any personal information, like your email address or phone number. Many ontologies have limited resources and personnel for development and maintenance. Please be patient with your requests. If your ticket/request has been unanswered for a long period of time, feel free to kindly check in by commenting on the ticket. Including a deadline or priority on the ticket can help the ontology curators with triaging tickets. Reading READMEs \u00b6 A README is a text file that introduces and explains a project. It is intended for everyone , not just the software or ontology developers. Ideally, the README file will include detailed information about the ontology, how to get started with using any of the files, license information and other details. The README is usually on the front page of the GitHub repository. Basics of ontology development workflows \u00b6 Ontology development workflows \u00b6 The steps below describe how to make changes to an ontology. Go to the GitHub repository for your ontology, and clone the repository. The example below describes how to clone the Mondo Disease Ontology repo, but this can be applied to any ontology that is stored in GitHub. Clone the Mondo repo \u00b6 Open the Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time depending on how big the file is and how much memory your computer has. Create a branch using GitHub Desktop \u00b6 Click the little arrow in Current Branch Click New Branch Give your branch a name: training-initials (ie training-NV ) Open the Ontology edit file in Protege \u00b6 Open Protege Go to: File -> Open Navigate to [ontology-name]/src/ontology/[ontology-name]-edit.obo and open this file in Protege. For example: mondo/src/ontology/mondo-edit.obo Note: all ontologies that use the Ontology Development Kit (ODK) will have the 'edit' files stored in the same folder path: src/ontology/[ontology-name]-edit.owl (or [ontology-name]-edit.obo) Browsing and Searching in Protege \u00b6 The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology. Open the Mondo in Prot\u00e9g\u00e9 \u00b6 Note: Windows users should open Protege using run.bat Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web. The Prot\u00e9g\u00e9 UI \u00b6 The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information. Running the reasoner \u00b6 Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner: For more details on why it is important to have the reasoner on when using the editors version of an ontology, see the Reasoning reference guide . But for now, you don't need a deeper understanding, just be sure that you always have the reasoner on. Entities tab \u00b6 You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query. Searching in Protege \u00b6 You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: 'congenital heart disease' 'Kindler syndrome' 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get. Use GitHub: make pull requests \u00b6 Committing, pushing and making pull requests \u00b6 Changes made to the ontology can be viewed in GitHub Desktop. Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the ontology editors for help instead. Example 1: Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:0001012 episodic angioedema with eosinophilia NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the cl-edit.owl file. Push: To incorporate the changes into the remote repository, click Publish branch. Add New Terms to an Ontology: \u00b6 The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology. Setup \u00b6 Setting Preferences for New entities \u00b6 Ontology terms have separate names and IDs. The names are annotation values (labels) and the IDs are represented using IRIs. The OBO foundry has a policy on IRI (or ID) generation ( http://www.obofoundry.org/principles/fp-003-uris.html ). You can set an ID strategy using the \"New Entities\" tab under the Prot\u00e9g\u00e9 Preferences -- on the top toolbar, click the \"Prot\u00e9g\u00e9 dropdown, then click Preferences. Set your new entity preferences precisely as in the following screenshot of the New Entities tab. Note - you have been assigned an ID range in the Mondo idranges file - you should be able to find your own range assigned there. DIY (only if you know what you are doing!) To add your own ID ranges: Go into src/ontology create a branch Find and edit mondo-idranges.owl by adding the following: Datatype: idrange:10 #update this to next following integer from previous Annotations: allocatedto: \"Your Name\" #change to your name EquivalentTo: xsd:integer[>= 0806000 , <= 0806999]. #add a range of 999 above the previous integer Be sure to change \"Your Name\" to your actual name! And note that this value should almost always be an individual, and not an organization or group. create a pull request and add matentzn or nicolevasilevsky as a reviewer proceed to settting up as below: Specified IRI: http://purl.obolibrary.org/obo/ Note - if you edit more than one ontology in Protege, you will need to update your Preferences for each ontology before you edit. Setting Preferences for User details \u00b6 User name: click Use supplied user name and enter your username in the field below Click Use Git user name when available In the ORCID field, add your ORCID ID (in the format 0000-0000-0000-0000) Setting Preferences for New entities metadata \u00b6 The current recommendation of the OBO Foundry Technical Working Group is that an editor who creates a new term SHOULD add a http://purl.org/dc/terms/contributor annotation, set to the ORCID or GitHub username of the editor, and a http://purl.org/dc/terms/date annotation, set to the current date. You can have Prot\u00e9g\u00e9 automatically add those annotations by setting your preferences to match the screenshot below, in the New entities metadata tab (under preferences). If you do not have an ORCID, register for for free here: https://orcid.org/ Protege editing \u00b6 Creating a new class \u00b6 Before you start: make sure you are working on a branch - see quick guide here . make sure you have the editor's file open in Protege as detailed here . New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle). Practice adding a new term: We will work on these two tickets: https://github.com/monarch-initiative/mondo/issues/616 https://github.com/monarch-initiative/mondo/issues/2541 https://github.com/monarch-initiative/mondo/issues/616 \u00b6 Search for the parent term 'hypereosinophilic syndrome' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'hypereosinophilic syndrome' A dialog will popup. Name this new subclass: migratory muscle precursor. Click \"OK\" to add the class. Adding annotations \u00b6 Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. CL includes a pre-declared set of annotation properties. The most commonly used annotations are below. rdfs:label definition has_exact_synonym has_broad_synonym has_narrow_synonym has_related synonym database_cross_reference rdfs:comment Note, most of these are bold in the annotation property list: Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK. Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns. Definitions in Mondo should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space) To add a dbxref to the definition: Click the @ symbol next to the definition Click the + button next in the pop-up window Scroll up on the left hand side until you find 'database_cross_reference', and click it Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces) Click OK Add the additional dbxref: GARD:0013029 The dbxrefs should appear as below. Add Synonyms and Database cross reference \u00b6 Add synonyms Click the add annotations button Add the following synonyms as 'has_exact_synonym': EAE Gleich's syndrome Gleich syndrome All synonyms in Mondo should have a dbxref on the synonym Click the @ symbol next to the synonym Click the + button Add the dbxref to each synonym: GARD:0013029 Add database cross reference Click the add annotations button Add the following database_cross_reference': GARD:0013029 Click the @ symbol next to the synonym Click the + button Add source: MONDO:equivalentTo The Class description view \u00b6 We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class. Note the Anonymous Ancestors. This is a difficult concept we will return to later, and the contents of this portion may seem confusing at first (some of these may be clearer after you complete the \"Basics of OWL\" section below). These are OWL expressions that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the expression from. For many ontologies, you will see some quite abstract expressions in here inherited from upper ontologies, but these can generally be ignored for most purposes. Revising a superclass: \u00b6 If you want to revise the superclass, click the 'o' symbol next to the superclass and replace the text. Try to revise 'musculoskeletal system disease' to 'disease by anatomical system'. If you want to delete a superclass, click the 'x' button next to the superclass. Delete the 'disease by anatomical system' superclass. Close this window without saving. Save your work. Make a Pull Request \u00b6 Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release. Ontology design patterns \u00b6 Dead Simple Ontology Design Patterns (DOSDPs) are specifications, written in yaml format, that specify how ontology terms should be created (see article here ). They can be used to: generate documentation generate new terms retrofit existing ontology terms DOSDPs have some key features: Description : that describes the purpose of the patterns Examples : Provides examples of terms that use the Patterns Declared classes : these are the classes that are used in the pattern. Any subclass of the declared class can be used in this pattern. Declared relationships : the relationships used in the logical axioms vars : the variable classes that are used in the pattern. Any subclass of the 'var' can be used in this pattern. Pattern for class name, annotations, text definition and equivalentTo (logical definition) : Ontology classes are used as 'fillers' to create new classes that conform to the specific patterns. Examples of design patterns are available here: uPheno pattern library Mondo Disease Ontology pattern library Use templates: ROBOT, DOSDP \u00b6 under development Basics of OWL \u00b6 Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Logic and debugging \u00b6 Below are exercises to demonstrate how to: Add equivalent axioms (logical definitions) to ontology terms Run the reasoner and view the inferred hierarchy Debugging and viewing explanations Practice adding logic and debugging \u00b6 These instructions will use the Mondo disease ontology as an example. Practice 1 \u00b6 Add New Terms with an Equivalance Axiom to Mondo: \u00b6 Creating a new class \u00b6 New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle). Practice adding a new term: \u00b6 Add the new term 'mycotoxin allergy' \u00b6 Navigate to the Mondo repo in GitHub Desktop, create a branch, and open mondo-edit.obo in Protege. Search for the parent term 'allergic disease' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'allergic disease' A dialog will popup. Name this new subclass: mycotoxin allergy. Click \"OK\" to add the class. Add annotations and a logical axiom \u00b6 Equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/ For this class, we want to follow the design pattern for allergy . Review this pattern before proceeding. Based on the pattern specifications, add a text definition to this term. Add the database cross reference to this term: MONDO:patterns/allergy Add a synonym that is consistent with this pattern. Add the equivalence axiom according to the pattern specifications. Run the reasoner View the inferred hierarchy. You should see a child of owl:Nothing (and you should see this in the Description pane as well.) Click on the ? button next to the owl:Nothing in the Description view The explanation tells you the reason why this is unsatisfiable Mycotoxin is a child of a 'specifically dependent continuant' and and 'independent continuant' is disjoint with 'specifically dependent continuant'. This logical axiom uses the relation 'realized in response to stimulus' Click on this relation in Protege (you can click on the relation name in the equivalence axiom, or you can click on the Object properties tab and search for this relation. There are domain and range restrictions on this property, where the range has to be a material enity. Chebi mycotoxin is actually a role, so it is not a material entity. Go back to the Classes or Entities pane and remove the equivalence axiom and run the reasoner again. You should now see no unsatisfiable classes. Practice 2 \u00b6 Practice adding a new term: \u00b6 Add the new term 'acquired alacrima' \u00b6 Add 'acquired candidiasis as a subclass of MONDO_0002026 candidiasis. Add annotations and a logical axiom \u00b6 As noted above, equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/ For this class, we want to follow the design pattern for acquired . Review this pattern before proceeding. Based on the pattern specifications, add a text definition to this term. Add the database cross reference to the definition: MONDO:patterns/allergy Add the equivalence axiom according to the pattern specifications. Run the reasoner View the inferred hierarchy. Further reading \u00b6 Debugging ontologies using OWL reasoning Chris Mungall on how to write great textual definitions","title":"Contributing to OBO ontologies"},{"location":"lesson/contributing-to-obo-ontologies/#contributing-to-obo-ontologies","text":"","title":"Contributing to OBO ontologies"},{"location":"lesson/contributing-to-obo-ontologies/#prerequisites","text":"Participants will need to have access to the following resources and tools prior to the training: GitHub account - register for a free GitHub account here Protege - Install Protege 5.5, download it here Install ELK 0.5 Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop","title":"Prerequisites"},{"location":"lesson/contributing-to-obo-ontologies/#preparation","text":"Review tutorial on Ontology Term Use","title":"Preparation"},{"location":"lesson/contributing-to-obo-ontologies/#what-is-delivered-as-part-of-the-course","text":"Description: How to contribute terms to existing ontologies.","title":"What is delivered as part of the course"},{"location":"lesson/contributing-to-obo-ontologies/#learning-objectives","text":"How to use GitHub GitHub workflows Branch vs Fork How to create GitHub Issues Understand basic Open Source etiquette Reading READMEs Understand basics of ontology development workflows Browsing and Searching in Protege Add new terms to an ontology Initial Protege setup Protege editing The Class description view Use GitHub: make pull requests Understand ontology design patterns Use templates: ROBOT, DOSDP ( under development ) Basics of OWL Logic and debugging","title":"Learning objectives"},{"location":"lesson/contributing-to-obo-ontologies/#tutorials","text":"N/A","title":"Tutorials"},{"location":"lesson/contributing-to-obo-ontologies/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/contributing-to-obo-ontologies/#contributors","text":"Nicole Vasilevsky Rebecca Jackson Melissa Haendel Chris Mungall David Osumi-Sutherland Matt Yoder Carlo Torniai Simon Jupp","title":"Contributors"},{"location":"lesson/contributing-to-obo-ontologies/#use-github","text":"","title":"Use GitHub"},{"location":"lesson/contributing-to-obo-ontologies/#github-workflows","text":"GitHub - distributed version control (Git) + social media for geeks who like to build code/documented collaboratively. A Git repo consists of a set of branches each with a complete history of all changes ever made to the files and directories. This is true for a local copy you check out to your computer from GitHub or for a copy (fork) you make on GitHub. A Git repo typically has a master or main branch that is not directly editing. Changes are made by creating a branch from Master (complete copy of the Master + its history).","title":"GitHub workflows"},{"location":"lesson/contributing-to-obo-ontologies/#branch-vs-fork","text":"You can copy (fork) any GitHub repo to some other location on GitHub without having to ask permission from the owners. If you modify some files in that repo, e.g. to fix a bug in some code, or a typo in a document, you can then suggest to the owners (via a Pull Request) that they adopt (merge) you your changes back into their repo. If you have permission from the owners, you can instead make a new branch. For this training, we gave you access to the repository. See the Appendix for instructions on how to make a fork.","title":"Branch vs Fork"},{"location":"lesson/contributing-to-obo-ontologies/#create-github-issues","text":"Go to GitHub tracker for the ontology where you'd like to create an issue Select New issue Pick appropriate template (if applicable) Fill in the information that is requested on the template below each header For a new term request, please include: The parent ID and label A definition in the proper format Sources/cross references for synonyms Your ORCID Add any additional comments at the end If you are requesting changes to an existing term, include as much information as possible, including the term ID and label. If you use a template, an ontology curator may automatically be assigned. Tip : you can easily obtain term metadata like OBO ID, IRI, or the term label by clicking the three lines above the Annotations box (next to the term name) in Protege, see screenshot below. You can also copy the IRI in markdown, which is really convenient for pasting into GitHub.","title":"Create GitHub Issues"},{"location":"lesson/contributing-to-obo-ontologies/#video-explanation","text":"See this example video on creating a new term request to the Mondo Disease Ontology:","title":"Video Explanation"},{"location":"lesson/contributing-to-obo-ontologies/#basic-open-source-etiquette","text":"Keep in mind that open source ontology repositories on GitHub are public and open to all. Be respectful in your requests and comments. Do not include any private information. GitHub sends notifications to your email, and you can respond via your email client. Keep in mind, the responses are posted publicly. Be sure to delete your email signature that includes any personal information, like your email address or phone number. Many ontologies have limited resources and personnel for development and maintenance. Please be patient with your requests. If your ticket/request has been unanswered for a long period of time, feel free to kindly check in by commenting on the ticket. Including a deadline or priority on the ticket can help the ontology curators with triaging tickets.","title":"Basic Open Source etiquette"},{"location":"lesson/contributing-to-obo-ontologies/#reading-readmes","text":"A README is a text file that introduces and explains a project. It is intended for everyone , not just the software or ontology developers. Ideally, the README file will include detailed information about the ontology, how to get started with using any of the files, license information and other details. The README is usually on the front page of the GitHub repository.","title":"Reading READMEs"},{"location":"lesson/contributing-to-obo-ontologies/#basics-of-ontology-development-workflows","text":"","title":"Basics of ontology development workflows"},{"location":"lesson/contributing-to-obo-ontologies/#ontology-development-workflows","text":"The steps below describe how to make changes to an ontology. Go to the GitHub repository for your ontology, and clone the repository. The example below describes how to clone the Mondo Disease Ontology repo, but this can be applied to any ontology that is stored in GitHub.","title":"Ontology development workflows"},{"location":"lesson/contributing-to-obo-ontologies/#clone-the-mondo-repo","text":"Open the Mondo GitHub repository Click Code Click 'Open with GitHub Desktop' You will be given an option as to where to save the repository. I have a folder called 'git' where I save all of my local repos. This will open GitHub Desktop and the repo should start downloading. This could take some time depending on how big the file is and how much memory your computer has.","title":"Clone the Mondo repo"},{"location":"lesson/contributing-to-obo-ontologies/#create-a-branch-using-github-desktop","text":"Click the little arrow in Current Branch Click New Branch Give your branch a name: training-initials (ie training-NV )","title":"Create a branch using GitHub Desktop"},{"location":"lesson/contributing-to-obo-ontologies/#open-the-ontology-edit-file-in-protege","text":"Open Protege Go to: File -> Open Navigate to [ontology-name]/src/ontology/[ontology-name]-edit.obo and open this file in Protege. For example: mondo/src/ontology/mondo-edit.obo Note: all ontologies that use the Ontology Development Kit (ODK) will have the 'edit' files stored in the same folder path: src/ontology/[ontology-name]-edit.owl (or [ontology-name]-edit.obo)","title":"Open the Ontology edit file in Protege"},{"location":"lesson/contributing-to-obo-ontologies/#browsing-and-searching-in-protege","text":"The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology.","title":"Browsing and Searching in Protege"},{"location":"lesson/contributing-to-obo-ontologies/#open-the-mondo-in-protege","text":"Note: Windows users should open Protege using run.bat Navigate to where you downloaded the repository and open the mondo-edit.obo file (src/ontology/mondo-edit.obo) When you open Protege, you will be on the Active Ontology tab Note the Ontology IRI field. The IRI is used to identify the ontology on the Web.","title":"Open the Mondo in Prot\u00e9g\u00e9"},{"location":"lesson/contributing-to-obo-ontologies/#the-protege-ui","text":"The Prot\u00e9g\u00e9 interface follows a basic paradigm of Tabs and Panels. By default, Prot\u00e9g\u00e9 launches with the main tabs seen below. The layout of tabs and panels is configurable by the user. The Tab list will have slight differences from version to version, and depending on your configuration. It will also reflect your customizations. To customize your view, go to the Window tab on the toolbar and select Views. Here you can customize which panels you see in each tab. In the tabs view, you can select which tabs you will see. You will commonly want to see the Entities tab, which has the Classes tab and the Object Properties tab. Note: if you open a new ontology while viewing your current ontology, Prot\u00e9g\u00e9 will ask you if you'd like to open it in a new window. For most normal usage you should answer no. This will open in a new window. The panel in the center is the ontology annotations panel. This panel contains basic metadata about the ontology, such as the authors, a short description and license information.","title":"The Prot\u00e9g\u00e9 UI"},{"location":"lesson/contributing-to-obo-ontologies/#running-the-reasoner","text":"Before browsing or searching an ontology, it is useful to run an OWL reasoner first. This ensures that you can view the full, intended classification and allows you to run queries. Navigate to the query menu, and run the ELK reasoner: For more details on why it is important to have the reasoner on when using the editors version of an ontology, see the Reasoning reference guide . But for now, you don't need a deeper understanding, just be sure that you always have the reasoner on.","title":"Running the reasoner"},{"location":"lesson/contributing-to-obo-ontologies/#entities-tab","text":"You will see various tabs along the top of the screen. Each tab provides a different perspective on the ontology. For the purposes of this tutorial, we care mostly about the Entities tab, the DL query tab and the search tool. OWL Entities include Classes (which we are focussed on editing in this tutorial), relations (OWL Object Properties) and Annotation Properties (terms like, 'definition' and 'label' which we use to annotate OWL entities. Select the Entities tab and then the Classes sub-tab. Now choose the inferred view (as shown below). The Entities tab is split into two halves. The left-hand side provides a suite of panels for selecting various entities in your ontology. When a particular entity is selected the panels on the right-hand side display information about that entity. The entities panel is context specific, so if you have a class selected (like Thing) then the panels on the right are aimed at editing classes. The panels on the right are customizable. Based on prior use you may see new panes or alternate arrangements. You should see the class OWL:Thing. You could start browsing from here, but the upper level view of the ontology is too abstract for our purposes. To find something more interesting to look at we need to search or query.","title":"Entities tab"},{"location":"lesson/contributing-to-obo-ontologies/#searching-in-protege","text":"You can search for any entity using the search bar on the right: The search window will open on top of your Protege pane, we recommend resizing it and moving it to the side of the main window so you can view together. Here's an example search for 'COVID-19': It shows results found in display names, definitions, synonyms and more. The default results list is truncated. To see full results check the 'Show all results option'. You may need to resize the box to show all results. Double clicking on a result, displays details about it in the entities tab, e.g. In the Entities, tab, you can browse related types, opening/closing branches and clicking on terms to see details on the right. In the default layout, annotations on a term are displayed in the top panel and logical assertions in the 'Description' panel at the bottom. Try to find these specific classes: 'congenital heart disease' 'Kindler syndrome' 'kidney failure' Note - a cool feature in the search tool in Protege is you can search on partial string matching. For example, if you want to search for \u2018down syndrome\u2019, you could search on a partial string: \u2018do synd\u2019. Try searching for \u2018br car and see what kind of results are returned. Question: The search will also search on synonyms. Try searching for \u2018shingles\u2019 and see what results are returned. Were you able to find the term? Note - if the search is slow, you can uncheck the box \u2018Search in annotation values. Try this and search for a term and note if the search is faster. Then search for \u2018shingles\u2019 again and note what results you get.","title":"Searching in Protege"},{"location":"lesson/contributing-to-obo-ontologies/#use-github-make-pull-requests","text":"","title":"Use GitHub: make pull requests"},{"location":"lesson/contributing-to-obo-ontologies/#committing-pushing-and-making-pull-requests","text":"Changes made to the ontology can be viewed in GitHub Desktop. Before committing, check the diff. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask the ontology editors for help instead. Example 1: Commit: Add a meaningful message in the Commit field in the lower left, for example: add new class MONDO:0001012 episodic angioedema with eosinophilia NOTE: You can use the word 'fixes' or 'closes' in the description of the commit message, followed by the corresponding ticket number (in the format #1234) - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . Note: 'Fixes' and \"Closes' are case-insensitive. If you don't want to close the ticket, just refer to the ticket # without the word 'Fixes' or use 'Adresses'. The commit will be associated with the correct ticket but the ticket will remain open. NOTE: It is also possible to type a longer message than allowed when using the '-m' argument; to do this, skip the -m, and a vi window (on mac) will open in which an unlimited description may be typed. Click Commit to [branch]. This will save the changes to the cl-edit.owl file. Push: To incorporate the changes into the remote repository, click Publish branch.","title":"Committing, pushing and making pull requests"},{"location":"lesson/contributing-to-obo-ontologies/#add-new-terms-to-an-ontology","text":"The instructions below are using the Mondo Disease Ontology as an example, but this can be applied to any ontology.","title":"Add New Terms to an Ontology:"},{"location":"lesson/contributing-to-obo-ontologies/#setup","text":"","title":"Setup"},{"location":"lesson/contributing-to-obo-ontologies/#setting-preferences-for-new-entities","text":"Ontology terms have separate names and IDs. The names are annotation values (labels) and the IDs are represented using IRIs. The OBO foundry has a policy on IRI (or ID) generation ( http://www.obofoundry.org/principles/fp-003-uris.html ). You can set an ID strategy using the \"New Entities\" tab under the Prot\u00e9g\u00e9 Preferences -- on the top toolbar, click the \"Prot\u00e9g\u00e9 dropdown, then click Preferences. Set your new entity preferences precisely as in the following screenshot of the New Entities tab. Note - you have been assigned an ID range in the Mondo idranges file - you should be able to find your own range assigned there. DIY (only if you know what you are doing!) To add your own ID ranges: Go into src/ontology create a branch Find and edit mondo-idranges.owl by adding the following: Datatype: idrange:10 #update this to next following integer from previous Annotations: allocatedto: \"Your Name\" #change to your name EquivalentTo: xsd:integer[>= 0806000 , <= 0806999]. #add a range of 999 above the previous integer Be sure to change \"Your Name\" to your actual name! And note that this value should almost always be an individual, and not an organization or group. create a pull request and add matentzn or nicolevasilevsky as a reviewer proceed to settting up as below: Specified IRI: http://purl.obolibrary.org/obo/ Note - if you edit more than one ontology in Protege, you will need to update your Preferences for each ontology before you edit.","title":"Setting Preferences for New entities"},{"location":"lesson/contributing-to-obo-ontologies/#setting-preferences-for-user-details","text":"User name: click Use supplied user name and enter your username in the field below Click Use Git user name when available In the ORCID field, add your ORCID ID (in the format 0000-0000-0000-0000)","title":"Setting Preferences for User details"},{"location":"lesson/contributing-to-obo-ontologies/#setting-preferences-for-new-entities-metadata","text":"The current recommendation of the OBO Foundry Technical Working Group is that an editor who creates a new term SHOULD add a http://purl.org/dc/terms/contributor annotation, set to the ORCID or GitHub username of the editor, and a http://purl.org/dc/terms/date annotation, set to the current date. You can have Prot\u00e9g\u00e9 automatically add those annotations by setting your preferences to match the screenshot below, in the New entities metadata tab (under preferences). If you do not have an ORCID, register for for free here: https://orcid.org/","title":"Setting Preferences for New entities metadata"},{"location":"lesson/contributing-to-obo-ontologies/#protege-editing","text":"","title":"Protege editing"},{"location":"lesson/contributing-to-obo-ontologies/#creating-a-new-class","text":"Before you start: make sure you are working on a branch - see quick guide here . make sure you have the editor's file open in Protege as detailed here . New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle). Practice adding a new term: We will work on these two tickets: https://github.com/monarch-initiative/mondo/issues/616 https://github.com/monarch-initiative/mondo/issues/2541","title":"Creating a new class"},{"location":"lesson/contributing-to-obo-ontologies/#httpsgithubcommonarch-initiativemondoissues616","text":"Search for the parent term 'hypereosinophilic syndrome' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'hypereosinophilic syndrome' A dialog will popup. Name this new subclass: migratory muscle precursor. Click \"OK\" to add the class.","title":"https://github.com/monarch-initiative/mondo/issues/616"},{"location":"lesson/contributing-to-obo-ontologies/#adding-annotations","text":"Using Prot\u00e9g\u00e9 you can add annotations such as labels, definitions, synonyms, database cross references (dbxrefs) to any OWL entity. The panel on the right, named Annotations, is where these annotations are added. CL includes a pre-declared set of annotation properties. The most commonly used annotations are below. rdfs:label definition has_exact_synonym has_broad_synonym has_narrow_synonym has_related synonym database_cross_reference rdfs:comment Note, most of these are bold in the annotation property list: Use this panel to add a definition to the class you created. Select the + button to add an annotation to the selected entity. Click on the annotation 'definition' on the left and copy and paste in the definition to the white editing box on the right. Click OK. Definition: A disorder characterized by episodes of swelling under the skin (angioedema) and an elevated number of the white blood cells known as eosinophils (eosinophilia). During these episodes, symptoms of hives (urticaria), fever, swelling, weight gain and eosinophilia may occur. Symptoms usually appear every 3-4 weeks and resolve on their own within several days. Other cells may be elevated during the episodes, such as neutrophils and lymphocytes. Although the syndrome is often considered a subtype of the idiopathic hypereosinophilic syndromes, it does not typically have organ involvement or lead to other health concerns. Definitions in Mondo should have a 'database cross reference' (dbxref), which is a reference to the definition source, such as a paper from the primary literature or another database. For references to papers, we cross reference the PubMed Identifier in the format, PMID:XXXXXXXX. (Note, no space) To add a dbxref to the definition: Click the @ symbol next to the definition Click the + button next in the pop-up window Scroll up on the left hand side until you find 'database_cross_reference', and click it Add the PMID in the editing box (PMID:25527564). _Note: the PMID should not have any spaces) Click OK Add the additional dbxref: GARD:0013029 The dbxrefs should appear as below.","title":"Adding annotations"},{"location":"lesson/contributing-to-obo-ontologies/#add-synonyms-and-database-cross-reference","text":"Add synonyms Click the add annotations button Add the following synonyms as 'has_exact_synonym': EAE Gleich's syndrome Gleich syndrome All synonyms in Mondo should have a dbxref on the synonym Click the @ symbol next to the synonym Click the + button Add the dbxref to each synonym: GARD:0013029 Add database cross reference Click the add annotations button Add the following database_cross_reference': GARD:0013029 Click the @ symbol next to the synonym Click the + button Add source: MONDO:equivalentTo","title":"Add Synonyms and Database cross reference"},{"location":"lesson/contributing-to-obo-ontologies/#the-class-description-view","text":"We have seen how to add sub/superclasses and annotate the class hierarchy. Another way to do the same thing is via the Class description view. When an OWL class is selected in the entities view, the right-hand side of the tab shows the class description panel. If we select the 'vertebral column disease' class, we see in the class description view that this class is a \"SubClass Of\" (= has a SuperClass) the 'musculoskeletal system disease' class. Using the (+) button beside \"SubClass Of\" we could add another superclass to the 'skeletal system disease' class. Note the Anonymous Ancestors. This is a difficult concept we will return to later, and the contents of this portion may seem confusing at first (some of these may be clearer after you complete the \"Basics of OWL\" section below). These are OWL expressions that are inherited from the parents. If you hover over the Subclass Of (Anonymous Ancestor) you can see the parent that the class inherited the expression from. For many ontologies, you will see some quite abstract expressions in here inherited from upper ontologies, but these can generally be ignored for most purposes.","title":"The Class description view"},{"location":"lesson/contributing-to-obo-ontologies/#revising-a-superclass","text":"If you want to revise the superclass, click the 'o' symbol next to the superclass and replace the text. Try to revise 'musculoskeletal system disease' to 'disease by anatomical system'. If you want to delete a superclass, click the 'x' button next to the superclass. Delete the 'disease by anatomical system' superclass. Close this window without saving. Save your work.","title":"Revising a superclass:"},{"location":"lesson/contributing-to-obo-ontologies/#make-a-pull-request","text":"Click: Create Pull Request in GitHub Desktop This will automatically open GitHub Desktop Click the green button 'Create pull request' You may now add comments to your pull request. The CL editors team will review your PR and either ask for changes or merge it. The changes will be available in the next release.","title":"Make a Pull Request"},{"location":"lesson/contributing-to-obo-ontologies/#ontology-design-patterns","text":"Dead Simple Ontology Design Patterns (DOSDPs) are specifications, written in yaml format, that specify how ontology terms should be created (see article here ). They can be used to: generate documentation generate new terms retrofit existing ontology terms DOSDPs have some key features: Description : that describes the purpose of the patterns Examples : Provides examples of terms that use the Patterns Declared classes : these are the classes that are used in the pattern. Any subclass of the declared class can be used in this pattern. Declared relationships : the relationships used in the logical axioms vars : the variable classes that are used in the pattern. Any subclass of the 'var' can be used in this pattern. Pattern for class name, annotations, text definition and equivalentTo (logical definition) : Ontology classes are used as 'fillers' to create new classes that conform to the specific patterns. Examples of design patterns are available here: uPheno pattern library Mondo Disease Ontology pattern library","title":"Ontology design patterns"},{"location":"lesson/contributing-to-obo-ontologies/#use-templates-robot-dosdp","text":"under development","title":"Use templates: ROBOT, DOSDP"},{"location":"lesson/contributing-to-obo-ontologies/#basics-of-owl","text":"Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner?","title":"Basics of OWL"},{"location":"lesson/contributing-to-obo-ontologies/#logic-and-debugging","text":"Below are exercises to demonstrate how to: Add equivalent axioms (logical definitions) to ontology terms Run the reasoner and view the inferred hierarchy Debugging and viewing explanations","title":"Logic and debugging"},{"location":"lesson/contributing-to-obo-ontologies/#practice-adding-logic-and-debugging","text":"These instructions will use the Mondo disease ontology as an example.","title":"Practice adding logic and debugging"},{"location":"lesson/contributing-to-obo-ontologies/#practice-1","text":"","title":"Practice 1"},{"location":"lesson/contributing-to-obo-ontologies/#add-new-terms-with-an-equivalance-axiom-to-mondo","text":"","title":"Add New Terms with an Equivalance Axiom to Mondo:"},{"location":"lesson/contributing-to-obo-ontologies/#creating-a-new-class_1","text":"New classes are created in the Class hierarchy panel on the left. There are three buttons at the top of the class hierarchy view. These allow you to add a subclass (L-shaped icon), add a sibling class (c-shaped icon), or delete a selected class (x'd circle).","title":"Creating a new class"},{"location":"lesson/contributing-to-obo-ontologies/#practice-adding-a-new-term","text":"","title":"Practice adding a new term:"},{"location":"lesson/contributing-to-obo-ontologies/#add-the-new-term-mycotoxin-allergy","text":"Navigate to the Mondo repo in GitHub Desktop, create a branch, and open mondo-edit.obo in Protege. Search for the parent term 'allergic disease' (see search guide if you are unsure how to do this). When you are clicked on the term in the Class hierarchy pane, click the add subclass button to add a child class to 'allergic disease' A dialog will popup. Name this new subclass: mycotoxin allergy. Click \"OK\" to add the class.","title":"Add the new term 'mycotoxin allergy'"},{"location":"lesson/contributing-to-obo-ontologies/#add-annotations-and-a-logical-axiom","text":"Equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/ For this class, we want to follow the design pattern for allergy . Review this pattern before proceeding. Based on the pattern specifications, add a text definition to this term. Add the database cross reference to this term: MONDO:patterns/allergy Add a synonym that is consistent with this pattern. Add the equivalence axiom according to the pattern specifications. Run the reasoner View the inferred hierarchy. You should see a child of owl:Nothing (and you should see this in the Description pane as well.) Click on the ? button next to the owl:Nothing in the Description view The explanation tells you the reason why this is unsatisfiable Mycotoxin is a child of a 'specifically dependent continuant' and and 'independent continuant' is disjoint with 'specifically dependent continuant'. This logical axiom uses the relation 'realized in response to stimulus' Click on this relation in Protege (you can click on the relation name in the equivalence axiom, or you can click on the Object properties tab and search for this relation. There are domain and range restrictions on this property, where the range has to be a material enity. Chebi mycotoxin is actually a role, so it is not a material entity. Go back to the Classes or Entities pane and remove the equivalence axiom and run the reasoner again. You should now see no unsatisfiable classes.","title":"Add annotations and a logical axiom"},{"location":"lesson/contributing-to-obo-ontologies/#practice-2","text":"","title":"Practice 2"},{"location":"lesson/contributing-to-obo-ontologies/#practice-adding-a-new-term_1","text":"","title":"Practice adding a new term:"},{"location":"lesson/contributing-to-obo-ontologies/#add-the-new-term-acquired-alacrima","text":"Add 'acquired candidiasis as a subclass of MONDO_0002026 candidiasis.","title":"Add the new term 'acquired alacrima'"},{"location":"lesson/contributing-to-obo-ontologies/#add-annotations-and-a-logical-axiom_1","text":"As noted above, equivalence axioms in Mondo are added according to Dead Simple Ontology Design Patterns (DOSDPs). You can view all of the design patterns in Mondo by going to code/src/patterns/dosdp-patterns/ For this class, we want to follow the design pattern for acquired . Review this pattern before proceeding. Based on the pattern specifications, add a text definition to this term. Add the database cross reference to the definition: MONDO:patterns/allergy Add the equivalence axiom according to the pattern specifications. Run the reasoner View the inferred hierarchy.","title":"Add annotations and a logical axiom"},{"location":"lesson/contributing-to-obo-ontologies/#further-reading","text":"Debugging ontologies using OWL reasoning Chris Mungall on how to write great textual definitions","title":"Further reading"},{"location":"lesson/developing-an-obo-ontology/","text":"Developing an OBO Reference Ontology \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Ontology Development Automation Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: Leading a new or existing OBO project; maybe reference ontology develoment? Learning objectives \u00b6 detailed knowledge of OBO principles and best practises use OBO Dashboard use OBO Registry use PURL system Tutorials \u00b6 Week 8: Ontology Development \u00b6 First Instructor: Nico Matentzoglu, Becky Jackson Description \u00b6 By the end of this session, you should be able to: Merge ontology modules & imports with robot merge Create a classified version of an ontology with robot reason Add metadata to an ontology with robot annotate Create a simple release workflow using ROBOT commands in a Makefile Create a new ontology with ODK Preparation \u00b6 Please complete the following and then read the section below: ROBOT Mini-Tutorial, part 2 Software Carpentry: Automation and Make What is an ontology release? \u00b6 Like software, official OBO Foundry ontologies have versioned releases . This is important because OBO Foundry ontologies are expected to be shared and reused. Since ontologies are bound to change over time as more terms are added and refined, other developers need stable versions to point to so that there are no surprises. OBO Foundry ontologies use GitHub releases to maintain these stable copies of older versions. Generally, OBO Foundry ontologies maintain an \"edit\" version of their file that changes without notice and should not be used by external ontology developers because of this. The edit file is used to create releases on a (hopefully) regular basis. The released version of an OBO Foundry ontology generally a merged and reasoned version of the edit file. This means that all modules and imports are combined into one file, and that file has the inferred class hierarchy actually asserted. It also often has some extra metadata, including a version IRI . OBO Foundry defines the requirements for version IRIs here . The release workflow process should be stable and can be written as a series of steps, e.g.: Update modules from templates Merge ontology modules & the main edit file into one Assert the inferred class hierarchy Add a version IRI & other important metadata This series of steps can be turned into ROBOT commands: robot template robot merge robot reason robot annotate Since we can turn these steps into a series of commands, we can create a Makefile that stores these as \"recipes\" for our ontology release! Review the ROBOT commands we've learned so far (Becky; review; 30 minutes) Week 5: report and query Week 6: convert , extract , and template New: merge , reason , annotate , and diff Chaining ROBOT commands Specifying custom prefixes Introduction to Makefiles & workflows (Becky; review; 30 minutes) Review Software Carpentry course content How can the ROBOT commands be combined to create an ontology release? Practice writing recipes using ROBOT commands Using the ODK to bootstrap a new ontology (Nico; 45 minutes) Introduction to the OBO Foundry Registry (if time) Additional materials and resources \u00b6 Contributors \u00b6 Nico Matentzoglu","title":"Developing an OBO Ontology"},{"location":"lesson/developing-an-obo-ontology/#developing-an-obo-reference-ontology","text":"","title":"Developing an OBO Reference Ontology"},{"location":"lesson/developing-an-obo-ontology/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/developing-an-obo-ontology/#prerequisites","text":"Review tutorial on Ontology Development Automation","title":"Prerequisites"},{"location":"lesson/developing-an-obo-ontology/#preparation","text":"TBD","title":"Preparation"},{"location":"lesson/developing-an-obo-ontology/#what-is-delivered-as-part-of-the-course","text":"Description: Leading a new or existing OBO project; maybe reference ontology develoment?","title":"What is delivered as part of the course"},{"location":"lesson/developing-an-obo-ontology/#learning-objectives","text":"detailed knowledge of OBO principles and best practises use OBO Dashboard use OBO Registry use PURL system","title":"Learning objectives"},{"location":"lesson/developing-an-obo-ontology/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/developing-an-obo-ontology/#week-8-ontology-development","text":"First Instructor: Nico Matentzoglu, Becky Jackson","title":"Week 8: Ontology Development"},{"location":"lesson/developing-an-obo-ontology/#description","text":"By the end of this session, you should be able to: Merge ontology modules & imports with robot merge Create a classified version of an ontology with robot reason Add metadata to an ontology with robot annotate Create a simple release workflow using ROBOT commands in a Makefile Create a new ontology with ODK","title":"Description"},{"location":"lesson/developing-an-obo-ontology/#preparation_1","text":"Please complete the following and then read the section below: ROBOT Mini-Tutorial, part 2 Software Carpentry: Automation and Make","title":"Preparation"},{"location":"lesson/developing-an-obo-ontology/#what-is-an-ontology-release","text":"Like software, official OBO Foundry ontologies have versioned releases . This is important because OBO Foundry ontologies are expected to be shared and reused. Since ontologies are bound to change over time as more terms are added and refined, other developers need stable versions to point to so that there are no surprises. OBO Foundry ontologies use GitHub releases to maintain these stable copies of older versions. Generally, OBO Foundry ontologies maintain an \"edit\" version of their file that changes without notice and should not be used by external ontology developers because of this. The edit file is used to create releases on a (hopefully) regular basis. The released version of an OBO Foundry ontology generally a merged and reasoned version of the edit file. This means that all modules and imports are combined into one file, and that file has the inferred class hierarchy actually asserted. It also often has some extra metadata, including a version IRI . OBO Foundry defines the requirements for version IRIs here . The release workflow process should be stable and can be written as a series of steps, e.g.: Update modules from templates Merge ontology modules & the main edit file into one Assert the inferred class hierarchy Add a version IRI & other important metadata This series of steps can be turned into ROBOT commands: robot template robot merge robot reason robot annotate Since we can turn these steps into a series of commands, we can create a Makefile that stores these as \"recipes\" for our ontology release! Review the ROBOT commands we've learned so far (Becky; review; 30 minutes) Week 5: report and query Week 6: convert , extract , and template New: merge , reason , annotate , and diff Chaining ROBOT commands Specifying custom prefixes Introduction to Makefiles & workflows (Becky; review; 30 minutes) Review Software Carpentry course content How can the ROBOT commands be combined to create an ontology release? Practice writing recipes using ROBOT commands Using the ODK to bootstrap a new ontology (Nico; 45 minutes) Introduction to the OBO Foundry Registry (if time)","title":"What is an ontology release?"},{"location":"lesson/developing-an-obo-ontology/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/developing-an-obo-ontology/#contributors","text":"Nico Matentzoglu","title":"Contributors"},{"location":"lesson/developing-application-ontologies/","text":"Developing an Application Ontology \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Ontology Contribution Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: Combining ontology subsets for use in a project. Learning objectives \u00b6 manage GitHub manage ontology imports use ROBOT extract: MIREOT, SLME use ROBOT report pruning trees Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 Contributors \u00b6 Nico Matentzoglu","title":"Developing an Application Ontology"},{"location":"lesson/developing-application-ontologies/#developing-an-application-ontology","text":"","title":"Developing an Application Ontology"},{"location":"lesson/developing-application-ontologies/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/developing-application-ontologies/#prerequisites","text":"Review tutorial on Ontology Contribution","title":"Prerequisites"},{"location":"lesson/developing-application-ontologies/#preparation","text":"TBD","title":"Preparation"},{"location":"lesson/developing-application-ontologies/#what-is-delivered-as-part-of-the-course","text":"Description: Combining ontology subsets for use in a project.","title":"What is delivered as part of the course"},{"location":"lesson/developing-application-ontologies/#learning-objectives","text":"manage GitHub manage ontology imports use ROBOT extract: MIREOT, SLME use ROBOT report pruning trees","title":"Learning objectives"},{"location":"lesson/developing-application-ontologies/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"lesson/developing-application-ontologies/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/developing-application-ontologies/#contributors","text":"Nico Matentzoglu","title":"Contributors"},{"location":"lesson/modelling-with-object-properties/","text":"Modeling with Object Properties \u00b6 In this lesson, we will give an intuition of how to work with object properties in OBO ontologies, also referred to as \"relations\". We will cover, in particular, the following subjects: What is the role of object properties in OBO ontologies, and how should we model them? What is the relation ontology (RO), and how do we add object properties to it? Preparation \u00b6 We have worked with the University of Manchester to incorporate the Family History Knowledge Base Tutorial fully into OBO Academy. This is it: OBOAcademy: Family History - Modelling with Object Properties . In contrast to the Pizza tutorial, the Family history tutorial focuses on modelling with individuals. Chapters 4, 5, 8 and 9 are full of object property modelling, and are not only great to get a basic understanding of using them in your ontology, but also give good hints at where OWL and object properties fall short. We refer to the FHKB in the following and expect you to have completed at least chapter 5 before reading on. The Role of Object Properties in the OBO-sphere \u00b6 To remind ourselves, there are three different types of relations in OWL: Data properties (DatatypeProperty) connect your classes and individuals to data values, such as strings or numbers. In OBO, these are the least frequently used kinds of properties, used for example by CIDO and ONS. For some example usage, run the following query in the ontobee OLS endpoint: http://www.ontobee.org/sparql prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct * WHERE { GRAPH ?graph_uri { ?dp rdf:type owl:DatatypeProperty . ?sub ?dp ?obj } } Note that many uses of data properties across OBO are a bit questionable, for example, you do never want to attach a modification dates or similar to your classes using data properties, as these fall under OWL semantics . This means that logically, if a superclass has a relation using a DatatypeProperty, then this relation _holds for all subclasses of that class as well. Annotation properties are similar to data properties, but they are outside of OWL semantics , i.e. OWL reasoners and reasoning do not care, in fact ignore, anything related to annotation properties. This makes them suitable for attaching metadata like labels etc to our classes and properties. We sometimes use annotation properties even to describe relationships between classes if we want reasoners to ignore them. The most typical example is IAO:replaced_by, which connects an obsolete term with its replacement. Widely used annotation properties in the OBO-sphere are standardised in the OBO Metadata Ontology (OMO) . The main type of relation we use in OBO Foundry are object properties . Object properties relate two individuals or classes with each other, for example: OWLObjectPropertyAssertion(:part_of, :heart, :cardiovascular_system) In the same way as annotation properties are maintained in OMO (see above), object properties are maintained in the Relation Ontology (RO) . Object properties are of central importance to all ontological modelling in the OBO sphere, and understanding their semantics is critical for any put the most trivial ontologies. We assume the reader to have completed the Family History Tutorial mentioned above . Object property semantics in OBO \u00b6 In our experience, these are the most widely used characteristics we specify about object properties (OP): Sub-property: if an OP is a sub-property of another parent OP, it inherits all its semantic characteristics. Most importantly: if OP1 is a sub-property of OP2, then, if (a)--[OP1]-->(b), we infer that (a)--[OP2]-->(b). Domain: if OP has a domain C, it means that every time (a)--[OP]-->(b), (a) must be a C. For example, ecologically co-occurs with in RO has the domain 'organism or virus or viroid' , which means that whenever anything ecologically co-occurs with something else, it will be inferred to be a 'organism or virus or viroid' . Range: if OP has a range C, it means that every time (a)--[OP]-->(b), (b) must be a C. For example produced by has the domain material entity . Note that in ontologies, ranges are slightly less powerful then domains: If we have a class Moderna Vaccine which is SubClass of 'produced by' some 'Moderna' we get that Moderna Vaccine is a material entity due to the domain constraint, but NOT that Moderna is a material entity due to the range constraint (explanation to this is a bit complicated, sorry). Transitivity: if an OP is transitive, it means that if (a)--[OP]-->(b)--[OP]-->(c), (a)--[OP]-->(c). For example, if the eye is part of the head, which is part of the body, we can infer that the eye must be part of the body. Property chains: Similar to transitive properties, property chains allow us to bridge across multiple properties. The FHKB tutorial above is all about amazing property chains so you should have a deep understanding of these if you followed the tutorial. Other characteristics like functionality and symmetry are used across OBO ontologies, but not nearly to the same extend as the 5 described above. The Relation Ontology (RO) \u00b6 The Relation Ontology serves two main purposes in the OBO world: As a place to standardise object properties. The idea is this: many ontologies are modelling mereological relations, such as partonomies, which requires relationships such as \"part of\" and \"has part\". To ensure that ontologies are interoperable, we need to make sure that all ontologies use the same \"part of\" relationship. Historically this is not always been true, and still is not. At the time of this writing, running: prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct ?graph_uri ?s WHERE { GRAPH ?graph_uri { ?s rdf:type owl:ObjectProperty ; rdfs:label \"part of\" . } } On the OntoBee SPARQL endpoint still reveals a number of ontologies using non-standard part-of relations. In our experience, most of these are accidental due to past format conversions, but not all. This problem was much worse before RO came along, and our goal is to unify the representation of key properties like \"part of\" across all OBO ontologies. The OBO Dashboard checks for object properties that are not aligned with RO. As a place to encode and negotiate object property semantics. Object properties (OP) can have domains and ranges, can have characteristics such as functionality and transitivity, see above . Arguing the exact semantics of an OP can be a difficult and lengthy collaborative process, esp. since OP semantics can have a huge impact on ontology reasoning. Detailed RO documentation (modelling patterns and practices) can be found in here . The process of how relationships are added to RO is discussed in the next section. Adding relationships to RO \u00b6 To add a relationship we usually follow the following process. For details, please refer to the RO documentation . Check whether the OP is already in RO . Search for synonyms - often the relationship you are looking exist, but under a different name. If you cant find the exact OP, see whether you can find similar OPs - this may help you also identify suitable parent OPs. Make an RO issue . Take care to not only describe the name of your relationship, but also intended application areas with examples, a good definition , potential parent relationships, domains and ranges. The more detail you provide, the easier it will be for the community to review your request. Make a pull request . This involves the same steps as usual . If you are unsure what annotations need to be added and how to reflect the intended semantics, it may be useful to look at past pull requests . Join our quarterly RO calls and check out the RO documentation .","title":"Modelling with Object Properties"},{"location":"lesson/modelling-with-object-properties/#modeling-with-object-properties","text":"In this lesson, we will give an intuition of how to work with object properties in OBO ontologies, also referred to as \"relations\". We will cover, in particular, the following subjects: What is the role of object properties in OBO ontologies, and how should we model them? What is the relation ontology (RO), and how do we add object properties to it?","title":"Modeling with Object Properties"},{"location":"lesson/modelling-with-object-properties/#preparation","text":"We have worked with the University of Manchester to incorporate the Family History Knowledge Base Tutorial fully into OBO Academy. This is it: OBOAcademy: Family History - Modelling with Object Properties . In contrast to the Pizza tutorial, the Family history tutorial focuses on modelling with individuals. Chapters 4, 5, 8 and 9 are full of object property modelling, and are not only great to get a basic understanding of using them in your ontology, but also give good hints at where OWL and object properties fall short. We refer to the FHKB in the following and expect you to have completed at least chapter 5 before reading on.","title":"Preparation"},{"location":"lesson/modelling-with-object-properties/#the-role-of-object-properties-in-the-obo-sphere","text":"To remind ourselves, there are three different types of relations in OWL: Data properties (DatatypeProperty) connect your classes and individuals to data values, such as strings or numbers. In OBO, these are the least frequently used kinds of properties, used for example by CIDO and ONS. For some example usage, run the following query in the ontobee OLS endpoint: http://www.ontobee.org/sparql prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct * WHERE { GRAPH ?graph_uri { ?dp rdf:type owl:DatatypeProperty . ?sub ?dp ?obj } } Note that many uses of data properties across OBO are a bit questionable, for example, you do never want to attach a modification dates or similar to your classes using data properties, as these fall under OWL semantics . This means that logically, if a superclass has a relation using a DatatypeProperty, then this relation _holds for all subclasses of that class as well. Annotation properties are similar to data properties, but they are outside of OWL semantics , i.e. OWL reasoners and reasoning do not care, in fact ignore, anything related to annotation properties. This makes them suitable for attaching metadata like labels etc to our classes and properties. We sometimes use annotation properties even to describe relationships between classes if we want reasoners to ignore them. The most typical example is IAO:replaced_by, which connects an obsolete term with its replacement. Widely used annotation properties in the OBO-sphere are standardised in the OBO Metadata Ontology (OMO) . The main type of relation we use in OBO Foundry are object properties . Object properties relate two individuals or classes with each other, for example: OWLObjectPropertyAssertion(:part_of, :heart, :cardiovascular_system) In the same way as annotation properties are maintained in OMO (see above), object properties are maintained in the Relation Ontology (RO) . Object properties are of central importance to all ontological modelling in the OBO sphere, and understanding their semantics is critical for any put the most trivial ontologies. We assume the reader to have completed the Family History Tutorial mentioned above .","title":"The Role of Object Properties in the OBO-sphere"},{"location":"lesson/modelling-with-object-properties/#object-property-semantics-in-obo","text":"In our experience, these are the most widely used characteristics we specify about object properties (OP): Sub-property: if an OP is a sub-property of another parent OP, it inherits all its semantic characteristics. Most importantly: if OP1 is a sub-property of OP2, then, if (a)--[OP1]-->(b), we infer that (a)--[OP2]-->(b). Domain: if OP has a domain C, it means that every time (a)--[OP]-->(b), (a) must be a C. For example, ecologically co-occurs with in RO has the domain 'organism or virus or viroid' , which means that whenever anything ecologically co-occurs with something else, it will be inferred to be a 'organism or virus or viroid' . Range: if OP has a range C, it means that every time (a)--[OP]-->(b), (b) must be a C. For example produced by has the domain material entity . Note that in ontologies, ranges are slightly less powerful then domains: If we have a class Moderna Vaccine which is SubClass of 'produced by' some 'Moderna' we get that Moderna Vaccine is a material entity due to the domain constraint, but NOT that Moderna is a material entity due to the range constraint (explanation to this is a bit complicated, sorry). Transitivity: if an OP is transitive, it means that if (a)--[OP]-->(b)--[OP]-->(c), (a)--[OP]-->(c). For example, if the eye is part of the head, which is part of the body, we can infer that the eye must be part of the body. Property chains: Similar to transitive properties, property chains allow us to bridge across multiple properties. The FHKB tutorial above is all about amazing property chains so you should have a deep understanding of these if you followed the tutorial. Other characteristics like functionality and symmetry are used across OBO ontologies, but not nearly to the same extend as the 5 described above.","title":"Object property semantics in OBO"},{"location":"lesson/modelling-with-object-properties/#the-relation-ontology-ro","text":"The Relation Ontology serves two main purposes in the OBO world: As a place to standardise object properties. The idea is this: many ontologies are modelling mereological relations, such as partonomies, which requires relationships such as \"part of\" and \"has part\". To ensure that ontologies are interoperable, we need to make sure that all ontologies use the same \"part of\" relationship. Historically this is not always been true, and still is not. At the time of this writing, running: prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT distinct ?graph_uri ?s WHERE { GRAPH ?graph_uri { ?s rdf:type owl:ObjectProperty ; rdfs:label \"part of\" . } } On the OntoBee SPARQL endpoint still reveals a number of ontologies using non-standard part-of relations. In our experience, most of these are accidental due to past format conversions, but not all. This problem was much worse before RO came along, and our goal is to unify the representation of key properties like \"part of\" across all OBO ontologies. The OBO Dashboard checks for object properties that are not aligned with RO. As a place to encode and negotiate object property semantics. Object properties (OP) can have domains and ranges, can have characteristics such as functionality and transitivity, see above . Arguing the exact semantics of an OP can be a difficult and lengthy collaborative process, esp. since OP semantics can have a huge impact on ontology reasoning. Detailed RO documentation (modelling patterns and practices) can be found in here . The process of how relationships are added to RO is discussed in the next section.","title":"The Relation Ontology (RO)"},{"location":"lesson/modelling-with-object-properties/#adding-relationships-to-ro","text":"To add a relationship we usually follow the following process. For details, please refer to the RO documentation . Check whether the OP is already in RO . Search for synonyms - often the relationship you are looking exist, but under a different name. If you cant find the exact OP, see whether you can find similar OPs - this may help you also identify suitable parent OPs. Make an RO issue . Take care to not only describe the name of your relationship, but also intended application areas with examples, a good definition , potential parent relationships, domains and ranges. The more detail you provide, the easier it will be for the community to review your request. Make a pull request . This involves the same steps as usual . If you are unsure what annotations need to be added and how to reflect the intended semantics, it may be useful to look at past pull requests . Join our quarterly RO calls and check out the RO documentation .","title":"Adding relationships to RO"},{"location":"lesson/ontology-design/","text":"Ontology Design \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Participants will need to have access to the following resources and tools prior to the training: GitHub account - register for a free GitHub account here Protege - Install Protege 5.5, download it here Install ELK 0.5 Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop Preparation \u00b6 Review tutorial on Ontology Term Use Review tutorial on Contributing to OBO Ontologies Clone Mondo repo: Follow these instructions to clone the Mondo repo What is delivered as part of this course \u00b6 Description : This course will cover reasoning with OWL. Learning objectives \u00b6 At the end of this lesson, you should know how to do: Add existential restrictions Add defined classes Add disjoint axioms Debug unsatisfiable classes Tutorials \u00b6 OpenHPI Course Content Ontologies and Logic Videos 3.0-3.10 | Duration: ~3.5 hrs OWL, Rules, and Reasoning Videos 4.0-4.8 | Duration: ~2.7 hrs Additional Materials and Resources \u00b6 Monkeying around with OWL: Musings on building and using ontologies, posts by Chris Mungall Documentation on Cell Ontology relations Guidelines for writing definitions in Ontologies (paper) How to deal with unintentional equivalent classes : Cool post by Chris Mungall on how to deal with an important reasoning issue. Semantic Engineer Toolbox \u00b6 Protege ELK Protege Plugin GitHub Desktop Contributors \u00b6 Nicole Vasilevsky Nico Matentzoglu Acknowledgement \u00b6 Content was adapted from Ontology 101 Tutorial OWL class restrictions \u00b6 In OWL, we use object properties to describe binary relationships between two individuals (or instances). We can also use the properties to describe new classes (or sets of individuals) using restrictions . A restriction describes a class of individuals based on the relationships that members of the class participate in. In other words, a restriction is a kind of class, in the same way that a named class is a kind of class. For example, we can use a named class to capture all the individuals that are idiopathic diseases. But we could also describe the class of idiopathic disease as all the instances that are ' has modifier' idiopathic disease. In OWL, there are three main types of restrictions that can be placed on classes. These are quantifier restriction , cardinality restrictions , and hasValue restriction. In this tutorial will initially focus on quantifier restrictions. Quantifier restrictions are further categorized into two types, the existential and the universal restriction. Existential restrictions describe classes of individuals that participate in at least one relationship along a specified property to individuals that are members of a specified class. For example, the class of individuals that have at least one ( some ) 'has modifier' relationship to members of the idiopathic disease class. In Protege, the keyword 'some' is used to denote existential restrictions. Universal restrictions describe classes of individuals that for a given property only have relationships along this property to individuals that are members of a specified class. For example, we can say a cellular component is capable of many functions using the existential quantifier, however, OWL semantics assume that there could be more. We can use the universal quantifier to add closure to the existential. That is, we can assert that a cellular component is capable of these functions, and is only capable of those functions and no other. Another example is that the process of hair growth is found only in instances of the class Mammalia. In Protege the keyword ' only ' is used. In this tutorial, we will deal exclusively with the existential (some) quantifier. Superclass restrictions \u00b6 Strictly speaking in OWL, you don't make relationships between classes , however, using OWL restrictions we essentially achieve the same thing. We wanted to capture the knowledge that the named class ' idiopathic achalasia ' is an idiopathic disease . In OWL speak, we want to say that every instance of an ' idiopathic achalasia ' is also an instance of the class of things that have at least one 'has modifier' relationship to an idiopathic disease . In OWL, we do this by creating an existential restriction on the idiopathic achalasia class. In the Entities tab, select ' idiopathic achalasia ' in the class hierarchy and look at its current class description in the bottom right box. Note that there are two superclasses (as denoted by the SubClass Of list). ' 'gastroesophageal disease' ' and 'has modifier' some idiopathic . Run the reasoner. You should see that this class is now inferred to be an idiopathic disease because of this SubClassOf (superclass) restriction. Equivalence Axioms and Automatic classification \u00b6 This example introduces equivalence axioms or defined classes (also called logical definitions ) and automatic classification. The example involves classification of Mendelian diseases that have a monogenic (single gene) varation. These equivalence axioms are based off the Mondo Deisgn Pattern disease_series_by_gene . Constructs: and (intersection) equivalence (logical definitions) existential restrictions (e.g. 'disease has basis in dysfunction of') Add an equivalence axiom to an existing Mondo term \u00b6 Create a new branch and open (or re-open) mondo-edit.obo Navigate to the class 'cardioacrofacial dysplasia 1' According to OMIM , this disease is caused by a variation in the gene PRKACA. We want to add an equivalence axiom that says every instance of this class is a type of 'cardioacrofacial dysplasia' that has dysfunction in the PRKACA gene. To do this, click the + next to Equivalent To in the lower right Description box. Add the following equivalence axiom: 'cardioacrofacial dysplasia' and ('disease has basis in dysfunction of' some PRKACA) Run the reasoner. You shouldn't see any change, but try deleting the superclass assertion to 'cardioacrofacial dysplasia' and re-running the reasoner. You should see that 'cardioacrofacial dysplasia' is an inferred superclass. Undo the last change and save your work and commmit and create a pull request. Adding classes and automatically classifying them \u00b6 For teaching purposes, let's say we need a new class that is 'fungal allergy'. Create a branch and re-open mondo-edit.obo Add a new term under owl:Thing named 'fungal allergy'. Following the design pattern allergy.yaml , add the text definition, synonym and equivalentTo axiom, using the substance ECTO_0000524 'exposure to mycotoxin'. Run the reasoner and note where the class is automatically classified. Create a pull request and note in the PR what the parent class is. Debugging automatic classifications \u00b6 On the same branch, add a new term under owl:Thing named 'oral cavity neoplasm'. Following the design pattern neoplasm_by_origin , add the term label and the equivalence axiom. Run the reasoner and note where the term in automatically classified. You should see it is under owl:Nothing. Click the ? next to owl:Nothing in the Description box to see the explanation. Can you determine why this is an unsatisfiable class? Create a pull request and add a comment explaining why this is unsatisfiable. Disjointness \u00b6 By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom. Create a branch in the Mondo repo (name it: disjoint-[your initials]. For example: disjoint-nv) Open the mondo-edit.obo file Per this ticket , we want to assert that infectious disease and 'syndromic disease' are disjoint. To do this first search for and select the infectious disease class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the Expression editor, add 'syndromic disease' as disjoint with 'infectious disease'. Run the ELK reasoner. Scroll to the top of your hierarchy and note that owl:Nothing has turned red. This is because there are unsatisfiable classes. Review and fix one unsatisfiable class \u00b6 Below we'll review an example of one class and how to fix it. Next you should review and fix another one on your own and create a pull request for Nicole or Nico to review. Note, fixing these may require a bit of review and subjective decision making and the fix described below may not necessarily apply to each case. Review Bickerstaff brainstem encephalitis : To understand why this class appeared under owl:Nothing, first click the ? next to owl:Nothing in the Description box. (Note, this can take a few minutes). The explanation is displayed above - it is because this class is a descedent of Guillain-Barre syndrome , which is a child of syndromic disease . Next, we have to ask if Bickerstaff brainstem encephalitis is an appropriate child of regional variant of Guillain-Barre syndrome . Note, Mondo integrates several disease terminologies and ontologies, and brought in all the subclass hierarchies from these source ontologies. To see the source of this superclass assertion, click the @ next to the assertion. This source came from Orphanet, see below. Based on the text definition, there does not seem to be any suggestion that this disease is a type of Guillain-Barre syndrome. Assuming that this disease is not a type of Guillain-Barre syndrome, we should exclude the superclass regional variant of Guillain-Barre syndrome (see this paper and this paper . It seems a bit unclear what the relationship of BBE is to Guillain-Barre syndrome. This also brings into the question if a disease can be syndromic and an infectious disease - maybe this disjoint axiom is wrong, but let's not worry about this for the teaching purposes.) To exclude a superclass, follow the instructions here .","title":"Ontology Design"},{"location":"lesson/ontology-design/#ontology-design","text":"","title":"Ontology Design"},{"location":"lesson/ontology-design/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/ontology-design/#prerequisites","text":"Participants will need to have access to the following resources and tools prior to the training: GitHub account - register for a free GitHub account here Protege - Install Protege 5.5, download it here Install ELK 0.5 Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop","title":"Prerequisites"},{"location":"lesson/ontology-design/#preparation","text":"Review tutorial on Ontology Term Use Review tutorial on Contributing to OBO Ontologies Clone Mondo repo: Follow these instructions to clone the Mondo repo","title":"Preparation"},{"location":"lesson/ontology-design/#what-is-delivered-as-part-of-this-course","text":"Description : This course will cover reasoning with OWL.","title":"What is delivered as part of this course"},{"location":"lesson/ontology-design/#learning-objectives","text":"At the end of this lesson, you should know how to do: Add existential restrictions Add defined classes Add disjoint axioms Debug unsatisfiable classes","title":"Learning objectives"},{"location":"lesson/ontology-design/#tutorials","text":"OpenHPI Course Content Ontologies and Logic Videos 3.0-3.10 | Duration: ~3.5 hrs OWL, Rules, and Reasoning Videos 4.0-4.8 | Duration: ~2.7 hrs","title":"Tutorials"},{"location":"lesson/ontology-design/#additional-materials-and-resources","text":"Monkeying around with OWL: Musings on building and using ontologies, posts by Chris Mungall Documentation on Cell Ontology relations Guidelines for writing definitions in Ontologies (paper) How to deal with unintentional equivalent classes : Cool post by Chris Mungall on how to deal with an important reasoning issue.","title":"Additional Materials and Resources"},{"location":"lesson/ontology-design/#semantic-engineer-toolbox","text":"Protege ELK Protege Plugin GitHub Desktop","title":"Semantic Engineer Toolbox"},{"location":"lesson/ontology-design/#contributors","text":"Nicole Vasilevsky Nico Matentzoglu","title":"Contributors"},{"location":"lesson/ontology-design/#acknowledgement","text":"Content was adapted from Ontology 101 Tutorial","title":"Acknowledgement"},{"location":"lesson/ontology-design/#owl-class-restrictions","text":"In OWL, we use object properties to describe binary relationships between two individuals (or instances). We can also use the properties to describe new classes (or sets of individuals) using restrictions . A restriction describes a class of individuals based on the relationships that members of the class participate in. In other words, a restriction is a kind of class, in the same way that a named class is a kind of class. For example, we can use a named class to capture all the individuals that are idiopathic diseases. But we could also describe the class of idiopathic disease as all the instances that are ' has modifier' idiopathic disease. In OWL, there are three main types of restrictions that can be placed on classes. These are quantifier restriction , cardinality restrictions , and hasValue restriction. In this tutorial will initially focus on quantifier restrictions. Quantifier restrictions are further categorized into two types, the existential and the universal restriction. Existential restrictions describe classes of individuals that participate in at least one relationship along a specified property to individuals that are members of a specified class. For example, the class of individuals that have at least one ( some ) 'has modifier' relationship to members of the idiopathic disease class. In Protege, the keyword 'some' is used to denote existential restrictions. Universal restrictions describe classes of individuals that for a given property only have relationships along this property to individuals that are members of a specified class. For example, we can say a cellular component is capable of many functions using the existential quantifier, however, OWL semantics assume that there could be more. We can use the universal quantifier to add closure to the existential. That is, we can assert that a cellular component is capable of these functions, and is only capable of those functions and no other. Another example is that the process of hair growth is found only in instances of the class Mammalia. In Protege the keyword ' only ' is used. In this tutorial, we will deal exclusively with the existential (some) quantifier.","title":"OWL class restrictions"},{"location":"lesson/ontology-design/#superclass-restrictions","text":"Strictly speaking in OWL, you don't make relationships between classes , however, using OWL restrictions we essentially achieve the same thing. We wanted to capture the knowledge that the named class ' idiopathic achalasia ' is an idiopathic disease . In OWL speak, we want to say that every instance of an ' idiopathic achalasia ' is also an instance of the class of things that have at least one 'has modifier' relationship to an idiopathic disease . In OWL, we do this by creating an existential restriction on the idiopathic achalasia class. In the Entities tab, select ' idiopathic achalasia ' in the class hierarchy and look at its current class description in the bottom right box. Note that there are two superclasses (as denoted by the SubClass Of list). ' 'gastroesophageal disease' ' and 'has modifier' some idiopathic . Run the reasoner. You should see that this class is now inferred to be an idiopathic disease because of this SubClassOf (superclass) restriction.","title":"Superclass restrictions"},{"location":"lesson/ontology-design/#equivalence-axioms-and-automatic-classification","text":"This example introduces equivalence axioms or defined classes (also called logical definitions ) and automatic classification. The example involves classification of Mendelian diseases that have a monogenic (single gene) varation. These equivalence axioms are based off the Mondo Deisgn Pattern disease_series_by_gene . Constructs: and (intersection) equivalence (logical definitions) existential restrictions (e.g. 'disease has basis in dysfunction of')","title":"Equivalence Axioms and Automatic classification"},{"location":"lesson/ontology-design/#add-an-equivalence-axiom-to-an-existing-mondo-term","text":"Create a new branch and open (or re-open) mondo-edit.obo Navigate to the class 'cardioacrofacial dysplasia 1' According to OMIM , this disease is caused by a variation in the gene PRKACA. We want to add an equivalence axiom that says every instance of this class is a type of 'cardioacrofacial dysplasia' that has dysfunction in the PRKACA gene. To do this, click the + next to Equivalent To in the lower right Description box. Add the following equivalence axiom: 'cardioacrofacial dysplasia' and ('disease has basis in dysfunction of' some PRKACA) Run the reasoner. You shouldn't see any change, but try deleting the superclass assertion to 'cardioacrofacial dysplasia' and re-running the reasoner. You should see that 'cardioacrofacial dysplasia' is an inferred superclass. Undo the last change and save your work and commmit and create a pull request.","title":"Add an equivalence axiom to an existing Mondo term"},{"location":"lesson/ontology-design/#adding-classes-and-automatically-classifying-them","text":"For teaching purposes, let's say we need a new class that is 'fungal allergy'. Create a branch and re-open mondo-edit.obo Add a new term under owl:Thing named 'fungal allergy'. Following the design pattern allergy.yaml , add the text definition, synonym and equivalentTo axiom, using the substance ECTO_0000524 'exposure to mycotoxin'. Run the reasoner and note where the class is automatically classified. Create a pull request and note in the PR what the parent class is.","title":"Adding classes and automatically classifying them"},{"location":"lesson/ontology-design/#debugging-automatic-classifications","text":"On the same branch, add a new term under owl:Thing named 'oral cavity neoplasm'. Following the design pattern neoplasm_by_origin , add the term label and the equivalence axiom. Run the reasoner and note where the term in automatically classified. You should see it is under owl:Nothing. Click the ? next to owl:Nothing in the Description box to see the explanation. Can you determine why this is an unsatisfiable class? Create a pull request and add a comment explaining why this is unsatisfiable.","title":"Debugging automatic classifications"},{"location":"lesson/ontology-design/#disjointness","text":"By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom. Create a branch in the Mondo repo (name it: disjoint-[your initials]. For example: disjoint-nv) Open the mondo-edit.obo file Per this ticket , we want to assert that infectious disease and 'syndromic disease' are disjoint. To do this first search for and select the infectious disease class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the Expression editor, add 'syndromic disease' as disjoint with 'infectious disease'. Run the ELK reasoner. Scroll to the top of your hierarchy and note that owl:Nothing has turned red. This is because there are unsatisfiable classes.","title":"Disjointness"},{"location":"lesson/ontology-design/#review-and-fix-one-unsatisfiable-class","text":"Below we'll review an example of one class and how to fix it. Next you should review and fix another one on your own and create a pull request for Nicole or Nico to review. Note, fixing these may require a bit of review and subjective decision making and the fix described below may not necessarily apply to each case. Review Bickerstaff brainstem encephalitis : To understand why this class appeared under owl:Nothing, first click the ? next to owl:Nothing in the Description box. (Note, this can take a few minutes). The explanation is displayed above - it is because this class is a descedent of Guillain-Barre syndrome , which is a child of syndromic disease . Next, we have to ask if Bickerstaff brainstem encephalitis is an appropriate child of regional variant of Guillain-Barre syndrome . Note, Mondo integrates several disease terminologies and ontologies, and brought in all the subclass hierarchies from these source ontologies. To see the source of this superclass assertion, click the @ next to the assertion. This source came from Orphanet, see below. Based on the text definition, there does not seem to be any suggestion that this disease is a type of Guillain-Barre syndrome. Assuming that this disease is not a type of Guillain-Barre syndrome, we should exclude the superclass regional variant of Guillain-Barre syndrome (see this paper and this paper . It seems a bit unclear what the relationship of BBE is to Guillain-Barre syndrome. This also brings into the question if a disease can be syndromic and an infectious disease - maybe this disjoint axiom is wrong, but let's not worry about this for the teaching purposes.) To exclude a superclass, follow the instructions here .","title":"Review and fix one unsatisfiable class"},{"location":"lesson/ontology-development/","text":"Ontology Development: Release Management, Quality Control and Collaborative Methods \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 You have a GitHub account You have set up docker and installed the ODK ( how to ) Preparation \u00b6 You have prepared your ODK set-up Read ODK reference overview What is delivered as part of the course \u00b6 Learning objectives \u00b6 Tutorials \u00b6 Additional materials and resources \u00b6 Contributors \u00b6","title":"Ontology Development"},{"location":"lesson/ontology-development/#ontology-development-release-management-quality-control-and-collaborative-methods","text":"","title":"Ontology Development: Release Management, Quality Control and Collaborative Methods"},{"location":"lesson/ontology-development/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/ontology-development/#prerequisites","text":"You have a GitHub account You have set up docker and installed the ODK ( how to )","title":"Prerequisites"},{"location":"lesson/ontology-development/#preparation","text":"You have prepared your ODK set-up Read ODK reference overview","title":"Preparation"},{"location":"lesson/ontology-development/#what-is-delivered-as-part-of-the-course","text":"","title":"What is delivered as part of the course"},{"location":"lesson/ontology-development/#learning-objectives","text":"","title":"Learning objectives"},{"location":"lesson/ontology-development/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/ontology-development/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/ontology-development/#contributors","text":"","title":"Contributors"},{"location":"lesson/ontology-fundamentals/","text":"Ontologies: Fundamentals \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Install Protege Preparation \u00b6 Complete OpenHPI Week 5: Ontology Engineering videos 5.1, 5.2, and 5.4 - 5.6 (~2.5 hours) We are skipping 5.3: Ontology Learning and both sections on MORE Ontology Evaluation (5.7 and 5.8) Complete part of the Ontologies 101 Tutorial (~2 hours) Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Don't spend too much time on this if you get stuck, we'll look at an example of an inconsistent ontology in our session. What is delivered as part of the course \u00b6 Description: Learn the fundamentals of ontologies. Learning objectives \u00b6 OpenHPI course review: questions? (~15 minutes) OWL ontology serializations (\"formats\") (~15 minutes) Converting between serializations with robot convert (Review; ~15 minutes) Creating modules from existing ontologies (~30 minutes) What is a module? How do we use the modules in our ontologies? Extraction methods: MIREOT vs. SLME Creating a module to import with robot extract (Review; ~15 minutes) Ontology design patterns (~15 minutes) Real world example: Ontology for Biomedical Investigations (OBI) Using design patterns in robot template (Review; ~15 minutes) Including your modules in your ontology as imports Tutorials \u00b6 Additional materials and resources \u00b6 Contributors \u00b6 add name/ORCID here","title":"Ontology Fundamentals"},{"location":"lesson/ontology-fundamentals/#ontologies-fundamentals","text":"","title":"Ontologies: Fundamentals"},{"location":"lesson/ontology-fundamentals/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/ontology-fundamentals/#prerequisites","text":"Install Protege","title":"Prerequisites"},{"location":"lesson/ontology-fundamentals/#preparation","text":"Complete OpenHPI Week 5: Ontology Engineering videos 5.1, 5.2, and 5.4 - 5.6 (~2.5 hours) We are skipping 5.3: Ontology Learning and both sections on MORE Ontology Evaluation (5.7 and 5.8) Complete part of the Ontologies 101 Tutorial (~2 hours) Clone the Ontologies 101 repository, then open the folder BDK14_exercises from your file system Open basic-subclass/chromosome-parts.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Subclass Hierarchy (review) - make sure to look at the \"detailed instructions\" for adding annotations here, as it will go over adding annotations on annotation assertions Disjointness Object Properties - note that you will rarely, if ever, be making object properties, as most of the properties you'll ever need are defined in the Relation Ontology OWL Class Restrictions Open basic-restriction/er-sec-complex.owl in Prot\u00e9g\u00e9, then do the following exercise: Basic Restrictions Open basic-dl-query/cc.owl in Prot\u00e9g\u00e9, then do the following exercises: DL Query Tab - note that owl:Nothing is defined as the very bottom node of an ontology, therefore the DL query results will show owl:Nothing as a subclass. This is expected and does not mean there is a problem with your ontology! It's only bad when something is a subclass of owl:Nothing and therefore unsatisfiable (more on that below). Basic DL Queries Open basic-classification/ubiq-ligase-complex.owl in Prot\u00e9g\u00e9, then do the following exercises: Basic Classification Read (I can't get no) satisfiability (~10 minutes) Optional : Open a new ontology in Prot\u00e9g\u00e9. Try creating an inconsistent ontology using the classes and instances in the first Pets example (hint: you'll also need to create the \"eats\" object property)... what happens when you run the reasoner? Don't spend too much time on this if you get stuck, we'll look at an example of an inconsistent ontology in our session.","title":"Preparation"},{"location":"lesson/ontology-fundamentals/#what-is-delivered-as-part-of-the-course","text":"Description: Learn the fundamentals of ontologies.","title":"What is delivered as part of the course"},{"location":"lesson/ontology-fundamentals/#learning-objectives","text":"OpenHPI course review: questions? (~15 minutes) OWL ontology serializations (\"formats\") (~15 minutes) Converting between serializations with robot convert (Review; ~15 minutes) Creating modules from existing ontologies (~30 minutes) What is a module? How do we use the modules in our ontologies? Extraction methods: MIREOT vs. SLME Creating a module to import with robot extract (Review; ~15 minutes) Ontology design patterns (~15 minutes) Real world example: Ontology for Biomedical Investigations (OBI) Using design patterns in robot template (Review; ~15 minutes) Including your modules in your ontology as imports","title":"Learning objectives"},{"location":"lesson/ontology-fundamentals/#tutorials","text":"","title":"Tutorials"},{"location":"lesson/ontology-fundamentals/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/ontology-fundamentals/#contributors","text":"add name/ORCID here","title":"Contributors"},{"location":"lesson/ontology-pipelines/","text":"Ontology Pipelines with ROBOT and SPARQL \u00b6 Warning \u00b6 These materials are under construction and may be incomplete. Prerequisites \u00b6 Install ROBOT so you can use it outside of Docker (scroll down to the end of the ROBOT page to find the Windows instructions) Optional Install ODK . The ODK includes ROBOT. In the more advanced parts of the course, you will need the ODK installed for some of the other dependencies it includes, and for Windows users it is often easier to follow the tutorials from inside the docker container rather than the Windows CMD. Familiarise yourself with the ROBOT documentation , to the point that you are aware of the various commands that exist. Tutorials \u00b6 Complete the ROBOT Mini-Tutorial 1 to learn your first ROBOT commands: convert , extract and template Complete the ROBOT Mini-Tutorial 2 to learn about annotate , merge , reason and diff Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour) What is delivered as part of the course \u00b6 Description: There are two basic ways to edit an ontology: (1) manually, using tools such as Protege, or (2) using computational tools such as ROBOT. Both have their advantages and disadvantages: manual curation is often more practical when the required ontology change follows a non-standard pattern, such as adding a textual definition or a synonym, while automated approaches are usually much more scalable (ensure that all axioms in the ontology are consistent, or that imported terms from external ontologies are up-to-date or that all labels start with a lower-case letter). Here, we will do a first dive into the \"computational tools\" side of the edit process. We strongly believe that the modern ontology curator should have a basic set of computational tools in their Semantic Engineering toolbox, and many of the lessons in this course should apply to this role of the modern ontology curator . ROBOT is one of the most important tools in the Semantic Engineering Toolbox. For a bit more background on the tool, please refer to the paper . We also recommend to get a basic familiarity with SPARQL, the query language of the semantic web, that can be a powerful combination with ROBOT to perform changes and quality control checks on your ontology. Additional materials and resources \u00b6 Contributors \u00b6 Becky Jackson Nico Matentzoglu","title":"Ontology Pipelines with ROBOT"},{"location":"lesson/ontology-pipelines/#ontology-pipelines-with-robot-and-sparql","text":"","title":"Ontology Pipelines with ROBOT and SPARQL"},{"location":"lesson/ontology-pipelines/#warning","text":"These materials are under construction and may be incomplete.","title":"Warning"},{"location":"lesson/ontology-pipelines/#prerequisites","text":"Install ROBOT so you can use it outside of Docker (scroll down to the end of the ROBOT page to find the Windows instructions) Optional Install ODK . The ODK includes ROBOT. In the more advanced parts of the course, you will need the ODK installed for some of the other dependencies it includes, and for Windows users it is often easier to follow the tutorials from inside the docker container rather than the Windows CMD. Familiarise yourself with the ROBOT documentation , to the point that you are aware of the various commands that exist.","title":"Prerequisites"},{"location":"lesson/ontology-pipelines/#tutorials","text":"Complete the ROBOT Mini-Tutorial 1 to learn your first ROBOT commands: convert , extract and template Complete the ROBOT Mini-Tutorial 2 to learn about annotate , merge , reason and diff Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour)","title":"Tutorials"},{"location":"lesson/ontology-pipelines/#what-is-delivered-as-part-of-the-course","text":"Description: There are two basic ways to edit an ontology: (1) manually, using tools such as Protege, or (2) using computational tools such as ROBOT. Both have their advantages and disadvantages: manual curation is often more practical when the required ontology change follows a non-standard pattern, such as adding a textual definition or a synonym, while automated approaches are usually much more scalable (ensure that all axioms in the ontology are consistent, or that imported terms from external ontologies are up-to-date or that all labels start with a lower-case letter). Here, we will do a first dive into the \"computational tools\" side of the edit process. We strongly believe that the modern ontology curator should have a basic set of computational tools in their Semantic Engineering toolbox, and many of the lessons in this course should apply to this role of the modern ontology curator . ROBOT is one of the most important tools in the Semantic Engineering Toolbox. For a bit more background on the tool, please refer to the paper . We also recommend to get a basic familiarity with SPARQL, the query language of the semantic web, that can be a powerful combination with ROBOT to perform changes and quality control checks on your ontology.","title":"What is delivered as part of the course"},{"location":"lesson/ontology-pipelines/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/ontology-pipelines/#contributors","text":"Becky Jackson Nico Matentzoglu","title":"Contributors"},{"location":"lesson/ontology-term-use/","text":"Using Ontologies and Ontology Terms \u00b6 Warning \u00b6 These materials are under construction and may be incomplete. Prerequisites \u00b6 Sign up for a free GitHub account Preparation \u00b6 None What is delivered as part of the course \u00b6 Description: Using ontology terms for annotations and structuring data. Learning objectives \u00b6 Explain why ontologies are useful Find good ontologies: ontology repositories, OBO Find terms using ontology browsers Assess ontologies for use: license, quality Map local terminology to ontology terms Identify missing terms Make term requests to existing ontologies Understand the differences between IRIs, CURIEs, and labels Tutorials \u00b6 None Additional materials and resources \u00b6 How select and request terms from ontologies - Blog post by Chris Mungall Guidelines for writing definitions in Ontologies (paper) OntoTips - A guide by Chris Mungall covering various aspects of ontology engineering. Contributors \u00b6 Nicole Vasilevsky 1. Why ontologies are useful \u00b6 Ontologies provide a logical classification of information in a particular domain or subject area. Ontologies can be used for data annotations, for structuring disparate data types, classifying information, for inferencing and reasoning across data and computational analyses. Difference between a terminology and an ontology \u00b6 Terminology \u00b6 A terminology is a collection of terms; a term can have a definition and synonyms. Ontology \u00b6 An ontology contains a formal classification of terminology in a domain that provides textual and machine readable definitions, and defines the relationships between terms. An ontology is a terminology, but a terminology is not (necessarily) an ontology. 2. Finding good ontologies \u00b6 Numerous ontologies exist. Some recommended sources to find community developed, high quality and frequently used ontologies are listed below. OBO Foundry . Read more below The Ontology Lookup Service (OLS) . The OLS contains over 200 ontologies. BioPortal . BioPortal aggregates almost 900 biomedical ontologies, and provides a search interface to look up terms. It is a popular repository for ontologies, but as only a fraction of the ontologies are reviewed by the OBO Foundry, you should carefully review any ontologies found on BioPortal before committing to use them. Ontobee . Ontobee indexes all 200+ OBO Foundry ontologies and is the default browser for OBO: For example, when you click http://purl.obolibrary.org/obo/IAO_0000112, you will be redirected to the a page in the Ontobee browser that describes the annotation property example of usage . 3. Ontology repositories \u00b6 OBO Foundry \u00b6 The OBO Foundry is a community of ontology developers that are committed to developing a library of ontologies that are open, interoperable ontologies, logically well-formed and scientifically accurate. OBO Foundry participants follow and contribute to the development of an evolving set of principles including open use, collaborative development, non-overlapping and strictly-scoped content, and common syntax and relations, based on ontology models that work well, such as the Gene Ontology (GO) . The OBO Foundry is overseen by an Operations Committee with Editorial, Technical and Outreach working groups. Find terms using ontology browsers \u00b6 Various ontology browsers are available, we recommend using one of the ontology browsers listed below. Find terms: Ontology Lookup Service BioPortal Ontobee 4. Assessing ontologies for use \u00b6 Some considerations for determining which ontologies to use include the license and quality of the ontology. License \u00b6 Licenses define how an ontology can legally be used or reused. One requirement for OBO Foundry Ontologies is that they are open, meaning that the ontologies are openly and freely available for use with acknowledgement and without alteration. OBO ontologies are required to be released under a Creative Commons CC-BY license version 3.0 or later , OR released into the public domain under CC0 . The license should be clearly stated in the ontology file. Quality \u00b6 Some criteria that can be applied to determine the quality of an ontology include: Is there an ontology tracker to report issues? All open ontologies should have some form of an issue tracker to report bugs, make new term requests or request other changes to the ontology. Many ontologies use GitHub to track their issues. Is it currently active? Are there a large number of open tickets on the ontology tracker that have not been commented on or otherwise addressed? Are the tickets very old, have been sitting for years? Commmunity involvement On the issue tracker, is there evidence of community involvement, such as issues and comments from outside community members? Scientifically sound Does the ontology accurately represent the domain in a scientifically sound way? How to determine which is the right ontology to use? \u00b6 There are multiple ontologies that exist, start by selecting the appropriate ontology, then search and restrict your search to that ontology. Recommend using ontologies that are open and interoperable. Focusing on OBO foundry ontologies are a good place to start Make informed decision about which ontology to use Maybe the ontology you want to use does not have the term you want, so make a term request to that ontology 5. Mapping local terminology to ontology terms \u00b6 Data can mapped to ontology terms manually, using spreadsheets, or via curation tools such as: Zooma BioPortal Annotator Canto - a web-based literature curation tools Textpresso - designed for C. elegans curation OntoBrowser - an online collaborative curation tool 6. Identifying missing terms \u00b6 The figure below by Chris Mungall on his blog post on How to select and request terms from ontologies describes a workflow on searching for identifying missing terms from an ontology. 7. Make term requests to existing ontologies \u00b6 See separate lesson on Making term requests to existing ontologies . 8. Differences between IRIs, CURIEs, and labels \u00b6 URI \u00b6 A uniform resource identifier (URI) is a string of characters used to identify a name or a resource. URL \u00b6 A URL is a URI that, in addition to identifying a network-homed resource, specifies the means of acting upon or obtaining the representation. A URL such as this one: https://github.com/obophenotype/uberon/blob/master/uberon_edit.obo has three main parts: Protocol, e.g. https Host, e.g. github.com Path, e.g. /obophenotype/uberon/blob/master/uberon_edit.obo The protocol tells you how to get the resource. Common protocols for web pages are http (HyperText Transfer Protocol) and https (HTTP Secure). The host is the name of the server to contact (the where), which can be a numeric IP address, but is more often a domain name. The path is the name of the resource on that server (the what), here the Uberon anatomy ontology file. IRI \u00b6 A Internationalized Resource Identifiers (IRI) is an internet protocol standard that allows permitted characters from a wide range of scripts. While URIs are limited to a subset of the ASCII character set, IRIs may contain characters from the Universal Character Set (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. It is defined by RFC 3987 . More information is available here . CURIEs \u00b6 A Compact URI (CURIE) consists of a prefix and a suffix, where the prefix stands in place of a longer base IRI. By converting the prefix and appending the suffix we get back to full IRI. For example, if we define the obo prefix to stand in place of the IRI as: http://purl.obolibrary.org/obo/, then the CURIE obo:UBERON_0002280 can be expanded to http://purl.obolibrary.org/obo/UBERON_0002280, which is the UBERON Anatomy term for \u2018otolith\u2019. Any file that contains CURIEs need to define the prefixes in the file header. Label \u00b6 A label is the textual, human readable name that is given to a term, class property or instance in an ontology.","title":"Ontology Term Use"},{"location":"lesson/ontology-term-use/#using-ontologies-and-ontology-terms","text":"","title":"Using Ontologies and Ontology Terms"},{"location":"lesson/ontology-term-use/#warning","text":"These materials are under construction and may be incomplete.","title":"Warning"},{"location":"lesson/ontology-term-use/#prerequisites","text":"Sign up for a free GitHub account","title":"Prerequisites"},{"location":"lesson/ontology-term-use/#preparation","text":"None","title":"Preparation"},{"location":"lesson/ontology-term-use/#what-is-delivered-as-part-of-the-course","text":"Description: Using ontology terms for annotations and structuring data.","title":"What is delivered as part of the course"},{"location":"lesson/ontology-term-use/#learning-objectives","text":"Explain why ontologies are useful Find good ontologies: ontology repositories, OBO Find terms using ontology browsers Assess ontologies for use: license, quality Map local terminology to ontology terms Identify missing terms Make term requests to existing ontologies Understand the differences between IRIs, CURIEs, and labels","title":"Learning objectives"},{"location":"lesson/ontology-term-use/#tutorials","text":"None","title":"Tutorials"},{"location":"lesson/ontology-term-use/#additional-materials-and-resources","text":"How select and request terms from ontologies - Blog post by Chris Mungall Guidelines for writing definitions in Ontologies (paper) OntoTips - A guide by Chris Mungall covering various aspects of ontology engineering.","title":"Additional materials and resources"},{"location":"lesson/ontology-term-use/#contributors","text":"Nicole Vasilevsky","title":"Contributors"},{"location":"lesson/ontology-term-use/#1-why-ontologies-are-useful","text":"Ontologies provide a logical classification of information in a particular domain or subject area. Ontologies can be used for data annotations, for structuring disparate data types, classifying information, for inferencing and reasoning across data and computational analyses.","title":"1. Why ontologies are useful"},{"location":"lesson/ontology-term-use/#difference-between-a-terminology-and-an-ontology","text":"","title":"Difference between a terminology and an ontology"},{"location":"lesson/ontology-term-use/#terminology","text":"A terminology is a collection of terms; a term can have a definition and synonyms.","title":"Terminology"},{"location":"lesson/ontology-term-use/#ontology","text":"An ontology contains a formal classification of terminology in a domain that provides textual and machine readable definitions, and defines the relationships between terms. An ontology is a terminology, but a terminology is not (necessarily) an ontology.","title":"Ontology"},{"location":"lesson/ontology-term-use/#2-finding-good-ontologies","text":"Numerous ontologies exist. Some recommended sources to find community developed, high quality and frequently used ontologies are listed below. OBO Foundry . Read more below The Ontology Lookup Service (OLS) . The OLS contains over 200 ontologies. BioPortal . BioPortal aggregates almost 900 biomedical ontologies, and provides a search interface to look up terms. It is a popular repository for ontologies, but as only a fraction of the ontologies are reviewed by the OBO Foundry, you should carefully review any ontologies found on BioPortal before committing to use them. Ontobee . Ontobee indexes all 200+ OBO Foundry ontologies and is the default browser for OBO: For example, when you click http://purl.obolibrary.org/obo/IAO_0000112, you will be redirected to the a page in the Ontobee browser that describes the annotation property example of usage .","title":"2. Finding good ontologies"},{"location":"lesson/ontology-term-use/#3-ontology-repositories","text":"","title":"3. Ontology repositories"},{"location":"lesson/ontology-term-use/#obo-foundry","text":"The OBO Foundry is a community of ontology developers that are committed to developing a library of ontologies that are open, interoperable ontologies, logically well-formed and scientifically accurate. OBO Foundry participants follow and contribute to the development of an evolving set of principles including open use, collaborative development, non-overlapping and strictly-scoped content, and common syntax and relations, based on ontology models that work well, such as the Gene Ontology (GO) . The OBO Foundry is overseen by an Operations Committee with Editorial, Technical and Outreach working groups.","title":"OBO Foundry"},{"location":"lesson/ontology-term-use/#find-terms-using-ontology-browsers","text":"Various ontology browsers are available, we recommend using one of the ontology browsers listed below. Find terms: Ontology Lookup Service BioPortal Ontobee","title":"Find terms using ontology browsers"},{"location":"lesson/ontology-term-use/#4-assessing-ontologies-for-use","text":"Some considerations for determining which ontologies to use include the license and quality of the ontology.","title":"4. Assessing ontologies for use"},{"location":"lesson/ontology-term-use/#license","text":"Licenses define how an ontology can legally be used or reused. One requirement for OBO Foundry Ontologies is that they are open, meaning that the ontologies are openly and freely available for use with acknowledgement and without alteration. OBO ontologies are required to be released under a Creative Commons CC-BY license version 3.0 or later , OR released into the public domain under CC0 . The license should be clearly stated in the ontology file.","title":"License"},{"location":"lesson/ontology-term-use/#quality","text":"Some criteria that can be applied to determine the quality of an ontology include: Is there an ontology tracker to report issues? All open ontologies should have some form of an issue tracker to report bugs, make new term requests or request other changes to the ontology. Many ontologies use GitHub to track their issues. Is it currently active? Are there a large number of open tickets on the ontology tracker that have not been commented on or otherwise addressed? Are the tickets very old, have been sitting for years? Commmunity involvement On the issue tracker, is there evidence of community involvement, such as issues and comments from outside community members? Scientifically sound Does the ontology accurately represent the domain in a scientifically sound way?","title":"Quality"},{"location":"lesson/ontology-term-use/#how-to-determine-which-is-the-right-ontology-to-use","text":"There are multiple ontologies that exist, start by selecting the appropriate ontology, then search and restrict your search to that ontology. Recommend using ontologies that are open and interoperable. Focusing on OBO foundry ontologies are a good place to start Make informed decision about which ontology to use Maybe the ontology you want to use does not have the term you want, so make a term request to that ontology","title":"How to determine which is the right ontology to use?"},{"location":"lesson/ontology-term-use/#5-mapping-local-terminology-to-ontology-terms","text":"Data can mapped to ontology terms manually, using spreadsheets, or via curation tools such as: Zooma BioPortal Annotator Canto - a web-based literature curation tools Textpresso - designed for C. elegans curation OntoBrowser - an online collaborative curation tool","title":"5. Mapping local terminology to ontology terms"},{"location":"lesson/ontology-term-use/#6-identifying-missing-terms","text":"The figure below by Chris Mungall on his blog post on How to select and request terms from ontologies describes a workflow on searching for identifying missing terms from an ontology.","title":"6. Identifying missing terms"},{"location":"lesson/ontology-term-use/#7-make-term-requests-to-existing-ontologies","text":"See separate lesson on Making term requests to existing ontologies .","title":"7. Make term requests to existing ontologies"},{"location":"lesson/ontology-term-use/#8-differences-between-iris-curies-and-labels","text":"","title":"8. Differences between IRIs, CURIEs, and labels"},{"location":"lesson/ontology-term-use/#uri","text":"A uniform resource identifier (URI) is a string of characters used to identify a name or a resource.","title":"URI"},{"location":"lesson/ontology-term-use/#url","text":"A URL is a URI that, in addition to identifying a network-homed resource, specifies the means of acting upon or obtaining the representation. A URL such as this one: https://github.com/obophenotype/uberon/blob/master/uberon_edit.obo has three main parts: Protocol, e.g. https Host, e.g. github.com Path, e.g. /obophenotype/uberon/blob/master/uberon_edit.obo The protocol tells you how to get the resource. Common protocols for web pages are http (HyperText Transfer Protocol) and https (HTTP Secure). The host is the name of the server to contact (the where), which can be a numeric IP address, but is more often a domain name. The path is the name of the resource on that server (the what), here the Uberon anatomy ontology file.","title":"URL"},{"location":"lesson/ontology-term-use/#iri","text":"A Internationalized Resource Identifiers (IRI) is an internet protocol standard that allows permitted characters from a wide range of scripts. While URIs are limited to a subset of the ASCII character set, IRIs may contain characters from the Universal Character Set (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. It is defined by RFC 3987 . More information is available here .","title":"IRI"},{"location":"lesson/ontology-term-use/#curies","text":"A Compact URI (CURIE) consists of a prefix and a suffix, where the prefix stands in place of a longer base IRI. By converting the prefix and appending the suffix we get back to full IRI. For example, if we define the obo prefix to stand in place of the IRI as: http://purl.obolibrary.org/obo/, then the CURIE obo:UBERON_0002280 can be expanded to http://purl.obolibrary.org/obo/UBERON_0002280, which is the UBERON Anatomy term for \u2018otolith\u2019. Any file that contains CURIEs need to define the prefixes in the file header.","title":"CURIEs"},{"location":"lesson/ontology-term-use/#label","text":"A label is the textual, human readable name that is given to a term, class property or instance in an ontology.","title":"Label"},{"location":"lesson/rdf/","text":"Introduction to RDF \u00b6 First Instructor: James Overton Second Instructor: Becky Jackson Warning \u00b6 These materials are under construction and incomplete. Description \u00b6 Modelling and querying data with RDF triples, and working with RDF using tables Topics \u00b6 RDF modelling RDFS SPARQL OpenHPI Course Content \u00b6 OpenHPI Linked Data Engineering (2016) Lesson 2 RDF Lesson 4 SPARQL Software Carpentry Lessons \u00b6 Using Databases and SQL New Material \u00b6 Tables and Triples Optional material \u00b6 Linking data Semantic Engineer Toolbox \u00b6","title":"RDF Fundamentals"},{"location":"lesson/rdf/#introduction-to-rdf","text":"First Instructor: James Overton Second Instructor: Becky Jackson","title":"Introduction to RDF"},{"location":"lesson/rdf/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/rdf/#description","text":"Modelling and querying data with RDF triples, and working with RDF using tables","title":"Description"},{"location":"lesson/rdf/#topics","text":"RDF modelling RDFS SPARQL","title":"Topics"},{"location":"lesson/rdf/#openhpi-course-content","text":"OpenHPI Linked Data Engineering (2016) Lesson 2 RDF Lesson 4 SPARQL","title":"OpenHPI Course Content"},{"location":"lesson/rdf/#software-carpentry-lessons","text":"Using Databases and SQL","title":"Software Carpentry Lessons"},{"location":"lesson/rdf/#new-material","text":"Tables and Triples","title":"New Material"},{"location":"lesson/rdf/#optional-material","text":"Linking data","title":"Optional material"},{"location":"lesson/rdf/#semantic-engineer-toolbox","text":"","title":"Semantic Engineer Toolbox"},{"location":"lesson/semantic-database-fundamentals/","text":"Semantic Databases: Fundamentals \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 Review tutorial on Project Ontology Development Preparation \u00b6 TBD What is delivered as part of the course \u00b6 Description: Using ontology terms in a database. Learning objectives \u00b6 advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs Tutorials \u00b6 in person or video (link videos here as they become available) Additional materials and resources \u00b6 Contributors \u00b6 Nico Matentzoglu","title":"Semantic Database Fundamentals"},{"location":"lesson/semantic-database-fundamentals/#semantic-databases-fundamentals","text":"","title":"Semantic Databases: Fundamentals"},{"location":"lesson/semantic-database-fundamentals/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/semantic-database-fundamentals/#prerequisites","text":"Review tutorial on Project Ontology Development","title":"Prerequisites"},{"location":"lesson/semantic-database-fundamentals/#preparation","text":"TBD","title":"Preparation"},{"location":"lesson/semantic-database-fundamentals/#what-is-delivered-as-part-of-the-course","text":"Description: Using ontology terms in a database.","title":"What is delivered as part of the course"},{"location":"lesson/semantic-database-fundamentals/#learning-objectives","text":"advanced term mapping ontology terms in SQL terminology table JOINs, constraints convert tables to triples triplestores knowledge graphs","title":"Learning objectives"},{"location":"lesson/semantic-database-fundamentals/#tutorials","text":"in person or video (link videos here as they become available)","title":"Tutorials"},{"location":"lesson/semantic-database-fundamentals/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/semantic-database-fundamentals/#contributors","text":"Nico Matentzoglu","title":"Contributors"},{"location":"lesson/templates-for-obo/","text":"Templating systems for OBO ontologies: a deep dive \u00b6 Ontologies are notoriously hard to edit. This makes it a very high burden to edit ontologies for anyone but a select few. However, many of the contents of ontologies are actually best edited by domain experts with often little or known ontological training - editing labels and synonyms, curating definitions, adding references to publications and many more. Furthermore, if we simply remove the burden of writing OWL axioms, editors with very little ontology training can actually curate even logical content: for example, if we want to describe that a class is restricted to a certain taxon (also known as taxon-restriction), the editor is often capable to select the appropriate taxon for a term (say, a \"mouse heart\" is restricted to the taxon of Mus musculus ), but maybe they would not know how to \"add that restriction to the ontology\". Tables are great (for a deep dive into tables and triples see here ). Scientists in particular love tables , and, even more importantly, can be trained easily to edit data in spreadsheet tools, such as Google Sheets or Microsoft Excel. Ontology templating systems, such as DOSDP templates , ROBOT templates and Reasonable Ontology Templates (OTTR) allow separating the raw data in the ontology (labels, synonyms, related ontological entities, descriptions, cross-references and other metadata) from the OWL language patterns that are used to manifest them in the ontology. There are three main ingredients to a templating system: A way to capture the data. In all the systems we care about, these are tables, usually manifested as spreadsheets in Excel or Google Sheets. A way to capture the template. In ROBOT templates the templates are captured in a header row of the same table that captures the data, in DOSDP templates the templates are captured in a separate YAML file and in OTTR typically the templates are serialised as and RDF-graph in a format like RDF/XML or Turtle. A toolkit that can combine the data and the template to generate OWL axioms and annotations. ROBOT templates can be compiled to OWL using ROBOT , DOSDP templates can be compiled using DOSDP tools and OTTR templates using Lutra . In OBO we are currently mostly concerned with ROBOT templates and DOSDP templates. Before moving on, we recommend to complete a basic tutorial in both: ROBOT template tutorial DOSDP template tutorial ROBOT template vs DOSDP template \u00b6 Ontologies, especially in the biomedical domain, are complex and, while growing in size, increasingly hard to manage for their curators. In this section, we will look at some of the key differences of two popular templating systems in the OBO domain: Dead Simple Ontology Design Patterns (DOSDPs) and ROBOT templates. We will not cover the rationale for templates in general in much depth (the interested reader should check ontology design patterns and Reasonable Ontology Templates (OTTR): Motivation and Overview , which pertains to a different system, but applies none-the-less in general), and focus on making it easier for developers to pick the right templating approach for their particular use case. We will first discuss in detail representational differences, before we go through the functional ones and delineate use cases. Structural differences, formats and tools \u00b6 DOSDP templates: structure and format \u00b6 DOSDP separates data and templates into two files: a yaml file which defines the template, and a TSV file which holds the data. Lets look at s example. The template: abnormalAnatomicalEntity pattern_name: abnormalAnatomicalEntity pattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml description: \"Any unspecified abnormality of an anatomical entity.\" contributors: - https://orcid.org/0000-0002-9900-7880 - https://orcid.org/0000-0001-9076-6015 - https://orcid.org/0000-0003-4148-4606 - https://orcid.org/0000-0002-3528-5267 classes: quality: PATO:0000001 abnormal: PATO:0000460 anatomical entity: UBERON:0001062 relations: inheres_in_part_of: RO:0002314 has_modifier: RO:0002573 has_part: BFO:0000051 annotationProperties: exact_synonym: oio:hasExactSynonym vars: anatomical_entity: \"'anatomical entity'\" name: text: \"abnormal %s\" vars: - anatomical_entity annotations: - annotationProperty: exact_synonym text: \"abnormality of %s\" vars: - anatomical_entity def: text: \"Abnormality of %s.\" vars: - anatomical_entity equivalentTo: text: \"'has_part' some ('quality' and ('inheres_in_part_of' some %s) and ('has_modifier' some 'abnormal'))\" vars: - anatomical_entity The data: abnormalAnatomicalEntity.tsv defined_class defined_class_label anatomical_entity anatomical_entity_label HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit ROBOT templates: structure and format \u00b6 ROBOT encodes both the template and the data in the same TSV; after the table header, the second row basically encodes the entire template logic, and the data follows in table row 3. ID Label EQ Anatomy Label ID LABEL EC 'has_part' some ('quality' and ('inheres_in_part_of' some %) and ('has_modifier' some 'abnormal')) HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit Note that for the Anatomy Label we deliberately left the second row empty, which instructs the ROBOT template tool to completely ignore this column . A discussion on the main differences \u00b6 Ontology Engineering perspective \u00b6 From an ontology engineering perspective, the essence of the difference between DOSDP and ROBOT templates could be captured as follows: DOSDP templates are more about generating annotations and axioms, while ROBOT templates are more about curating annotations and axioms. Curating annotations and axioms means that an editor, or ontology curator, manually enters the labels, synonyms, definitions and so forth into the spreadsheet. Generating axioms in the sense of this section means that we try to automatically generate labels, synonyms, definitions and so forth based on the related logical entities in the patterns. E.g., using the example template above, the label \"abnormal kidney\" would automatically be generated when the Uberon term for kidney is supplied. While both ROBOT and DOSDP can be used for \"curation\" of annotation of axioms, DOSDP seeks to apply generation rules to automatically generate synonyms, labels, definitions and so forth while for ROBOT template seeks to collect manually curated information in an easy-to-use table which is then compiled into OWL. In other words: the average DOSDP user will not write their own labels, definitions and synonyms - they will want those to be generated automatically from a set of simple rules; the average ROBOT template user will not want automatically generated definitions, labels and synonyms - they will want to capture their own. Sharing and Re-use \u00b6 However, there is another dimension in which both approaches differ widely: sharing and re-use. DOSDPs by far the most important feature is that it allows a community of developers to rally around a modelling problem, debate and establish consensus; for example, a pattern can be used to say: this is how we model abnormal anatomical entities. Consensus can be made explicit by \"signing off\" on the pattern (e.g. by adding your ORCId to the list of contributors), and due to the template/data separation, the template can be simply imported using its IRI (for example http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml) and re-used by everyone. Furthermore, additional metadata fields including textual descriptions, and more recently \"examples\", make DOSDP template files comparatively easy to understand, even by a less technically inclined editor. ROBOT templates on the other hand do not lend themselves to community debates in the same way; first of all, they are typically supplied including all data merged in; secondly, they do not provide additional metadata fields that could, for example, conveniently be used to represent a sign off (you could, of course, add the ORCId's into a non-functional column, or as a pipe-separated string into a cell in the first or second row; but its obvious that this would be quite clunky) or a textual description. A yaml file is much easier for a human to read and understand then the header of a TSV file, especially when the template becomes quite large. However, there is a flipside to the strict separation of data and templates. One is that DOSDP templates are really hard to change. Once, for example, a particular variable name was chosen, renaming the variable will require an excessive community-wide action to rename columns in all associated spreadsheets - which requires them all to be known beforehand (which is not always the case). You don't have such a problem with ROBOT templates; if you change a column name, or a template string, everything will continue to work without any additional coordination. Summary \u00b6 Both ROBOT templates and DOSDP templates are widely used. The author of this page uses both in most of the projects he is involved in, because of their different strengths and capabilities. You can use the following rules of thumb to inform your choice: Consider ROBOT templates if your emphasis is on manually curating labels, definitions, synonyms and axioms or other annotations managing your templates in the spreadsheet itself is a concern for you (this is often the case, for example, when turning an existing data table into a ROBOT template ad hoc) Consider DOSDP templates if your emphasis is on re-use, community-wide implementation of the same templates and community discussion, you should consider DOSDP templates automatically generating labels, definitions, synonyms from rules in the pattern. Detour: Concerns with Managing Tables \u00b6 There is a nice debate going on which questions the use of tables in ontology curation altogether. There are many nuances in this debate, but I want to stylise it here as two schools of thoughts (there are probably hundreds in between, but this makes it easier to follow): The one school (let's call them Tablosceptics) claims that using tables introduces a certain degree of fragility into the development process due to a number of factors, including: losing the immediateness of QC feedback; Table-based development, so the Tablosceptics, encourages lazy editing (adding stuff to a template and then not reviewing the consequence properly, which we will discuss in more depth later). losing track of the ID space (in a multi-table world, it becomes increasingly hard to manage IDs, making sure they are not double used etc) and 3) encouraging bad design (relying more on assertion than inference). They prefer to use tools like Protege that show the curator immediately the consequences of their actions, like reasoning errors (unintended equivalent classes, unsatisfiable classes and other unintended inferences). The Tablophile school of thought responds to these accusations in essence with \"tools\"; they say that tables are essentially a convenient matrix to input the data (which in turns opens ontology curation to a much wider range of people), and it is up to the tools to ensure that QC is run, hierarchies are being presented for review and weird ID space clashes are flagged up. Furthermore, they say, having a controlled input matrix will actually decrease the number of faulty annotations or axioms (which is evidenced by the large number of wrongful annotation assertions across OBO foundry ontologies I see every day as part of my work). At first sight, both template systems are affected equally by the war of the Tablosceptics and the Tablophile. Indeed, in my on practice, the ID space issue is really problematic when we manage 100s and more templates, and so far, I have not seen a nice and clear solution that ensures that no ID used twice unless it is so intended and respects ID spaces which are often semi-formally assigned to individual curators of an ontology. Generally in this course we do not want to take a 100% stance. The author of this page believes that the advantage of using tables and involving many more people in the development process outweighs any concerns, but tooling is required that can provide more immediate feedback when such tables such as the ones presented here are curated at scale.","title":"Templates for OBO ontologies"},{"location":"lesson/templates-for-obo/#templating-systems-for-obo-ontologies-a-deep-dive","text":"Ontologies are notoriously hard to edit. This makes it a very high burden to edit ontologies for anyone but a select few. However, many of the contents of ontologies are actually best edited by domain experts with often little or known ontological training - editing labels and synonyms, curating definitions, adding references to publications and many more. Furthermore, if we simply remove the burden of writing OWL axioms, editors with very little ontology training can actually curate even logical content: for example, if we want to describe that a class is restricted to a certain taxon (also known as taxon-restriction), the editor is often capable to select the appropriate taxon for a term (say, a \"mouse heart\" is restricted to the taxon of Mus musculus ), but maybe they would not know how to \"add that restriction to the ontology\". Tables are great (for a deep dive into tables and triples see here ). Scientists in particular love tables , and, even more importantly, can be trained easily to edit data in spreadsheet tools, such as Google Sheets or Microsoft Excel. Ontology templating systems, such as DOSDP templates , ROBOT templates and Reasonable Ontology Templates (OTTR) allow separating the raw data in the ontology (labels, synonyms, related ontological entities, descriptions, cross-references and other metadata) from the OWL language patterns that are used to manifest them in the ontology. There are three main ingredients to a templating system: A way to capture the data. In all the systems we care about, these are tables, usually manifested as spreadsheets in Excel or Google Sheets. A way to capture the template. In ROBOT templates the templates are captured in a header row of the same table that captures the data, in DOSDP templates the templates are captured in a separate YAML file and in OTTR typically the templates are serialised as and RDF-graph in a format like RDF/XML or Turtle. A toolkit that can combine the data and the template to generate OWL axioms and annotations. ROBOT templates can be compiled to OWL using ROBOT , DOSDP templates can be compiled using DOSDP tools and OTTR templates using Lutra . In OBO we are currently mostly concerned with ROBOT templates and DOSDP templates. Before moving on, we recommend to complete a basic tutorial in both: ROBOT template tutorial DOSDP template tutorial","title":"Templating systems for OBO ontologies: a deep dive"},{"location":"lesson/templates-for-obo/#robot-template-vs-dosdp-template","text":"Ontologies, especially in the biomedical domain, are complex and, while growing in size, increasingly hard to manage for their curators. In this section, we will look at some of the key differences of two popular templating systems in the OBO domain: Dead Simple Ontology Design Patterns (DOSDPs) and ROBOT templates. We will not cover the rationale for templates in general in much depth (the interested reader should check ontology design patterns and Reasonable Ontology Templates (OTTR): Motivation and Overview , which pertains to a different system, but applies none-the-less in general), and focus on making it easier for developers to pick the right templating approach for their particular use case. We will first discuss in detail representational differences, before we go through the functional ones and delineate use cases.","title":"ROBOT template vs DOSDP template"},{"location":"lesson/templates-for-obo/#structural-differences-formats-and-tools","text":"","title":"Structural differences, formats and tools"},{"location":"lesson/templates-for-obo/#dosdp-templates-structure-and-format","text":"DOSDP separates data and templates into two files: a yaml file which defines the template, and a TSV file which holds the data. Lets look at s example. The template: abnormalAnatomicalEntity pattern_name: abnormalAnatomicalEntity pattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml description: \"Any unspecified abnormality of an anatomical entity.\" contributors: - https://orcid.org/0000-0002-9900-7880 - https://orcid.org/0000-0001-9076-6015 - https://orcid.org/0000-0003-4148-4606 - https://orcid.org/0000-0002-3528-5267 classes: quality: PATO:0000001 abnormal: PATO:0000460 anatomical entity: UBERON:0001062 relations: inheres_in_part_of: RO:0002314 has_modifier: RO:0002573 has_part: BFO:0000051 annotationProperties: exact_synonym: oio:hasExactSynonym vars: anatomical_entity: \"'anatomical entity'\" name: text: \"abnormal %s\" vars: - anatomical_entity annotations: - annotationProperty: exact_synonym text: \"abnormality of %s\" vars: - anatomical_entity def: text: \"Abnormality of %s.\" vars: - anatomical_entity equivalentTo: text: \"'has_part' some ('quality' and ('inheres_in_part_of' some %s) and ('has_modifier' some 'abnormal'))\" vars: - anatomical_entity The data: abnormalAnatomicalEntity.tsv defined_class defined_class_label anatomical_entity anatomical_entity_label HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit","title":"DOSDP templates: structure and format"},{"location":"lesson/templates-for-obo/#robot-templates-structure-and-format","text":"ROBOT encodes both the template and the data in the same TSV; after the table header, the second row basically encodes the entire template logic, and the data follows in table row 3. ID Label EQ Anatomy Label ID LABEL EC 'has_part' some ('quality' and ('inheres_in_part_of' some %) and ('has_modifier' some 'abnormal')) HP:0040286 Abnormal axial muscle morphology UBERON:0003897 axial muscle HP:0011297 Abnormal digit morphology UBERON:0002544 digit Note that for the Anatomy Label we deliberately left the second row empty, which instructs the ROBOT template tool to completely ignore this column .","title":"ROBOT templates: structure and format"},{"location":"lesson/templates-for-obo/#a-discussion-on-the-main-differences","text":"","title":"A discussion on the main differences"},{"location":"lesson/templates-for-obo/#ontology-engineering-perspective","text":"From an ontology engineering perspective, the essence of the difference between DOSDP and ROBOT templates could be captured as follows: DOSDP templates are more about generating annotations and axioms, while ROBOT templates are more about curating annotations and axioms. Curating annotations and axioms means that an editor, or ontology curator, manually enters the labels, synonyms, definitions and so forth into the spreadsheet. Generating axioms in the sense of this section means that we try to automatically generate labels, synonyms, definitions and so forth based on the related logical entities in the patterns. E.g., using the example template above, the label \"abnormal kidney\" would automatically be generated when the Uberon term for kidney is supplied. While both ROBOT and DOSDP can be used for \"curation\" of annotation of axioms, DOSDP seeks to apply generation rules to automatically generate synonyms, labels, definitions and so forth while for ROBOT template seeks to collect manually curated information in an easy-to-use table which is then compiled into OWL. In other words: the average DOSDP user will not write their own labels, definitions and synonyms - they will want those to be generated automatically from a set of simple rules; the average ROBOT template user will not want automatically generated definitions, labels and synonyms - they will want to capture their own.","title":"Ontology Engineering perspective"},{"location":"lesson/templates-for-obo/#sharing-and-re-use","text":"However, there is another dimension in which both approaches differ widely: sharing and re-use. DOSDPs by far the most important feature is that it allows a community of developers to rally around a modelling problem, debate and establish consensus; for example, a pattern can be used to say: this is how we model abnormal anatomical entities. Consensus can be made explicit by \"signing off\" on the pattern (e.g. by adding your ORCId to the list of contributors), and due to the template/data separation, the template can be simply imported using its IRI (for example http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml) and re-used by everyone. Furthermore, additional metadata fields including textual descriptions, and more recently \"examples\", make DOSDP template files comparatively easy to understand, even by a less technically inclined editor. ROBOT templates on the other hand do not lend themselves to community debates in the same way; first of all, they are typically supplied including all data merged in; secondly, they do not provide additional metadata fields that could, for example, conveniently be used to represent a sign off (you could, of course, add the ORCId's into a non-functional column, or as a pipe-separated string into a cell in the first or second row; but its obvious that this would be quite clunky) or a textual description. A yaml file is much easier for a human to read and understand then the header of a TSV file, especially when the template becomes quite large. However, there is a flipside to the strict separation of data and templates. One is that DOSDP templates are really hard to change. Once, for example, a particular variable name was chosen, renaming the variable will require an excessive community-wide action to rename columns in all associated spreadsheets - which requires them all to be known beforehand (which is not always the case). You don't have such a problem with ROBOT templates; if you change a column name, or a template string, everything will continue to work without any additional coordination.","title":"Sharing and Re-use"},{"location":"lesson/templates-for-obo/#summary","text":"Both ROBOT templates and DOSDP templates are widely used. The author of this page uses both in most of the projects he is involved in, because of their different strengths and capabilities. You can use the following rules of thumb to inform your choice: Consider ROBOT templates if your emphasis is on manually curating labels, definitions, synonyms and axioms or other annotations managing your templates in the spreadsheet itself is a concern for you (this is often the case, for example, when turning an existing data table into a ROBOT template ad hoc) Consider DOSDP templates if your emphasis is on re-use, community-wide implementation of the same templates and community discussion, you should consider DOSDP templates automatically generating labels, definitions, synonyms from rules in the pattern.","title":"Summary"},{"location":"lesson/templates-for-obo/#detour-concerns-with-managing-tables","text":"There is a nice debate going on which questions the use of tables in ontology curation altogether. There are many nuances in this debate, but I want to stylise it here as two schools of thoughts (there are probably hundreds in between, but this makes it easier to follow): The one school (let's call them Tablosceptics) claims that using tables introduces a certain degree of fragility into the development process due to a number of factors, including: losing the immediateness of QC feedback; Table-based development, so the Tablosceptics, encourages lazy editing (adding stuff to a template and then not reviewing the consequence properly, which we will discuss in more depth later). losing track of the ID space (in a multi-table world, it becomes increasingly hard to manage IDs, making sure they are not double used etc) and 3) encouraging bad design (relying more on assertion than inference). They prefer to use tools like Protege that show the curator immediately the consequences of their actions, like reasoning errors (unintended equivalent classes, unsatisfiable classes and other unintended inferences). The Tablophile school of thought responds to these accusations in essence with \"tools\"; they say that tables are essentially a convenient matrix to input the data (which in turns opens ontology curation to a much wider range of people), and it is up to the tools to ensure that QC is run, hierarchies are being presented for review and weird ID space clashes are flagged up. Furthermore, they say, having a controlled input matrix will actually decrease the number of faulty annotations or axioms (which is evidenced by the large number of wrongful annotation assertions across OBO foundry ontologies I see every day as part of my work). At first sight, both template systems are affected equally by the war of the Tablosceptics and the Tablophile. Indeed, in my on practice, the ID space issue is really problematic when we manage 100s and more templates, and so far, I have not seen a nice and clear solution that ensures that no ID used twice unless it is so intended and respects ID spaces which are often semi-formally assigned to individual curators of an ontology. Generally in this course we do not want to take a 100% stance. The author of this page believes that the advantage of using tables and involving many more people in the development process outweighs any concerns, but tooling is required that can provide more immediate feedback when such tables such as the ones presented here are curated at scale.","title":"Detour: Concerns with Managing Tables"},{"location":"lesson/using-disease-and-phenotype-ontologies/","text":"Finding and using Disease and Phenotype Ontologies \u00b6 Warning \u00b6 These materials are under construction and incomplete. Prerequisites \u00b6 None Preparation \u00b6 Review tutorial on Ontology Term Use What is delivered as part of the course \u00b6 Description: An introduction to the landscape of disease and phenotype terminologies and ontologies, and how they can be used to add value to your analysis. Learning objectives \u00b6 Become aware of the major disease and phenotype ontologies that are available Be able to decide which phenotype or disease ontology to use for different use cases Understand how to leverage disease and phenotype ontologies for advanced data analytics Have a basic understanding of how to integrate other data Tutorials \u00b6 Video from Disease and Phenotypes c-path lesson 2021-06-16 Additional materials and resources \u00b6 Contributors \u00b6 Nicole Vasilevsky Nico Matentzoglu Major disease and phenotype ontologies that are available \u00b6 A landscape analysis of major disease and phenotype ontologies that are currently available is here (also available in Zenodo here ). Decide which phenotype or disease ontology to use for different use cases \u00b6 Different ontologies are build for different purposes and were created for various reasons. For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. The unified phenotype ontology (uPheno) aggregates species-specific phenotype ontologies into a unified resource. Several species-specific phenotype ontologies exist, such as the Human Phenotype Ontology , Mammalian Phenotype Ontology (http://www.informatics.jax.org/searches/MP_form.shtml) and many more. Similarly to the phenotype ontologies, there are many disease ontologies that exist that are specific to certain areas of diseases, such as infectious diseases (e.g. Infectious Disease Ontology ), cancer (e.g. National Cancer Institute Thesaurus ), rare diseases (e.g. Orphanet ), etc. In addition, there are several more general disease ontologies, such as the Mondo Disease Ontology , the Human Disease Ontology (DO) , SNOMED , etc. Different disease ontologies may be built for different purposes; for example, ontologies like Mondo and DO are intended to be used for classifying data, and downstream computational analyses. Some terminologies are used for indexing purposes, such as International classification of Diseases (ICD). ICD-11 is intended for indexing medical encounters for the purposes of billing and coding. Some of the disease ontologies listed on the landscape contain terms that define diseases, such as Ontology for General Medical Sciences (OGMS) are upper-level ontologies and are intended for integration with other ontologies. When deciding on which phenotype or disease ontology to use, some things to consider: Do you need a more specific ontology, such as a species-specific ontology, or do you need a more general ontology that is cross-species or covers more aspects of diseases? Is the ontology open and free to use? Does the description of the ontology describe it's intended use? For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. Is the ontology actively maintained? Does the ontology contain the terms you need? If not, is there a mechanism to request changes and new terms and are the ontology developers responsive to change requests on their tracker? Is the ontology widely used by the community? You can check things like active contributors on GitHub, usages described on the OBO Foundry page (for example http://obofoundry.org/ontology/mondo.html), published papers and citations. Understand how to leverage disease and phenotype ontologies for advanced data analytics \u00b6 How to integrate other data \u00b6","title":"Disease and Phenotype Ontologies"},{"location":"lesson/using-disease-and-phenotype-ontologies/#finding-and-using-disease-and-phenotype-ontologies","text":"","title":"Finding and using Disease and Phenotype Ontologies"},{"location":"lesson/using-disease-and-phenotype-ontologies/#warning","text":"These materials are under construction and incomplete.","title":"Warning"},{"location":"lesson/using-disease-and-phenotype-ontologies/#prerequisites","text":"None","title":"Prerequisites"},{"location":"lesson/using-disease-and-phenotype-ontologies/#preparation","text":"Review tutorial on Ontology Term Use","title":"Preparation"},{"location":"lesson/using-disease-and-phenotype-ontologies/#what-is-delivered-as-part-of-the-course","text":"Description: An introduction to the landscape of disease and phenotype terminologies and ontologies, and how they can be used to add value to your analysis.","title":"What is delivered as part of the course"},{"location":"lesson/using-disease-and-phenotype-ontologies/#learning-objectives","text":"Become aware of the major disease and phenotype ontologies that are available Be able to decide which phenotype or disease ontology to use for different use cases Understand how to leverage disease and phenotype ontologies for advanced data analytics Have a basic understanding of how to integrate other data","title":"Learning objectives"},{"location":"lesson/using-disease-and-phenotype-ontologies/#tutorials","text":"Video from Disease and Phenotypes c-path lesson 2021-06-16","title":"Tutorials"},{"location":"lesson/using-disease-and-phenotype-ontologies/#additional-materials-and-resources","text":"","title":"Additional materials and resources"},{"location":"lesson/using-disease-and-phenotype-ontologies/#contributors","text":"Nicole Vasilevsky Nico Matentzoglu","title":"Contributors"},{"location":"lesson/using-disease-and-phenotype-ontologies/#major-disease-and-phenotype-ontologies-that-are-available","text":"A landscape analysis of major disease and phenotype ontologies that are currently available is here (also available in Zenodo here ).","title":"Major disease and phenotype ontologies that are available"},{"location":"lesson/using-disease-and-phenotype-ontologies/#decide-which-phenotype-or-disease-ontology-to-use-for-different-use-cases","text":"Different ontologies are build for different purposes and were created for various reasons. For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. The unified phenotype ontology (uPheno) aggregates species-specific phenotype ontologies into a unified resource. Several species-specific phenotype ontologies exist, such as the Human Phenotype Ontology , Mammalian Phenotype Ontology (http://www.informatics.jax.org/searches/MP_form.shtml) and many more. Similarly to the phenotype ontologies, there are many disease ontologies that exist that are specific to certain areas of diseases, such as infectious diseases (e.g. Infectious Disease Ontology ), cancer (e.g. National Cancer Institute Thesaurus ), rare diseases (e.g. Orphanet ), etc. In addition, there are several more general disease ontologies, such as the Mondo Disease Ontology , the Human Disease Ontology (DO) , SNOMED , etc. Different disease ontologies may be built for different purposes; for example, ontologies like Mondo and DO are intended to be used for classifying data, and downstream computational analyses. Some terminologies are used for indexing purposes, such as International classification of Diseases (ICD). ICD-11 is intended for indexing medical encounters for the purposes of billing and coding. Some of the disease ontologies listed on the landscape contain terms that define diseases, such as Ontology for General Medical Sciences (OGMS) are upper-level ontologies and are intended for integration with other ontologies. When deciding on which phenotype or disease ontology to use, some things to consider: Do you need a more specific ontology, such as a species-specific ontology, or do you need a more general ontology that is cross-species or covers more aspects of diseases? Is the ontology open and free to use? Does the description of the ontology describe it's intended use? For example, some ontologies are built for text mining purposes, some are built for annotating data and downstream computational analysis. Is the ontology actively maintained? Does the ontology contain the terms you need? If not, is there a mechanism to request changes and new terms and are the ontology developers responsive to change requests on their tracker? Is the ontology widely used by the community? You can check things like active contributors on GitHub, usages described on the OBO Foundry page (for example http://obofoundry.org/ontology/mondo.html), published papers and citations.","title":"Decide which phenotype or disease ontology to use for different use cases"},{"location":"lesson/using-disease-and-phenotype-ontologies/#understand-how-to-leverage-disease-and-phenotype-ontologies-for-advanced-data-analytics","text":"","title":"Understand how to leverage disease and phenotype ontologies for advanced data analytics"},{"location":"lesson/using-disease-and-phenotype-ontologies/#how-to-integrate-other-data","text":"","title":"How to integrate other data"},{"location":"pathways/ontology-contributor/","text":"Ontology Contributor Pathway \u00b6 Description: These are guidelines are developed for anyone interested in contributing to ontologies to provide guidance on how to contribute to OBO Foundry ontologies. Why you should contribute to ontology development efforts? \u00b6 Ontologies are routinely used for data standardization and in the analytical analysis, but the ontologies themselves are under constant revisions and iterative development. Building ontologies is a community effort and we need expertise from different areas: Technical expertise Domain expertise User experiences The OBO foundry ontologies are open, which means anyone can access them, and anyone can contribute to them. The types of contributions may include reporting issues, identifying bugs, making requests for new terms or changes, and you can also contribute directly to the ontology itself- if you are familiar with ontology editing workflows, you can download our ontologies and make edits on a branch and do a pull request in GitHub. Providing Feedback to an Ontology \u00b6 The preferred mechanism for feedback for almost all OBO Foundry ontologies is via the ontology's GitHub issue tracker. To find the correct GitHub issue tracker, go to the OBO Foundry website website and search for a particular ontology. For example, click on go (Gene Ontology) and you should see a link to the tracker ( https://github.com/geneontology/go-ontology/issues/ ) The OBO Metadata also has a link to a contact but this is generally for OBO administrative purposes, and should not be used for general questions, new term requests, or general feedback Some ontologies may have other means of engaging their use community, and these should all be listed in a standard way on the OBO page. This includes mailing lists, slack groups, and Twitter or other social media accounts. Getting Started with GitHub \u00b6 Open a free account at https://github.com/ . GitHub Fundamentals for OBO Engineers . Provides an introduction to GitHub including : how to get started an overview of the organization of GitHub an introduction to Markdown (the simple markup language used in GitHub to do format text, like bold or italics ) types of content that can be added to GitHub (e.g. you can attach a screenshot to an issue) Intro to managing and tracking issues in GitHub . This tutorial walks you through creating issues in GitHub. Using Ontologies and Ontology Terms \u00b6 Why ontologies are useful Finding good ontologies Ontology repositories Assessing ontologies for use Contributing to Ontologies \u00b6 Community feedback is welcome for all open, OBO Foundry ontologies. Feedback is often provided in the form of: New terms requests Add/revise synonyms, definitions Reclassify a term Report a bug etc. Ways to provide feedback \u00b6 Create a new issue on a GitHub issue tracker See lesson on Identifying missing terms See lesson on Making term requests to existing ontologies Join the discussion: Comment on tickets or discussion board Join the conversation: Attend ontology calls (many ontology developer groups have recurring calls that are open to the community. Contact the ontology owner to request information about calls.) Edit the ontology file: make changes on a branch and do a pull request ( more advanced ) See lesson on Contributing to OBO ontologies Relevant Presentations \u00b6 Using ontologies to standardize rare disease data collection - By Nicole Vasilevsky, Presentation on June 15, 2022 RDCA-DAP: Searchability and Standardized Ontologies - By Ramona Walls, Recording from October 13, 2020","title":"Ontology contributor"},{"location":"pathways/ontology-contributor/#ontology-contributor-pathway","text":"Description: These are guidelines are developed for anyone interested in contributing to ontologies to provide guidance on how to contribute to OBO Foundry ontologies.","title":"Ontology Contributor Pathway"},{"location":"pathways/ontology-contributor/#why-you-should-contribute-to-ontology-development-efforts","text":"Ontologies are routinely used for data standardization and in the analytical analysis, but the ontologies themselves are under constant revisions and iterative development. Building ontologies is a community effort and we need expertise from different areas: Technical expertise Domain expertise User experiences The OBO foundry ontologies are open, which means anyone can access them, and anyone can contribute to them. The types of contributions may include reporting issues, identifying bugs, making requests for new terms or changes, and you can also contribute directly to the ontology itself- if you are familiar with ontology editing workflows, you can download our ontologies and make edits on a branch and do a pull request in GitHub.","title":"Why you should contribute to ontology development efforts?"},{"location":"pathways/ontology-contributor/#providing-feedback-to-an-ontology","text":"The preferred mechanism for feedback for almost all OBO Foundry ontologies is via the ontology's GitHub issue tracker. To find the correct GitHub issue tracker, go to the OBO Foundry website website and search for a particular ontology. For example, click on go (Gene Ontology) and you should see a link to the tracker ( https://github.com/geneontology/go-ontology/issues/ ) The OBO Metadata also has a link to a contact but this is generally for OBO administrative purposes, and should not be used for general questions, new term requests, or general feedback Some ontologies may have other means of engaging their use community, and these should all be listed in a standard way on the OBO page. This includes mailing lists, slack groups, and Twitter or other social media accounts.","title":"Providing Feedback to an Ontology"},{"location":"pathways/ontology-contributor/#getting-started-with-github","text":"Open a free account at https://github.com/ . GitHub Fundamentals for OBO Engineers . Provides an introduction to GitHub including : how to get started an overview of the organization of GitHub an introduction to Markdown (the simple markup language used in GitHub to do format text, like bold or italics ) types of content that can be added to GitHub (e.g. you can attach a screenshot to an issue) Intro to managing and tracking issues in GitHub . This tutorial walks you through creating issues in GitHub.","title":"Getting Started with GitHub"},{"location":"pathways/ontology-contributor/#using-ontologies-and-ontology-terms","text":"Why ontologies are useful Finding good ontologies Ontology repositories Assessing ontologies for use","title":"Using Ontologies and Ontology Terms"},{"location":"pathways/ontology-contributor/#contributing-to-ontologies","text":"Community feedback is welcome for all open, OBO Foundry ontologies. Feedback is often provided in the form of: New terms requests Add/revise synonyms, definitions Reclassify a term Report a bug etc.","title":"Contributing to Ontologies"},{"location":"pathways/ontology-contributor/#ways-to-provide-feedback","text":"Create a new issue on a GitHub issue tracker See lesson on Identifying missing terms See lesson on Making term requests to existing ontologies Join the discussion: Comment on tickets or discussion board Join the conversation: Attend ontology calls (many ontology developer groups have recurring calls that are open to the community. Contact the ontology owner to request information about calls.) Edit the ontology file: make changes on a branch and do a pull request ( more advanced ) See lesson on Contributing to OBO ontologies","title":"Ways to provide feedback"},{"location":"pathways/ontology-contributor/#relevant-presentations","text":"Using ontologies to standardize rare disease data collection - By Nicole Vasilevsky, Presentation on June 15, 2022 RDCA-DAP: Searchability and Standardized Ontologies - By Ramona Walls, Recording from October 13, 2020","title":"Relevant Presentations"},{"location":"pathways/ontology-curator-go-style/","text":"Ontology Curator Pathway: GO-Style \u00b6 Note: There is no one single accepted way of doing ontology curation in the OBO-World, see here . This guide reflects the practice of the GO-style ontology curation, as it is used by GO, Uberon, CL, PATO and others. Note: Work on this document is still in progress, items that are not linked are currently being worked on. Getting Set-up \u00b6 Download and install GitHub Desktop Download and install Protege . See instructions on how to set up Protege here Install ELK reasoner in protege Setting up your ID range Setting up ODK Learning \u00b6 Learning about Ontologies \u00b6 Fundamentals of Ontologies Learning Git and GitHub \u00b6 Fundamentals of GitHub Github issues Cloning a Repo Creating pull requests Introduction into the command line Learning protege \u00b6 Protege interface Browse and Search DL query Editing Terms Creating New Terms Adding disjointness Logical axiomatization of classes & use of reasoning Ontology Relations Updating Imports with ODK OBO-style term annotation Obsoleting terms Merging terms General mindset \u00b6 How to be a team open science player Ontology Curator How To Collection \u00b6 This section is a non-ordered collection of how to documents that a curator might needs Adding taxon restrictions Changing files in pull requests Cloning a repo Creating new terms Daily Workflow Fixing Conflicts Creating a github fork Creating a github pull request Setting up your ID range Setting up ELK reasoner in protege Obsoleting terms Merging terms Creating slims Setting up ODK Switching ontologies Updating Imports with ODK","title":"Ontology Curator GO-style"},{"location":"pathways/ontology-curator-go-style/#ontology-curator-pathway-go-style","text":"Note: There is no one single accepted way of doing ontology curation in the OBO-World, see here . This guide reflects the practice of the GO-style ontology curation, as it is used by GO, Uberon, CL, PATO and others. Note: Work on this document is still in progress, items that are not linked are currently being worked on.","title":"Ontology Curator Pathway: GO-Style"},{"location":"pathways/ontology-curator-go-style/#getting-set-up","text":"Download and install GitHub Desktop Download and install Protege . See instructions on how to set up Protege here Install ELK reasoner in protege Setting up your ID range Setting up ODK","title":"Getting Set-up"},{"location":"pathways/ontology-curator-go-style/#learning","text":"","title":"Learning"},{"location":"pathways/ontology-curator-go-style/#learning-about-ontologies","text":"Fundamentals of Ontologies","title":"Learning about Ontologies"},{"location":"pathways/ontology-curator-go-style/#learning-git-and-github","text":"Fundamentals of GitHub Github issues Cloning a Repo Creating pull requests Introduction into the command line","title":"Learning Git and GitHub"},{"location":"pathways/ontology-curator-go-style/#learning-protege","text":"Protege interface Browse and Search DL query Editing Terms Creating New Terms Adding disjointness Logical axiomatization of classes & use of reasoning Ontology Relations Updating Imports with ODK OBO-style term annotation Obsoleting terms Merging terms","title":"Learning protege"},{"location":"pathways/ontology-curator-go-style/#general-mindset","text":"How to be a team open science player","title":"General mindset"},{"location":"pathways/ontology-curator-go-style/#ontology-curator-how-to-collection","text":"This section is a non-ordered collection of how to documents that a curator might needs Adding taxon restrictions Changing files in pull requests Cloning a repo Creating new terms Daily Workflow Fixing Conflicts Creating a github fork Creating a github pull request Setting up your ID range Setting up ELK reasoner in protege Obsoleting terms Merging terms Creating slims Setting up ODK Switching ontologies Updating Imports with ODK","title":"Ontology Curator How To Collection"},{"location":"pathways/ontology-curator-obi-style/","text":"Ontology Curator Pathway: OBI-style \u00b6 Note: There is no one single accepted way of doing ontology curation in the OBO-World, see here . This guide reflects the practice of the OBI-style ontology curation, as it is used by OBI, IAO and others. Getting Set-up \u00b6 Learning \u00b6 Learning Git and GitHub \u00b6 Fundamentals of GitHub Github issues Cloning a Repo Creating pull requests Learning how to edit ontologies \u00b6 Ontology Curator How To Collection \u00b6","title":"Ontology Curator OBI-style"},{"location":"pathways/ontology-curator-obi-style/#ontology-curator-pathway-obi-style","text":"Note: There is no one single accepted way of doing ontology curation in the OBO-World, see here . This guide reflects the practice of the OBI-style ontology curation, as it is used by OBI, IAO and others.","title":"Ontology Curator Pathway: OBI-style"},{"location":"pathways/ontology-curator-obi-style/#getting-set-up","text":"","title":"Getting Set-up"},{"location":"pathways/ontology-curator-obi-style/#learning","text":"","title":"Learning"},{"location":"pathways/ontology-curator-obi-style/#learning-git-and-github","text":"Fundamentals of GitHub Github issues Cloning a Repo Creating pull requests","title":"Learning Git and GitHub"},{"location":"pathways/ontology-curator-obi-style/#learning-how-to-edit-ontologies","text":"","title":"Learning how to edit ontologies"},{"location":"pathways/ontology-curator-obi-style/#ontology-curator-how-to-collection","text":"","title":"Ontology Curator How To Collection"},{"location":"pathways/ontology-curator/","text":"Ontology Curator Pathway \u00b6 There is no one single accepted methodology for building ontologies in the OBO-World. We can distinguish at least two major schools of ontology curation GO-style ontology curation OBI-style ontology curation Note that there are many more variants, probably as many as there are ontologies. Both schools differ only in how they curate their ontologies - the final product is always an ontology in accordance with OBO Principles . These are some of the main differences of the two schools: GO-style OBI-style Edit format Historically developed in OBO format Developed in an OWL format Annotation properties Many annotation properties from the oboInOwl namespace, for example for synonyms and provenance. Many annotation properties from the IAO namespace. Upper Ontology Hesitant alignment with BFO, often uncommitted. Strong alignment with BFO. Logic Tend do be simple existential restrictions ( some ), ontologies in OWL 2 EL. No class expression nesting. Simple logical definition patterns geared towards automating classification Tend to use a lot more expressive logic, including only and not . Class expression nesting can be more complex. Examples GO, Uberon, Mondo, HPO, PATO, CL, BSPO OBI, IAO, OGMS There are a lot of processes happening that are bringing these schools together, sharing best practices (GitHub, documentation) and reconciling metadata conventions and annotation properties in the OBO Metadata Ontology (OMO) . The Upper Level alignment is now done by members of both schools through the Core Ontology for Biology and Biomedicine (COB) . While these processes are ongoing, we decided to curate separate pathways for both schools: Pathway for GO-style ontology curation Pathway for OBI-style ontology curation","title":"Ontology Curator"},{"location":"pathways/ontology-curator/#ontology-curator-pathway","text":"There is no one single accepted methodology for building ontologies in the OBO-World. We can distinguish at least two major schools of ontology curation GO-style ontology curation OBI-style ontology curation Note that there are many more variants, probably as many as there are ontologies. Both schools differ only in how they curate their ontologies - the final product is always an ontology in accordance with OBO Principles . These are some of the main differences of the two schools: GO-style OBI-style Edit format Historically developed in OBO format Developed in an OWL format Annotation properties Many annotation properties from the oboInOwl namespace, for example for synonyms and provenance. Many annotation properties from the IAO namespace. Upper Ontology Hesitant alignment with BFO, often uncommitted. Strong alignment with BFO. Logic Tend do be simple existential restrictions ( some ), ontologies in OWL 2 EL. No class expression nesting. Simple logical definition patterns geared towards automating classification Tend to use a lot more expressive logic, including only and not . Class expression nesting can be more complex. Examples GO, Uberon, Mondo, HPO, PATO, CL, BSPO OBI, IAO, OGMS There are a lot of processes happening that are bringing these schools together, sharing best practices (GitHub, documentation) and reconciling metadata conventions and annotation properties in the OBO Metadata Ontology (OMO) . The Upper Level alignment is now done by members of both schools through the Core Ontology for Biology and Biomedicine (COB) . While these processes are ongoing, we decided to curate separate pathways for both schools: Pathway for GO-style ontology curation Pathway for OBI-style ontology curation","title":"Ontology Curator Pathway"},{"location":"pathways/ontology-engineer/","text":"Ontology Engineer/Developer Pathway \u00b6 Getting Set-up \u00b6 Download and install GitHub Desktop Download and install Protege Install ELK reasoner in protege Setting up your ID range Setting up ODK Learning \u00b6 As a ontology engineer, it would be useful for you to know how curators work, as such, it would be useful to be familiar with all the concepts in the ontology curator pathways document . This pathways will however be focusing on the engineering side of things. Very basics \u00b6 Basic introduction to CLI 1 Basic introduction to CLI 2 Learning Git and GitHub \u00b6 Fundamentals of GitHub Github issues Cloning a Repo Creating pull requests Learning ontology engineering \u00b6 Setting up a ODK repository Developing an obo ontology Understanding product variants Dealing with large ontologies ROBOT tutorial pt. 1 ROBOT tutorial pt. 2 Templates Getting started with DOSDP templates DOSDP Templates Basic Tutorial Introduction to Managing DOSDP Templates in ODK Basics of SPARQL Setting up slims Ontology Engineer How To Collection \u00b6 This section is a non-ordered collection of how to documents that an engineer might need (this includes everything from the curators list as they may be pertinent knowledge to an engineer). Adding taxon restrictions Changing files in pull requests Cloning a repo Creating new terms Daily Curator Workflow Fixing Conflicts Creating a github fork Creating a github pull request Setting up your ID range Setting up ELK reasoner in protege Obsoleting terms Merging terms Creating slims Setting up ODK Switching ontologies Dealing with large ontologies","title":"Ontology Engineer/Developer"},{"location":"pathways/ontology-engineer/#ontology-engineerdeveloper-pathway","text":"","title":"Ontology Engineer/Developer Pathway"},{"location":"pathways/ontology-engineer/#getting-set-up","text":"Download and install GitHub Desktop Download and install Protege Install ELK reasoner in protege Setting up your ID range Setting up ODK","title":"Getting Set-up"},{"location":"pathways/ontology-engineer/#learning","text":"As a ontology engineer, it would be useful for you to know how curators work, as such, it would be useful to be familiar with all the concepts in the ontology curator pathways document . This pathways will however be focusing on the engineering side of things.","title":"Learning"},{"location":"pathways/ontology-engineer/#very-basics","text":"Basic introduction to CLI 1 Basic introduction to CLI 2","title":"Very basics"},{"location":"pathways/ontology-engineer/#learning-git-and-github","text":"Fundamentals of GitHub Github issues Cloning a Repo Creating pull requests","title":"Learning Git and GitHub"},{"location":"pathways/ontology-engineer/#learning-ontology-engineering","text":"Setting up a ODK repository Developing an obo ontology Understanding product variants Dealing with large ontologies ROBOT tutorial pt. 1 ROBOT tutorial pt. 2 Templates Getting started with DOSDP templates DOSDP Templates Basic Tutorial Introduction to Managing DOSDP Templates in ODK Basics of SPARQL Setting up slims","title":"Learning ontology engineering"},{"location":"pathways/ontology-engineer/#ontology-engineer-how-to-collection","text":"This section is a non-ordered collection of how to documents that an engineer might need (this includes everything from the curators list as they may be pertinent knowledge to an engineer). Adding taxon restrictions Changing files in pull requests Cloning a repo Creating new terms Daily Curator Workflow Fixing Conflicts Creating a github fork Creating a github pull request Setting up your ID range Setting up ELK reasoner in protege Obsoleting terms Merging terms Creating slims Setting up ODK Switching ontologies Dealing with large ontologies","title":"Ontology Engineer How To Collection"},{"location":"pathways/pathways/","text":"Introduction to pathways \u00b6 Pathways are materials from OBOOK in a linear fashion for the purpose of helping people in different roles finding the materials relevant to their work more easily. To browse through the pathways, look under the \"Pathways\" menu item.","title":"What is a pathway?"},{"location":"pathways/pathways/#introduction-to-pathways","text":"Pathways are materials from OBOOK in a linear fashion for the purpose of helping people in different roles finding the materials relevant to their work more easily. To browse through the pathways, look under the \"Pathways\" menu item.","title":"Introduction to pathways"},{"location":"reference/formatting-license/","text":"Formatting your ontology annotations correctly \u00b6 The new OBO Foundry guidelines encourage the annotation of ontologies with an appropriately formatted description, title and license. Here are some examples that can be used as a guide to implement those in your ontology. Note : these examples purposefully do not include version information, this should not be manually added, instead it should be added by ROBOT as part of a pipeline. An ontology set up with the ODK will take care of all of this for you. RDF/XML Example: \u00b6 <?xml version=\"1.0\"?> <rdf:RDF xmlns=\"http://purl.obolibrary.org/obo/license.owl#\" xml:base=\"http://purl.obolibrary.org/obo/license.owl\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:owl=\"http://www.w3.org/2002/07/owl#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:xml=\"http://www.w3.org/XML/1998/namespace\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\" xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\" xmlns:terms=\"http://purl.org/dc/terms/\"> <owl:Ontology rdf:about=\"http://purl.obolibrary.org/obo/license.owl\"> <dc:description rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">An integrated and fictional ontology for the description of abnormal tomato phenotypes.</dc:description> <dc:title rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">Tomato Phenotype Ontology (TPO)</dc:title> <terms:license rdf:resource=\"https://creativecommons.org/licenses/by/3.0/\"/> </owl:Ontology> <owl:AnnotationProperty rdf:about=\"http://purl.org/dc/elements/1.1/description\"/> <owl:AnnotationProperty rdf:about=\"http://purl.org/dc/elements/1.1/title\"/> <owl:AnnotationProperty rdf:about=\"http://purl.org/dc/terms/license\"/> </rdf:RDF> Functional Syntax Example: \u00b6 Prefix(:=<http://purl.obolibrary.org/obo/license.owl#>) Prefix(owl:=<http://www.w3.org/2002/07/owl#>) Prefix(rdf:=<http://www.w3.org/1999/02/22-rdf-syntax-ns#>) Prefix(xml:=<http://www.w3.org/XML/1998/namespace>) Prefix(xsd:=<http://www.w3.org/2001/XMLSchema#>) Prefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>) Ontology(<http://purl.obolibrary.org/obo/license.owl> Annotation(<http://purl.org/dc/elements/1.1/description> \"An integrated and fictional ontology for the description of abnormal tomato phenotypes.\"^^xsd:string) Annotation(<http://purl.org/dc/elements/1.1/title> \"Tomato Phenotype Ontology (TPO)\"^^xsd:string) Annotation(<http://purl.org/dc/terms/license> <https://creativecommons.org/licenses/by/3.0/>) ) OWL/XML Example: \u00b6 <?xml version=\"1.0\"?> <Ontology xmlns=\"http://www.w3.org/2002/07/owl#\" xml:base=\"http://purl.obolibrary.org/obo/license.owl\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:xml=\"http://www.w3.org/XML/1998/namespace\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\" xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\" ontologyIRI=\"http://purl.obolibrary.org/obo/license.owl\"> <Prefix name=\"\" IRI=\"http://purl.obolibrary.org/obo/license.owl#\"/> <Prefix name=\"owl\" IRI=\"http://www.w3.org/2002/07/owl#\"/> <Prefix name=\"rdf\" IRI=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"/> <Prefix name=\"xml\" IRI=\"http://www.w3.org/XML/1998/namespace\"/> <Prefix name=\"xsd\" IRI=\"http://www.w3.org/2001/XMLSchema#\"/> <Prefix name=\"rdfs\" IRI=\"http://www.w3.org/2000/01/rdf-schema#\"/> <Annotation> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/description\"/> <Literal>An integrated and fictional ontology for the description of abnormal tomato phenotypes.</Literal> </Annotation> <Annotation> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/title\"/> <Literal>Tomato Phenotype Ontology (TPO)</Literal> </Annotation> <Annotation> <AnnotationProperty abbreviatedIRI=\"terms:license\"/> <IRI>https://creativecommons.org/licenses/by/3.0/</IRI> </Annotation> <Declaration> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/title\"/> </Declaration> <Declaration> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/description\"/> </Declaration> <Declaration> <AnnotationProperty IRI=\"http://purl.org/dc/terms/license\"/> </Declaration> </Ontology> OBO Example: \u00b6 format-version: 1.2 ontology: license property_value: http://purl.org/dc/elements/1.1/description \"An integrated and fictional ontology for the description of abnormal tomato phenotypes.\" xsd:string property_value: http://purl.org/dc/elements/1.1/title \"Tomato Phenotype Ontology (TPO)\" xsd:string property_value: http://purl.org/dc/terms/license https://creativecommons.org/licenses/by/3.0/","title":"Formatting your ontology annotations correctly"},{"location":"reference/formatting-license/#formatting-your-ontology-annotations-correctly","text":"The new OBO Foundry guidelines encourage the annotation of ontologies with an appropriately formatted description, title and license. Here are some examples that can be used as a guide to implement those in your ontology. Note : these examples purposefully do not include version information, this should not be manually added, instead it should be added by ROBOT as part of a pipeline. An ontology set up with the ODK will take care of all of this for you.","title":"Formatting your ontology annotations correctly"},{"location":"reference/formatting-license/#rdfxml-example","text":"<?xml version=\"1.0\"?> <rdf:RDF xmlns=\"http://purl.obolibrary.org/obo/license.owl#\" xml:base=\"http://purl.obolibrary.org/obo/license.owl\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:owl=\"http://www.w3.org/2002/07/owl#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:xml=\"http://www.w3.org/XML/1998/namespace\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\" xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\" xmlns:terms=\"http://purl.org/dc/terms/\"> <owl:Ontology rdf:about=\"http://purl.obolibrary.org/obo/license.owl\"> <dc:description rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">An integrated and fictional ontology for the description of abnormal tomato phenotypes.</dc:description> <dc:title rdf:datatype=\"http://www.w3.org/2001/XMLSchema#string\">Tomato Phenotype Ontology (TPO)</dc:title> <terms:license rdf:resource=\"https://creativecommons.org/licenses/by/3.0/\"/> </owl:Ontology> <owl:AnnotationProperty rdf:about=\"http://purl.org/dc/elements/1.1/description\"/> <owl:AnnotationProperty rdf:about=\"http://purl.org/dc/elements/1.1/title\"/> <owl:AnnotationProperty rdf:about=\"http://purl.org/dc/terms/license\"/> </rdf:RDF>","title":"RDF/XML Example:"},{"location":"reference/formatting-license/#functional-syntax-example","text":"Prefix(:=<http://purl.obolibrary.org/obo/license.owl#>) Prefix(owl:=<http://www.w3.org/2002/07/owl#>) Prefix(rdf:=<http://www.w3.org/1999/02/22-rdf-syntax-ns#>) Prefix(xml:=<http://www.w3.org/XML/1998/namespace>) Prefix(xsd:=<http://www.w3.org/2001/XMLSchema#>) Prefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>) Ontology(<http://purl.obolibrary.org/obo/license.owl> Annotation(<http://purl.org/dc/elements/1.1/description> \"An integrated and fictional ontology for the description of abnormal tomato phenotypes.\"^^xsd:string) Annotation(<http://purl.org/dc/elements/1.1/title> \"Tomato Phenotype Ontology (TPO)\"^^xsd:string) Annotation(<http://purl.org/dc/terms/license> <https://creativecommons.org/licenses/by/3.0/>) )","title":"Functional Syntax Example:"},{"location":"reference/formatting-license/#owlxml-example","text":"<?xml version=\"1.0\"?> <Ontology xmlns=\"http://www.w3.org/2002/07/owl#\" xml:base=\"http://purl.obolibrary.org/obo/license.owl\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:xml=\"http://www.w3.org/XML/1998/namespace\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\" xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\" ontologyIRI=\"http://purl.obolibrary.org/obo/license.owl\"> <Prefix name=\"\" IRI=\"http://purl.obolibrary.org/obo/license.owl#\"/> <Prefix name=\"owl\" IRI=\"http://www.w3.org/2002/07/owl#\"/> <Prefix name=\"rdf\" IRI=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"/> <Prefix name=\"xml\" IRI=\"http://www.w3.org/XML/1998/namespace\"/> <Prefix name=\"xsd\" IRI=\"http://www.w3.org/2001/XMLSchema#\"/> <Prefix name=\"rdfs\" IRI=\"http://www.w3.org/2000/01/rdf-schema#\"/> <Annotation> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/description\"/> <Literal>An integrated and fictional ontology for the description of abnormal tomato phenotypes.</Literal> </Annotation> <Annotation> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/title\"/> <Literal>Tomato Phenotype Ontology (TPO)</Literal> </Annotation> <Annotation> <AnnotationProperty abbreviatedIRI=\"terms:license\"/> <IRI>https://creativecommons.org/licenses/by/3.0/</IRI> </Annotation> <Declaration> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/title\"/> </Declaration> <Declaration> <AnnotationProperty IRI=\"http://purl.org/dc/elements/1.1/description\"/> </Declaration> <Declaration> <AnnotationProperty IRI=\"http://purl.org/dc/terms/license\"/> </Declaration> </Ontology>","title":"OWL/XML Example:"},{"location":"reference/formatting-license/#obo-example","text":"format-version: 1.2 ontology: license property_value: http://purl.org/dc/elements/1.1/description \"An integrated and fictional ontology for the description of abnormal tomato phenotypes.\" xsd:string property_value: http://purl.org/dc/elements/1.1/title \"Tomato Phenotype Ontology (TPO)\" xsd:string property_value: http://purl.org/dc/terms/license https://creativecommons.org/licenses/by/3.0/","title":"OBO Example:"},{"location":"reference/frequently-used-odk-commands/","text":"Frequently used ODK commands \u00b6 Updates the Makefile to the latest ODK \u00b6 sh run.sh make update_repo Recreates and deploys the automated documentation \u00b6 sh run.sh make update_docs Preparing a new release \u00b6 sh run.sh make prepare_release Refreshing a single import \u00b6 sh run.sh make refresh-% Example: sh run.sh make refresh-chebi Refresh all imports \u00b6 sh run.sh make refresh-imports Refresh all imports excluding large ones \u00b6 sh run.sh make refresh-imports-excluding-large Run all the QC checks \u00b6 sh run.sh make test Print the version of the currently installed ODK \u00b6 sh run.sh make odkversion Checks the OWL2 DL profile validity \u00b6 (of a specific file) sh run.sh make validate_profile_% Example: sh run.sh make validate_profile_hp-edit.owl","title":"Frequently used ODK commands"},{"location":"reference/frequently-used-odk-commands/#frequently-used-odk-commands","text":"","title":"Frequently used ODK commands"},{"location":"reference/frequently-used-odk-commands/#updates-the-makefile-to-the-latest-odk","text":"sh run.sh make update_repo","title":"Updates the Makefile to the latest ODK"},{"location":"reference/frequently-used-odk-commands/#recreates-and-deploys-the-automated-documentation","text":"sh run.sh make update_docs","title":"Recreates and deploys the automated documentation"},{"location":"reference/frequently-used-odk-commands/#preparing-a-new-release","text":"sh run.sh make prepare_release","title":"Preparing a new release"},{"location":"reference/frequently-used-odk-commands/#refreshing-a-single-import","text":"sh run.sh make refresh-% Example: sh run.sh make refresh-chebi","title":"Refreshing a single import"},{"location":"reference/frequently-used-odk-commands/#refresh-all-imports","text":"sh run.sh make refresh-imports","title":"Refresh all imports"},{"location":"reference/frequently-used-odk-commands/#refresh-all-imports-excluding-large-ones","text":"sh run.sh make refresh-imports-excluding-large","title":"Refresh all imports excluding large ones"},{"location":"reference/frequently-used-odk-commands/#run-all-the-qc-checks","text":"sh run.sh make test","title":"Run all the QC checks"},{"location":"reference/frequently-used-odk-commands/#print-the-version-of-the-currently-installed-odk","text":"sh run.sh make odkversion","title":"Print the version of the currently installed ODK"},{"location":"reference/frequently-used-odk-commands/#checks-the-owl2-dl-profile-validity","text":"(of a specific file) sh run.sh make validate_profile_% Example: sh run.sh make validate_profile_hp-edit.owl","title":"Checks the OWL2 DL profile validity"},{"location":"reference/gh-actions-errors/","text":"Common Errors in GitHub actions \u00b6 Killed : Running out of memory \u00b6 Running the same workflow several times simultaneously (e.g. if two PRs are submitted in a short time, and the second PR triggers the CI workflow while the CI workflow triggered by the first PR is still running) could lead to lack-of-memory situations because all concurrent workflows have to share a single memory limit. (Note: it isn't really clear with documentation of GitHub Actions on whether concurrent workflow runs share a single memory limit.) What could possibly be done is to forbid a given workflow from ever running as long as there is already a run of the same workflow ongoing, using the concurrency property .","title":"Common Errors in GitHub actions"},{"location":"reference/gh-actions-errors/#common-errors-in-github-actions","text":"","title":"Common Errors in GitHub actions"},{"location":"reference/gh-actions-errors/#killed-running-out-of-memory","text":"Running the same workflow several times simultaneously (e.g. if two PRs are submitted in a short time, and the second PR triggers the CI workflow while the CI workflow triggered by the first PR is still running) could lead to lack-of-memory situations because all concurrent workflows have to share a single memory limit. (Note: it isn't really clear with documentation of GitHub Actions on whether concurrent workflow runs share a single memory limit.) What could possibly be done is to forbid a given workflow from ever running as long as there is already a run of the same workflow ongoing, using the concurrency property .","title":"Killed: Running out of memory"},{"location":"reference/git-faq/","text":"Git FAQs \u00b6 This page aims to consolidate some tips and tricks that ontology editors have found useful in using git . It is not meant to be a tutorial of git , but rather as a page with tips that could help in certain specialised situations. Reverting Commits \u00b6 Reverting particular files back to master version \u00b6 If you want to revert only certain files (eg import files), you can do it using Terminal. For this example, we will use uberon_import.owl as the file we want reverted back to the version in master branch, however, this can be done on any file. Assuming your directory is set to src/ontology , in terminal use: git checkout master -- imports/uberon_import.owl . Commit the change to the branch as normal. Reverting particular files back to a previous version \u00b6 If you want to revert a file back to a previous version instead of master, you can use the commit ID. To do this, in Terminal use: git log to list out the previous commits and copy the commit code of the commit you would like to revert to (example: see yellow string of text in screenshot below). Press q on your keyboard to quit git log (or down arrow to scroll down to continue to find the commit ID you want to revert if it is further down). In terminal use: git checkout ff18c9482035062bbbbb27aaeb50e658298fb635 -- imports/uberon_import.owl using whichever commit code you want instead of the commit code in this example. commit the change to the branch as normal.","title":"Git FAQ"},{"location":"reference/git-faq/#git-faqs","text":"This page aims to consolidate some tips and tricks that ontology editors have found useful in using git . It is not meant to be a tutorial of git , but rather as a page with tips that could help in certain specialised situations.","title":"Git FAQs"},{"location":"reference/git-faq/#reverting-commits","text":"","title":"Reverting Commits"},{"location":"reference/git-faq/#reverting-particular-files-back-to-master-version","text":"If you want to revert only certain files (eg import files), you can do it using Terminal. For this example, we will use uberon_import.owl as the file we want reverted back to the version in master branch, however, this can be done on any file. Assuming your directory is set to src/ontology , in terminal use: git checkout master -- imports/uberon_import.owl . Commit the change to the branch as normal.","title":"Reverting particular files back to master version"},{"location":"reference/git-faq/#reverting-particular-files-back-to-a-previous-version","text":"If you want to revert a file back to a previous version instead of master, you can use the commit ID. To do this, in Terminal use: git log to list out the previous commits and copy the commit code of the commit you would like to revert to (example: see yellow string of text in screenshot below). Press q on your keyboard to quit git log (or down arrow to scroll down to continue to find the commit ID you want to revert if it is further down). In terminal use: git checkout ff18c9482035062bbbbb27aaeb50e658298fb635 -- imports/uberon_import.owl using whichever commit code you want instead of the commit code in this example. commit the change to the branch as normal.","title":"Reverting particular files back to a previous version"},{"location":"reference/github-desktop/","text":"GitHub Desktop \u00b6 For most of our training activities, we recommend using GitHub Desktop . It provides a very convenient way to push and pull changes, and inspect the \"diff\". It is, however, not mandatory if you are already familiar with other git workflows (such as command line, or Sourcetree).","title":"GitHub Desktop"},{"location":"reference/github-desktop/#github-desktop","text":"For most of our training activities, we recommend using GitHub Desktop . It provides a very convenient way to push and pull changes, and inspect the \"diff\". It is, however, not mandatory if you are already familiar with other git workflows (such as command line, or Sourcetree).","title":"GitHub Desktop"},{"location":"reference/github-intro/","text":"Git, GitHub and GitHub Desktop (version control) \u00b6 A repository can consist of many files with several users simultaneously editing those files at any moment in time. In order to ensure conflicting edits between the users are not made and a history of the edits are tracked, software classified as a \"distributed version control system\" is used. All OBO repositories are managed by the Git version control system. This allows users to make their own local branch of the repository, i.e., making a mirror copy of the repository directories and files on their own computers, and make edits as desired. The edits can then be reviewed by other users before the changes are incorporated in the 'main' or 'master' branch of the repository. This process can be executed by running Git line commands and/or by using a web interface (Github.com) along with a desktop application (GitHub Desktop). Documentation, including an introduction to GitHub, can be found here: Hello World .","title":"Intro to GitHub"},{"location":"reference/github-intro/#git-github-and-github-desktop-version-control","text":"A repository can consist of many files with several users simultaneously editing those files at any moment in time. In order to ensure conflicting edits between the users are not made and a history of the edits are tracked, software classified as a \"distributed version control system\" is used. All OBO repositories are managed by the Git version control system. This allows users to make their own local branch of the repository, i.e., making a mirror copy of the repository directories and files on their own computers, and make edits as desired. The edits can then be reviewed by other users before the changes are incorporated in the 'main' or 'master' branch of the repository. This process can be executed by running Git line commands and/or by using a web interface (Github.com) along with a desktop application (GitHub Desktop). Documentation, including an introduction to GitHub, can be found here: Hello World .","title":"Git, GitHub and GitHub Desktop (version control)"},{"location":"reference/glossary/","text":"Glossary for concepts in and around OBO \u00b6 Tools \u00b6 Term Definition Type Docs Ontology Development Kit (ODK) A toolkit and docker image for managing ontology releases. Tool docs ROBOT A toolkit for transforming and interacting with ontologies. Tool docs rdflib A python library to interact with RDF data Library docs OWL API A java-based API to interact with OWL ontologies Library docs Protege A typical ontology development tool used by ontology developers in the OBO-sphere Tool docs ROBOT templates A templating system based on tables, where the templates are integrated in the same table as the data Standard docs Dead Simple Ontology Design Patterns (DOSDP) A templating system for ontologies with well-documented patterns and templates. Standard docs DOSDP tools DOSDP is the open source reference implementation of the DOSDP templating language. Tool docs Reasonable Ontology Templates (OTTR) A system for composable ontology templates and documentation Standard docs Lutra Lutra is the open source reference implementation of the OTTR templating language. Tool docs","title":"Glossary"},{"location":"reference/glossary/#glossary-for-concepts-in-and-around-obo","text":"","title":"Glossary for concepts in and around OBO"},{"location":"reference/glossary/#tools","text":"Term Definition Type Docs Ontology Development Kit (ODK) A toolkit and docker image for managing ontology releases. Tool docs ROBOT A toolkit for transforming and interacting with ontologies. Tool docs rdflib A python library to interact with RDF data Library docs OWL API A java-based API to interact with OWL ontologies Library docs Protege A typical ontology development tool used by ontology developers in the OBO-sphere Tool docs ROBOT templates A templating system based on tables, where the templates are integrated in the same table as the data Standard docs Dead Simple Ontology Design Patterns (DOSDP) A templating system for ontologies with well-documented patterns and templates. Standard docs DOSDP tools DOSDP is the open source reference implementation of the DOSDP templating language. Tool docs Reasonable Ontology Templates (OTTR) A system for composable ontology templates and documentation Standard docs Lutra Lutra is the open source reference implementation of the OTTR templating language. Tool docs","title":"Tools"},{"location":"reference/go-style-annotation-property-practice/","text":"Recommended metadata properties to use in curating OBO ontologies (GO-style) \u00b6 Note that while most of the practices documented here apply to all OBO ontologies this recommendation applies only to ontologies that are developed using GO-style curation workflows . Type Property to use Required Number/Limit Description Format Annotation Reference/Comments Label rdfs:label Y Max 1 * Full name of the term, must be unique. Free text None * some ontologies have multiple labels for different languages, in which case, there should maximum be one label per language Definition IAO:0000115 Y Max 1 A textual definition of ther term. In most ontologies, must be unique. Free text database_cross_reference: reference materials used and contributors (in ORCID ID link format) See this document for guide on writing definitions Contributor dcterms:contributor N (though highly reccomended) No limit The ORCID ID of people who contributed to the creation of the term. ORCID ID (using full link) None Synonyms http://www.geneontology.org/formats/oboInOwl#hasExactSynonym, http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym, http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym, http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym N No limit Synonyms of the term. Free text database_cross_reference: reference material in which the synonymn is used See synonyms documentation for guide on using synonyms Comments rdfs:comment N Max 1 Comments about the term, extended descriptions that might be useful, notes on modelling choices, other misc notes. Free text database_cross_reference: reference material relating to the comment See documentation on comments for more information about comments Editor note IAO:0000116 N Max 1 A note that is not relevant to front users, but might be to editors Free text database_cross_reference: reference material relating to the note Subset http://www.geneontology.org/formats/oboInOwl#inSubset N No limit A tag that marks a term as being part of a subset annotation property that is a subproperty of subset_property (see guide on how to select this) None See Slim documentation for more information on subsets Database Cross Reference http://www.geneontology.org/formats/oboInOwl#hasDbXref N No limit Links out to external references. string and should* take the form {prefix}:{accession}; see db-xrefs yaml for prefixes None *Some ontologies allow full URLS in specific cases, but this is controversial Date created dcterms:created N Max 1 Date in which the term was created ISO-8601 format None Date last updated dcterms:date N Max 1 Date in which the term was last updated ISO-8601 format None Deprecation http://www.w3.org/2002/07/owl#deprecated N Max 1 A tag that marks a term as being obsolete/deprecated xsd:boolean (true/false) None See obsoletion guide for more details Replaced by IAO:0100001 N Max 1 Term that has replaced an obsoleted term IRI/ID (e.g. CL:0000001) None See obsoletion guide and merging terms guide for more details Consider oboInOwl:consider N No limit Term that can be considered from manual replacement of an obsoleted term IRI/ID (e.g. CL:0000001) None See obsoletion guide and merging terms guide for more details","title":"OBO-style term annotation"},{"location":"reference/go-style-annotation-property-practice/#recommended-metadata-properties-to-use-in-curating-obo-ontologies-go-style","text":"Note that while most of the practices documented here apply to all OBO ontologies this recommendation applies only to ontologies that are developed using GO-style curation workflows . Type Property to use Required Number/Limit Description Format Annotation Reference/Comments Label rdfs:label Y Max 1 * Full name of the term, must be unique. Free text None * some ontologies have multiple labels for different languages, in which case, there should maximum be one label per language Definition IAO:0000115 Y Max 1 A textual definition of ther term. In most ontologies, must be unique. Free text database_cross_reference: reference materials used and contributors (in ORCID ID link format) See this document for guide on writing definitions Contributor dcterms:contributor N (though highly reccomended) No limit The ORCID ID of people who contributed to the creation of the term. ORCID ID (using full link) None Synonyms http://www.geneontology.org/formats/oboInOwl#hasExactSynonym, http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym, http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym, http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym N No limit Synonyms of the term. Free text database_cross_reference: reference material in which the synonymn is used See synonyms documentation for guide on using synonyms Comments rdfs:comment N Max 1 Comments about the term, extended descriptions that might be useful, notes on modelling choices, other misc notes. Free text database_cross_reference: reference material relating to the comment See documentation on comments for more information about comments Editor note IAO:0000116 N Max 1 A note that is not relevant to front users, but might be to editors Free text database_cross_reference: reference material relating to the note Subset http://www.geneontology.org/formats/oboInOwl#inSubset N No limit A tag that marks a term as being part of a subset annotation property that is a subproperty of subset_property (see guide on how to select this) None See Slim documentation for more information on subsets Database Cross Reference http://www.geneontology.org/formats/oboInOwl#hasDbXref N No limit Links out to external references. string and should* take the form {prefix}:{accession}; see db-xrefs yaml for prefixes None *Some ontologies allow full URLS in specific cases, but this is controversial Date created dcterms:created N Max 1 Date in which the term was created ISO-8601 format None Date last updated dcterms:date N Max 1 Date in which the term was last updated ISO-8601 format None Deprecation http://www.w3.org/2002/07/owl#deprecated N Max 1 A tag that marks a term as being obsolete/deprecated xsd:boolean (true/false) None See obsoletion guide for more details Replaced by IAO:0100001 N Max 1 Term that has replaced an obsoleted term IRI/ID (e.g. CL:0000001) None See obsoletion guide and merging terms guide for more details Consider oboInOwl:consider N No limit Term that can be considered from manual replacement of an obsoleted term IRI/ID (e.g. CL:0000001) None See obsoletion guide and merging terms guide for more details","title":"Recommended metadata properties to use in curating OBO ontologies (GO-style)"},{"location":"reference/managing-issues/","text":"Tools for Managing Issues \u00b6 Based on Intro to GitHub (GO-Centric) with credit to Nomi Harris and Chris Mungall Labels \u00b6 Labels are a useful tool to help group and organize issues, allowing people to filter issues by grouping. Note: Only project contributors can add/change labels Best Practices for Labels \u00b6 Make use of use GitHub's default labels: bug, question, enhancement, good first issue, etc. Define new labels as needed for project management Lightly coordinate labels across repos in an organization Labels are not ontologies; don\u2019t overload them. A small simple set consistently applied is better than overly specific inconsistently applied labels Superissues \u00b6 Superissues are issues that have checklists (added using -[] on items). These are useful as they show progress towards completion. These can be used for issues that require multiple steps to solve. Milestones \u00b6 Milestones are used for issues with a specific date/deadline. Milestones contain issues and issues can be filtered by milestones. They are also useful for visualizing how many issues in it is completed. Project Boards \u00b6 Project boards are a useful tool to organise, as the name implies, projects. They can span multiple repos (though the repos need to be in the same organisation). Notes can also be added.","title":"Tools for Managing Issues"},{"location":"reference/managing-issues/#tools-for-managing-issues","text":"Based on Intro to GitHub (GO-Centric) with credit to Nomi Harris and Chris Mungall","title":"Tools for Managing Issues"},{"location":"reference/managing-issues/#labels","text":"Labels are a useful tool to help group and organize issues, allowing people to filter issues by grouping. Note: Only project contributors can add/change labels","title":"Labels"},{"location":"reference/managing-issues/#best-practices-for-labels","text":"Make use of use GitHub's default labels: bug, question, enhancement, good first issue, etc. Define new labels as needed for project management Lightly coordinate labels across repos in an organization Labels are not ontologies; don\u2019t overload them. A small simple set consistently applied is better than overly specific inconsistently applied labels","title":"Best Practices for Labels"},{"location":"reference/managing-issues/#superissues","text":"Superissues are issues that have checklists (added using -[] on items). These are useful as they show progress towards completion. These can be used for issues that require multiple steps to solve.","title":"Superissues"},{"location":"reference/managing-issues/#milestones","text":"Milestones are used for issues with a specific date/deadline. Milestones contain issues and issues can be filtered by milestones. They are also useful for visualizing how many issues in it is completed.","title":"Milestones"},{"location":"reference/managing-issues/#project-boards","text":"Project boards are a useful tool to organise, as the name implies, projects. They can span multiple repos (though the repos need to be in the same organisation). Notes can also be added.","title":"Project Boards"},{"location":"reference/medical-ontology-landscape/","text":"The Landscape of Disease and Phenotype Ontologies \u00b6 Compiled by Nicole Vasilevsky. Feel free to make pull requests to suggest edits. Note: This currently just provides an overview of disease and phenotype ontologies. Contributors are welcome to add more descriptions of other medical ontologies. This was last updated in 2021. Disease Ontologies & Terminologies \u00b6 Disease Summary Table \u00b6 Name Disease Area Artificial Intelligence Rheumatology Consultant System Ontology (AI-RHEUM) Rheumatic diseases Autism DSM-ADI-R Ontology (ADAR) Autism Autism Spectrum Disorder Phenotype Ontology (ASDPTO) Autism Brucellosis Ontology (IDOBRU) brucellosis Cardiovascular Disease Ontology (CVDO) Cardiovascular Chronic Kidney Disease Ontology (CKDO) Chronic kidney disease Chronic Obstructive Pulmonary Disease Ontology (COPDO) Chronic obstructive pulmonary disease (COPD) Coronavirus Infectious Disease Ontology (CIDO) Coronavirus infectious diseases Diagnostic and Statistical Manual of Mental Disorders (DSM) Mental disorders Dispedia Core Ontology (DCO) Rare diseases Experimental Factor Ontology (EFO) Broad disease coverage Fibrotic Interstitial Lung Disease Ontology (FILDO) Fibrotic interstitial lung disease Genetic and Rare Diseases Information Center (GARD) Rare diseases Holistic Ontology of Rare Diseases (HORD) Rare disease Human Dermatological Disease Ontology (DERMO) Dermatology (skin) Human Disease Ontology (DO) Human disease Infectious Disease Ontology (IDO) Infectious disease International Classification of Functioning, Disability and Health (ICF) Cross-discipline, focuses disabilities International Statistical Classification of Diseases and Related Health Problems (ICD-11) Broad coverage International Classification of Diseases for Oncology (ICD-O) Cancer Logical Observation Identifier Names and Codes (LOINC) Broad coverage Medical Subject Headings (MeSH) Broad coverage MedGen Human medical genetics Medical Dictionary for Regulatory Activities (MedDRA) Broad coverage Mental Disease Ontology (MDO) Mental functioning Mondo Disease Ontology (Mondo) Broad coverage, Cross species National Cancer Institute Thesaurus (NCIT) Humam cancer and neoplasms Neurological Disease Ontology (ND) Neurology Online Mendelian Inheritance in Man (OMIM) Mendelian, genetic diseases. Ontology of Cardiovascular Drug Adverse Events (OCVDAE) Cardiovascular Ontology for General Medical Science (OGMS) Broad coverage Ontology for Genetic Susceptibility Factor (OGSF) Genetic disease Ontology of Glucose Metabolism Disorder (OGMD) Metabolic disorders Ontology of Language Disorder in Autism (LDA) Austism The Oral Health and Disease Ontology (OHD) Oral health and disease Orphanet (ORDO) Rare diseases Parkinson Disease Ontology (PDO) Parkinson disease Pathogenic Disease Ontology (PDO) Pathogenic diseases PolyCystic Ovary Syndrome Knowledgebase (PCOSKB) Polycystic ovary syndrome Rat Disease Ontology (RDO) Broad coverage Removable Partial Denture Ontology (RPDO) Oral health Resource of Asian Primary Immunodeficiency Diseases (RPO) Immunodeficiencies Sickle Cell Disease Ontology (SCDO) Sickle Cell Disease SNOMED Clinical Terminology (SNOMED CT) Broad disease representation for human diseases. Symptom Ontology Human diseases Unified Medical Language System Broad coverage Artificial Intelligence Rheumatology Consultant System ontology (AI-RHEUM) \u00b6 Description: Contains findings, such as clinical signs, symptoms, laboratory test results, radiologic observations, tissue biopsy results, and intermediate diagnosis hypotheses, for the diagnosis of rheumatic diseases. Disease area: Rheumatic diseases Use Cases: Used by clinicians and informatics researchers. Website: https://bioportal.bioontology.org/ontologies/AI-RHEUM Open: Yes Autism DSM-ADI-R Ontology (ADAR) \u00b6 Description: An ontology of autism spectrum disorder (ASD) and related neurodevelopmental disorders. Disease area: Autism Use Cases: It extends an existing autism ontology to allow automatic inference of ASD phenotypes and Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria based on subjects\u2019 Autism Diagnostic Interview\u2013Revised (ADI-R) assessment data. Website: https://bioportal.bioontology.org/ontologies/ADAR Open: Yes Autism Spectrum Disorder Phenotype Ontology (ASDPTO) \u00b6 Description: Encapsulates the ASD behavioral phenotype, informed by the standard ASD assessment instruments and the currently known characteristics of this disorder. Disease area: Autism Use Cases: Intended for use in research settings where extensive phenotypic data have been collected, allowing a concept-based approach to identifying behavioral features of importance and for correlating these with genotypic data. Website: https://bioportal.bioontology.org/ontologies/ASDPTO Open: Yes Brucellosis Ontology (IDOBRU) \u00b6 Description: Describes the most common zoonotic disease, brucellosis, which is caused by Brucella, a type of facultative intracellular bacteria. Disease area: brucellosis bacteria Use Cases: An extension ontology of the core Infectious Disease Ontology (IDO-core). This project appears to be inactive. Website: https://github.com/biomedontology/idobru Open: Yes Cardiovascular Disease Ontology (CVDO) \u00b6 Description: An ontology to describe entities related to cardiovascular diseases. Disease area: Cardiovascular Use Cases: Describes entities related to cardiovascular diseases including the diseases themselves, the underlying disorders, and the related pathological processes. Imports upper level terms from OGMS and imports some terms from Disease Ontology (DO). GitHub repo: https://github.com/OpenLHS/CVDO/ Website: https://github.com/OpenLHS/CVDO OBO Foundry webpage: http://obofoundry.org/ontology/cvdo.html Open: Yes Chronic Kidney Disease Ontology (CKDO) \u00b6 Description: An ontology of chronic kidney disease in primary care. Disease area: Chronic kidney disease Use Cases: CKDDO was developed to assist routine data studies and case identification of CKD in primary care. Website: http://purl.bioontology.org/ontology/CKDO Open: Yes Chronic Obstructive Pulmonary Disease Ontology (COPDO) \u00b6 Description: Models concepts associated with chronic obstructive pulmonary disease in routine clinical databases. Disease area: Chronic obstructive pulmonary disease (COPD) Use Cases: Clinical use. Website: https://bioportal.bioontology.org/ontologies/COPDO Open: Yes Coronavirus Infectious Disease Ontology (CIDO) \u00b6 Description: Aims to ontologically represent and standardize various aspects of coronavirus infectious diseases, including their etiology, transmission, epidemiology, pathogenesis, diagnosis, prevention, and treatment. Disease area: Coronavirus infectious diseases, including COVID-19, SARS, MERS; covers etiology, transmission, epidemiology, pathogenesis, diagnosis, prevention, and treatment. Use Cases: Used for disease annotations related to coronavirus infections. GitHub repo: https://github.com/cido-ontology/cido OBO Foundry webpage: http://obofoundry.org/ontology/cido.html Open: Yes Diagnostic and Statistical Manual of Mental Disorders (DSM) \u00b6 Description: Authoritative source to define and classify mental disorders to improve diagnoses, treatment, and research. Disease area: Mental disorders Use Cases: Used in clinical healthcare and research by pyschiatrists and psychologists. Website: https://www.psychiatry.org/psychiatrists/practice/dsm Open: No, must be purchased Dispedia Core Ontology (DCO) \u00b6 Description: A schema for information brokering and knowledge management in the complex field of rare diseases. DCO describes patients affected by rare diseases and records expertise about diseases in machine-readable form. Disease area: Rare disease Use Cases: DCO was initially created with amyotrophic lateral sclerosis as a use case. Website: http://purl.bioontology.org/ontology/DCO Open: Yes Experimental Factor Ontology (EFO) \u00b6 Description: Provides a systematic description of many experimental variables available in EBI databases, and for projects such as the GWAS catalog. Disease area: Broad disease coverage, integrates the Mondo disease ontology. Use Cases: Application ontology build for European Bioinformatics (EBI) tools and databases and Open Targets Genetics Portal . Website: https://www.ebi.ac.uk/efo/ Open: Yes Fibrotic Interstitial Lung Disease Ontology (FILDO) \u00b6 Description: An in-progress, four-tiered ontology proposed to standardize the diagnostic classification of patients with fibrotic interstitial lung disease. Disease area: Fibrotic interstitial lung disease Use Cases: Goal is to standardize the diagnostic classification of patients with fibrotic ILD. A paper was published in 2017 and an ontology is not publicly available. Publication: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5803648/ Open: No Genetic and Rare Diseases Information Center (GARD) \u00b6 Description: NIH resource that provides the public with access to current, reliable, and easy-to-understand information about rare or genetic diseases in English or Spanish. Disease area: Rare diseases Use Cases: Patient portal. Integrates defintions and synonyms from Orphanet, maps to HPO phenotypes, and is integrated by Mondo. Website: https://rarediseases.info.nih.gov/ Open: Yes Holistic Ontology of Rare Diseases (HORD) \u00b6 Description: Describes the biopsychosocial state (i.e., disease, psychological, social, and environmental state) of persons with rare diseases in a holistic way. Disease area: Rare disease Use Cases: Rehabilita, Disruptive Technologies for the Rehabilitation of the Future , a project that aims to enhance rehabilitation transforming it to a more personalized, ubiquitous and evidence-based rehabilitation. Website: http://purl.bioontology.org/ontology/HORD Open: Yes Human Dermatological Disease Ontology (DERMO) \u00b6 Description: The most comprehensive dermatological disease ontology available, with over 3,500 classes available. There are 20 upper-level disease entities, with features such as anatomical location, heritability, and affected cell or tissue type. Disease area: Dermatology (skin) Use Cases: DermO can be used to extract data from patient electronic health records using text mining, or to translate existing variable-granularity coding such as ICD-10 to allow capture and standardization of patient/disease annotations. Website: https://bioportal.bioontology.org/ontologies/DERMO Open: Yes Human Disease Ontology (DO) \u00b6 Description: An ontology for describing the classification of human diseases organized by etiology. Disease area: Human disease terms, phenotype characteristics and related medical vocabulary disease concepts. Use Cases: Used by Model Organism Databases (MOD), such as Mouse Genome Informatics disease model for diseae annotations, and Alliance for Genome Resources for disease annotations. In 2018, DO tracked over 300 DO project citations suggesting wide adoption and usage for disease annotations. GitHub repo: https://github.com/DiseaseOntology/HumanDiseaseOntology/ Website: http://www.disease-ontology.org/ OBO Foundry webpage: http://obofoundry.org/ontology/doid.html Open: Yes Infectious Disease Ontology (IDO) \u00b6 Description: A set of interoperable ontologies that will together provide coverage of the infectious disease domain. IDO core is the upper-level ontology that hosts terms of general relevance across the domain, while extension ontologies host terms to specific to a particular part of the domain. Disease area: Infectious disease features, such as acute, primary, secondary infection, and chronic, hospital acquired and local infection. Use Cases: Does not seem active, has not been released since 2017. GitHub repo: https://github.com/infectious-disease-ontology/infectious-disease-ontology/ Website: http://www.bioontology.org/wiki/index.php/Infectious_Disease_Ontology OBO Foundry webpage: http://obofoundry.org/ontology/ido.html Open: Yes International Classification of Functioning, Disability and Health (ICF) \u00b6 Description: Represents diseases and provides a conceptual basis for the definition and measurement of health and disability as organized by patient-oriented outcomes of function and disability. ICF considers environmental factors as well as the relevance of associated health conditions in recognizing major models of disability. Disease area: Cross-discipline, focuses on health and disability Use Cases: ICF is the World Health Organization (WHO) framework for measuring health and disability at both individual and population levels. ICF was officially endorsed by the WHO as the international standard to describe and measure health and disability. Website: https://www.who.int/standards/classifications/international-classification-of-functioning-disability-and-health Open: Yes International Statistical Classification of Diseases and Related Health Problems (ICD-11) \u00b6 Description: A medical classification list by the World Health Organization (WHO) that contains codes for diseases, signs and symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or diseases. Disease area: Broad coverage of human disease features, such as disease of anatomical systems, infectious diseases, injuries, external causes of morbidity and mortality. Use Cases: The main purpose of ICD-11 is for clinical care, billing and coding for insurance companies. Website: https://www.who.int/standards/classifications/classification-of-diseases Open: Yes International Classification of Diseases for Oncology (ICD-O) \u00b6 Description: A domain-specific extension of the International Statistical Classification of Diseases and Related Health Problems for tumor diseases. Disease area: A multi-axial classification of the site, morphology, behaviour, and grading of neoplasms. Use Cases: Used principally in tumour or cancer registries for coding the site (topography) and the histology (morphology) of neoplasms, usually obtained from a pathology report. Website: https://www.who.int/standards/classifications/other-classifications/international-classification-of-diseases-for-oncology Open: Yes Logical Observation Identifier Names and Codes (LOINC) \u00b6 Description: Identifies medical laboratory observations. Disease area: Broad coverage Use Cases: The Regenstrief Institute first developed LOINC in 1994 in response to the demand for an electronic database for clinical care and management. LOINC is publicly available at no cost and is endorsed by the American Clinical Laboratory Association and the College of American Pathologists. Since its inception, LOINC has expanded to include not just medical laboratory code names but also nursing diagnoses, nursing interventions, outcome classifications, and patient care data sets. Website: https://loinc.org/ Open: Yes, registration is required. Medical Subject Headings (MeSH) \u00b6 Description: Medical Subject Headings (MeSH) thesaurus is a controlled and hierarchically-organized vocabulary produced by the National Library of Medicine. Disease area: Broad coverage Use Cases: It is used for indexing, cataloging, and searching of biomedical and health-related information. Integrated into Mondo. Website: https://meshb.nlm.nih.gov/search Open: Yes MedGen \u00b6 Description: Organizes information related to human medical genetics, such as attributes of conditions and phenotypes of genetic contributions. Disease area: Human medical genetics Use Cases: MedGen is NCBI's portal to information about conditions and phenotypes related to Medical Genetics. Terms from the NIH Genetic Testing Registry (GTR), UMLS, HPO, Orphanet, ClinVar and other sources are aggregated into concepts, each of which is assigned a unique identifier and a preferred name and symbol. The core content of the record may include names, identifiers used by other databases, mode of inheritance, clinical features, and map location of the loci affecting the disorder. The concept identifier (CUI) is used to aggregate information about that concept, similar to the way NCBI Gene serves as a gateway to gene-related information. Website: https://www.ncbi.nlm.nih.gov/medgen/ Open: Yes Medical Dictionary for Regulatory Activities (MedDRA) \u00b6 Description: Provides a standardized international medical terminology to be used for regulatory communication and evaluation of data about medicinal products for human use. Disease area: Broad coverage Use Cases: Mainly targeted towards industry and regulatory users. Website: https://www.meddra.org/ Open: Yes Mental Disease Ontology (MDO) \u00b6 Description: An ontology to describe and classify mental diseases such as schizophrenia, annotated with DSM-IV and ICD codes where applicable. Disease area: Mental functioning, including mental processes such as cognition and traits such as intelligence. Use Cases: The ontology has been partially aligned with the related projects Cognitive Atlas , knowledge base on cognitive science and the Cognitive Paradigm Ontology , which is used in the Brainmap , a database of neuroimaging experiments. GitHub repo: https://github.com/jannahastings/mental-functioning-ontology OBO Foundry webpage: http://obofoundry.org/ontology/mfomd.html Open: yes Mondo Disease Ontology (Mondo) \u00b6 Description: An integrated disease ontology that provides precise mappings between source ontologies that comprehensively covers cross-species diseases, from common to rare diseases. Disease area: Cross species, intended to cover all areas of diseases, integrating source ontologies that cover Mendelian diseases (OMIM), rare diseases (Orphanet), neoplasms (NCIt), human diseases (DO), and others. See all sources here . Use Cases: Mondo was developed for usage in the Monarch Initiative , a discovery system that allows navigation of similarities between phenotypes, organisms, and human diseases across many data sources and organisms. Mondo is also used by ClinGen for disease curations, the Kids First Data Resource Portal for disease annotations and others, see an extensive list here . GitHub repo: https://github.com/monarch-initiative/mondo Website: https://mondo.monarchinitiative.org/ OBO Foundry webpage: http://obofoundry.org/ontology/mondo.html Open: yes National Cancer Institute Thesaurus (NCIT) \u00b6 Description: NCI Thesaurus (NCIt)is a reference terminology that includes broad coverage of the cancer domain, including cancer related diseases, findings and abnormalities. The NCIt OBO Edition aims to increase integration of the NCIt with OBO Library ontologies. NCIt OBO Edition releases should be considered experimental. Disease area: Cancer and neoplasms Use Cases: NCI Thesaurus (NCIt) provides reference terminology for many National Cancer Institute and other systems. It is used by the Clinical Data Interchange Standards Consortium Terminology (CDISC), the U.S. Food and Drug Administration (FDA), the Federal Medication Terminologies (FMT), and the National Council for Prescription Drug Programs (NCPDP). It provides extensive coverage of neoplasms and cancers. GitHub repo: https://github.com/NCI-Thesaurus/thesaurus-obo-edition/issues Website: https://ncithesaurus.nci.nih.gov/ncitbrowser/pages/home.jsf?version=20.11e OBO Foundry webpage: http://obofoundry.org/ontology/ncit.html Open: Yes Neurological Disease Ontology (ND) \u00b6 Description: A framework for the representation of key aspects of neurological disease. Disease area: Neurology Use Cases: Goal is to provide a framework to enable representation of aspects of neurological diseases that are relevant to their treatment and study. This project may be inactive, the last commit to GitHub was in 2016. GitHub repo: https://github.com/addiehl/neurological-disease-ontology Open: Yes Online Mendelian Inheritance in Man (OMIM) \u00b6 Description: a comprehensive, authoritative compendium of human genes and genetic phenotypes that is freely available and updated daily. Disease area: Mendelian, genetic diseases. Use Cases: Integrated into the disease ontology, used by the Human Phenotype Ontology for disease annotations, patients and researchers. Website: https://omim.org/ Open: yes Ontology of Cardiovascular Drug Adverse Events (OCVDAE) \u00b6 Description: A biomedical ontology of cardiovascular drug\u2013associated adverse events. Disease area: Cardiovascular Use Cases: One novel study of the OCVDAE project is the development of the PCR method. Specifically, an AE-specific drug class effect is defined to exist when all the drugs (drug chemical ingredients or drug products) in a drug class are associated with an AE, which is formulated as a proportional class level ratio (\u201cPCR\u201d)\u2009=\u20091. See more information in the paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5653862/. This project may be inactive, the last GitHub commit was in 2019. GitHub repo: https://github.com/OCVDAE/OCVDAE Website: https://bioportal.bioontology.org/ontologies/OCVDAE Open: yes Ontology for General Medical Science (OGMS) \u00b6 Description: An ontology of entities involved in a clinical encounter. Use Cases: Provides a formal theory of disease that can be further elaborated by specific disease ontologies. It is intended to be used as a upper level ontology for other disease ontologies. Used by Cardiovascular Disease Ontology . GitHub repo: https://github.com/OGMS/ogms OBO Foundry webpage: http://obofoundry.org/ontology/ogms.html Open: Yes Ontology for Genetic Susceptibility Factor (OGSF) \u00b6 Description: An application ontology to represent genetic susceptibility to a specific disease, adverse event, or a pathological process. Use Cases: Modeling genetic susceptibility to vaccine adverse events. GitHub repo: https://github.com/linikujp/OGSF OBO Foundry webpage: http://obofoundry.org/ontology/ogsf.html Open: Yes Ontology of Glucose Metabolism Disorder (OGMD) \u00b6 Description: Represents glucose metabolism disorder and diabetes disease names, phenotypes, and their classifications. Disease area: Metabolic disorders Use Cases: Still under development (last verssion released in BioPortal was in 2021) but there is little information about its usage online. Website: https://bioportal.bioontology.org/ontologies/OGMD Open: Yes Ontology of Language Disorder in Autism (LDA) \u00b6 Description: An ontology assembled from a set of language terms mined from the autism literature. Disease area: Austism Use Cases: This has not been released since 2008 and looks like it is inactive. Website: https://bioportal.bioontology.org/ontologies/LDA Open: Yes The Oral Health and Disease Ontology (OHD) \u00b6 Description: Represents the content of dental practice health records and is intended to be further developed for use in translational medicine. OHD is structured using BFO (Basic Formal Ontology) and uses terms from many ontologies, NCBITaxon, and a subset of terms from the CDT (Current Dental Terminology). Disease area: Oral health and disease Use Cases: Used to represent the content of dental practice health records and is intended to be further developed for use in translation medicine. Appears to be inactive. OBO Foundry webpage: http://www.obofoundry.org/ontology/ohd.html Open: Yes Orphanet (ORDO) \u00b6 Description: The portal for rare diseases and orphan drugs. Contains a structured vocabulary for rare diseases capturing relationships between diseases, genes, and other relevant features, jointly developed by Orphanet and the EBI. It contains information on nearly 10,000 cancers and related diseases, 8,000 single agents and combination therapies, and a wide range of other topics related to cancer and biomedical research. Disease area: Rare diseases Use Cases: Used by rare disease research and clinical community. Integrated into the Mondo disease ontology, aligned with OMIM. Website: https://www.orpha.net/consor/cgi-bin/index.php Open: Yes Parkinson Disease ontology (PDO) \u00b6 Description: A comprehensive semantic framework with a subclass-based taxonomic hierarchy, covering the whole breadth of the Parkinson disease knowledge domain from major biomedical concepts to different views on disease features held by molecular biologists, clinicians, and drug developers. Disease area: Parkinson disease Use Cases: This resource has been created for use in the IMI-funded AETIONOMY project . Last release was in 2015, may be inactive. Website: https://bioportal.bioontology.org/ontologies/PDON Open: Yes Pathogenic Disease Ontology (PDO) \u00b6 Description: Provides information on infectious diseases, disease synonyms, transmission pathways, disease agents, affected populations, and disease properties. Diseases are grouped into syndromic disease categories, organisms are structured hierarchically, and both disease transmission and relevant disease properties are searchable. Disease area: human infectious diseases caused by microbes and the diseases that is related to microbial infection. Use Cases: Has not been released since 2016 and may be inactive. Website: https://bioportal.bioontology.org/ontologies/PDO Open: Yes. PolyCystic Ovary Syndrome Knowledgebase (PCOSKB) \u00b6 Description: Comprises genes, single nucleotide polymorphisms, diseases, gene ontology terms, and biochemical pathways associated with polycystic ovary syndrome, a major cause of female subfertility worldwide. Disease area: polycystic ovary syndrome Use Cases: Ontology underlying the Polycystic Ovary Syndrome Knowledgebase , a manually curated knowledgebase on PCOS. Website: http://pcoskb.bicnirrh.res.in/go_d.php Open: Yes Rat Disease Ontology (RDO) \u00b6 Description: Provides the foundation for ten comprehensive disease area\u2013related data sets at the Rat Genome Database Disease Portals. Disease area: Broad coverage including animal diseases, infectious diseases, chemically-induced disorders, occupational diseases, wounds and injuries and more. Use Cases: Developed for use with the Rat Genome Database Disease Portals. Website: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=DOID:4 Open: Yes Removable Partial Denture Ontology (RPDO) \u00b6 Description: Represents knowledge of a patient\u2019s oral conditions and denture component parts, originally developed to create a clinician decision support model. Disease area: Oral health and dentures Use Cases: A paper was published on this in 2016 but it does not appear any other information is available about this ontology on the website, presumably it is an inactive project. Publication: https://www.nature.com/articles/srep27855 Open: No Resource of Asian Primary Immunodeficiency Diseases (RPO) \u00b6 Description: Represents observed phenotypic terms, sequence variations, and messenger RNA and protein expression levels of all genes involved in primary immunodeficiency diseases. Disease area: Primary immunodeficiency diseases Use Cases: This terminology is used in a freely accessible, dynamic and integrated database for primary immunodeficiency diseases (PID) called Resource of Asian Primary Immunodeficiency Diseases (RAPID), which is available here . Open: Yes Sickle Cell Disease Ontology (SCDO) \u00b6 Description: SCDO establishes (a) community-standardized sickle cell disease terms and descriptions, (b) canonical and hierarchical representation of knowledge on sickle cell disease, and (c) links to other ontologies and bodies of work. Disease area: Sickle Cell Disease (SCD). Use Cases: SCDO is intended to be a comprehensive collection of knowledge on SCD, facilitate exploration of new scientific questions and ideas, facilitate seamless data sharing and collaborations including meta-analysis within the SCD community, support the building of databasing and clinical informatics in SCD. GitHub repo: https://github.com/scdodev/scdo-ontology/issues Website: https://scdontology.h3abionet.org/ OBO Foundry webpage: http://obofoundry.org/ontology/scdo.html Open: Yes SNOMED Clinical Terminology (SNOMED CT) \u00b6 Description: A comprehensive clinical terminology/ontology used in healthcare settings. Disease area: Broad disease representation for human diseases. Use Cases: Main coding system used in Electronic Health Records (EHRs). Website: https://browser.ihtsdotools.org/? Open: No, requires a license for usage. Symptom Ontology \u00b6 Description: An ontology of disease symptoms, with symptoms encompasing perceived changes in function, sensations or appearance reported by a patient indicative of a disease. Disease area: Human diseases Use Cases: Developed by the Disease Ontology (DO) team and used for describing symptoms of human diseases in the DO. Website: http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page OBO Foundry webpage: http://obofoundry.org/ontology/symp.html Open: Yes Unified Medical Language System \u00b6 Description: The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services. Disease area: Broad coverage Use Cases: Healthcare settings including electronic health records and HL7. Website: https://www.nlm.nih.gov/research/umls/index.html Open: Yes Phenotype ontologies \u00b6 Phenotype Summary Table \u00b6 Name Species Area Ascomycete phenotype ontology (APO) Ascomycota C. elegans phenotype (wbphenotype) C elegans Dictyostelium discoideum phenotype ontology (ddpheno) Dictyostelium discoideum Drosophila Phenotype Ontology (DPO) Drosophila Flora Phenotype Ontology (FLOPO) Viridiplantae Fission Yeast Phenotype Ontology (FYPO) S. pombe Human Phenotype Ontology (HPO) Human HPO - ORDO Ontological Module (HOOM) Human Mammalian Phenotype Ontology (MP) Mammals Ontology of Microbial Phenotypes (OMP) Microbe Ontology of Prokaryotic Phenotypic and Metabolic Characters Prokaryotes Pathogen Host Interaction Phenotype Ontology pathogens Planarian Phenotype Ontology (PLANP) Schmidtea mediterranea Plant Trait Ontology (TO) Viridiplantae Plant Phenology Ontology Plants Unified Phenotype Ontology (uPheno) Cross-species coverage Xenopus Phenotype Ontology (XPO) Xenopus Zebrafish Phenotype Ontology (ZP) Zebrafish Ascomycete phenotype ontology (APO) \u00b6 Description: A structured controlled vocabulary for the phenotypes of Ascomycete fungi. Species: Ascomycota GitHub repo: https://github.com/obophenotype/ascomycete-phenotype-ontology/ Webpage: http://www.yeastgenome.org/ OBO Foundry webpage: http://obofoundry.org/ontology/wbphenotype.html Open: Yes C. elegans phenotype (wbphenotype) \u00b6 Description: A structured controlled vocabulary of Caenorhabditis elegans phenotypes. Species: C elegans GitHub repo: https://github.com/obophenotype/c-elegans-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/wbphenotype.html Open: Yes Dictyostelium discoideum phenotype ontology (ddpheno) \u00b6 Description: A structured controlled vocabulary of phenotypes of the slime-mould Dictyostelium discoideum. Species: Dictyostelium discoideum GitHub repo: https://github.com/obophenotype/dicty-phenotype-ontology/issues Webpage: http://dictybase.org/ OBO Foundry webpage: http://obofoundry.org/ontology/ddpheno.html Open: Yes Drosophila Phenotype Ontology (DPO) \u00b6 Description: An ontology of commonly encountered and/or high level Drosophila phenotypes. Species: Drosophila GitHub repo: https://github.com/obophenotype/c-elegans-phenotype-ontology Webpage: http://purl.obolibrary.org/obo/fbcv OBO Foundry webpage: http://obofoundry.org/ontology/dpo.html Open: Yes Flora Phenotype Ontology (FLOPO) \u00b6 Description: Traits and phenotypes of flowering plants occurring in digitized Floras. Species: Viridiplantae GitHub repo: https://github.com/flora-phenotype-ontology/flopoontology/ OBO Foundry webpage: http://obofoundry.org/ontology/flopo.html Open: Yes Fission Yeast Phenotype Ontology (FYPO) \u00b6 Description: FYPO is a formal ontology of phenotypes observed in fission yeast. Species: S. pombe GitHub repo: https://github.com/pombase/fypo OBO Foundry webpage: http://obofoundry.org/ontology/fypo.html Open: Yes Human Phenotype Ontology (HPO) \u00b6 Description: HPO provides a standardized vocabulary of phenotypic abnormalities encountered in human disease. Each term in the HPO describes a phenotypic abnormality. Species: Human GitHub repo: https://github.com/obophenotype/human-phenotype-ontology Website: https://hpo.jax.org/app/ OBO Foundry webpage: http://obofoundry.org/ontology/hp.html Open: yes HPO - ORDO Ontological Module (HOOM) \u00b6 Description: Orphanet provides phenotypic annotations of the rare diseases in the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion. In ORDO a clinical entity is either a group of rare disorders, a rare disorder or a subtype of disorder. The phenomes branch of ORDO has been refactored as a logical import of HPO, and the HPO-ORDO phenotype disease-annotations have been provided in a series of triples in OBAN format in which associations, frequency and provenance are modeled. HOOM is provided as an OWL (Ontologies Web Languages) file, using OBAN, the Orphanet Rare Disease Ontology (ORDO), and HPO ontological models. HOOM provides extra possibilities for researchers, pharmaceutical companies and others wishing to co-analyse rare and common disease phenotype associations, or re-use the integrated ontologies in genomic variants repositories or match-making tools. Species: Human Website: http://www.orphadata.org/cgi-bin/img/PDF/WhatIsHOOM.pdf BioPortal: https://bioportal.bioontology.org/ontologies/HOOM Open: yes Mammalian Phenotype Ontology (MP) \u00b6 Description: Standard terms for annotating mammalian phenotypic data. Species: Mammals (main focus is on mouse and rodents) GitHub repo: https://github.com/obophenotype/mammalian-phenotype-ontology Website: http://www.informatics.jax.org/searches/MP_form.shtml OBO Foundry webpage: http://obofoundry.org/ontology/mp.html Open: Yes Ontology of Microbial Phenotypes (OMP) \u00b6 Description: An ontology of phenotypes covering microbes. Species: microbes GitHub repo: https://github.com/microbialphenotypes/OMP-ontology Website: http://microbialphenotypes.org OBO Foundry webpage: http://obofoundry.org/ontology/omp.html Open: Yes Ontology of Prokaryotic Phenotypic and Metabolic Characters \u00b6 Description: An ontology of phenotypes covering microbes. Species: Prokaryotes GitHub repo: https://github.com/microbialphenotypes/OMP-ontology/issues Website: http://microbialphenotypes.org/ OBO Foundry webpage: http://obofoundry.org/ontology/omp.html Open: Yes Pathogen Host Interaction Phenotype Ontology \u00b6 Description: PHIPO is a formal ontology of species-neutral phenotypes observed in pathogen-host interactions. Species: pathogens GitHub repo: https://github.com/PHI-base/phipo Website: http://www.phi-base.org OBO Foundry webpage: http://obofoundry.org/ontology/phipo.html Open: Yes Planarian Phenotype Ontology (PLANP) \u00b6 Description: Planarian Phenotype Ontology is an ontology of phenotypes observed in the planarian Schmidtea mediterranea. Species: Schmidtea mediterranea GitHub repo: https://github.com/obophenotype/planarian-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/planp.html Open: Yes Plant Trait Ontology (TO) \u00b6 Description: A controlled vocabulary of describe phenotypic traits in plants. Species: Viridiplantae GitHub repo: https://github.com/Planteome/plant-trait-ontology/ OBO Foundry webpage: http://obofoundry.org/ontology/to.html Open: Yes Plant Phenology Ontology \u00b6 Description: An ontology for describing the phenology of individual plants and populations of plants, and for integrating plant phenological data across sources and scales. Species: Plants GitHub repo: https://github.com/PlantPhenoOntology/PPO OBO Foundry webpage: http://obofoundry.org/ontology/ppo.html Open: Yes Unified Phenotype Ontology (uPheno) \u00b6 Description: The uPheno ontology integrates multiple phenotype ontologies into a unified cross-species phenotype ontology. Species: Cross-species coverage GitHub repo: https://github.com/obophenotype/upheno OBO Foundry webpage: http://obofoundry.org/ontology/upheno.html Open: Yes Xenopus Phenotype Ontology (XPO) \u00b6 Description: XPO represents anatomical, cellular, and gene function phenotypes occurring throughout the development of the African frogs Xenopus laevis and tropicalis. Species: Xenopus GitHub repo: https://github.com/obophenotype/xenopus-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/xpo.html Open: Yes Zebrafish Phenotype Ontology (ZP) \u00b6 Description: The Zebrafish Phenotype Ontology formally defines all phenotypes of the Zebrafish model organism. Species: Zebrafish GitHub repo: https://github.com/obophenotype/zebrafish-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/zp.html Open: Yes References \u00b6 A Census of Disease Ontologies Melissa A. Haendel, Julie A. McMurry, Rose Relevo, Christopher J. Mungall, Peter N. Robinson, Christopher G. Chute Annual Review of Biomedical Data Science 2018 1:1, 305-331 OMOP2OBO repository","title":"Medical Ontology landscape"},{"location":"reference/medical-ontology-landscape/#the-landscape-of-disease-and-phenotype-ontologies","text":"Compiled by Nicole Vasilevsky. Feel free to make pull requests to suggest edits. Note: This currently just provides an overview of disease and phenotype ontologies. Contributors are welcome to add more descriptions of other medical ontologies. This was last updated in 2021.","title":"The Landscape of Disease and Phenotype Ontologies"},{"location":"reference/medical-ontology-landscape/#disease-ontologies-terminologies","text":"","title":"Disease Ontologies &amp; Terminologies"},{"location":"reference/medical-ontology-landscape/#disease-summary-table","text":"Name Disease Area Artificial Intelligence Rheumatology Consultant System Ontology (AI-RHEUM) Rheumatic diseases Autism DSM-ADI-R Ontology (ADAR) Autism Autism Spectrum Disorder Phenotype Ontology (ASDPTO) Autism Brucellosis Ontology (IDOBRU) brucellosis Cardiovascular Disease Ontology (CVDO) Cardiovascular Chronic Kidney Disease Ontology (CKDO) Chronic kidney disease Chronic Obstructive Pulmonary Disease Ontology (COPDO) Chronic obstructive pulmonary disease (COPD) Coronavirus Infectious Disease Ontology (CIDO) Coronavirus infectious diseases Diagnostic and Statistical Manual of Mental Disorders (DSM) Mental disorders Dispedia Core Ontology (DCO) Rare diseases Experimental Factor Ontology (EFO) Broad disease coverage Fibrotic Interstitial Lung Disease Ontology (FILDO) Fibrotic interstitial lung disease Genetic and Rare Diseases Information Center (GARD) Rare diseases Holistic Ontology of Rare Diseases (HORD) Rare disease Human Dermatological Disease Ontology (DERMO) Dermatology (skin) Human Disease Ontology (DO) Human disease Infectious Disease Ontology (IDO) Infectious disease International Classification of Functioning, Disability and Health (ICF) Cross-discipline, focuses disabilities International Statistical Classification of Diseases and Related Health Problems (ICD-11) Broad coverage International Classification of Diseases for Oncology (ICD-O) Cancer Logical Observation Identifier Names and Codes (LOINC) Broad coverage Medical Subject Headings (MeSH) Broad coverage MedGen Human medical genetics Medical Dictionary for Regulatory Activities (MedDRA) Broad coverage Mental Disease Ontology (MDO) Mental functioning Mondo Disease Ontology (Mondo) Broad coverage, Cross species National Cancer Institute Thesaurus (NCIT) Humam cancer and neoplasms Neurological Disease Ontology (ND) Neurology Online Mendelian Inheritance in Man (OMIM) Mendelian, genetic diseases. Ontology of Cardiovascular Drug Adverse Events (OCVDAE) Cardiovascular Ontology for General Medical Science (OGMS) Broad coverage Ontology for Genetic Susceptibility Factor (OGSF) Genetic disease Ontology of Glucose Metabolism Disorder (OGMD) Metabolic disorders Ontology of Language Disorder in Autism (LDA) Austism The Oral Health and Disease Ontology (OHD) Oral health and disease Orphanet (ORDO) Rare diseases Parkinson Disease Ontology (PDO) Parkinson disease Pathogenic Disease Ontology (PDO) Pathogenic diseases PolyCystic Ovary Syndrome Knowledgebase (PCOSKB) Polycystic ovary syndrome Rat Disease Ontology (RDO) Broad coverage Removable Partial Denture Ontology (RPDO) Oral health Resource of Asian Primary Immunodeficiency Diseases (RPO) Immunodeficiencies Sickle Cell Disease Ontology (SCDO) Sickle Cell Disease SNOMED Clinical Terminology (SNOMED CT) Broad disease representation for human diseases. Symptom Ontology Human diseases Unified Medical Language System Broad coverage","title":"Disease Summary Table"},{"location":"reference/medical-ontology-landscape/#artificial-intelligence-rheumatology-consultant-system-ontology-ai-rheum","text":"Description: Contains findings, such as clinical signs, symptoms, laboratory test results, radiologic observations, tissue biopsy results, and intermediate diagnosis hypotheses, for the diagnosis of rheumatic diseases. Disease area: Rheumatic diseases Use Cases: Used by clinicians and informatics researchers. Website: https://bioportal.bioontology.org/ontologies/AI-RHEUM Open: Yes","title":"Artificial Intelligence Rheumatology Consultant System ontology (AI-RHEUM)"},{"location":"reference/medical-ontology-landscape/#autism-dsm-adi-r-ontology-adar","text":"Description: An ontology of autism spectrum disorder (ASD) and related neurodevelopmental disorders. Disease area: Autism Use Cases: It extends an existing autism ontology to allow automatic inference of ASD phenotypes and Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria based on subjects\u2019 Autism Diagnostic Interview\u2013Revised (ADI-R) assessment data. Website: https://bioportal.bioontology.org/ontologies/ADAR Open: Yes","title":"Autism DSM-ADI-R Ontology (ADAR)"},{"location":"reference/medical-ontology-landscape/#autism-spectrum-disorder-phenotype-ontology-asdpto","text":"Description: Encapsulates the ASD behavioral phenotype, informed by the standard ASD assessment instruments and the currently known characteristics of this disorder. Disease area: Autism Use Cases: Intended for use in research settings where extensive phenotypic data have been collected, allowing a concept-based approach to identifying behavioral features of importance and for correlating these with genotypic data. Website: https://bioportal.bioontology.org/ontologies/ASDPTO Open: Yes","title":"Autism Spectrum Disorder Phenotype Ontology (ASDPTO)"},{"location":"reference/medical-ontology-landscape/#brucellosis-ontology-idobru","text":"Description: Describes the most common zoonotic disease, brucellosis, which is caused by Brucella, a type of facultative intracellular bacteria. Disease area: brucellosis bacteria Use Cases: An extension ontology of the core Infectious Disease Ontology (IDO-core). This project appears to be inactive. Website: https://github.com/biomedontology/idobru Open: Yes","title":"Brucellosis Ontology (IDOBRU)"},{"location":"reference/medical-ontology-landscape/#cardiovascular-disease-ontology-cvdo","text":"Description: An ontology to describe entities related to cardiovascular diseases. Disease area: Cardiovascular Use Cases: Describes entities related to cardiovascular diseases including the diseases themselves, the underlying disorders, and the related pathological processes. Imports upper level terms from OGMS and imports some terms from Disease Ontology (DO). GitHub repo: https://github.com/OpenLHS/CVDO/ Website: https://github.com/OpenLHS/CVDO OBO Foundry webpage: http://obofoundry.org/ontology/cvdo.html Open: Yes","title":"Cardiovascular Disease Ontology (CVDO)"},{"location":"reference/medical-ontology-landscape/#chronic-kidney-disease-ontology-ckdo","text":"Description: An ontology of chronic kidney disease in primary care. Disease area: Chronic kidney disease Use Cases: CKDDO was developed to assist routine data studies and case identification of CKD in primary care. Website: http://purl.bioontology.org/ontology/CKDO Open: Yes","title":"Chronic Kidney Disease Ontology (CKDO)"},{"location":"reference/medical-ontology-landscape/#chronic-obstructive-pulmonary-disease-ontology-copdo","text":"Description: Models concepts associated with chronic obstructive pulmonary disease in routine clinical databases. Disease area: Chronic obstructive pulmonary disease (COPD) Use Cases: Clinical use. Website: https://bioportal.bioontology.org/ontologies/COPDO Open: Yes","title":"Chronic Obstructive Pulmonary Disease Ontology (COPDO)"},{"location":"reference/medical-ontology-landscape/#coronavirus-infectious-disease-ontology-cido","text":"Description: Aims to ontologically represent and standardize various aspects of coronavirus infectious diseases, including their etiology, transmission, epidemiology, pathogenesis, diagnosis, prevention, and treatment. Disease area: Coronavirus infectious diseases, including COVID-19, SARS, MERS; covers etiology, transmission, epidemiology, pathogenesis, diagnosis, prevention, and treatment. Use Cases: Used for disease annotations related to coronavirus infections. GitHub repo: https://github.com/cido-ontology/cido OBO Foundry webpage: http://obofoundry.org/ontology/cido.html Open: Yes","title":"Coronavirus Infectious Disease Ontology (CIDO)"},{"location":"reference/medical-ontology-landscape/#diagnostic-and-statistical-manual-of-mental-disorders-dsm","text":"Description: Authoritative source to define and classify mental disorders to improve diagnoses, treatment, and research. Disease area: Mental disorders Use Cases: Used in clinical healthcare and research by pyschiatrists and psychologists. Website: https://www.psychiatry.org/psychiatrists/practice/dsm Open: No, must be purchased","title":"Diagnostic and Statistical Manual of Mental Disorders (DSM)"},{"location":"reference/medical-ontology-landscape/#dispedia-core-ontology-dco","text":"Description: A schema for information brokering and knowledge management in the complex field of rare diseases. DCO describes patients affected by rare diseases and records expertise about diseases in machine-readable form. Disease area: Rare disease Use Cases: DCO was initially created with amyotrophic lateral sclerosis as a use case. Website: http://purl.bioontology.org/ontology/DCO Open: Yes","title":"Dispedia Core Ontology (DCO)"},{"location":"reference/medical-ontology-landscape/#experimental-factor-ontology-efo","text":"Description: Provides a systematic description of many experimental variables available in EBI databases, and for projects such as the GWAS catalog. Disease area: Broad disease coverage, integrates the Mondo disease ontology. Use Cases: Application ontology build for European Bioinformatics (EBI) tools and databases and Open Targets Genetics Portal . Website: https://www.ebi.ac.uk/efo/ Open: Yes","title":"Experimental Factor Ontology (EFO)"},{"location":"reference/medical-ontology-landscape/#fibrotic-interstitial-lung-disease-ontology-fildo","text":"Description: An in-progress, four-tiered ontology proposed to standardize the diagnostic classification of patients with fibrotic interstitial lung disease. Disease area: Fibrotic interstitial lung disease Use Cases: Goal is to standardize the diagnostic classification of patients with fibrotic ILD. A paper was published in 2017 and an ontology is not publicly available. Publication: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5803648/ Open: No","title":"Fibrotic Interstitial Lung Disease Ontology (FILDO)"},{"location":"reference/medical-ontology-landscape/#genetic-and-rare-diseases-information-center-gard","text":"Description: NIH resource that provides the public with access to current, reliable, and easy-to-understand information about rare or genetic diseases in English or Spanish. Disease area: Rare diseases Use Cases: Patient portal. Integrates defintions and synonyms from Orphanet, maps to HPO phenotypes, and is integrated by Mondo. Website: https://rarediseases.info.nih.gov/ Open: Yes","title":"Genetic and Rare Diseases Information Center (GARD)"},{"location":"reference/medical-ontology-landscape/#holistic-ontology-of-rare-diseases-hord","text":"Description: Describes the biopsychosocial state (i.e., disease, psychological, social, and environmental state) of persons with rare diseases in a holistic way. Disease area: Rare disease Use Cases: Rehabilita, Disruptive Technologies for the Rehabilitation of the Future , a project that aims to enhance rehabilitation transforming it to a more personalized, ubiquitous and evidence-based rehabilitation. Website: http://purl.bioontology.org/ontology/HORD Open: Yes","title":"Holistic Ontology of Rare Diseases (HORD)"},{"location":"reference/medical-ontology-landscape/#human-dermatological-disease-ontology-dermo","text":"Description: The most comprehensive dermatological disease ontology available, with over 3,500 classes available. There are 20 upper-level disease entities, with features such as anatomical location, heritability, and affected cell or tissue type. Disease area: Dermatology (skin) Use Cases: DermO can be used to extract data from patient electronic health records using text mining, or to translate existing variable-granularity coding such as ICD-10 to allow capture and standardization of patient/disease annotations. Website: https://bioportal.bioontology.org/ontologies/DERMO Open: Yes","title":"Human Dermatological Disease Ontology (DERMO)"},{"location":"reference/medical-ontology-landscape/#human-disease-ontology-do","text":"Description: An ontology for describing the classification of human diseases organized by etiology. Disease area: Human disease terms, phenotype characteristics and related medical vocabulary disease concepts. Use Cases: Used by Model Organism Databases (MOD), such as Mouse Genome Informatics disease model for diseae annotations, and Alliance for Genome Resources for disease annotations. In 2018, DO tracked over 300 DO project citations suggesting wide adoption and usage for disease annotations. GitHub repo: https://github.com/DiseaseOntology/HumanDiseaseOntology/ Website: http://www.disease-ontology.org/ OBO Foundry webpage: http://obofoundry.org/ontology/doid.html Open: Yes","title":"Human Disease Ontology (DO)"},{"location":"reference/medical-ontology-landscape/#infectious-disease-ontology-ido","text":"Description: A set of interoperable ontologies that will together provide coverage of the infectious disease domain. IDO core is the upper-level ontology that hosts terms of general relevance across the domain, while extension ontologies host terms to specific to a particular part of the domain. Disease area: Infectious disease features, such as acute, primary, secondary infection, and chronic, hospital acquired and local infection. Use Cases: Does not seem active, has not been released since 2017. GitHub repo: https://github.com/infectious-disease-ontology/infectious-disease-ontology/ Website: http://www.bioontology.org/wiki/index.php/Infectious_Disease_Ontology OBO Foundry webpage: http://obofoundry.org/ontology/ido.html Open: Yes","title":"Infectious Disease Ontology (IDO)"},{"location":"reference/medical-ontology-landscape/#international-classification-of-functioning-disability-and-health-icf","text":"Description: Represents diseases and provides a conceptual basis for the definition and measurement of health and disability as organized by patient-oriented outcomes of function and disability. ICF considers environmental factors as well as the relevance of associated health conditions in recognizing major models of disability. Disease area: Cross-discipline, focuses on health and disability Use Cases: ICF is the World Health Organization (WHO) framework for measuring health and disability at both individual and population levels. ICF was officially endorsed by the WHO as the international standard to describe and measure health and disability. Website: https://www.who.int/standards/classifications/international-classification-of-functioning-disability-and-health Open: Yes","title":"International Classification of Functioning, Disability and Health (ICF)"},{"location":"reference/medical-ontology-landscape/#international-statistical-classification-of-diseases-and-related-health-problems-icd-11","text":"Description: A medical classification list by the World Health Organization (WHO) that contains codes for diseases, signs and symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or diseases. Disease area: Broad coverage of human disease features, such as disease of anatomical systems, infectious diseases, injuries, external causes of morbidity and mortality. Use Cases: The main purpose of ICD-11 is for clinical care, billing and coding for insurance companies. Website: https://www.who.int/standards/classifications/classification-of-diseases Open: Yes","title":"International Statistical Classification of Diseases and Related Health Problems (ICD-11)"},{"location":"reference/medical-ontology-landscape/#international-classification-of-diseases-for-oncology-icd-o","text":"Description: A domain-specific extension of the International Statistical Classification of Diseases and Related Health Problems for tumor diseases. Disease area: A multi-axial classification of the site, morphology, behaviour, and grading of neoplasms. Use Cases: Used principally in tumour or cancer registries for coding the site (topography) and the histology (morphology) of neoplasms, usually obtained from a pathology report. Website: https://www.who.int/standards/classifications/other-classifications/international-classification-of-diseases-for-oncology Open: Yes","title":"International Classification of Diseases for Oncology (ICD-O)"},{"location":"reference/medical-ontology-landscape/#logical-observation-identifier-names-and-codes-loinc","text":"Description: Identifies medical laboratory observations. Disease area: Broad coverage Use Cases: The Regenstrief Institute first developed LOINC in 1994 in response to the demand for an electronic database for clinical care and management. LOINC is publicly available at no cost and is endorsed by the American Clinical Laboratory Association and the College of American Pathologists. Since its inception, LOINC has expanded to include not just medical laboratory code names but also nursing diagnoses, nursing interventions, outcome classifications, and patient care data sets. Website: https://loinc.org/ Open: Yes, registration is required.","title":"Logical Observation Identifier Names and Codes (LOINC)"},{"location":"reference/medical-ontology-landscape/#medical-subject-headings-mesh","text":"Description: Medical Subject Headings (MeSH) thesaurus is a controlled and hierarchically-organized vocabulary produced by the National Library of Medicine. Disease area: Broad coverage Use Cases: It is used for indexing, cataloging, and searching of biomedical and health-related information. Integrated into Mondo. Website: https://meshb.nlm.nih.gov/search Open: Yes","title":"Medical Subject Headings (MeSH)"},{"location":"reference/medical-ontology-landscape/#medgen","text":"Description: Organizes information related to human medical genetics, such as attributes of conditions and phenotypes of genetic contributions. Disease area: Human medical genetics Use Cases: MedGen is NCBI's portal to information about conditions and phenotypes related to Medical Genetics. Terms from the NIH Genetic Testing Registry (GTR), UMLS, HPO, Orphanet, ClinVar and other sources are aggregated into concepts, each of which is assigned a unique identifier and a preferred name and symbol. The core content of the record may include names, identifiers used by other databases, mode of inheritance, clinical features, and map location of the loci affecting the disorder. The concept identifier (CUI) is used to aggregate information about that concept, similar to the way NCBI Gene serves as a gateway to gene-related information. Website: https://www.ncbi.nlm.nih.gov/medgen/ Open: Yes","title":"MedGen"},{"location":"reference/medical-ontology-landscape/#medical-dictionary-for-regulatory-activities-meddra","text":"Description: Provides a standardized international medical terminology to be used for regulatory communication and evaluation of data about medicinal products for human use. Disease area: Broad coverage Use Cases: Mainly targeted towards industry and regulatory users. Website: https://www.meddra.org/ Open: Yes","title":"Medical Dictionary for Regulatory Activities (MedDRA)"},{"location":"reference/medical-ontology-landscape/#mental-disease-ontology-mdo","text":"Description: An ontology to describe and classify mental diseases such as schizophrenia, annotated with DSM-IV and ICD codes where applicable. Disease area: Mental functioning, including mental processes such as cognition and traits such as intelligence. Use Cases: The ontology has been partially aligned with the related projects Cognitive Atlas , knowledge base on cognitive science and the Cognitive Paradigm Ontology , which is used in the Brainmap , a database of neuroimaging experiments. GitHub repo: https://github.com/jannahastings/mental-functioning-ontology OBO Foundry webpage: http://obofoundry.org/ontology/mfomd.html Open: yes","title":"Mental Disease Ontology (MDO)"},{"location":"reference/medical-ontology-landscape/#mondo-disease-ontology-mondo","text":"Description: An integrated disease ontology that provides precise mappings between source ontologies that comprehensively covers cross-species diseases, from common to rare diseases. Disease area: Cross species, intended to cover all areas of diseases, integrating source ontologies that cover Mendelian diseases (OMIM), rare diseases (Orphanet), neoplasms (NCIt), human diseases (DO), and others. See all sources here . Use Cases: Mondo was developed for usage in the Monarch Initiative , a discovery system that allows navigation of similarities between phenotypes, organisms, and human diseases across many data sources and organisms. Mondo is also used by ClinGen for disease curations, the Kids First Data Resource Portal for disease annotations and others, see an extensive list here . GitHub repo: https://github.com/monarch-initiative/mondo Website: https://mondo.monarchinitiative.org/ OBO Foundry webpage: http://obofoundry.org/ontology/mondo.html Open: yes","title":"Mondo Disease Ontology (Mondo)"},{"location":"reference/medical-ontology-landscape/#national-cancer-institute-thesaurus-ncit","text":"Description: NCI Thesaurus (NCIt)is a reference terminology that includes broad coverage of the cancer domain, including cancer related diseases, findings and abnormalities. The NCIt OBO Edition aims to increase integration of the NCIt with OBO Library ontologies. NCIt OBO Edition releases should be considered experimental. Disease area: Cancer and neoplasms Use Cases: NCI Thesaurus (NCIt) provides reference terminology for many National Cancer Institute and other systems. It is used by the Clinical Data Interchange Standards Consortium Terminology (CDISC), the U.S. Food and Drug Administration (FDA), the Federal Medication Terminologies (FMT), and the National Council for Prescription Drug Programs (NCPDP). It provides extensive coverage of neoplasms and cancers. GitHub repo: https://github.com/NCI-Thesaurus/thesaurus-obo-edition/issues Website: https://ncithesaurus.nci.nih.gov/ncitbrowser/pages/home.jsf?version=20.11e OBO Foundry webpage: http://obofoundry.org/ontology/ncit.html Open: Yes","title":"National Cancer Institute Thesaurus (NCIT)"},{"location":"reference/medical-ontology-landscape/#neurological-disease-ontology-nd","text":"Description: A framework for the representation of key aspects of neurological disease. Disease area: Neurology Use Cases: Goal is to provide a framework to enable representation of aspects of neurological diseases that are relevant to their treatment and study. This project may be inactive, the last commit to GitHub was in 2016. GitHub repo: https://github.com/addiehl/neurological-disease-ontology Open: Yes","title":"Neurological Disease Ontology (ND)"},{"location":"reference/medical-ontology-landscape/#online-mendelian-inheritance-in-man-omim","text":"Description: a comprehensive, authoritative compendium of human genes and genetic phenotypes that is freely available and updated daily. Disease area: Mendelian, genetic diseases. Use Cases: Integrated into the disease ontology, used by the Human Phenotype Ontology for disease annotations, patients and researchers. Website: https://omim.org/ Open: yes","title":"Online Mendelian Inheritance in Man (OMIM)"},{"location":"reference/medical-ontology-landscape/#ontology-of-cardiovascular-drug-adverse-events-ocvdae","text":"Description: A biomedical ontology of cardiovascular drug\u2013associated adverse events. Disease area: Cardiovascular Use Cases: One novel study of the OCVDAE project is the development of the PCR method. Specifically, an AE-specific drug class effect is defined to exist when all the drugs (drug chemical ingredients or drug products) in a drug class are associated with an AE, which is formulated as a proportional class level ratio (\u201cPCR\u201d)\u2009=\u20091. See more information in the paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5653862/. This project may be inactive, the last GitHub commit was in 2019. GitHub repo: https://github.com/OCVDAE/OCVDAE Website: https://bioportal.bioontology.org/ontologies/OCVDAE Open: yes","title":"Ontology of Cardiovascular Drug Adverse Events (OCVDAE)"},{"location":"reference/medical-ontology-landscape/#ontology-for-general-medical-science-ogms","text":"Description: An ontology of entities involved in a clinical encounter. Use Cases: Provides a formal theory of disease that can be further elaborated by specific disease ontologies. It is intended to be used as a upper level ontology for other disease ontologies. Used by Cardiovascular Disease Ontology . GitHub repo: https://github.com/OGMS/ogms OBO Foundry webpage: http://obofoundry.org/ontology/ogms.html Open: Yes","title":"Ontology for General Medical Science (OGMS)"},{"location":"reference/medical-ontology-landscape/#ontology-for-genetic-susceptibility-factor-ogsf","text":"Description: An application ontology to represent genetic susceptibility to a specific disease, adverse event, or a pathological process. Use Cases: Modeling genetic susceptibility to vaccine adverse events. GitHub repo: https://github.com/linikujp/OGSF OBO Foundry webpage: http://obofoundry.org/ontology/ogsf.html Open: Yes","title":"Ontology for Genetic Susceptibility Factor (OGSF)"},{"location":"reference/medical-ontology-landscape/#ontology-of-glucose-metabolism-disorder-ogmd","text":"Description: Represents glucose metabolism disorder and diabetes disease names, phenotypes, and their classifications. Disease area: Metabolic disorders Use Cases: Still under development (last verssion released in BioPortal was in 2021) but there is little information about its usage online. Website: https://bioportal.bioontology.org/ontologies/OGMD Open: Yes","title":"Ontology of Glucose Metabolism Disorder (OGMD)"},{"location":"reference/medical-ontology-landscape/#ontology-of-language-disorder-in-autism-lda","text":"Description: An ontology assembled from a set of language terms mined from the autism literature. Disease area: Austism Use Cases: This has not been released since 2008 and looks like it is inactive. Website: https://bioportal.bioontology.org/ontologies/LDA Open: Yes","title":"Ontology of Language Disorder in Autism (LDA)"},{"location":"reference/medical-ontology-landscape/#the-oral-health-and-disease-ontology-ohd","text":"Description: Represents the content of dental practice health records and is intended to be further developed for use in translational medicine. OHD is structured using BFO (Basic Formal Ontology) and uses terms from many ontologies, NCBITaxon, and a subset of terms from the CDT (Current Dental Terminology). Disease area: Oral health and disease Use Cases: Used to represent the content of dental practice health records and is intended to be further developed for use in translation medicine. Appears to be inactive. OBO Foundry webpage: http://www.obofoundry.org/ontology/ohd.html Open: Yes","title":"The Oral Health and Disease Ontology (OHD)"},{"location":"reference/medical-ontology-landscape/#orphanet-ordo","text":"Description: The portal for rare diseases and orphan drugs. Contains a structured vocabulary for rare diseases capturing relationships between diseases, genes, and other relevant features, jointly developed by Orphanet and the EBI. It contains information on nearly 10,000 cancers and related diseases, 8,000 single agents and combination therapies, and a wide range of other topics related to cancer and biomedical research. Disease area: Rare diseases Use Cases: Used by rare disease research and clinical community. Integrated into the Mondo disease ontology, aligned with OMIM. Website: https://www.orpha.net/consor/cgi-bin/index.php Open: Yes","title":"Orphanet (ORDO)"},{"location":"reference/medical-ontology-landscape/#parkinson-disease-ontology-pdo","text":"Description: A comprehensive semantic framework with a subclass-based taxonomic hierarchy, covering the whole breadth of the Parkinson disease knowledge domain from major biomedical concepts to different views on disease features held by molecular biologists, clinicians, and drug developers. Disease area: Parkinson disease Use Cases: This resource has been created for use in the IMI-funded AETIONOMY project . Last release was in 2015, may be inactive. Website: https://bioportal.bioontology.org/ontologies/PDON Open: Yes","title":"Parkinson Disease ontology (PDO)"},{"location":"reference/medical-ontology-landscape/#pathogenic-disease-ontology-pdo","text":"Description: Provides information on infectious diseases, disease synonyms, transmission pathways, disease agents, affected populations, and disease properties. Diseases are grouped into syndromic disease categories, organisms are structured hierarchically, and both disease transmission and relevant disease properties are searchable. Disease area: human infectious diseases caused by microbes and the diseases that is related to microbial infection. Use Cases: Has not been released since 2016 and may be inactive. Website: https://bioportal.bioontology.org/ontologies/PDO Open: Yes.","title":"Pathogenic Disease Ontology (PDO)"},{"location":"reference/medical-ontology-landscape/#polycystic-ovary-syndrome-knowledgebase-pcoskb","text":"Description: Comprises genes, single nucleotide polymorphisms, diseases, gene ontology terms, and biochemical pathways associated with polycystic ovary syndrome, a major cause of female subfertility worldwide. Disease area: polycystic ovary syndrome Use Cases: Ontology underlying the Polycystic Ovary Syndrome Knowledgebase , a manually curated knowledgebase on PCOS. Website: http://pcoskb.bicnirrh.res.in/go_d.php Open: Yes","title":"PolyCystic Ovary Syndrome Knowledgebase (PCOSKB)"},{"location":"reference/medical-ontology-landscape/#rat-disease-ontology-rdo","text":"Description: Provides the foundation for ten comprehensive disease area\u2013related data sets at the Rat Genome Database Disease Portals. Disease area: Broad coverage including animal diseases, infectious diseases, chemically-induced disorders, occupational diseases, wounds and injuries and more. Use Cases: Developed for use with the Rat Genome Database Disease Portals. Website: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=DOID:4 Open: Yes","title":"Rat Disease Ontology (RDO)"},{"location":"reference/medical-ontology-landscape/#removable-partial-denture-ontology-rpdo","text":"Description: Represents knowledge of a patient\u2019s oral conditions and denture component parts, originally developed to create a clinician decision support model. Disease area: Oral health and dentures Use Cases: A paper was published on this in 2016 but it does not appear any other information is available about this ontology on the website, presumably it is an inactive project. Publication: https://www.nature.com/articles/srep27855 Open: No","title":"Removable Partial Denture Ontology (RPDO)"},{"location":"reference/medical-ontology-landscape/#resource-of-asian-primary-immunodeficiency-diseases-rpo","text":"Description: Represents observed phenotypic terms, sequence variations, and messenger RNA and protein expression levels of all genes involved in primary immunodeficiency diseases. Disease area: Primary immunodeficiency diseases Use Cases: This terminology is used in a freely accessible, dynamic and integrated database for primary immunodeficiency diseases (PID) called Resource of Asian Primary Immunodeficiency Diseases (RAPID), which is available here . Open: Yes","title":"Resource of Asian Primary Immunodeficiency Diseases (RPO)"},{"location":"reference/medical-ontology-landscape/#sickle-cell-disease-ontology-scdo","text":"Description: SCDO establishes (a) community-standardized sickle cell disease terms and descriptions, (b) canonical and hierarchical representation of knowledge on sickle cell disease, and (c) links to other ontologies and bodies of work. Disease area: Sickle Cell Disease (SCD). Use Cases: SCDO is intended to be a comprehensive collection of knowledge on SCD, facilitate exploration of new scientific questions and ideas, facilitate seamless data sharing and collaborations including meta-analysis within the SCD community, support the building of databasing and clinical informatics in SCD. GitHub repo: https://github.com/scdodev/scdo-ontology/issues Website: https://scdontology.h3abionet.org/ OBO Foundry webpage: http://obofoundry.org/ontology/scdo.html Open: Yes","title":"Sickle Cell Disease Ontology (SCDO)"},{"location":"reference/medical-ontology-landscape/#snomed-clinical-terminology-snomed-ct","text":"Description: A comprehensive clinical terminology/ontology used in healthcare settings. Disease area: Broad disease representation for human diseases. Use Cases: Main coding system used in Electronic Health Records (EHRs). Website: https://browser.ihtsdotools.org/? Open: No, requires a license for usage.","title":"SNOMED Clinical Terminology (SNOMED CT)"},{"location":"reference/medical-ontology-landscape/#symptom-ontology","text":"Description: An ontology of disease symptoms, with symptoms encompasing perceived changes in function, sensations or appearance reported by a patient indicative of a disease. Disease area: Human diseases Use Cases: Developed by the Disease Ontology (DO) team and used for describing symptoms of human diseases in the DO. Website: http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page OBO Foundry webpage: http://obofoundry.org/ontology/symp.html Open: Yes","title":"Symptom Ontology"},{"location":"reference/medical-ontology-landscape/#unified-medical-language-system","text":"Description: The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services. Disease area: Broad coverage Use Cases: Healthcare settings including electronic health records and HL7. Website: https://www.nlm.nih.gov/research/umls/index.html Open: Yes","title":"Unified Medical Language System"},{"location":"reference/medical-ontology-landscape/#phenotype-ontologies","text":"","title":"Phenotype ontologies"},{"location":"reference/medical-ontology-landscape/#phenotype-summary-table","text":"Name Species Area Ascomycete phenotype ontology (APO) Ascomycota C. elegans phenotype (wbphenotype) C elegans Dictyostelium discoideum phenotype ontology (ddpheno) Dictyostelium discoideum Drosophila Phenotype Ontology (DPO) Drosophila Flora Phenotype Ontology (FLOPO) Viridiplantae Fission Yeast Phenotype Ontology (FYPO) S. pombe Human Phenotype Ontology (HPO) Human HPO - ORDO Ontological Module (HOOM) Human Mammalian Phenotype Ontology (MP) Mammals Ontology of Microbial Phenotypes (OMP) Microbe Ontology of Prokaryotic Phenotypic and Metabolic Characters Prokaryotes Pathogen Host Interaction Phenotype Ontology pathogens Planarian Phenotype Ontology (PLANP) Schmidtea mediterranea Plant Trait Ontology (TO) Viridiplantae Plant Phenology Ontology Plants Unified Phenotype Ontology (uPheno) Cross-species coverage Xenopus Phenotype Ontology (XPO) Xenopus Zebrafish Phenotype Ontology (ZP) Zebrafish","title":"Phenotype Summary Table"},{"location":"reference/medical-ontology-landscape/#ascomycete-phenotype-ontology-apo","text":"Description: A structured controlled vocabulary for the phenotypes of Ascomycete fungi. Species: Ascomycota GitHub repo: https://github.com/obophenotype/ascomycete-phenotype-ontology/ Webpage: http://www.yeastgenome.org/ OBO Foundry webpage: http://obofoundry.org/ontology/wbphenotype.html Open: Yes","title":"Ascomycete phenotype ontology (APO)"},{"location":"reference/medical-ontology-landscape/#c-elegans-phenotype-wbphenotype","text":"Description: A structured controlled vocabulary of Caenorhabditis elegans phenotypes. Species: C elegans GitHub repo: https://github.com/obophenotype/c-elegans-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/wbphenotype.html Open: Yes","title":"C. elegans phenotype (wbphenotype)"},{"location":"reference/medical-ontology-landscape/#dictyostelium-discoideum-phenotype-ontology-ddpheno","text":"Description: A structured controlled vocabulary of phenotypes of the slime-mould Dictyostelium discoideum. Species: Dictyostelium discoideum GitHub repo: https://github.com/obophenotype/dicty-phenotype-ontology/issues Webpage: http://dictybase.org/ OBO Foundry webpage: http://obofoundry.org/ontology/ddpheno.html Open: Yes","title":"Dictyostelium discoideum phenotype ontology (ddpheno)"},{"location":"reference/medical-ontology-landscape/#drosophila-phenotype-ontology-dpo","text":"Description: An ontology of commonly encountered and/or high level Drosophila phenotypes. Species: Drosophila GitHub repo: https://github.com/obophenotype/c-elegans-phenotype-ontology Webpage: http://purl.obolibrary.org/obo/fbcv OBO Foundry webpage: http://obofoundry.org/ontology/dpo.html Open: Yes","title":"Drosophila Phenotype Ontology (DPO)"},{"location":"reference/medical-ontology-landscape/#flora-phenotype-ontology-flopo","text":"Description: Traits and phenotypes of flowering plants occurring in digitized Floras. Species: Viridiplantae GitHub repo: https://github.com/flora-phenotype-ontology/flopoontology/ OBO Foundry webpage: http://obofoundry.org/ontology/flopo.html Open: Yes","title":"Flora Phenotype Ontology (FLOPO)"},{"location":"reference/medical-ontology-landscape/#fission-yeast-phenotype-ontology-fypo","text":"Description: FYPO is a formal ontology of phenotypes observed in fission yeast. Species: S. pombe GitHub repo: https://github.com/pombase/fypo OBO Foundry webpage: http://obofoundry.org/ontology/fypo.html Open: Yes","title":"Fission Yeast Phenotype Ontology (FYPO)"},{"location":"reference/medical-ontology-landscape/#human-phenotype-ontology-hpo","text":"Description: HPO provides a standardized vocabulary of phenotypic abnormalities encountered in human disease. Each term in the HPO describes a phenotypic abnormality. Species: Human GitHub repo: https://github.com/obophenotype/human-phenotype-ontology Website: https://hpo.jax.org/app/ OBO Foundry webpage: http://obofoundry.org/ontology/hp.html Open: yes","title":"Human Phenotype Ontology (HPO)"},{"location":"reference/medical-ontology-landscape/#hpo-ordo-ontological-module-hoom","text":"Description: Orphanet provides phenotypic annotations of the rare diseases in the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion. In ORDO a clinical entity is either a group of rare disorders, a rare disorder or a subtype of disorder. The phenomes branch of ORDO has been refactored as a logical import of HPO, and the HPO-ORDO phenotype disease-annotations have been provided in a series of triples in OBAN format in which associations, frequency and provenance are modeled. HOOM is provided as an OWL (Ontologies Web Languages) file, using OBAN, the Orphanet Rare Disease Ontology (ORDO), and HPO ontological models. HOOM provides extra possibilities for researchers, pharmaceutical companies and others wishing to co-analyse rare and common disease phenotype associations, or re-use the integrated ontologies in genomic variants repositories or match-making tools. Species: Human Website: http://www.orphadata.org/cgi-bin/img/PDF/WhatIsHOOM.pdf BioPortal: https://bioportal.bioontology.org/ontologies/HOOM Open: yes","title":"HPO - ORDO Ontological Module (HOOM)"},{"location":"reference/medical-ontology-landscape/#mammalian-phenotype-ontology-mp","text":"Description: Standard terms for annotating mammalian phenotypic data. Species: Mammals (main focus is on mouse and rodents) GitHub repo: https://github.com/obophenotype/mammalian-phenotype-ontology Website: http://www.informatics.jax.org/searches/MP_form.shtml OBO Foundry webpage: http://obofoundry.org/ontology/mp.html Open: Yes","title":"Mammalian Phenotype Ontology (MP)"},{"location":"reference/medical-ontology-landscape/#ontology-of-microbial-phenotypes-omp","text":"Description: An ontology of phenotypes covering microbes. Species: microbes GitHub repo: https://github.com/microbialphenotypes/OMP-ontology Website: http://microbialphenotypes.org OBO Foundry webpage: http://obofoundry.org/ontology/omp.html Open: Yes","title":"Ontology of Microbial Phenotypes (OMP)"},{"location":"reference/medical-ontology-landscape/#ontology-of-prokaryotic-phenotypic-and-metabolic-characters","text":"Description: An ontology of phenotypes covering microbes. Species: Prokaryotes GitHub repo: https://github.com/microbialphenotypes/OMP-ontology/issues Website: http://microbialphenotypes.org/ OBO Foundry webpage: http://obofoundry.org/ontology/omp.html Open: Yes","title":"Ontology of Prokaryotic Phenotypic and Metabolic Characters"},{"location":"reference/medical-ontology-landscape/#pathogen-host-interaction-phenotype-ontology","text":"Description: PHIPO is a formal ontology of species-neutral phenotypes observed in pathogen-host interactions. Species: pathogens GitHub repo: https://github.com/PHI-base/phipo Website: http://www.phi-base.org OBO Foundry webpage: http://obofoundry.org/ontology/phipo.html Open: Yes","title":"Pathogen Host Interaction Phenotype Ontology"},{"location":"reference/medical-ontology-landscape/#planarian-phenotype-ontology-planp","text":"Description: Planarian Phenotype Ontology is an ontology of phenotypes observed in the planarian Schmidtea mediterranea. Species: Schmidtea mediterranea GitHub repo: https://github.com/obophenotype/planarian-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/planp.html Open: Yes","title":"Planarian Phenotype Ontology (PLANP)"},{"location":"reference/medical-ontology-landscape/#plant-trait-ontology-to","text":"Description: A controlled vocabulary of describe phenotypic traits in plants. Species: Viridiplantae GitHub repo: https://github.com/Planteome/plant-trait-ontology/ OBO Foundry webpage: http://obofoundry.org/ontology/to.html Open: Yes","title":"Plant Trait Ontology (TO)"},{"location":"reference/medical-ontology-landscape/#plant-phenology-ontology","text":"Description: An ontology for describing the phenology of individual plants and populations of plants, and for integrating plant phenological data across sources and scales. Species: Plants GitHub repo: https://github.com/PlantPhenoOntology/PPO OBO Foundry webpage: http://obofoundry.org/ontology/ppo.html Open: Yes","title":"Plant Phenology Ontology"},{"location":"reference/medical-ontology-landscape/#unified-phenotype-ontology-upheno","text":"Description: The uPheno ontology integrates multiple phenotype ontologies into a unified cross-species phenotype ontology. Species: Cross-species coverage GitHub repo: https://github.com/obophenotype/upheno OBO Foundry webpage: http://obofoundry.org/ontology/upheno.html Open: Yes","title":"Unified Phenotype Ontology (uPheno)"},{"location":"reference/medical-ontology-landscape/#xenopus-phenotype-ontology-xpo","text":"Description: XPO represents anatomical, cellular, and gene function phenotypes occurring throughout the development of the African frogs Xenopus laevis and tropicalis. Species: Xenopus GitHub repo: https://github.com/obophenotype/xenopus-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/xpo.html Open: Yes","title":"Xenopus Phenotype Ontology (XPO)"},{"location":"reference/medical-ontology-landscape/#zebrafish-phenotype-ontology-zp","text":"Description: The Zebrafish Phenotype Ontology formally defines all phenotypes of the Zebrafish model organism. Species: Zebrafish GitHub repo: https://github.com/obophenotype/zebrafish-phenotype-ontology OBO Foundry webpage: http://obofoundry.org/ontology/zp.html Open: Yes","title":"Zebrafish Phenotype Ontology (ZP)"},{"location":"reference/medical-ontology-landscape/#references","text":"A Census of Disease Ontologies Melissa A. Haendel, Julie A. McMurry, Rose Relevo, Christopher J. Mungall, Peter N. Robinson, Christopher G. Chute Annual Review of Biomedical Data Science 2018 1:1, 305-331 OMOP2OBO repository","title":"References"},{"location":"reference/mungall-blog-radar/","text":"Monkeying around with OWL \u00b6 An index page to find some of our favourite articles on Chris' blog. These are not all articles, but I selection we found useful during our every work. Ontology development and modelling \u00b6 OntoTips Series . Must read series for the beginning ontology developer. Warning about complex modelling . Chris is generally big on Occam's Razor solutions: given two solutions that solve a use case, the simpler is better. OntoTip: Don\u2019t over-specify OWL definitions . From the above OntoTip series. How to deal with unintentional equivalent classes Ontology curation \u00b6 OntoTip: Write simple, concise, clear, operational textual definitions . One of our favourite blog posts of Chris. Must read!","title":"Monkeying around with OWL"},{"location":"reference/mungall-blog-radar/#monkeying-around-with-owl","text":"An index page to find some of our favourite articles on Chris' blog. These are not all articles, but I selection we found useful during our every work.","title":"Monkeying around with OWL"},{"location":"reference/mungall-blog-radar/#ontology-development-and-modelling","text":"OntoTips Series . Must read series for the beginning ontology developer. Warning about complex modelling . Chris is generally big on Occam's Razor solutions: given two solutions that solve a use case, the simpler is better. OntoTip: Don\u2019t over-specify OWL definitions . From the above OntoTip series. How to deal with unintentional equivalent classes","title":"Ontology development and modelling"},{"location":"reference/mungall-blog-radar/#ontology-curation","text":"OntoTip: Write simple, concise, clear, operational textual definitions . One of our favourite blog posts of Chris. Must read!","title":"Ontology curation"},{"location":"reference/odk/","text":"Ontology Development Kit (ODK) Reference \u00b6 The ODK is essentially two things: A toolbox. All frequently used tools for managing the ontology life cycle are bundled together into a Docker image: ROBOT, owltools, fastobo-validator, dosdp-tools, riot and many, many more. A system, you could have say \"methodology\" for managing the ontology life cycle from continious integration and quality control to imports and release management. The Toolbox \u00b6 The ODK bundles a lot of tools together, such as ROBOT, owltools, fastobo-validator and dosdp-tools. To get a better idea, its best to simply read the Dockerfile specifications of the ODK image: ODK Lite Image . This contains the most essentials tools related to ODK development. Most of the day to day activities of ontology developers with ROBOT are well covered by odklite . ODK Full Image . Extends the ODK Lite image with a further round of powerful tools. It contains for example Apache Jena, the OBO Dashboard, the Konclude reasoner and a large array of command line tools. The system for ontology life cycle management \u00b6 One of the tools in the toolbox, the \"seed my repo\" function, allows us to generate a complete GitHub repository with everything needed to manage an OBO ontology according to OBO best practices. The two central components are A Makefile that encodes the rules by which ontology release files should be derived from the source of truth (the edit file). A support for CI such as GitHub actions or Travis for running continuous integration checks.","title":"Ontology Development Kit (ODK)"},{"location":"reference/odk/#ontology-development-kit-odk-reference","text":"The ODK is essentially two things: A toolbox. All frequently used tools for managing the ontology life cycle are bundled together into a Docker image: ROBOT, owltools, fastobo-validator, dosdp-tools, riot and many, many more. A system, you could have say \"methodology\" for managing the ontology life cycle from continious integration and quality control to imports and release management.","title":"Ontology Development Kit (ODK) Reference"},{"location":"reference/odk/#the-toolbox","text":"The ODK bundles a lot of tools together, such as ROBOT, owltools, fastobo-validator and dosdp-tools. To get a better idea, its best to simply read the Dockerfile specifications of the ODK image: ODK Lite Image . This contains the most essentials tools related to ODK development. Most of the day to day activities of ontology developers with ROBOT are well covered by odklite . ODK Full Image . Extends the ODK Lite image with a further round of powerful tools. It contains for example Apache Jena, the OBO Dashboard, the Konclude reasoner and a large array of command line tools.","title":"The Toolbox"},{"location":"reference/odk/#the-system-for-ontology-life-cycle-management","text":"One of the tools in the toolbox, the \"seed my repo\" function, allows us to generate a complete GitHub repository with everything needed to manage an OBO ontology according to OBO best practices. The two central components are A Makefile that encodes the rules by which ontology release files should be derived from the source of truth (the edit file). A support for CI such as GitHub actions or Travis for running continuous integration checks.","title":"The system for ontology life cycle management"},{"location":"reference/ontology-curator/","text":"A Day in the Life of an Ontology Curator \u00b6 Review issues on the issue tracker . Tickets by organized by assigning labels (such as new term requests ) and milestones Can also sort tickets on Project boards Can search on labels, milestones , assignee , etc. In Mondo, we set priorities based on user requests, size of the ticket (ie amount of work required), if it is blocking something else, etc. Edits to the Mondo ontology are made on Branches and via Pull Requests on the mondo-edit.obo file. Example: work on an open ticket to add a new term using Protege. Detailed instructions on how to add a new term are here . Example: User request to add 50+ subtyps of acute myeloid leukemia. We used a ROBOT template . Do you want to contribute? See tickets labeled good first issue . Documentation and more instructions are available in the Mondo editors guide .","title":"A day in the life of an ontology curator"},{"location":"reference/ontology-curator/#a-day-in-the-life-of-an-ontology-curator","text":"Review issues on the issue tracker . Tickets by organized by assigning labels (such as new term requests ) and milestones Can also sort tickets on Project boards Can search on labels, milestones , assignee , etc. In Mondo, we set priorities based on user requests, size of the ticket (ie amount of work required), if it is blocking something else, etc. Edits to the Mondo ontology are made on Branches and via Pull Requests on the mondo-edit.obo file. Example: work on an open ticket to add a new term using Protege. Detailed instructions on how to add a new term are here . Example: User request to add 50+ subtyps of acute myeloid leukemia. We used a ROBOT template . Do you want to contribute? See tickets labeled good first issue . Documentation and more instructions are available in the Mondo editors guide .","title":"A Day in the Life of an Ontology Curator"},{"location":"reference/other-resources/","text":"Other Resources \u00b6 Here's a collection of links about the Open Biological and Biomedical Ontologies (OBO), and related topics. If you're completely new to OBO, I suggest starting with Ontologies 101 : Unit 1: Controlled Vocabularies, Ontologies, and Data Linking (PowerPoint Slides) Unit 2: An Introduction to OWL (Powerpoint Slides) Unit 3: Ontology Community (Powerpoint Slides) BDK14 Ontologies 101 repository If you're new to scientific computing more generally, then I strongly recommend Software Carpentry , which provides a set of very pragmatic introductions to the Unix command line, git, Python, Make, and other tools widely used by OBO developers. Open Biological and Biomedical Ontologies \u00b6 OBO is a community of people collaborating on open source ontologies for science. We have a set of shared principles and best practises to help people and data work together effectively. OBO Foundry Homepage The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration (journal article) OBO Discuss mailing list Services \u00b6 Here is a very incomplete list of some excellent services to help you find an use OBO terms and ontologies. EMBL-EBI OLS: Ontology Lookup Service is an excellent ontology browser and search service OxO shows mappings between ontologies and terms Zooma for mapping free text to ontology terms Onto-Animals Ontobee is an ontology browser Ontofox is an ontology extraction tool Bioportal provides ontology browsing, search, mapping, etc. Tools \u00b6 This is the suite of open source software that most OBO developers use. OBO Tools mailing list GitHub is where most OBO projects are hosted and what we use to manage code, issues, etc. GitHub tutorial Prot\u00e9g\u00e9 is a graphical user interface for editing OWL ontologies. (Java) ROBOT is a command-line tool for automating ontology tasks. (Java) ROBOT tutorial ROBOT: A Tool for Automating Ontology Workflows (journal article) ENVO ROBOT Template and Merge Workflow DOS-DP is a command-line tool for working with ontology design patterns. (Python) ODK: Ontology Development Kit is a collection of tools for building and maintaining an OBO project. (Docker) OBO Tools and Workflows (Google Slides) A good overview of technical and advanced topics of OBO practises, including the Ontology Development Kit. OWLAPI is a Java library for working with ontologies, and is the foundation for Prot\u00e9g\u00e9 and ROBOT. OBO PURL System is used to redirect OBO terms from their IRIs to the right resource String of PURLs \u2013 frugal migration and maintenance of persistent identifiers (journal article) Technical \u00b6 This section is for technical reference, not beginners. OBO projects use Semantic Web and Linked Data technologies: W3C Semantic Web overview Search for W3C data standards W3C Data on the Web Best Practices These standards form layers: IRI: Internationalized Resource Identifiers are a superset of the familiar URLs used to locate resources on the web. Every ontology term has a globally unique IRI. RDF: Resource Description Format is a standard for combining IRIs into subject-predicate-object \"triples\" that make a statement about some thing. Sets of triples form a graph (i.e. network), and graphs can easily be merged to form larger graphs. SPARQL is the language for querying RDF graphs. RDF 1.1 Primer SPARQL 1.1 Overview RDFS: RDF Schema 1.1 extends RDF with classes, hierarchies, and other features. XSD: W3C XML Schema Definition Language (XSD) 1.1 Part 2: Datatypes is the common standard for datatypes in RDF OWL: Web Ontology Language extends RDF and RDFS to provide more powerful logic OWL 2 Web Ontology Language Primer (Second Edition) Other useful resources on technical topics: Monkeying around with OWL Chris Mungall's blog, mostly on technical topics for ontologies.","title":"Other useful resources"},{"location":"reference/other-resources/#other-resources","text":"Here's a collection of links about the Open Biological and Biomedical Ontologies (OBO), and related topics. If you're completely new to OBO, I suggest starting with Ontologies 101 : Unit 1: Controlled Vocabularies, Ontologies, and Data Linking (PowerPoint Slides) Unit 2: An Introduction to OWL (Powerpoint Slides) Unit 3: Ontology Community (Powerpoint Slides) BDK14 Ontologies 101 repository If you're new to scientific computing more generally, then I strongly recommend Software Carpentry , which provides a set of very pragmatic introductions to the Unix command line, git, Python, Make, and other tools widely used by OBO developers.","title":"Other Resources"},{"location":"reference/other-resources/#open-biological-and-biomedical-ontologies","text":"OBO is a community of people collaborating on open source ontologies for science. We have a set of shared principles and best practises to help people and data work together effectively. OBO Foundry Homepage The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration (journal article) OBO Discuss mailing list","title":"Open Biological and Biomedical Ontologies"},{"location":"reference/other-resources/#services","text":"Here is a very incomplete list of some excellent services to help you find an use OBO terms and ontologies. EMBL-EBI OLS: Ontology Lookup Service is an excellent ontology browser and search service OxO shows mappings between ontologies and terms Zooma for mapping free text to ontology terms Onto-Animals Ontobee is an ontology browser Ontofox is an ontology extraction tool Bioportal provides ontology browsing, search, mapping, etc.","title":"Services"},{"location":"reference/other-resources/#tools","text":"This is the suite of open source software that most OBO developers use. OBO Tools mailing list GitHub is where most OBO projects are hosted and what we use to manage code, issues, etc. GitHub tutorial Prot\u00e9g\u00e9 is a graphical user interface for editing OWL ontologies. (Java) ROBOT is a command-line tool for automating ontology tasks. (Java) ROBOT tutorial ROBOT: A Tool for Automating Ontology Workflows (journal article) ENVO ROBOT Template and Merge Workflow DOS-DP is a command-line tool for working with ontology design patterns. (Python) ODK: Ontology Development Kit is a collection of tools for building and maintaining an OBO project. (Docker) OBO Tools and Workflows (Google Slides) A good overview of technical and advanced topics of OBO practises, including the Ontology Development Kit. OWLAPI is a Java library for working with ontologies, and is the foundation for Prot\u00e9g\u00e9 and ROBOT. OBO PURL System is used to redirect OBO terms from their IRIs to the right resource String of PURLs \u2013 frugal migration and maintenance of persistent identifiers (journal article)","title":"Tools"},{"location":"reference/other-resources/#technical","text":"This section is for technical reference, not beginners. OBO projects use Semantic Web and Linked Data technologies: W3C Semantic Web overview Search for W3C data standards W3C Data on the Web Best Practices These standards form layers: IRI: Internationalized Resource Identifiers are a superset of the familiar URLs used to locate resources on the web. Every ontology term has a globally unique IRI. RDF: Resource Description Format is a standard for combining IRIs into subject-predicate-object \"triples\" that make a statement about some thing. Sets of triples form a graph (i.e. network), and graphs can easily be merged to form larger graphs. SPARQL is the language for querying RDF graphs. RDF 1.1 Primer SPARQL 1.1 Overview RDFS: RDF Schema 1.1 extends RDF with classes, hierarchies, and other features. XSD: W3C XML Schema Definition Language (XSD) 1.1 Part 2: Datatypes is the common standard for datatypes in RDF OWL: Web Ontology Language extends RDF and RDFS to provide more powerful logic OWL 2 Web Ontology Language Primer (Second Edition) Other useful resources on technical topics: Monkeying around with OWL Chris Mungall's blog, mostly on technical topics for ontologies.","title":"Technical"},{"location":"reference/protege-faq/","text":"Proteg\u00e9 FAQs \u00b6 How to escape characters in the class expression editor \u00b6 To add an ontology term (such as a GO term) that contains ' in its name (e.g. RNA-directed 5'-3' RNA polymerase activity ) in the class expression editor, you need to escape the ' characters. In Proteg\u00e9 5.5 this is not automatically handled when you auto-complete with tab. To escape the character append \\ before the ' -> RNA-directed 5\\'-3\\' RNA polymerase activity . You won't be able to add the annotation otherwise. As in Proteg\u00e9 5.5, the \\ characters will show up in the description window, and when hovering over the term, you won't be able to click on it with a link. However, when you save the file, the relationship is saved correctly. You can double-check by going to the ontology text file and see that the term is correctly mentioned in the relationship.","title":"Protege FAQ"},{"location":"reference/protege-faq/#protege-faqs","text":"","title":"Proteg\u00e9 FAQs"},{"location":"reference/protege-faq/#how-to-escape-characters-in-the-class-expression-editor","text":"To add an ontology term (such as a GO term) that contains ' in its name (e.g. RNA-directed 5'-3' RNA polymerase activity ) in the class expression editor, you need to escape the ' characters. In Proteg\u00e9 5.5 this is not automatically handled when you auto-complete with tab. To escape the character append \\ before the ' -> RNA-directed 5\\'-3\\' RNA polymerase activity . You won't be able to add the annotation otherwise. As in Proteg\u00e9 5.5, the \\ characters will show up in the description window, and when hovering over the term, you won't be able to click on it with a link. However, when you save the file, the relationship is saved correctly. You can double-check by going to the ontology text file and see that the term is correctly mentioned in the relationship.","title":"How to escape characters in the class expression editor"},{"location":"reference/protege-interface/","text":"Reference document for protege interface \u00b6 For this reference, we will use the cell ontology to highlight the key information on the user interface in Protege General interface buttons \u00b6 '+' button (not shown above) = add '?' button = explain axiom '@' button = annotate 'x' button = remove 'o' button = edit Active Ontology tab \u00b6 Overview \u00b6 When you open the ontology on protege, you should land on the Active ontology tab, alternatively, it is available on the top as one of your tabs. Ontology Level Annotations \u00b6 Annotations on the active ontology tab are ontology level annotations and contain metadata about the ontology. This includes: title (name of the ontology) description license contributors (ideally this should be in ORCID but many ontologies use names instead) references (under rdfs:comment) preferred_root (this allows certain browsers to know which root to display the ontology from) Entities tab \u00b6 Entities are where your \"entries\" in the ontology live and where you can add terms etc.","title":"Protege interface"},{"location":"reference/protege-interface/#reference-document-for-protege-interface","text":"For this reference, we will use the cell ontology to highlight the key information on the user interface in Protege","title":"Reference document for protege interface"},{"location":"reference/protege-interface/#general-interface-buttons","text":"'+' button (not shown above) = add '?' button = explain axiom '@' button = annotate 'x' button = remove 'o' button = edit","title":"General interface buttons"},{"location":"reference/protege-interface/#active-ontology-tab","text":"","title":"Active Ontology tab"},{"location":"reference/protege-interface/#overview","text":"When you open the ontology on protege, you should land on the Active ontology tab, alternatively, it is available on the top as one of your tabs.","title":"Overview"},{"location":"reference/protege-interface/#ontology-level-annotations","text":"Annotations on the active ontology tab are ontology level annotations and contain metadata about the ontology. This includes: title (name of the ontology) description license contributors (ideally this should be in ORCID but many ontologies use names instead) references (under rdfs:comment) preferred_root (this allows certain browsers to know which root to display the ontology from)","title":"Ontology Level Annotations"},{"location":"reference/protege-interface/#entities-tab","text":"Entities are where your \"entries\" in the ontology live and where you can add terms etc.","title":"Entities tab"},{"location":"reference/reasoning/","text":"Why do we need reasoning? \u00b6 A quick personal perspective up-front. When I was finishing my undergrad, I barely had heard the term Semantic Web. What I had heard vaguely intrigued me, so I decided that for my final project, I would try to combine something Semantic Web related with my other major, Law and build a tool that could automatically infer the applicability of a law (written in OWL) given a legal case. Super naively, I just went went ahead, read a few papers about legal ontologies, build a simple one, loaded it into my application and somehow got it to work, with reasoning and all, without even having heard of Description Logic. In my PhD, I worked on actual reasoning algorithms, which meant, no more avoiding logic. But - I did not get it. Up until this point in my life, I could just study harder and harder, and in the end I was confident with what I learned, but First Order Logic, in particular model theory and proofs, caused me anxiety until the date of my viva. In the end, a very basic understanding of model theory and Tableau did help me with charactering the algorithms I was working with (I was studying the effect of modularity, cutting out logically connected subsets of an ontology, on reasoning performance) but I can confidently say today: I never really, like deeply, understood logical proofs. I still cant read them - and I have a PhD in Reasoning (albeit from an empirical angle). If you followed the Open HPI courses on logic, and you are anything like me, your head will hurt and you will want to hide under your blankets. Most students feel like that. For a complete education in Semantic Web technologies, going through this part once is essential: it tells you something about how difficult some stuff is under the hood, and how much work has been done to make something like OWL work for knowledge representation. You should have gained some appreciation of the domain, which is no less complex than Machine Learning or Stochastic Processes. But, in my experience, some of the most effective ontology engineers barely understand reasoning - definitely have no idea how it works - and still do amazing work. In that spirit, I would like to invite you at this stage to put logic and reasoning behind you (unless it made you curious of course) - you won't need to know much of that for being an effective Semantic Engineer. In the following, I will summarise some of the key take-aways that I find useful to keep in mind. Semantics define how to interpret an ontology . For example, in OWL, the statement Human SubClassOf: Mammal means that all instances of the Human class, like me, are also instances of the Mammal class. Or, in other words, from the statements: Human SubClassOf: Mammal Nico type: Human Semantics allow as to deduce that Nico:Mammal . What are semantics practically ? Show me your semantics? Look at something like the OWL semantics . In there, you will find language statements (syntax) like X SubClassOf: Y and a bunch of formulae from model theory that describe how to interpret it - no easy read, and not really important for you now. OWL has a number of profiles , basically sub-languages where you can say less things. Why would we want to restrict our \"expressivity\"? Because their is a trade-off. An important slide I remember from when I learned about ontology languages was the triangle of complexity (here only paraphrased from memory): When expressivity goes up, cognitive complexity and computational complexity go up. When we want to decrease cognitive complexity (make it easier to build ontologies), expressivity goes down. When we want reasoners to be faster at making inferences (computational complexity), we need to decrease expressivity. So we need to find a way to balance. What are the most important practical applications of reasoning? There are many, and there will be many opinions, but in the OBO world, by far (95%) of all uses of reasoners pertain to the following: Classification. Most, if not all, of our ontologies are conceptually hierarchies of classes we use reasoners to automatically infer hierarchies. Look for example at the Xenopus Phenotype Ontology - the class hierarchy is entirely build with a reasoner - no Human intervention! Debugging. There are two major threats to ontologies. In the worst case, they can be inconsistent - which means, totally broken. A slightly less bad, but still undesirable situation is that some of the classes in your ontologies break (in parlance, become unsatisfiable). This happens when you say some contradictory things about them. Reasoners help you find these unsatisfiable classes, and there is a special reasoning algorithm that can generate an explanation for you - to help fixing your problem. So in general, what is reasoning? There are probably a dozen or more official characterisations in the scientific literature, but from the perspective of biomedical ontologies, the question can be roughly split like this: How can we capture what we know? This is the (research-) area of knowledge representation, logical formalisms, such as First Order Logic, Description Logic, etc. It is concerned with how we write down what we now: All cars have four wheels If you are a human, you are also a mammal If you are a bird, you can fly (unless you are a penguin) How can we uncover implicit knowledge efficiently? This is the area of reasoning, and while being closely related to the formalisms above, it makes sense to think of them in a distinct manner, as the problems are very different in practice. It can typically be grouped into the following two categories: deductive reasoning infers by Lets think about a naive approach: using a fact-, or data-, base.","title":"Reasoning"},{"location":"reference/reasoning/#why-do-we-need-reasoning","text":"A quick personal perspective up-front. When I was finishing my undergrad, I barely had heard the term Semantic Web. What I had heard vaguely intrigued me, so I decided that for my final project, I would try to combine something Semantic Web related with my other major, Law and build a tool that could automatically infer the applicability of a law (written in OWL) given a legal case. Super naively, I just went went ahead, read a few papers about legal ontologies, build a simple one, loaded it into my application and somehow got it to work, with reasoning and all, without even having heard of Description Logic. In my PhD, I worked on actual reasoning algorithms, which meant, no more avoiding logic. But - I did not get it. Up until this point in my life, I could just study harder and harder, and in the end I was confident with what I learned, but First Order Logic, in particular model theory and proofs, caused me anxiety until the date of my viva. In the end, a very basic understanding of model theory and Tableau did help me with charactering the algorithms I was working with (I was studying the effect of modularity, cutting out logically connected subsets of an ontology, on reasoning performance) but I can confidently say today: I never really, like deeply, understood logical proofs. I still cant read them - and I have a PhD in Reasoning (albeit from an empirical angle). If you followed the Open HPI courses on logic, and you are anything like me, your head will hurt and you will want to hide under your blankets. Most students feel like that. For a complete education in Semantic Web technologies, going through this part once is essential: it tells you something about how difficult some stuff is under the hood, and how much work has been done to make something like OWL work for knowledge representation. You should have gained some appreciation of the domain, which is no less complex than Machine Learning or Stochastic Processes. But, in my experience, some of the most effective ontology engineers barely understand reasoning - definitely have no idea how it works - and still do amazing work. In that spirit, I would like to invite you at this stage to put logic and reasoning behind you (unless it made you curious of course) - you won't need to know much of that for being an effective Semantic Engineer. In the following, I will summarise some of the key take-aways that I find useful to keep in mind. Semantics define how to interpret an ontology . For example, in OWL, the statement Human SubClassOf: Mammal means that all instances of the Human class, like me, are also instances of the Mammal class. Or, in other words, from the statements: Human SubClassOf: Mammal Nico type: Human Semantics allow as to deduce that Nico:Mammal . What are semantics practically ? Show me your semantics? Look at something like the OWL semantics . In there, you will find language statements (syntax) like X SubClassOf: Y and a bunch of formulae from model theory that describe how to interpret it - no easy read, and not really important for you now. OWL has a number of profiles , basically sub-languages where you can say less things. Why would we want to restrict our \"expressivity\"? Because their is a trade-off. An important slide I remember from when I learned about ontology languages was the triangle of complexity (here only paraphrased from memory): When expressivity goes up, cognitive complexity and computational complexity go up. When we want to decrease cognitive complexity (make it easier to build ontologies), expressivity goes down. When we want reasoners to be faster at making inferences (computational complexity), we need to decrease expressivity. So we need to find a way to balance. What are the most important practical applications of reasoning? There are many, and there will be many opinions, but in the OBO world, by far (95%) of all uses of reasoners pertain to the following: Classification. Most, if not all, of our ontologies are conceptually hierarchies of classes we use reasoners to automatically infer hierarchies. Look for example at the Xenopus Phenotype Ontology - the class hierarchy is entirely build with a reasoner - no Human intervention! Debugging. There are two major threats to ontologies. In the worst case, they can be inconsistent - which means, totally broken. A slightly less bad, but still undesirable situation is that some of the classes in your ontologies break (in parlance, become unsatisfiable). This happens when you say some contradictory things about them. Reasoners help you find these unsatisfiable classes, and there is a special reasoning algorithm that can generate an explanation for you - to help fixing your problem. So in general, what is reasoning? There are probably a dozen or more official characterisations in the scientific literature, but from the perspective of biomedical ontologies, the question can be roughly split like this: How can we capture what we know? This is the (research-) area of knowledge representation, logical formalisms, such as First Order Logic, Description Logic, etc. It is concerned with how we write down what we now: All cars have four wheels If you are a human, you are also a mammal If you are a bird, you can fly (unless you are a penguin) How can we uncover implicit knowledge efficiently? This is the area of reasoning, and while being closely related to the formalisms above, it makes sense to think of them in a distinct manner, as the problems are very different in practice. It can typically be grouped into the following two categories: deductive reasoning infers by Lets think about a naive approach: using a fact-, or data-, base.","title":"Why do we need reasoning?"},{"location":"reference/release-artefacts/","text":"Release artefacts \u00b6 For explanation of different release artefacts, please see discussion documentation on owl format variants We made a first stab add defining release artefacts that should cover all use cases community-wide. We need to (1) agree they are all that is needed and (2) they are defined correctly in terms of ROBOT commands. This functionality replaces what was previously done using OORT. Terminology: \u00b6 The source ontology is the ontology we are talking about. A release artefact is a version of the ontology modified in some specific way, intended for public use. An import is a module of an external ontology which contains all the axioms necessary for the source ontology. A component is a file containing axioms that belong to the source ontology (but are for one reason or another, like definitions.owl, managed in a separate file). An axiom is said to be foreign if it 'belongs' to a different ontology, and native if it belongs to the source ontology. For example, the source ontology might have, for one reason or another, been physically asserted (rather than imported) the axiom TransitiveObjectProperty(BFO:000005). If the source ontology does not 'own' the BFO namespace, this axiom will be considered foreign. There are currently 6 release defined in the ODK: base (required) full (required) non-classified (optional) simple (optional) basic (optional) simple-non-classified (optional, transient) We discuss all of them here in detail. Release artefact 1: base (required) \u00b6 The base file contains all and only native axioms. No further manipulation is performed, in particular no reasoning, redundancy stripping or relaxation. This release artefact is going to be the new backbone of the OBO strategy to combat incompatible imports and consequent lack of interoperability. (Detailed discussions elsewhere, @balhoff has documentation). Every OBO ontology will contain a mandatory base release (should be in the official OBO recommendations as well). The ROBOT command generating the base artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONT)-base.owl: $(SRC) $(OTHER_SRC) $(ROBOT) remove --input $< --select imports --trim false \\ merge $(patsubst %, -i %, $(OTHER_SRC)) \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@ Release artefact 2: full (required) \u00b6 The full release artefact contains all logical axioms, including inferred subsumptions. Redundancy stripping (i.e. redundant subclass of axioms) and typical relaxation operations are performed. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. Every OBO ontology will contain a mandatory full release. The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONT)-full.owl: $(SRC) $(OTHER_SRC) $(ROBOT) merge --input $< \\ reason --reasoner ELK \\ relax \\ reduce -r ELK \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@ Release artefact 3: non-classified (optional) \u00b6 The non-classified release artefact reflects the 'unmodified state' of the editors file at release time. No operations are performed that modify the axioms in any way, in particular no redundancy stripping. As opposed to the base artefact, both component and imported ontologies are merged into the non-classified release. The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONT)-non-classified.owl: $(SRC) $(OTHER_SRC) $(ROBOT) merge --input $< \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@ Release artefact 4: simple (optional) \u00b6 Many users want a release that can be treated as a simple existential graph of the terms defined in an ontology. This corresponds to the state of OBO ontologies before logical definitions and imports. For example, the only logical axioms in -simple release of CL will contain be of the form CL1 subClassOf CL2 or CL1 subClassOf R some CL3 where R is any objectProperty and CLn is a CL class. This role has be fulfilled by the -simple artefact, which up to now has been supported by OORT. To construct this, we first need to assert inferred classifications, relax equivalentClass axioms to sets of subClassOf axioms and then strip all axioms referencing foreign (imported) classes. As ontologies occasionally end up with forieign classes and axioms merged into the editors file, we achieve this will a filter based on obo-namespace. (e.g. finding all terms with iri matching http://purl.obolibrary.org/obo/CL_{\\d}7). The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(SIMPLESEED): all terms that 'belong' to the ontology $(ROBOT) merge --input $< $(patsubst %, -i %, $(OTHER_SRC)) \\ reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} \\ relax \\ remove --axioms equivalent \\ relax \\ filter --term-file $(SIMPLESEED) --select \"annotations ontology anonymous self\" --trim true --signature true \\ reduce -r {{ project.reasoner }} \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@.tmp.owl && mv $@.tmp.owl $@ NOTES: This requires $(ONTOLOGYTERMS) to include all ObjectProperties usesd. --select parents is required for logical axioms to be retained, but results in a few upper-level classes bleeding through. We hope this will be fixed by further improvments to Monarch. Release artefact 5: basic \u00b6 Some legacy users (e.g. MGI) require an OBO DAG version of -simple. OBO files derived from OWL are not guarenteed to be acyclic, but acyclic graphs can be achieved using judicious filtering of relationships (simple existential restrictions) by objectProperty. The -basic release artefact has historically fulfilled this function as part of OORT driven ontology releases. The default -basic version corresponds to the -simple artefact with only 'part of' relationships (BFO:0000050), but others may be added where ontology editors judge these to be useful and safe to add without adding cycles. We generate by taking the simple release and filtering it The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(KEEPRELATIONS): all relations that should be preserved. $(SIMPLESEED): all terms that 'belong' to the ontology $(ROBOT) merge --input $< $(patsubst %, -i %, $(OTHER_SRC)) \\ reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} \\ relax \\ remove --axioms equivalent \\ remove --axioms disjoint \\ remove --term-file $(KEEPRELATIONS) --select complement --select object-properties --trim true \\ relax \\ filter --term-file $(SIMPLESEED) --select \"annotations ontology anonymous self\" --trim true --signature true \\ reduce -r {{ project.reasoner }} \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@.tmp.owl && mv $@.tmp.owl $@ Release artefact 6: simple-non-classified (optional) \u00b6 This artefact caters to the very special and hopefully transient case of some ontologies that do not yet trust reasoning (MP, HP). The simple-non-classified artefact corresponds to the simple artefact, just without the reasoning step. $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONTOLOGYTERMS): all terms that 'belong' to the ontology $(ONT)-simple-non-classified.owl: $(SRC) $(OTHER_SRC) $(ONTOLOGYTERMS) $(ROBOT) remove --input $< --select imports \\ merge $(patsubst %, -i %, $(OTHER_SRC)) \\ relax \\ reduce -r ELK \\ filter --term-file $(ONTOLOGYTERMS) --trim true \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@","title":"Release Artefacts"},{"location":"reference/release-artefacts/#release-artefacts","text":"For explanation of different release artefacts, please see discussion documentation on owl format variants We made a first stab add defining release artefacts that should cover all use cases community-wide. We need to (1) agree they are all that is needed and (2) they are defined correctly in terms of ROBOT commands. This functionality replaces what was previously done using OORT.","title":"Release artefacts"},{"location":"reference/release-artefacts/#terminology","text":"The source ontology is the ontology we are talking about. A release artefact is a version of the ontology modified in some specific way, intended for public use. An import is a module of an external ontology which contains all the axioms necessary for the source ontology. A component is a file containing axioms that belong to the source ontology (but are for one reason or another, like definitions.owl, managed in a separate file). An axiom is said to be foreign if it 'belongs' to a different ontology, and native if it belongs to the source ontology. For example, the source ontology might have, for one reason or another, been physically asserted (rather than imported) the axiom TransitiveObjectProperty(BFO:000005). If the source ontology does not 'own' the BFO namespace, this axiom will be considered foreign. There are currently 6 release defined in the ODK: base (required) full (required) non-classified (optional) simple (optional) basic (optional) simple-non-classified (optional, transient) We discuss all of them here in detail.","title":"Terminology:"},{"location":"reference/release-artefacts/#release-artefact-1-base-required","text":"The base file contains all and only native axioms. No further manipulation is performed, in particular no reasoning, redundancy stripping or relaxation. This release artefact is going to be the new backbone of the OBO strategy to combat incompatible imports and consequent lack of interoperability. (Detailed discussions elsewhere, @balhoff has documentation). Every OBO ontology will contain a mandatory base release (should be in the official OBO recommendations as well). The ROBOT command generating the base artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONT)-base.owl: $(SRC) $(OTHER_SRC) $(ROBOT) remove --input $< --select imports --trim false \\ merge $(patsubst %, -i %, $(OTHER_SRC)) \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@","title":"Release artefact 1: base (required)"},{"location":"reference/release-artefacts/#release-artefact-2-full-required","text":"The full release artefact contains all logical axioms, including inferred subsumptions. Redundancy stripping (i.e. redundant subclass of axioms) and typical relaxation operations are performed. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. Every OBO ontology will contain a mandatory full release. The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONT)-full.owl: $(SRC) $(OTHER_SRC) $(ROBOT) merge --input $< \\ reason --reasoner ELK \\ relax \\ reduce -r ELK \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@","title":"Release artefact 2: full (required)"},{"location":"reference/release-artefacts/#release-artefact-3-non-classified-optional","text":"The non-classified release artefact reflects the 'unmodified state' of the editors file at release time. No operations are performed that modify the axioms in any way, in particular no redundancy stripping. As opposed to the base artefact, both component and imported ontologies are merged into the non-classified release. The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONT)-non-classified.owl: $(SRC) $(OTHER_SRC) $(ROBOT) merge --input $< \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@","title":"Release artefact 3: non-classified (optional)"},{"location":"reference/release-artefacts/#release-artefact-4-simple-optional","text":"Many users want a release that can be treated as a simple existential graph of the terms defined in an ontology. This corresponds to the state of OBO ontologies before logical definitions and imports. For example, the only logical axioms in -simple release of CL will contain be of the form CL1 subClassOf CL2 or CL1 subClassOf R some CL3 where R is any objectProperty and CLn is a CL class. This role has be fulfilled by the -simple artefact, which up to now has been supported by OORT. To construct this, we first need to assert inferred classifications, relax equivalentClass axioms to sets of subClassOf axioms and then strip all axioms referencing foreign (imported) classes. As ontologies occasionally end up with forieign classes and axioms merged into the editors file, we achieve this will a filter based on obo-namespace. (e.g. finding all terms with iri matching http://purl.obolibrary.org/obo/CL_{\\d}7). The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(SIMPLESEED): all terms that 'belong' to the ontology $(ROBOT) merge --input $< $(patsubst %, -i %, $(OTHER_SRC)) \\ reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} \\ relax \\ remove --axioms equivalent \\ relax \\ filter --term-file $(SIMPLESEED) --select \"annotations ontology anonymous self\" --trim true --signature true \\ reduce -r {{ project.reasoner }} \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@.tmp.owl && mv $@.tmp.owl $@ NOTES: This requires $(ONTOLOGYTERMS) to include all ObjectProperties usesd. --select parents is required for logical axioms to be retained, but results in a few upper-level classes bleeding through. We hope this will be fixed by further improvments to Monarch.","title":"Release artefact 4: simple (optional)"},{"location":"reference/release-artefacts/#release-artefact-5-basic","text":"Some legacy users (e.g. MGI) require an OBO DAG version of -simple. OBO files derived from OWL are not guarenteed to be acyclic, but acyclic graphs can be achieved using judicious filtering of relationships (simple existential restrictions) by objectProperty. The -basic release artefact has historically fulfilled this function as part of OORT driven ontology releases. The default -basic version corresponds to the -simple artefact with only 'part of' relationships (BFO:0000050), but others may be added where ontology editors judge these to be useful and safe to add without adding cycles. We generate by taking the simple release and filtering it The ROBOT command generating the full artefact: $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(KEEPRELATIONS): all relations that should be preserved. $(SIMPLESEED): all terms that 'belong' to the ontology $(ROBOT) merge --input $< $(patsubst %, -i %, $(OTHER_SRC)) \\ reason --reasoner {{ project.reasoner }} --equivalent-classes-allowed {{ project.allow_equivalents }} \\ relax \\ remove --axioms equivalent \\ remove --axioms disjoint \\ remove --term-file $(KEEPRELATIONS) --select complement --select object-properties --trim true \\ relax \\ filter --term-file $(SIMPLESEED) --select \"annotations ontology anonymous self\" --trim true --signature true \\ reduce -r {{ project.reasoner }} \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@ --output $@.tmp.owl && mv $@.tmp.owl $@","title":"Release artefact 5: basic"},{"location":"reference/release-artefacts/#release-artefact-6-simple-non-classified-optional","text":"This artefact caters to the very special and hopefully transient case of some ontologies that do not yet trust reasoning (MP, HP). The simple-non-classified artefact corresponds to the simple artefact, just without the reasoning step. $(SRC): source ontology $(OTHER_SRC): set of component ontologies $(ONTOLOGYTERMS): all terms that 'belong' to the ontology $(ONT)-simple-non-classified.owl: $(SRC) $(OTHER_SRC) $(ONTOLOGYTERMS) $(ROBOT) remove --input $< --select imports \\ merge $(patsubst %, -i %, $(OTHER_SRC)) \\ relax \\ reduce -r ELK \\ filter --term-file $(ONTOLOGYTERMS) --trim true \\ annotate --ontology-iri $(ONTBASE)/$@ --version-iri $(ONTBASE)/releases/$(TODAY)/$@","title":"Release artefact 6: simple-non-classified (optional)"},{"location":"reference/semantic-engineering-toolbox/","text":"The Semantic OBO Engineer's Toolbox \u00b6 Essentials Prot\u00e9g\u00e9 DL Query Tab ROBOT OBO Dashboard : OBO-wide quality control monitor for OBO ontologies. Automation GNU Make Ontology Development Kit (ODK) DROID : DROID is a web-based interface for working with make , managed by git . Text editors: Kakoune text/code editor Sublime Atom SPARQL query tool: Yasgui ROBOT query SPARQL endpoints Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies Ubergraph SPARQL endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Templating systems DOSDP with DOSDP Tools ROBOT template Ontology Mappings SSSOM and sssom-py : Toolkit and framework for managing mappings across and beyond ontologies. AgreementMakerLight (AML) : Matching tool for ontologies RDF Matcher : Experimental SSSOM based matching tool LogMap : Matching tool for ontologies Where to find ontologies and terms: Term browsers and ontology repositories OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. In practice, there is no particular reason why you would favour Ontobee over OLS for example - I just sometimes prefer the way Ontobee presents annotations and \"uses\" by other ontologies, so I use both. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. OBO Foundry Ontology Library . The OBO Foundry works with other repositories and term browsers such as OLS, Ontobee and BioPortal. For example, OLS directly reads the OBO Foundry registry metadata, and automatically loads new ontologies added to the OBO Foundry Ontology Library. BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Ontology visualisation OBO Graphviz : Library to visualise ontologies in beautifully readable graphics based on Dot . Nico's top 10 tools for the Semantic OBO Engineer's Toolbox \u00b6 ROBOT Prot\u00e9g\u00e9 Term browsers ( OLS , Ontobee ) Ontology Development Kit (ODK) SPARQL (e.g. ROBOT query and Yasgui ) GNU Make Text editor workflows (i.e. Atom , Sublime, VIM): a bit of regex Basic Shell scripting and pipelining From tables to ontologies: DOSDP templates and ROBOT templates GitHub Actions Other tools in my toolbox These are a bit less essential than the above, but I consider them still tremendously useful. Cogs (experimental) for automatically synchronising your spreadsheets with Google Sheets. Basic Dockerfile development : This can help you automate processes that go beyond usual ODK day-to-day business, such as automated mapping tools, graph machine learning, NLP etc. GitHub community management and git version control: Learn how to effectively manage your contributors, issue requests and code reviews. Also get your git commands straight - these can be life savers! Basics in python scripting : This is always useful, and python is our go-to language for most of automation nowadays - this used to be Java. Most of the Java heavy lifting is done in ROBOT now! SSSOM and sssom-py : Toolkit and framework for managing mappings between ontologies. DROID : DROID is a web-based interface for working with make , managed by git . OBO Dashboard : OBO-wide quality control monitor for OBO ontologies.","title":"Semantic Engineering Toolbox"},{"location":"reference/semantic-engineering-toolbox/#the-semantic-obo-engineers-toolbox","text":"Essentials Prot\u00e9g\u00e9 DL Query Tab ROBOT OBO Dashboard : OBO-wide quality control monitor for OBO ontologies. Automation GNU Make Ontology Development Kit (ODK) DROID : DROID is a web-based interface for working with make , managed by git . Text editors: Kakoune text/code editor Sublime Atom SPARQL query tool: Yasgui ROBOT query SPARQL endpoints Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies Ubergraph SPARQL endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Templating systems DOSDP with DOSDP Tools ROBOT template Ontology Mappings SSSOM and sssom-py : Toolkit and framework for managing mappings across and beyond ontologies. AgreementMakerLight (AML) : Matching tool for ontologies RDF Matcher : Experimental SSSOM based matching tool LogMap : Matching tool for ontologies Where to find ontologies and terms: Term browsers and ontology repositories OLS : The boss of the current term browsers out there. While the code base is a bit dated, it still gives access to a wide range of relevant open biomedical ontology terms. Note, while being a bit painful, it is possible to set up your own OLS (for your organisation) which only contains those terms/ontologies that are relevant for your work. Ontobee : The default term browser for OBO term purls. For example, click on http://purl.obolibrary.org/obo/OBI_0000070. This will redirect you directly to Ontobee, to show you the terms location in the hierarchy. In practice, there is no particular reason why you would favour Ontobee over OLS for example - I just sometimes prefer the way Ontobee presents annotations and \"uses\" by other ontologies, so I use both. AberOWL : Another ontology repository and semantic search engine. Some ontologies such as PhenomeNet can only be found on AberOWL, however, I personally prefer OLS. identifiers.org : A centralised registry for identifiers used in the life sciences. This is one of the tools that bridge the gap between CURIEs and URLs, but it does not cover (OBO) ontologies very well, and if so, is not aware of the proper URI prefixes (see for example here , and HP term resolution that does not list the proper persistent URL of the HP identifier (http://purl.obolibrary.org/obo/HP_0000001)). Identifiers.org has mainly good coverage for databases/resources that use CURIE type identifiers. But: you can enter any ID you find in your data and it will tell you what it is associated with. OBO Foundry Ontology Library . The OBO Foundry works with other repositories and term browsers such as OLS, Ontobee and BioPortal. For example, OLS directly reads the OBO Foundry registry metadata, and automatically loads new ontologies added to the OBO Foundry Ontology Library. BioPortal CPT Story . The Current Procedural Terminology was the by far most highly accessed Terminology on Bioportal - for many years. Due to license concerns, it had to be withdrawn from the repository. This story serves a cautionary tale of using terminologies with non-open or non-transparent licensing schemes. AgroPortal : Like BioPortal, but focussed on the Agronomy domain. Linked Open Data Vocabularies (LOV) : Lists the most important vocabularies in the Linked Data space, such as Dublin Core , SKOS and Friend-of-a-Friend (FOAF). Ontology visualisation OBO Graphviz : Library to visualise ontologies in beautifully readable graphics based on Dot .","title":"The Semantic OBO Engineer's Toolbox"},{"location":"reference/semantic-engineering-toolbox/#nicos-top-10-tools-for-the-semantic-obo-engineers-toolbox","text":"ROBOT Prot\u00e9g\u00e9 Term browsers ( OLS , Ontobee ) Ontology Development Kit (ODK) SPARQL (e.g. ROBOT query and Yasgui ) GNU Make Text editor workflows (i.e. Atom , Sublime, VIM): a bit of regex Basic Shell scripting and pipelining From tables to ontologies: DOSDP templates and ROBOT templates GitHub Actions Other tools in my toolbox These are a bit less essential than the above, but I consider them still tremendously useful. Cogs (experimental) for automatically synchronising your spreadsheets with Google Sheets. Basic Dockerfile development : This can help you automate processes that go beyond usual ODK day-to-day business, such as automated mapping tools, graph machine learning, NLP etc. GitHub community management and git version control: Learn how to effectively manage your contributors, issue requests and code reviews. Also get your git commands straight - these can be life savers! Basics in python scripting : This is always useful, and python is our go-to language for most of automation nowadays - this used to be Java. Most of the Java heavy lifting is done in ROBOT now! SSSOM and sssom-py : Toolkit and framework for managing mappings between ontologies. DROID : DROID is a web-based interface for working with make , managed by git . OBO Dashboard : OBO-wide quality control monitor for OBO ontologies.","title":"Nico's top 10 tools for the Semantic OBO Engineer's Toolbox"},{"location":"reference/sparql-basics/","text":"Basic SPARQL commands useful for OBO Engineers \u00b6 Basic SELECT query \u00b6 A basic SELECT query contains a set of prefixes, a SELECT clause and a WHERE clause. PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?term ?value WHERE { ?term rdfs:label ?value . } Prefixes \u00b6 Prefixes allow you to specify shortcuts. For example, instead of using the prefixes above, you could have simply said: SELECT ?term ?value WHERE { ?term <http://www.w3.org/2000/01/rdf-schema#label> ?value . } Without the prefix. It means the exact same thing. But it looks nicer. Some people even go as far as adding entire entities into the prefix header: PREFIX label: <http://www.w3.org/2000/01/rdf-schema#label> SELECT ?term ?value WHERE { ?term label: ?value . } This query is, again, the same as the ones above, but even more concise. SELECT clause \u00b6 The SELECT clause defines what you part of you query you want to show, for example, as a table. SELECT ?term ?value means: \"return\" or \"show\" whatever you find for the variable ?term and the variable ?value . There are other cool things you can do in the SELECT clause: Maths. You can count.","title":"SPARQL basics"},{"location":"reference/sparql-basics/#basic-sparql-commands-useful-for-obo-engineers","text":"","title":"Basic SPARQL commands useful for OBO Engineers"},{"location":"reference/sparql-basics/#basic-select-query","text":"A basic SELECT query contains a set of prefixes, a SELECT clause and a WHERE clause. PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?term ?value WHERE { ?term rdfs:label ?value . }","title":"Basic SELECT query"},{"location":"reference/sparql-basics/#prefixes","text":"Prefixes allow you to specify shortcuts. For example, instead of using the prefixes above, you could have simply said: SELECT ?term ?value WHERE { ?term <http://www.w3.org/2000/01/rdf-schema#label> ?value . } Without the prefix. It means the exact same thing. But it looks nicer. Some people even go as far as adding entire entities into the prefix header: PREFIX label: <http://www.w3.org/2000/01/rdf-schema#label> SELECT ?term ?value WHERE { ?term label: ?value . } This query is, again, the same as the ones above, but even more concise.","title":"Prefixes"},{"location":"reference/sparql-basics/#select-clause","text":"The SELECT clause defines what you part of you query you want to show, for example, as a table. SELECT ?term ?value means: \"return\" or \"show\" whatever you find for the variable ?term and the variable ?value . There are other cool things you can do in the SELECT clause: Maths. You can count.","title":"SELECT clause"},{"location":"reference/sparql-reference/","text":"Reference templates for SPARQL queries \u00b6 This document contains template SPARQL queries that can be adapted. Comments are added in-code with # above each step to explain them so that queries can be spliced together Checks/Report generation \u00b6 All terms native to ontology \u00b6 note: we assume that all native terms here have the same namespace - that of the ontology # select unique instances of the variable SELECT DISTINCT ?term WHERE { # selecting where the variable term is either used as a subject or object { ?s1 ?p1 ?term . } UNION { ?term ?p2 ?o2 . } # filtering out only terms that have the MONDO namespace (assumed to be native terms) FILTER(isIRI(?term) && (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/MONDO_\"))) } Definition lacks xref \u00b6 adaptable for lacking particular annotation # adding prefixes used prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix definition: <http://purl.obolibrary.org/obo/IAO_0000115> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT ?entity ?property ?value WHERE { # the variable property has to be defintion (IAO:0000115) VALUES ?property { definition: } # defining the order of variables in the triple ?entity ?property ?value . # selecting annotation on definition ?def_anno a owl:Axiom ; owl:annotatedSource ?entity ; owl:annotatedProperty definition: ; owl:annotatedTarget ?value . # filters out definitions which do not have a dbxref annotiton FILTER NOT EXISTS { ?def_anno oboInOwl:hasDbXref ?x . } # removes triples where entity is blank FILTER (!isBlank(?entity)) # selects entities that are native to ontology (in this case MONDO) FILTER (isIRI(?entity) && STRSTARTS(str(?entity), \"http://purl.obolibrary.org/obo/MONDO_\")) } # arrange report by entity variable ORDER BY ?entity Checks wether definitions contain underscore characters \u00b6 adaptable for checking if there is particular character in annotation # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> prefix IAO: <http://purl.obolibrary.org/obo/IAO_> prefix definition: <http://purl.obolibrary.org/obo/IAO_0000115> # selecting only unique instances of the three variables SELECT DISTINCT ?entity ?property ?value WHERE { # the variable property has to be definition (IAO:0000115) VALUES ?property { definition: } # defining the order of variables in the triple ?entity ?property ?value . # filtering out triples where the variable value has _ in it FILTER( regex(STR(?value), \"_\")) # removes triples where entity is blank FILTER (!isBlank(?entity)) } # arrange report by entity variable ORDER BY ?entity Only allowing a fix set of annotation properties \u00b6 # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix IAO: <http://purl.obolibrary.org/obo/IAO_> prefix RO: <http://purl.obolibrary.org/obo/RO_> prefix mondo: <http://purl.obolibrary.org/obo/mondo#> prefix skos: <http://www.w3.org/2004/02/skos/core#> prefix dce: <http://purl.org/dc/elements/1.1/> prefix dcterms: <http://purl.org/dc/terms/> # selecting only unique instances of the three variables SELECT DISTINCT ?term ?property ?value WHERE { # order of the variables in the triple ?term ?property ?value . # the variable property is an annotation property ?property a owl:AnnotationProperty . # selects entities that are native to ontology (in this case MONDO) FILTER (isIRI(?term) && regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\")) # removes triples where the variable value is blank FILTER(!isBlank(?value)) # listing the allowed annotation properties FILTER (?property NOT IN (dce:creator, dce:date, IAO:0000115, IAO:0000231, IAO:0100001, mondo:excluded_subClassOf, mondo:excluded_from_qc_check, mondo:excluded_synonym, mondo:pathogenesis, mondo:related, mondo:confidence, dcterms:conformsTo, mondo:should_conform_to, oboInOwl:consider, oboInOwl:created_by, oboInOwl:creation_date, oboInOwl:hasAlternativeId, oboInOwl:hasBroadSynonym, oboInOwl:hasDbXref, oboInOwl:hasExactSynonym, oboInOwl:hasNarrowSynonym, oboInOwl:hasRelatedSynonym, oboInOwl:id, oboInOwl:inSubset, owl:deprecated, rdfs:comment, rdfs:isDefinedBy, rdfs:label, rdfs:seeAlso, RO:0002161, skos:broadMatch, skos:closeMatch, skos:exactMatch, skos:narrowMatch)) } Checking for misused replaced_by \u00b6 adaptable for checking that a property is used in a certain way # adding prefixes used PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> PREFIX replacedBy: <http://purl.obolibrary.org/obo/IAO_0100001> # selecting only unique instances of the three variables SELECT DISTINCT ?entity ?property ?value WHERE { # the variable property is IAO_0100001 (item replaced by) VALUES ?property { replacedBy: } # order of the variables in the triple ?entity ?property ?value . # removing entities that have either owl:deprecated true or oboInOwl:ObsoleteClass (these entities are the only ones that should have replaced_by) FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (?entity != oboInOwl:ObsoleteClass) } # arrange report by entity variable ORDER BY ?entity Count \u00b6 Count class by prefixes \u00b6 # this query counts the number of classes you have with each prefix (eg number of MONDO terms, CL terms, etc.) # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix obo: <http://purl.obolibrary.org/obo/> # selecting 2 variables, prefix and numberOfClasses, where number of classes is a count of distinct cls SELECT ?prefix (COUNT(DISTINCT ?cls) AS ?numberOfClasses) WHERE { # the variable cls is a class ?cls a owl:Class . # removes any cases where the variable cls is blank FILTER (!isBlank(?cls)) # Binds the variable prefix as the prefix of the class (eg. MONDO, CL, etc.). classes that do not have obo purls will come out as blank in the report. BIND( STRBEFORE(STRAFTER(str(?cls),\"http://purl.obolibrary.org/obo/\"), \"_\") AS ?prefix) } # grouping the count by prefix GROUP BY ?prefix Counting subclasses in a namespace \u00b6 # this query counts the number of classes that are subclass of CL:0000003 (native cell) that are in the pcl namespace # adding prefixes used PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX CL: <http://purl.obolibrary.org/obo/CL_> PREFIX PCL: <http://purl.obolibrary.org/obo/PCL_> # count the number of unique term SELECT (COUNT (DISTINCT ?term) as ?pclcells) WHERE { # the variable term is a class ?term a owl:Class . # the variable term has to be a subclass of CL:0000003, including those that are subclassof by property path ?term rdfs:subClassOf* CL:0000003 # only count the term if it is in the pcl namespace FILTER(isIRI(?term) && (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/PCL_\"))) } Removing \u00b6 Removes all RO terms \u00b6 adaptable for removing all terms of a particular namespace # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> # removing triples DELETE { ?s ?p ?o } WHERE { { # the variable p must be a rdfs:label VALUES ?p { rdfs:label } # the variable s is an object property ?s a owl:ObjectProperty ; # the other variables can be anything else (note the above value restriction of p) ?p ?o # filter out triples where ?s starts with \"http://purl.obolibrary.org/obo/RO_\" FILTER (isIRI(?s) && STRSTARTS(str(?s), \"http://purl.obolibrary.org/obo/RO_\")) } } Deleting axiom annotations by prefix \u00b6 # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> # delete triples DELETE { ?anno ?property ?value . } WHERE { # the variable property is either synonym_type: or source: VALUES ?property { synonym_type: source: } # structure of variable value and variable anno ?anno a owl:Axiom ; owl:annotatedSource ?s ; owl:annotatedProperty ?p ; owl:annotatedTarget ?o ; ?property ?value . # filter out the variable value which start with \"ICD10EXP:\" FILTER(STRSTARTS(STR(?value),\"ICD10EXP:\")) } Replacing \u00b6 Replace oboInOwl:source with oboInOwl:hasDbXref in synonyms annotations \u00b6 adaptable for replacing annotations properties on particular axioms # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> # delete triples where the relation is oboInOwl:source DELETE { ?ax oboInOwl:source ?source . } # insert triples where the variables ax and source defined above are used, but using oboInOwl:hasDbXref instead INSERT { ?ax oboInOwl:hasDbXref ?source . } WHERE { # restricting to triples where the property variable is in this list VALUES ?property { oboInOwl:hasExactSynonym oboInOwl:hasNarrowSynonym oboInOwl:hasBroadSynonym oboInOwl:hasCloseSynonym oboInOwl:hasRelatedSynonym } . # order of the variables in the triple ?entity ?property ?value . # structure on which the variable ax and source applies ?ax rdf:type owl:Axiom ; owl:annotatedSource ?entity ; owl:annotatedTarget ?value ; owl:annotatedProperty ?property ; oboInOwl:source ?source . # filtering out triples where entity is an IRI FILTER (isIRI(?entity)) }","title":"SPARQL templates"},{"location":"reference/sparql-reference/#reference-templates-for-sparql-queries","text":"This document contains template SPARQL queries that can be adapted. Comments are added in-code with # above each step to explain them so that queries can be spliced together","title":"Reference templates for SPARQL queries"},{"location":"reference/sparql-reference/#checksreport-generation","text":"","title":"Checks/Report generation"},{"location":"reference/sparql-reference/#all-terms-native-to-ontology","text":"note: we assume that all native terms here have the same namespace - that of the ontology # select unique instances of the variable SELECT DISTINCT ?term WHERE { # selecting where the variable term is either used as a subject or object { ?s1 ?p1 ?term . } UNION { ?term ?p2 ?o2 . } # filtering out only terms that have the MONDO namespace (assumed to be native terms) FILTER(isIRI(?term) && (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/MONDO_\"))) }","title":"All terms native to ontology"},{"location":"reference/sparql-reference/#definition-lacks-xref","text":"adaptable for lacking particular annotation # adding prefixes used prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix definition: <http://purl.obolibrary.org/obo/IAO_0000115> prefix owl: <http://www.w3.org/2002/07/owl#> SELECT ?entity ?property ?value WHERE { # the variable property has to be defintion (IAO:0000115) VALUES ?property { definition: } # defining the order of variables in the triple ?entity ?property ?value . # selecting annotation on definition ?def_anno a owl:Axiom ; owl:annotatedSource ?entity ; owl:annotatedProperty definition: ; owl:annotatedTarget ?value . # filters out definitions which do not have a dbxref annotiton FILTER NOT EXISTS { ?def_anno oboInOwl:hasDbXref ?x . } # removes triples where entity is blank FILTER (!isBlank(?entity)) # selects entities that are native to ontology (in this case MONDO) FILTER (isIRI(?entity) && STRSTARTS(str(?entity), \"http://purl.obolibrary.org/obo/MONDO_\")) } # arrange report by entity variable ORDER BY ?entity","title":"Definition lacks xref"},{"location":"reference/sparql-reference/#checks-wether-definitions-contain-underscore-characters","text":"adaptable for checking if there is particular character in annotation # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> prefix IAO: <http://purl.obolibrary.org/obo/IAO_> prefix definition: <http://purl.obolibrary.org/obo/IAO_0000115> # selecting only unique instances of the three variables SELECT DISTINCT ?entity ?property ?value WHERE { # the variable property has to be definition (IAO:0000115) VALUES ?property { definition: } # defining the order of variables in the triple ?entity ?property ?value . # filtering out triples where the variable value has _ in it FILTER( regex(STR(?value), \"_\")) # removes triples where entity is blank FILTER (!isBlank(?entity)) } # arrange report by entity variable ORDER BY ?entity","title":"Checks wether definitions contain underscore characters"},{"location":"reference/sparql-reference/#only-allowing-a-fix-set-of-annotation-properties","text":"# adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix IAO: <http://purl.obolibrary.org/obo/IAO_> prefix RO: <http://purl.obolibrary.org/obo/RO_> prefix mondo: <http://purl.obolibrary.org/obo/mondo#> prefix skos: <http://www.w3.org/2004/02/skos/core#> prefix dce: <http://purl.org/dc/elements/1.1/> prefix dcterms: <http://purl.org/dc/terms/> # selecting only unique instances of the three variables SELECT DISTINCT ?term ?property ?value WHERE { # order of the variables in the triple ?term ?property ?value . # the variable property is an annotation property ?property a owl:AnnotationProperty . # selects entities that are native to ontology (in this case MONDO) FILTER (isIRI(?term) && regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\")) # removes triples where the variable value is blank FILTER(!isBlank(?value)) # listing the allowed annotation properties FILTER (?property NOT IN (dce:creator, dce:date, IAO:0000115, IAO:0000231, IAO:0100001, mondo:excluded_subClassOf, mondo:excluded_from_qc_check, mondo:excluded_synonym, mondo:pathogenesis, mondo:related, mondo:confidence, dcterms:conformsTo, mondo:should_conform_to, oboInOwl:consider, oboInOwl:created_by, oboInOwl:creation_date, oboInOwl:hasAlternativeId, oboInOwl:hasBroadSynonym, oboInOwl:hasDbXref, oboInOwl:hasExactSynonym, oboInOwl:hasNarrowSynonym, oboInOwl:hasRelatedSynonym, oboInOwl:id, oboInOwl:inSubset, owl:deprecated, rdfs:comment, rdfs:isDefinedBy, rdfs:label, rdfs:seeAlso, RO:0002161, skos:broadMatch, skos:closeMatch, skos:exactMatch, skos:narrowMatch)) }","title":"Only allowing a fix set of annotation properties"},{"location":"reference/sparql-reference/#checking-for-misused-replaced_by","text":"adaptable for checking that a property is used in a certain way # adding prefixes used PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> PREFIX replacedBy: <http://purl.obolibrary.org/obo/IAO_0100001> # selecting only unique instances of the three variables SELECT DISTINCT ?entity ?property ?value WHERE { # the variable property is IAO_0100001 (item replaced by) VALUES ?property { replacedBy: } # order of the variables in the triple ?entity ?property ?value . # removing entities that have either owl:deprecated true or oboInOwl:ObsoleteClass (these entities are the only ones that should have replaced_by) FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (?entity != oboInOwl:ObsoleteClass) } # arrange report by entity variable ORDER BY ?entity","title":"Checking for misused replaced_by"},{"location":"reference/sparql-reference/#count","text":"","title":"Count"},{"location":"reference/sparql-reference/#count-class-by-prefixes","text":"# this query counts the number of classes you have with each prefix (eg number of MONDO terms, CL terms, etc.) # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix obo: <http://purl.obolibrary.org/obo/> # selecting 2 variables, prefix and numberOfClasses, where number of classes is a count of distinct cls SELECT ?prefix (COUNT(DISTINCT ?cls) AS ?numberOfClasses) WHERE { # the variable cls is a class ?cls a owl:Class . # removes any cases where the variable cls is blank FILTER (!isBlank(?cls)) # Binds the variable prefix as the prefix of the class (eg. MONDO, CL, etc.). classes that do not have obo purls will come out as blank in the report. BIND( STRBEFORE(STRAFTER(str(?cls),\"http://purl.obolibrary.org/obo/\"), \"_\") AS ?prefix) } # grouping the count by prefix GROUP BY ?prefix","title":"Count class by prefixes"},{"location":"reference/sparql-reference/#counting-subclasses-in-a-namespace","text":"# this query counts the number of classes that are subclass of CL:0000003 (native cell) that are in the pcl namespace # adding prefixes used PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX CL: <http://purl.obolibrary.org/obo/CL_> PREFIX PCL: <http://purl.obolibrary.org/obo/PCL_> # count the number of unique term SELECT (COUNT (DISTINCT ?term) as ?pclcells) WHERE { # the variable term is a class ?term a owl:Class . # the variable term has to be a subclass of CL:0000003, including those that are subclassof by property path ?term rdfs:subClassOf* CL:0000003 # only count the term if it is in the pcl namespace FILTER(isIRI(?term) && (STRSTARTS(str(?term), \"http://purl.obolibrary.org/obo/PCL_\"))) }","title":"Counting subclasses in a namespace"},{"location":"reference/sparql-reference/#removing","text":"","title":"Removing"},{"location":"reference/sparql-reference/#removes-all-ro-terms","text":"adaptable for removing all terms of a particular namespace # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> # removing triples DELETE { ?s ?p ?o } WHERE { { # the variable p must be a rdfs:label VALUES ?p { rdfs:label } # the variable s is an object property ?s a owl:ObjectProperty ; # the other variables can be anything else (note the above value restriction of p) ?p ?o # filter out triples where ?s starts with \"http://purl.obolibrary.org/obo/RO_\" FILTER (isIRI(?s) && STRSTARTS(str(?s), \"http://purl.obolibrary.org/obo/RO_\")) } }","title":"Removes all RO terms"},{"location":"reference/sparql-reference/#deleting-axiom-annotations-by-prefix","text":"# adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> # delete triples DELETE { ?anno ?property ?value . } WHERE { # the variable property is either synonym_type: or source: VALUES ?property { synonym_type: source: } # structure of variable value and variable anno ?anno a owl:Axiom ; owl:annotatedSource ?s ; owl:annotatedProperty ?p ; owl:annotatedTarget ?o ; ?property ?value . # filter out the variable value which start with \"ICD10EXP:\" FILTER(STRSTARTS(STR(?value),\"ICD10EXP:\")) }","title":"Deleting axiom annotations by prefix"},{"location":"reference/sparql-reference/#replacing","text":"","title":"Replacing"},{"location":"reference/sparql-reference/#replace-oboinowlsource-with-oboinowlhasdbxref-in-synonyms-annotations","text":"adaptable for replacing annotations properties on particular axioms # adding prefixes used prefix owl: <http://www.w3.org/2002/07/owl#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> # delete triples where the relation is oboInOwl:source DELETE { ?ax oboInOwl:source ?source . } # insert triples where the variables ax and source defined above are used, but using oboInOwl:hasDbXref instead INSERT { ?ax oboInOwl:hasDbXref ?source . } WHERE { # restricting to triples where the property variable is in this list VALUES ?property { oboInOwl:hasExactSynonym oboInOwl:hasNarrowSynonym oboInOwl:hasBroadSynonym oboInOwl:hasCloseSynonym oboInOwl:hasRelatedSynonym } . # order of the variables in the triple ?entity ?property ?value . # structure on which the variable ax and source applies ?ax rdf:type owl:Axiom ; owl:annotatedSource ?entity ; owl:annotatedTarget ?value ; owl:annotatedProperty ?property ; oboInOwl:source ?source . # filtering out triples where entity is an IRI FILTER (isIRI(?entity)) }","title":"Replace oboInOwl:source with oboInOwl:hasDbXref in synonyms annotations"},{"location":"reference/synonyms-obo/","text":"Synonyms in OBO \u00b6 A synonym indicates an alternative name for a term. Terms can have multiple synonyms. The scope of a synonym may fall into one of four categories: \u00b6 Exact \u00b6 The definition of the synonym is exactly the same as primary term definition. This is used when the same class can have more than one name. For example, hereditary Wilms' tumor has the exact synonoym familial Wilms' tumor. Additionally, translations into other languages are listed as exact synonyms. For example, the Plant Ontology list both Spanish and Japanese translations as exact synonyms; e.g. anther wall has exact synonym \u2018pared de la antera\u2019 (Spanish) and \u2018\u846f\u58c1 \u2018(Japanese). Narrow \u00b6 The definition of the synonym is the same as the primary definition, but has additional qualifiers. For example, pod is a narrow synonym of fruit. Note - when adding a narrow synonym, please first consider whether a new subclass should be added instead of a narrow synonym. If there is any uncertainty, start a discussion on the GitHub issue tracker. Broad \u00b6 The primary definition accurately describes the synonym, but the definition of the synonym may encompass other structures as well. In some cases where a broad synonym is given, it will be a broad synonym for more than one ontology term. For example, Cyst of eyelid has the broad synonym Lesion of the eyelid. Note - when adding a broad synonym, please first consider whether a new superclass should be added instead of a broad synonym. If there is any uncertainty, start a discussion on the GitHub issue tracker. Related \u00b6 This scope is applied when a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct. That is, the synonym in fact has a slightly different meaning than the primary term name. Since users may not be aware that the synonym was being used incorrectly when searching for a term, related synonyms are included. For example, Autistic behavior has the related synonym Autism spectrum disorder. Synonym types \u00b6 Synonyms can also be classified by types. The default is no type. The synonym types vary in each ontology, but some commonly used synonym types include: abbreviation - to indicate the synonym is an abbreviation. Note the scope for an acronym should be determined on a case-by-case basis. Not all acronyms are necessarily exact. ambiguous - to indicate the synonym is open to more than one interpretation; may have a double meaning dubious synonym - to indicate the synonym may be suspect layperson term - to indicate the synonym is common language (used by the Human Phenotype Ontology) plural form - indicating the form of the term that means more than one UK spelling - the english language spelling that is used in the United Kingdom (UK) but not in the United States (US) Database cross references \u00b6 Whenever possible, database cross-references (dbxrefs) for synonyms should be provided, to indicate the publication that used the synonym. References to PubMed IDs should be in the format PMID:XXXXXXX (no space). However, dbxrefs for synonyms are not mandatory in most ontologies.","title":"Synonyms"},{"location":"reference/synonyms-obo/#synonyms-in-obo","text":"A synonym indicates an alternative name for a term. Terms can have multiple synonyms.","title":"Synonyms in OBO"},{"location":"reference/synonyms-obo/#the-scope-of-a-synonym-may-fall-into-one-of-four-categories","text":"","title":"The scope of a synonym may fall into one of four categories:"},{"location":"reference/synonyms-obo/#exact","text":"The definition of the synonym is exactly the same as primary term definition. This is used when the same class can have more than one name. For example, hereditary Wilms' tumor has the exact synonoym familial Wilms' tumor. Additionally, translations into other languages are listed as exact synonyms. For example, the Plant Ontology list both Spanish and Japanese translations as exact synonyms; e.g. anther wall has exact synonym \u2018pared de la antera\u2019 (Spanish) and \u2018\u846f\u58c1 \u2018(Japanese).","title":"Exact"},{"location":"reference/synonyms-obo/#narrow","text":"The definition of the synonym is the same as the primary definition, but has additional qualifiers. For example, pod is a narrow synonym of fruit. Note - when adding a narrow synonym, please first consider whether a new subclass should be added instead of a narrow synonym. If there is any uncertainty, start a discussion on the GitHub issue tracker.","title":"Narrow"},{"location":"reference/synonyms-obo/#broad","text":"The primary definition accurately describes the synonym, but the definition of the synonym may encompass other structures as well. In some cases where a broad synonym is given, it will be a broad synonym for more than one ontology term. For example, Cyst of eyelid has the broad synonym Lesion of the eyelid. Note - when adding a broad synonym, please first consider whether a new superclass should be added instead of a broad synonym. If there is any uncertainty, start a discussion on the GitHub issue tracker.","title":"Broad"},{"location":"reference/synonyms-obo/#related","text":"This scope is applied when a word of phrase has been used synonymously with the primary term name in the literature, but the usage is not strictly correct. That is, the synonym in fact has a slightly different meaning than the primary term name. Since users may not be aware that the synonym was being used incorrectly when searching for a term, related synonyms are included. For example, Autistic behavior has the related synonym Autism spectrum disorder.","title":"Related"},{"location":"reference/synonyms-obo/#synonym-types","text":"Synonyms can also be classified by types. The default is no type. The synonym types vary in each ontology, but some commonly used synonym types include: abbreviation - to indicate the synonym is an abbreviation. Note the scope for an acronym should be determined on a case-by-case basis. Not all acronyms are necessarily exact. ambiguous - to indicate the synonym is open to more than one interpretation; may have a double meaning dubious synonym - to indicate the synonym may be suspect layperson term - to indicate the synonym is common language (used by the Human Phenotype Ontology) plural form - indicating the form of the term that means more than one UK spelling - the english language spelling that is used in the United Kingdom (UK) but not in the United States (US)","title":"Synonym types"},{"location":"reference/synonyms-obo/#database-cross-references","text":"Whenever possible, database cross-references (dbxrefs) for synonyms should be provided, to indicate the publication that used the synonym. References to PubMed IDs should be in the format PMID:XXXXXXX (no space). However, dbxrefs for synonyms are not mandatory in most ontologies.","title":"Database cross references"},{"location":"reference/tables-and-triples/","text":"Tables and Triples \u00b6 Tables and triples seem very different. Tables are familiar and predictable. Triples are weird and floppy. SQL is normal, SPARQL is bizarre, at least at first. Tables are great, and they're the right tool for a lot of jobs, but they have their limitations. Triples shine when it comes to merging heterogeneous data. But it turns out that there's a clear path from tables to triples, which should help make RDF make more sense. Tables \u00b6 Tables are great! Here's a table! first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo You won't be surprised to find out that tables have rows and columns . Often each row corresponds to some thing that we want to talk about, such as a fictional character from Star Wars . Each column usually corresponds to some sort of property that those things might have. Then the cells contain the values of those properties for their respective row. We take some sort of complex information about the world, and we break it down along two dimensions: the things (rows) and their properties (columns). Primary Keys \u00b6 Tables are great! We can add another name to our table: first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo Anakin Skywalker Hmm. That's a perfectly good table, but it's not capturing the information that we wanted. It turns out ( Spoiler Alert! ) that Anakin Skywalker is Darth Vader! We might have thought that the rows of our table were describing individual people, but it turns out that they're just describing individual names. A person can change their name or have more than one name. We want some sort of identifier that lets us pick out the same person, and distinguish them from all the other people. Sometimes there's a \"natural key\" that we can use for this purpose: some bit of information that uniquely identifies a thing. When we don't have a natural key, we can generate an \"artificial key\". Random strings and number can be good artificial keys, but sometimes a simple incrementing integer is good enough. The main problem with artificial keys is that it's our job to maintain the link between the thing and the identifier that we gave it. We prefer natural keys because we just have to inspect that thing (in some way) to figure out what to call it. Even when it's possible, sometimes that's too much work. Maybe we could use a DNA sequence as a natural key for a person, but it probably isn't practical. We do use fingerprints and facial recognition, for similar things, though. (Do people in Star Wars even have DNA? Or just midichlorions?) Let's add a column with an artificial key to our table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker This is our table of names, allowing a given person to have multiple names. But what we thought we wanted was a person table with one row for each person, like this: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo In SQL we could assert that the \"sw_id\" column of the person table is a PRIMARY KEY. This means it must be unique. (It probably shouldn't be NULL either!) The names in the person table could be the primary names that we use in our Star Wars database system, and we could have another alternative_name table: sw_id first_name last_name 3 Anakin Skywalker Holes \u00b6 Tables are great! We can add more columns to our person table: sw_id first_name last_name occupation 1 Luke Skywalker Jedi 2 Leia Organa princess 3 Darth Vader 4 Han Solo scoundrel The 2D pattern of a table is a strong one. It not only provides a \"slot\" (cell) for every combination of row and column, it also makes it very obvious when one of those slots is empty . What does it mean for a slot to be empty? It could mean many things. For example, in the previous table in the row for Darth Vader, the cell for the \"occupation\" column is empty. This could mean that: we don't know whether he has an occupation we know that he has an occupation, but we don't know which occupation it is. we might know, but we haven't bothered to write it down yet we might know, but it doesn't fit nicely into the New Republic Standard Registry of Occupations; in other words, we know what his occupation is, but including it here would violate a constraint on our database we specifically know that he doesn't have an occupation; we triple-checked we know more generally ( Spoiler Alert!! ) that he's dead, and dead people can't have an occupation. I'm sure I haven't captured all the possibilities. The point is that there's lot of possible reasons why a cell would be blank. So what can we do about it? If our table is stored in a SQL database, then we have the option of putting a NULL value in the cell. NULL is pretty strange. It isn't TRUE and it isn't FALSE. Usually NULL values are excluded from SQL query results unless you are careful to ask for them. The way that NULL works in SQL eliminates some of the possibilities above. SQL uses the \"closed-world assumption\", which is the assumption that if a statement is true then it's known to be true, and conversely that if it's not known to be true then it's false. So if Anakin's occupation is NULL in a SQL database, then as far as SQL is concerned, we must know that he doesn't have an occupation. That might not be what you were expecting! The Software Carpentry module on Missing Data has more information. Multiple Values \u00b6 Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation enemy 1 Luke Skywalker Jedi 3 2 Leia Organa princess 3 3 Darth Vader 1,2,4 4 Han Solo scoundrel 3 We're trying to say that Darth Vader is the enemy of everybody else in our table. We're using the primary key of the person in the enemy column, which is good, but we've ended up with multiple values in the \"enemy\" column for Darth Vader. In any table or SQL database you could make the \"enemy\" column a string, pick a delimiter such as the comma, and concatenate your values into a comma-separated list. This works, but not very well. In some SQL databases, such as Postgres, you could given the \"enemy\" column an array type, so it can contain multiple values. You get special operators for querying inside arrays. This can work pretty well. The usual advice is to break this \"one to many\" information into a new \"enemy\" table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 Then you can JOIN the person table to the enemy table as needed. Sparse Tables \u00b6 Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation father lightsaber_color ship 1 Luke Skywalker Jedi 3 green 2 Leia Organa princess 3 3 Darth Vader red 4 Han Solo scoundrel Millennium Falcon A bunch of these columns only apply to a few rows. Now we've got a lot more NULLs to deal with. As the number of columns increases, this can become a problem. Property Tables \u00b6 Tables are great! If sparse tables are a problem, then let's try to apply the same solution that worked for the \"many to one\" problem in the previous section. name table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker occupation table: sw_id occupation 1 Jedi 2 princess 4 scoundrel enemy table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 father table: sw_id father 1 3 2 3 lightsaber_color table: sw_id lightsaber_color 1 green 3 red ship table: sw_id ship 4 Millennium Falcon Hmm. Yeah, that will work. But every query we write will need some JOINs. It feels like we've lost something. Entity, Attribute, Value \u00b6 Tables are great! But there's such a thing as too many tables. We started out with a table with a bunch of rows and a bunch of columns, and ended up with a bunch of tables with a bunch of rows but just a few columns. I have a brilliant idea! Let's combine all these property tables into just one table, by adding a \"property\" column! sw_id property value 1 first_name Luke 2 first_name Leia 3 first_name Darth 4 first_name Han 5 first_name Anakin 1 last_name Skywalker 2 last_name Skywalker 3 last_name Vader 4 last_name Solo 5 last_name Skywalker 1 occupation Jedi 2 occupation princess 4 occupation scoundrel 1 enemy 3 2 enemy 3 3 enemy 1 3 enemy 2 3 enemy 4 4 enemy 1 1 father 3 2 father 3 1 lightsaber_color green 3 lightsaber_color red 4 ship Millenium Falcon It turns out that I'm not the first one to think of this idea. People call it \"Entity, Attribute, Value\" or \"EAV\". People also call it an \"anti-pattern\", in other words: a clear sign that you've made a terrible mistake. There are lots of circumstances in which one big, extremely generic table is a bad idea. First of all, you can't do very much with the datatypes for the property and value columns. They kind of have to be strings. It's potentially difficult to index. And tables like this are miserable to query, because you end up with all sorts of self-joins to handle. But there's at least one use case where it turns out to work quite well... Merging Tables \u00b6 Tables are great! Until they're not. The strong row and column structure of tables makes them great for lots of things, but not so great for merging data from different sources. Before you can merge two tables you need to know all about: how the columns are structured what the rows mean what the cells mean So you need to know the schemas of the two tables before you can start merging them together. But if you happen to have two EAV tables then, as luck would have it, they already have the same schema! You also need to know that you're talking about the same things: the rows have to be about the same things, you need to be using the same property names for the same things, and the cell values also need to line up. If only there was an open standard for specifying globally unique identifiers... Yes, you guessed it: URLs (and URNs and URIs and IRIs)! Let's assume that we use the same URLs for the same things across the two tables. Since we're a close-knit community, we've come to an agreement on a Star Wars data vocabulary. URLs are annoyingly long to use in databases, so let's use standard \"sw\" prefix to shorten them. Now we have table 1: sw_id property value sw:1 sw:first_name Luke sw:2 sw:first_name Leia sw:3 sw:first_name Darth sw:4 sw:first_name Han sw:5 sw:first_name Anakin sw:1 sw:last_name Skywalker sw:2 sw:last_name Skywalker sw:3 sw:last_name Vader sw:4 sw:last_name Solo sw:5 sw:last_name Skywalker sw:1 sw:occupation sw:Jedi sw:2 sw:occupation sw:princess sw:4 sw:occupation sw:scoundrel and table 2: sw_id property value sw:1 sw:enemy sw:3 sw:2 sw:enemy sw:3 sw:3 sw:enemy sw:1 sw:3 sw:enemy sw:2 sw:3 sw:enemy sw:4 sw:4 sw:enemy sw:1 sw:1 sw:father sw:3 sw:2 sw:father sw:3 sw:1 sw:lightsaber_color green sw:3 sw:lightsaber_color red sw:4 sw:ship Millenium Falcon To merge these two tables, we simple concatenate them. It couldn't be simpler. Wait, this looks kinda familiar... RDF \u00b6 These tables are pretty much in RDF format. You just have to squint a little! sw_id == subject property == predicate value == object Each row of the table is a subject-predicate-object triple. Our subjects, predicates, and some objects are URLs. We also have some literal objects. We could turn this table directly into Turtle format with a little SQL magic (basically just concatenating strings): SELECT \"@prefix sw: <http://example.com/sw_> .\" UNION ALL SELECT \"\" UNION ALL SELECT sw_id || \" \" || property || \" \" || IF( INSTR(value, \":\"), value, -- CURIE \"\"\"\" || value || \"\"\"\" -- literal ) || \" .\" FROM triple_table; The first few lines will look like this: @prefix sw: <http://example.com/sw_> . sw:1 sw:first_name \"Luke\" . sw:2 sw:first_name \"Leia\" . sw:3 sw:first_name \"Darth\" . sw:4 sw:first_name \"Han\" . Two things we're missing from RDF are language tagged literals and typed literals. We also haven't used any blank nodes in our triple table. These are easy enough to add. The biggest thing that's different about RDF is that it uses the \"open-world assumption\", so something may be true even though we don't have a triple asserting that it's true. The open-world assumption is a better fit than the closed-world assumption when we're integrating data on the Web. Conclusion \u00b6 Tables are great! We use them all the time, they're strong and rigid, and we're comfortable with them. RDF, on the other hand, looks strange at first. For most common data processing, RDF is too flexible. But sometimes flexiblity is the most important thing. The greatest strength of tables is their rigid structure, but that's also their greatest weakness. We saw a number of problems with tables, and how they could be overcome by breaking tables apart into smaller tables, until we got down to the most basic pattern: subject-predicate-object. Step by step, we were pushed toward RDF. Merging tables is particularly painful. When working with data on the Web, merging is one of the most common and important operations, and so it makes sense to use RDF for these tasks. If self-joins with SQL is the worst problem for EAV tables, then SPARQL solves it. These examples show that it's not really very hard to convert tables to triples. And once you've seen SPARQL, the RDF query language, you've seen one good way to convert triples to tables: SPARQL SELECT results are just tables! Since it's straightforward to convert tables to triples and back again, make sure to use the right tool for the right job. When you need to merge heterogeneous data, reach for triples. For most other data processing tasks, use tables. They're great!","title":"Tables and Triples"},{"location":"reference/tables-and-triples/#tables-and-triples","text":"Tables and triples seem very different. Tables are familiar and predictable. Triples are weird and floppy. SQL is normal, SPARQL is bizarre, at least at first. Tables are great, and they're the right tool for a lot of jobs, but they have their limitations. Triples shine when it comes to merging heterogeneous data. But it turns out that there's a clear path from tables to triples, which should help make RDF make more sense.","title":"Tables and Triples"},{"location":"reference/tables-and-triples/#tables","text":"Tables are great! Here's a table! first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo You won't be surprised to find out that tables have rows and columns . Often each row corresponds to some thing that we want to talk about, such as a fictional character from Star Wars . Each column usually corresponds to some sort of property that those things might have. Then the cells contain the values of those properties for their respective row. We take some sort of complex information about the world, and we break it down along two dimensions: the things (rows) and their properties (columns).","title":"Tables"},{"location":"reference/tables-and-triples/#primary-keys","text":"Tables are great! We can add another name to our table: first_name last_name Luke Skywalker Leia Organa Darth Vader Han Solo Anakin Skywalker Hmm. That's a perfectly good table, but it's not capturing the information that we wanted. It turns out ( Spoiler Alert! ) that Anakin Skywalker is Darth Vader! We might have thought that the rows of our table were describing individual people, but it turns out that they're just describing individual names. A person can change their name or have more than one name. We want some sort of identifier that lets us pick out the same person, and distinguish them from all the other people. Sometimes there's a \"natural key\" that we can use for this purpose: some bit of information that uniquely identifies a thing. When we don't have a natural key, we can generate an \"artificial key\". Random strings and number can be good artificial keys, but sometimes a simple incrementing integer is good enough. The main problem with artificial keys is that it's our job to maintain the link between the thing and the identifier that we gave it. We prefer natural keys because we just have to inspect that thing (in some way) to figure out what to call it. Even when it's possible, sometimes that's too much work. Maybe we could use a DNA sequence as a natural key for a person, but it probably isn't practical. We do use fingerprints and facial recognition, for similar things, though. (Do people in Star Wars even have DNA? Or just midichlorions?) Let's add a column with an artificial key to our table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker This is our table of names, allowing a given person to have multiple names. But what we thought we wanted was a person table with one row for each person, like this: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo In SQL we could assert that the \"sw_id\" column of the person table is a PRIMARY KEY. This means it must be unique. (It probably shouldn't be NULL either!) The names in the person table could be the primary names that we use in our Star Wars database system, and we could have another alternative_name table: sw_id first_name last_name 3 Anakin Skywalker","title":"Primary Keys"},{"location":"reference/tables-and-triples/#holes","text":"Tables are great! We can add more columns to our person table: sw_id first_name last_name occupation 1 Luke Skywalker Jedi 2 Leia Organa princess 3 Darth Vader 4 Han Solo scoundrel The 2D pattern of a table is a strong one. It not only provides a \"slot\" (cell) for every combination of row and column, it also makes it very obvious when one of those slots is empty . What does it mean for a slot to be empty? It could mean many things. For example, in the previous table in the row for Darth Vader, the cell for the \"occupation\" column is empty. This could mean that: we don't know whether he has an occupation we know that he has an occupation, but we don't know which occupation it is. we might know, but we haven't bothered to write it down yet we might know, but it doesn't fit nicely into the New Republic Standard Registry of Occupations; in other words, we know what his occupation is, but including it here would violate a constraint on our database we specifically know that he doesn't have an occupation; we triple-checked we know more generally ( Spoiler Alert!! ) that he's dead, and dead people can't have an occupation. I'm sure I haven't captured all the possibilities. The point is that there's lot of possible reasons why a cell would be blank. So what can we do about it? If our table is stored in a SQL database, then we have the option of putting a NULL value in the cell. NULL is pretty strange. It isn't TRUE and it isn't FALSE. Usually NULL values are excluded from SQL query results unless you are careful to ask for them. The way that NULL works in SQL eliminates some of the possibilities above. SQL uses the \"closed-world assumption\", which is the assumption that if a statement is true then it's known to be true, and conversely that if it's not known to be true then it's false. So if Anakin's occupation is NULL in a SQL database, then as far as SQL is concerned, we must know that he doesn't have an occupation. That might not be what you were expecting! The Software Carpentry module on Missing Data has more information.","title":"Holes"},{"location":"reference/tables-and-triples/#multiple-values","text":"Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation enemy 1 Luke Skywalker Jedi 3 2 Leia Organa princess 3 3 Darth Vader 1,2,4 4 Han Solo scoundrel 3 We're trying to say that Darth Vader is the enemy of everybody else in our table. We're using the primary key of the person in the enemy column, which is good, but we've ended up with multiple values in the \"enemy\" column for Darth Vader. In any table or SQL database you could make the \"enemy\" column a string, pick a delimiter such as the comma, and concatenate your values into a comma-separated list. This works, but not very well. In some SQL databases, such as Postgres, you could given the \"enemy\" column an array type, so it can contain multiple values. You get special operators for querying inside arrays. This can work pretty well. The usual advice is to break this \"one to many\" information into a new \"enemy\" table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 Then you can JOIN the person table to the enemy table as needed.","title":"Multiple Values"},{"location":"reference/tables-and-triples/#sparse-tables","text":"Tables are great! Let's add even more information to our table: sw_id first_name last_name occupation father lightsaber_color ship 1 Luke Skywalker Jedi 3 green 2 Leia Organa princess 3 3 Darth Vader red 4 Han Solo scoundrel Millennium Falcon A bunch of these columns only apply to a few rows. Now we've got a lot more NULLs to deal with. As the number of columns increases, this can become a problem.","title":"Sparse Tables"},{"location":"reference/tables-and-triples/#property-tables","text":"Tables are great! If sparse tables are a problem, then let's try to apply the same solution that worked for the \"many to one\" problem in the previous section. name table: sw_id first_name last_name 1 Luke Skywalker 2 Leia Organa 3 Darth Vader 4 Han Solo 3 Anakin Skywalker occupation table: sw_id occupation 1 Jedi 2 princess 4 scoundrel enemy table: sw_id enemy 1 3 2 3 3 1 3 2 3 4 4 1 father table: sw_id father 1 3 2 3 lightsaber_color table: sw_id lightsaber_color 1 green 3 red ship table: sw_id ship 4 Millennium Falcon Hmm. Yeah, that will work. But every query we write will need some JOINs. It feels like we've lost something.","title":"Property Tables"},{"location":"reference/tables-and-triples/#entity-attribute-value","text":"Tables are great! But there's such a thing as too many tables. We started out with a table with a bunch of rows and a bunch of columns, and ended up with a bunch of tables with a bunch of rows but just a few columns. I have a brilliant idea! Let's combine all these property tables into just one table, by adding a \"property\" column! sw_id property value 1 first_name Luke 2 first_name Leia 3 first_name Darth 4 first_name Han 5 first_name Anakin 1 last_name Skywalker 2 last_name Skywalker 3 last_name Vader 4 last_name Solo 5 last_name Skywalker 1 occupation Jedi 2 occupation princess 4 occupation scoundrel 1 enemy 3 2 enemy 3 3 enemy 1 3 enemy 2 3 enemy 4 4 enemy 1 1 father 3 2 father 3 1 lightsaber_color green 3 lightsaber_color red 4 ship Millenium Falcon It turns out that I'm not the first one to think of this idea. People call it \"Entity, Attribute, Value\" or \"EAV\". People also call it an \"anti-pattern\", in other words: a clear sign that you've made a terrible mistake. There are lots of circumstances in which one big, extremely generic table is a bad idea. First of all, you can't do very much with the datatypes for the property and value columns. They kind of have to be strings. It's potentially difficult to index. And tables like this are miserable to query, because you end up with all sorts of self-joins to handle. But there's at least one use case where it turns out to work quite well...","title":"Entity, Attribute, Value"},{"location":"reference/tables-and-triples/#merging-tables","text":"Tables are great! Until they're not. The strong row and column structure of tables makes them great for lots of things, but not so great for merging data from different sources. Before you can merge two tables you need to know all about: how the columns are structured what the rows mean what the cells mean So you need to know the schemas of the two tables before you can start merging them together. But if you happen to have two EAV tables then, as luck would have it, they already have the same schema! You also need to know that you're talking about the same things: the rows have to be about the same things, you need to be using the same property names for the same things, and the cell values also need to line up. If only there was an open standard for specifying globally unique identifiers... Yes, you guessed it: URLs (and URNs and URIs and IRIs)! Let's assume that we use the same URLs for the same things across the two tables. Since we're a close-knit community, we've come to an agreement on a Star Wars data vocabulary. URLs are annoyingly long to use in databases, so let's use standard \"sw\" prefix to shorten them. Now we have table 1: sw_id property value sw:1 sw:first_name Luke sw:2 sw:first_name Leia sw:3 sw:first_name Darth sw:4 sw:first_name Han sw:5 sw:first_name Anakin sw:1 sw:last_name Skywalker sw:2 sw:last_name Skywalker sw:3 sw:last_name Vader sw:4 sw:last_name Solo sw:5 sw:last_name Skywalker sw:1 sw:occupation sw:Jedi sw:2 sw:occupation sw:princess sw:4 sw:occupation sw:scoundrel and table 2: sw_id property value sw:1 sw:enemy sw:3 sw:2 sw:enemy sw:3 sw:3 sw:enemy sw:1 sw:3 sw:enemy sw:2 sw:3 sw:enemy sw:4 sw:4 sw:enemy sw:1 sw:1 sw:father sw:3 sw:2 sw:father sw:3 sw:1 sw:lightsaber_color green sw:3 sw:lightsaber_color red sw:4 sw:ship Millenium Falcon To merge these two tables, we simple concatenate them. It couldn't be simpler. Wait, this looks kinda familiar...","title":"Merging Tables"},{"location":"reference/tables-and-triples/#rdf","text":"These tables are pretty much in RDF format. You just have to squint a little! sw_id == subject property == predicate value == object Each row of the table is a subject-predicate-object triple. Our subjects, predicates, and some objects are URLs. We also have some literal objects. We could turn this table directly into Turtle format with a little SQL magic (basically just concatenating strings): SELECT \"@prefix sw: <http://example.com/sw_> .\" UNION ALL SELECT \"\" UNION ALL SELECT sw_id || \" \" || property || \" \" || IF( INSTR(value, \":\"), value, -- CURIE \"\"\"\" || value || \"\"\"\" -- literal ) || \" .\" FROM triple_table; The first few lines will look like this: @prefix sw: <http://example.com/sw_> . sw:1 sw:first_name \"Luke\" . sw:2 sw:first_name \"Leia\" . sw:3 sw:first_name \"Darth\" . sw:4 sw:first_name \"Han\" . Two things we're missing from RDF are language tagged literals and typed literals. We also haven't used any blank nodes in our triple table. These are easy enough to add. The biggest thing that's different about RDF is that it uses the \"open-world assumption\", so something may be true even though we don't have a triple asserting that it's true. The open-world assumption is a better fit than the closed-world assumption when we're integrating data on the Web.","title":"RDF"},{"location":"reference/tables-and-triples/#conclusion","text":"Tables are great! We use them all the time, they're strong and rigid, and we're comfortable with them. RDF, on the other hand, looks strange at first. For most common data processing, RDF is too flexible. But sometimes flexiblity is the most important thing. The greatest strength of tables is their rigid structure, but that's also their greatest weakness. We saw a number of problems with tables, and how they could be overcome by breaking tables apart into smaller tables, until we got down to the most basic pattern: subject-predicate-object. Step by step, we were pushed toward RDF. Merging tables is particularly painful. When working with data on the Web, merging is one of the most common and important operations, and so it makes sense to use RDF for these tasks. If self-joins with SQL is the worst problem for EAV tables, then SPARQL solves it. These examples show that it's not really very hard to convert tables to triples. And once you've seen SPARQL, the RDF query language, you've seen one good way to convert triples to tables: SPARQL SELECT results are just tables! Since it's straightforward to convert tables to triples and back again, make sure to use the right tool for the right job. When you need to merge heterogeneous data, reach for triples. For most other data processing tasks, use tables. They're great!","title":"Conclusion"},{"location":"reference/troublehooting-robot/","text":"Lessons learned from troubleshooting ROBOT \u00b6 Prerequisites \u00b6 Review tutorial on Ontology pipelines with ROBOT and SPARQL Learning objectives \u00b6 Learn common mistakes when using ROBOT and how to troubleshoot and fix them. Lessons learned \u00b6 Copying-pasting (especially in google docs) can introduce unexpected format changes in row 2 of the template: \u00b6 Note that these format changes are not always visible. The most common typos are: introduction of space in cells single quotes are changed into apostrophes These errors are most commonly reported as \"MANCHESTER PARSE ERROR\" Restrictions for the first 2 rows of a ROBOT template : \u00b6 In the same column, it is OK to have a header string (row #1) with no template string (row #2). the information in the column is useful to curators (e.g. term labels) but will be ignored by ROBOT. In the same column, if there is a template string (row #2), there MUST be a header string (row #1) if the row #1 is missing, the error will be reported as: COLUMN MISMATCH ERROR the template string in column 1 must have a corresponding header in table \"tmp/merge_template.tsv\u201d The content of the template break some OBO or Protege rules \u00b6 for example, Protege only allows one comment on a class. If you are adding new comments to terms via ROBOT, you will get an error if a comment already exists on a term. error will be reported as: OBO STRUCTURE ERROR Ontology does not conform to OBO structure rules: multiple comment tags not allowed. Note: If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch. The error with the optional \u201cnull\u201d is when the mondo-edit file is empty. Optional.get() cannot be called on an absent value Use the -vvv option to show the stack trace. Use the --help option to see usage information make: *** [mondo.Makefile:454: merge_template] Error 1 New ID prefix : \u00b6 ROBOT template can be used to add axioms containing terms (and IDs) from other ontologies which were recently imported The ID prefix is not recognized by ROBOT, and the error is reported as MANCHESTER PARSE ERROR Resolution: the ontology Makefile should be updated to include the prefix in the merge_template. Note: If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch. Example templates \u00b6 Example templates from Mondo are available here Example templates from OBI are available here Contributors \u00b6 Sabrina Toro ( ORCID ) Nicole Vasilevsky ( ORCID )","title":"Troubleshooting ROBOT"},{"location":"reference/troublehooting-robot/#lessons-learned-from-troubleshooting-robot","text":"","title":"Lessons learned from troubleshooting ROBOT"},{"location":"reference/troublehooting-robot/#prerequisites","text":"Review tutorial on Ontology pipelines with ROBOT and SPARQL","title":"Prerequisites"},{"location":"reference/troublehooting-robot/#learning-objectives","text":"Learn common mistakes when using ROBOT and how to troubleshoot and fix them.","title":"Learning objectives"},{"location":"reference/troublehooting-robot/#lessons-learned","text":"","title":"Lessons learned"},{"location":"reference/troublehooting-robot/#copying-pasting-especially-in-google-docs-can-introduce-unexpected-format-changes-in-row-2-of-the-template","text":"Note that these format changes are not always visible. The most common typos are: introduction of space in cells single quotes are changed into apostrophes These errors are most commonly reported as \"MANCHESTER PARSE ERROR\"","title":"Copying-pasting (especially in google docs) can introduce unexpected format changes in row 2 of the template:"},{"location":"reference/troublehooting-robot/#restrictions-for-the-first-2-rows-of-a-robot-template","text":"In the same column, it is OK to have a header string (row #1) with no template string (row #2). the information in the column is useful to curators (e.g. term labels) but will be ignored by ROBOT. In the same column, if there is a template string (row #2), there MUST be a header string (row #1) if the row #1 is missing, the error will be reported as: COLUMN MISMATCH ERROR the template string in column 1 must have a corresponding header in table \"tmp/merge_template.tsv\u201d","title":"Restrictions for the first 2 rows of a ROBOT template:"},{"location":"reference/troublehooting-robot/#the-content-of-the-template-break-some-obo-or-protege-rules","text":"for example, Protege only allows one comment on a class. If you are adding new comments to terms via ROBOT, you will get an error if a comment already exists on a term. error will be reported as: OBO STRUCTURE ERROR Ontology does not conform to OBO structure rules: multiple comment tags not allowed. Note: If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch. The error with the optional \u201cnull\u201d is when the mondo-edit file is empty. Optional.get() cannot be called on an absent value Use the -vvv option to show the stack trace. Use the --help option to see usage information make: *** [mondo.Makefile:454: merge_template] Error 1","title":"The content of the template break some OBO or Protege rules"},{"location":"reference/troublehooting-robot/#new-id-prefix","text":"ROBOT template can be used to add axioms containing terms (and IDs) from other ontologies which were recently imported The ID prefix is not recognized by ROBOT, and the error is reported as MANCHESTER PARSE ERROR Resolution: the ontology Makefile should be updated to include the prefix in the merge_template. Note: If you run ROBOT and get an error, it may create a blank file. You need to discard the changes and/or open a new branch.","title":"New ID prefix:"},{"location":"reference/troublehooting-robot/#example-templates","text":"Example templates from Mondo are available here Example templates from OBI are available here","title":"Example templates"},{"location":"reference/troublehooting-robot/#contributors","text":"Sabrina Toro ( ORCID ) Nicole Vasilevsky ( ORCID )","title":"Contributors"},{"location":"teaching/case-studies/","text":"Learning Outcomes for Critical Path Tutorial \u00b6 Understand the value of URIs as global identifiers and the potential shortcomings. Having a basic picture of the flagship efforts of the Semantic Web. Being aware of some of the central Semantic Web applications in the biomedical domain. Having a cursory understanding of how linked data can help to power your Critical Path data analysis problems. Interesting Case Studies to talk about: \u00b6 The Experimental Factor Ontology: from controlled vocabulary to integrated application ontology driving drug target identification. From barely structured data via data dictionaries to semantic data integration: International HundredK+ Cohorts Consortium (IHCC) data harmonization case study: How to get from messy, individual data dictionaries for COHORT data to an integrated resource for browsing and grouping. The EJPRD story: Registry level integration using a semantic metadata model Common data elements in rare disease registration. EFO case study \u00b6 Build controlled vocabulary Look a bit at the anatomy of a term So what happens now? The story of scientific database curation The integrator hub with the killer use case comes along Now the vocabulary is getting \u201cforced\u201d onto other databases that want to be part (and have to be part) The number of terms needed shoot up exponentially - external ontologies need two be integrated Uberon Mondo Why Mondo and not DO? Finally: better, more specialised hierarchies Its hard to re-use. (Measurement story) Output data of integrator hub can now be integrated even higher (e.g. disease to gene networks) Individual sources can also be integrated individually Stories like this happen all the time: The SCDO story First started building a vocab Then using ROBOT Then linking OBO terms Then applying for OBO membership Then using OBO purls and re-using OBO terms More to come IHCC story \u00b6 Cohort data are scattered and there is no easy way to group data across cohorts Even just finding the right cohort can be difficult Data dictionaries are often just spreadsheets on someones computer Data dictionaries do not have rich metadata (you dont know data dictionary category or value pertains to a disease) What to do: Build controlled vocabulary Map data dictionaries to a controlled vocabulary Build ontological model from controlled terms rich enough to group the data for the use cases at hand Design a process that makes the above scalable Show examples So now, we want enable the discovery of data across these cohorts. Build GECKO Assign data dictionary elements to IDs and publish as \"Linked Data\" ( browse here ) Build mapping pipeline Check example google sheet Link IDs to ontology terms These links can now be used to group the metadata for identifying cohorts EJPRD story \u00b6 Rare disease registries are scattered across the web and there is no easy way to search across all EJPRD is developing two metadata schemas: On Registry level, they are building the metadata model which is reusing some standard vocabularies such as dcat. There is not that much \"semantics\" here - it really is a metadata model On Record level, they are building the Clinical Data Elements (CDE) Semantic Model , see for example the core model . The idea is that registries publish their metadata (and eventually data) as linked data that can be easily queried using the above models. One of the most major problems is the size of the project and competing voices (\"If its not RDF its not FAIR\"), but also the sheer scale of the technical issue: many of the so called registries are essentially excel spreadsheets on an FTP server.","title":"Case studies for teaching"},{"location":"teaching/case-studies/#learning-outcomes-for-critical-path-tutorial","text":"Understand the value of URIs as global identifiers and the potential shortcomings. Having a basic picture of the flagship efforts of the Semantic Web. Being aware of some of the central Semantic Web applications in the biomedical domain. Having a cursory understanding of how linked data can help to power your Critical Path data analysis problems.","title":"Learning Outcomes for Critical Path Tutorial"},{"location":"teaching/case-studies/#interesting-case-studies-to-talk-about","text":"The Experimental Factor Ontology: from controlled vocabulary to integrated application ontology driving drug target identification. From barely structured data via data dictionaries to semantic data integration: International HundredK+ Cohorts Consortium (IHCC) data harmonization case study: How to get from messy, individual data dictionaries for COHORT data to an integrated resource for browsing and grouping. The EJPRD story: Registry level integration using a semantic metadata model Common data elements in rare disease registration.","title":"Interesting Case Studies to talk about:"},{"location":"teaching/case-studies/#efo-case-study","text":"Build controlled vocabulary Look a bit at the anatomy of a term So what happens now? The story of scientific database curation The integrator hub with the killer use case comes along Now the vocabulary is getting \u201cforced\u201d onto other databases that want to be part (and have to be part) The number of terms needed shoot up exponentially - external ontologies need two be integrated Uberon Mondo Why Mondo and not DO? Finally: better, more specialised hierarchies Its hard to re-use. (Measurement story) Output data of integrator hub can now be integrated even higher (e.g. disease to gene networks) Individual sources can also be integrated individually Stories like this happen all the time: The SCDO story First started building a vocab Then using ROBOT Then linking OBO terms Then applying for OBO membership Then using OBO purls and re-using OBO terms More to come","title":"EFO case study"},{"location":"teaching/case-studies/#ihcc-story","text":"Cohort data are scattered and there is no easy way to group data across cohorts Even just finding the right cohort can be difficult Data dictionaries are often just spreadsheets on someones computer Data dictionaries do not have rich metadata (you dont know data dictionary category or value pertains to a disease) What to do: Build controlled vocabulary Map data dictionaries to a controlled vocabulary Build ontological model from controlled terms rich enough to group the data for the use cases at hand Design a process that makes the above scalable Show examples So now, we want enable the discovery of data across these cohorts. Build GECKO Assign data dictionary elements to IDs and publish as \"Linked Data\" ( browse here ) Build mapping pipeline Check example google sheet Link IDs to ontology terms These links can now be used to group the metadata for identifying cohorts","title":"IHCC story"},{"location":"teaching/case-studies/#ejprd-story","text":"Rare disease registries are scattered across the web and there is no easy way to search across all EJPRD is developing two metadata schemas: On Registry level, they are building the metadata model which is reusing some standard vocabularies such as dcat. There is not that much \"semantics\" here - it really is a metadata model On Record level, they are building the Clinical Data Elements (CDE) Semantic Model , see for example the core model . The idea is that registries publish their metadata (and eventually data) as linked data that can be easily queried using the above models. One of the most major problems is the size of the project and competing voices (\"If its not RDF its not FAIR\"), but also the sheer scale of the technical issue: many of the so called registries are essentially excel spreadsheets on an FTP server.","title":"EJPRD story"},{"location":"tutorial/basic-dl-query/","text":"DL query \u00b6 This tutorial is based off https://ontology101tutorial.readthedocs.io/en/latest/DL_QueryTab.html Created by: Melissa Haendel, Chris Mungall, David Osumi-Sutherland, Matt Yoder, Carlo Torniai, and Simon Jupp DL query tab \u00b6 The DL query tab shown below provides an interface for querying and searching an ontology. The ontology must be classified by a reasoner before it can be queried in the DL query tab. For this tutorial, we will be using cc.owl which can be found here . Open cc.owl in Protege (use Open from URL and enter the https://raw.githubusercontent.com/OHSUBD2K/BDK14-Ontologies-101/master/BDK14_exercises/basic-dl-query/cc.owl ). Run the reasoner. Navigate to the DL Query tab. Type organelle into the box, and make sure subclasses and direct subclasses are ticked. You can type any valid OWL class expression into the DL query tab. For example, to find all classes whose members are part_of a membrane, type part_of some membrane and click execute . Note the linking underscore for this relation in this ontology. Some ontologies do not use underscores for relations, whereby you'd need single quotes (i.e. part of ). The OWL keyword and can be used to make a class expression that is the intersection of two class expressions. For example, to find the classes in the red area below, we want to find subclasses of the intersection of the class organelle and the class endoplasmic reticulum part Note that we do not need to use the part grouping classes in the gene ontology (GO). The same results can be obtained by querying for the intersection of the class organelle and the restriction part_of some ER \u2013 try this and see. We can also ask for superclasses by ticking the boxes as below: The or keyword is to used to create a class expression that is the union of two class expressions. For example: (WARNING: or is not supported by ELK reasoner) This is illustrated by the red area in the following Venn diagram: For further exercises, please see https://ontology101tutorial.readthedocs.io/en/latest/EXERCISE_BasicDL_Queries.html","title":"DL queries"},{"location":"tutorial/basic-dl-query/#dl-query","text":"This tutorial is based off https://ontology101tutorial.readthedocs.io/en/latest/DL_QueryTab.html Created by: Melissa Haendel, Chris Mungall, David Osumi-Sutherland, Matt Yoder, Carlo Torniai, and Simon Jupp","title":"DL query"},{"location":"tutorial/basic-dl-query/#dl-query-tab","text":"The DL query tab shown below provides an interface for querying and searching an ontology. The ontology must be classified by a reasoner before it can be queried in the DL query tab. For this tutorial, we will be using cc.owl which can be found here . Open cc.owl in Protege (use Open from URL and enter the https://raw.githubusercontent.com/OHSUBD2K/BDK14-Ontologies-101/master/BDK14_exercises/basic-dl-query/cc.owl ). Run the reasoner. Navigate to the DL Query tab. Type organelle into the box, and make sure subclasses and direct subclasses are ticked. You can type any valid OWL class expression into the DL query tab. For example, to find all classes whose members are part_of a membrane, type part_of some membrane and click execute . Note the linking underscore for this relation in this ontology. Some ontologies do not use underscores for relations, whereby you'd need single quotes (i.e. part of ). The OWL keyword and can be used to make a class expression that is the intersection of two class expressions. For example, to find the classes in the red area below, we want to find subclasses of the intersection of the class organelle and the class endoplasmic reticulum part Note that we do not need to use the part grouping classes in the gene ontology (GO). The same results can be obtained by querying for the intersection of the class organelle and the restriction part_of some ER \u2013 try this and see. We can also ask for superclasses by ticking the boxes as below: The or keyword is to used to create a class expression that is the union of two class expressions. For example: (WARNING: or is not supported by ELK reasoner) This is illustrated by the red area in the following Venn diagram: For further exercises, please see https://ontology101tutorial.readthedocs.io/en/latest/EXERCISE_BasicDL_Queries.html","title":"DL query tab"},{"location":"tutorial/disjointness/","text":"Disjointness \u00b6 This tutorial is based off https://ontology101tutorial.readthedocs.io/en/latest/Disjointness.html Created by: Melissa Haendel, Chris Mungall, David Osumi-Sutherland, Matt Yoder, Carlo Torniai, and Simon Jupp For this excercise, we will be using chromosome-parts-interim.owl file that can be found here Disjointness \u00b6 In the chromosome-parts-interim.owl file, at the top of our class hierarchy we have cell, cell part, chromosomal part, intracellular part, organelle and organelle part. By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom. If you do not already have it open, load your previous ontology that was derived from the 'interim file'. Note: you can open a recent file by going to File-> Open Recent We want to assert that organelle and organelle part are disjoint. To do this first select the organelle class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the hierarchy panel, you can use CTRL to select multiple classes. Select 'organelle part' as disjoint with organelle. Note that the directionality is irrelevant. Prove this to yourself by deleting the disjoint axiom, and adding it back from organelle part . Reasoning and inconsistency checking \u00b6 We have introduced a deliberate mistake into the ontology. We previously asserted that intracellular organelle part is a subclass of both organelle part and organelle . We have now added an axiom stating that organelle and organelle part are disjoint. We can use the reasoner to check the consistency of our ontology. The reasoner should detect our contradiction. Prot\u00e9g\u00e9 comes with several reasoners, and more can be installed via the plugins mechanism (see plugins chapter). Select a reasoner from the Reasoner menu (Elk, HermiT, Pellet, or Fact++ will work - we mostly use ELK). Once a reasoner is highlighted, select 'Start reasoner' from the menu. Note: you may get several pop-boxes/warnings, ignore those. The intracellular organelle part class will have changed to red indicating that the class is now unsatisfiable . You can also see unsatisfiable classes by switching to the inferred view. Here you will a special class called Nothing . When we previously said that all OWL classes are subclasses of OWL Thing. OWL Nothing is a leaf class or bottom class of your ontology. Any classes that are deemed unsatisfiable by the reasoner are shown as subclasses or equivalent to OWL Nothing. The inferred view will show you all subclasses of Nothing. Once the ontology is classified, inferred statements or axioms are shown in the various panels with a light-yellow shading. The class description for intracellular organelle part should look something like the screen shot below. You will see that the class has been asserted equivalent to the Nothing class. Inside this statement, a small question mark icon appears, clicking this will get an explanation from the reasoner for this inconsistency. Select the (?) icon to get an explanation for this inconsistency. The explanation shows the axioms involved. We see the disjoint class axiom alongside the two subclass axioms are causing the inconsistency. We can simply repair this ontology by removing the intracellular organelle part subClassOf organelle axiom. Remove the Disjoint with axiom (click the (x) beside organelle in the Description pane for intracellular organelle part ), and resynchronise the reasoner from the reasoner menu.","title":"Disjointness"},{"location":"tutorial/disjointness/#disjointness","text":"This tutorial is based off https://ontology101tutorial.readthedocs.io/en/latest/Disjointness.html Created by: Melissa Haendel, Chris Mungall, David Osumi-Sutherland, Matt Yoder, Carlo Torniai, and Simon Jupp For this excercise, we will be using chromosome-parts-interim.owl file that can be found here","title":"Disjointness"},{"location":"tutorial/disjointness/#disjointness_1","text":"In the chromosome-parts-interim.owl file, at the top of our class hierarchy we have cell, cell part, chromosomal part, intracellular part, organelle and organelle part. By default, OWL assumes that these classes can overlap, i.e. there are individuals who can be instances of more than one of these classes. We want to create a restriction on our ontology that states these classes are different and that no individual can be a member of more than one of these classes. We can say this in OWL by creating a disjoint classes axiom. If you do not already have it open, load your previous ontology that was derived from the 'interim file'. Note: you can open a recent file by going to File-> Open Recent We want to assert that organelle and organelle part are disjoint. To do this first select the organelle class. In the class 'Description' view, scroll down and select the (+) button next to Disjoint With. You are presented with the now familiar window allowing you to select, or type, to choose a class. In the hierarchy panel, you can use CTRL to select multiple classes. Select 'organelle part' as disjoint with organelle. Note that the directionality is irrelevant. Prove this to yourself by deleting the disjoint axiom, and adding it back from organelle part .","title":"Disjointness"},{"location":"tutorial/disjointness/#reasoning-and-inconsistency-checking","text":"We have introduced a deliberate mistake into the ontology. We previously asserted that intracellular organelle part is a subclass of both organelle part and organelle . We have now added an axiom stating that organelle and organelle part are disjoint. We can use the reasoner to check the consistency of our ontology. The reasoner should detect our contradiction. Prot\u00e9g\u00e9 comes with several reasoners, and more can be installed via the plugins mechanism (see plugins chapter). Select a reasoner from the Reasoner menu (Elk, HermiT, Pellet, or Fact++ will work - we mostly use ELK). Once a reasoner is highlighted, select 'Start reasoner' from the menu. Note: you may get several pop-boxes/warnings, ignore those. The intracellular organelle part class will have changed to red indicating that the class is now unsatisfiable . You can also see unsatisfiable classes by switching to the inferred view. Here you will a special class called Nothing . When we previously said that all OWL classes are subclasses of OWL Thing. OWL Nothing is a leaf class or bottom class of your ontology. Any classes that are deemed unsatisfiable by the reasoner are shown as subclasses or equivalent to OWL Nothing. The inferred view will show you all subclasses of Nothing. Once the ontology is classified, inferred statements or axioms are shown in the various panels with a light-yellow shading. The class description for intracellular organelle part should look something like the screen shot below. You will see that the class has been asserted equivalent to the Nothing class. Inside this statement, a small question mark icon appears, clicking this will get an explanation from the reasoner for this inconsistency. Select the (?) icon to get an explanation for this inconsistency. The explanation shows the axioms involved. We see the disjoint class axiom alongside the two subclass axioms are causing the inconsistency. We can simply repair this ontology by removing the intracellular organelle part subClassOf organelle axiom. Remove the Disjoint with axiom (click the (x) beside organelle in the Description pane for intracellular organelle part ), and resynchronise the reasoner from the reasoner menu.","title":"Reasoning and inconsistency checking"},{"location":"tutorial/dosdp-odk/","text":"Using DOSDP templates in ODK Workflows \u00b6 Preparation \u00b6 You are set up for executing ODK workflows We assume you have a modern ODK-based repository (ODK version >= 1.2.32) set up. For a tutorial on creating a new ontology repo from scratch see here . We assume you have completed at least one of the general DOSDP tutorials . Tutorial \u00b6 Activate DOSDP in ODK Adding a first DOSDP template Video \u00b6 This is a very unprofessional video below recorded as part of one of our trainings. It walks you through this tutorial here, with some additional examples being given and a bit of Q&A. Glossary \u00b6 Template : A document with template strings that include variables which can be instantiated my a tool. For example, a ROBOT template may contain the template string SC 'part of' some % which can be instantiated by ROBOT to be transformed into an OWL axiom: SubClassOf(CATO:001 ObjectSomeValuesFrom(BFO:0000051 UBERON:123)) . Similarly, DOSDP YAML files are often referred to as \"templates\" (which is appropriate). Unfortunately, we often refer to them as \"patterns\" which is not strictly the right way to name them: they are templates that encode patterns (and that only to a limited extend). We recommend to refer to the DOSDP YAML files as \"templates\". Template string : See above: a single string with one or more slots for variables that can be instantiated and transformed into something else. The most important template string in DOSDP is the equivalentTo or subClassOf field : It tells DOSDP tools how to generate an OWL axiom, with which variable slots ( vars ). Pattern : See above, often misused to mean Template . In fact, a pattern provides a general modelling solution to a problem. Patterns can often be encoded in templates, but this is often incomplete. For example, a template typically does not care about the semantics of the properties it refers to, while this is critical for a pattern. (Avoid using this when talking about DOSDP YAML files). DOSDP template table : The spreadsheet (typically TSV or CSV) that contains the DOSDP variable data. The DOSDP template table is applied to the template string in the template to generate a set of OWL axioms and annotation assertions. Preparation \u00b6 This tutorial assumes you have set up an ODK repo with this config: id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: '-Xmx8G' Activate DOSDP in ODK \u00b6 In your src/ontology/{yourontology}-odk.yaml file, simply add the following: use_dosdps: true This flag activates DOSDP in ODK - without it, none of the DOSDP workflows in ODK can be used. Technically, this flag tells ODK the following things: The src/ontology/Makefile is extended as follows: A set of pipelines , or workflows, for processing patterns, e.g. pattern_schema_checks for validating all DOSDP templates, patterns to regenerate all patterns. A new directory, src/patterns , is created with the following files: src/patterns/pattern.owl : This is an ontology of your own patterns. This can be used to browse the your pattern in the form of a class hierarchy, which can help greatly to understand how they relate logically. There are some flaws in this system, like occasional unintended equivalencies between patterns, but for most uses, it is doing ok. src/patterns/definitions.owl : This is the merged ontology of all your DOSDP generated classes. Basically, if you manage your classes across multiple DOSDP patterns and tables, their generated OWL axioms will all be added to this file. src/patterns/external.txt : This file can be used to import external patterns. Just add the (p)URL to a pattern to the file, and the DOSDP pipeline will import it when you run it. We use this a lot when sharing DOSDP templates across ontologies. Two README files: one in the directory of the default DOSDP data pipeline ( src/patterns/data/default/ ) and one in the src/patterns directory. The former points you to the place where you should put, by default, any DOSDP data tables. More about that in the next sections. To fully activate DOSDP in your ontology, please run: sh run.sh make update_repo This will: Update your ontology repository to whatever ODK you have installed in docker ( v1.3 , for example) Apply any changes to your configuration file. For example, the fact that you have activated the DOSDP pipeline in your config file will lead to the ODK extending your Makefile in certain ways Adding a first, simple template \u00b6 (1) Create a new file src/patterns/dosdp-patterns/haircoat_colour_pattern.yaml and paste the following content: pattern_name: haircoat_colour_pattern pattern_iri: http://purl.obolibrary.org/obo/obo-academy/patterns/haircoat_colour_pattern.yaml description: \" Captures the multicoloured characteristic of the fur, i.e. spotted, dotted, motley etc.\" classes: colour_pattern: PATO:0001533 coat_of_hair: UBERON:0010166 relations: has_characteristic: RO:0000053 vars: colour_pattern: \"'colour_pattern'\" name: text: \"%s coat of hair\" vars: - colour_pattern def: text: \"A coat of hair with a %s colour pattern.\" vars: - colour_pattern equivalentTo: text: \"'coat_of_hair' and 'has_characteristic' some %s\" vars: - colour_pattern (2) Let's also create a simple template table to capture traits for our ontology. Note : the filename of the DOSDP template file ( haircoat_colour_pattern.yaml ) excluding the extension must be identical to the filename of the template table ( haircoat_colour_pattern.tsv ) excluding the extension . Let's create the new file at src/patterns/data/default/haircoat_colour_pattern.tsv . defined_class colour_pattern CATO:0000001 PATO:0000333 We are creating a minimal table here with just two columns: defined_class refers to the ID for the term that is being modelled by the template (mandatory for all DOSDP templates) colour_pattern refers to the variable of the same name specified in the vars: section of the DOSDP template YAML file. DOSDP generate: Turning the template tables into OWL axioms \u00b6 Next, we will get a bit used to various commands that help us with DOSDP-based ontology development. Lets first try to transform the simple table above to OWL using the ODK pipeline (we always use IMP=false to skip refreshing imports, which can be a lengthy process): sh run.sh make ../patterns/definitions.owl -B IMP=false This process will will create the ../patterns/definitions.owl file, which is the file that contains all axioms generated by all templates you have configured. In our simple scenario, this means a simple single pattern. Let us look at definitions.owl in your favourite text editor first. Tip: Remember, the `-B` tells `make` to run the make command no matter what - one of the advantages of `make` is that it only runs a command again if something changed, for example, you have added something to a DOSDP template table. Tip: Looking at ontologies in text editors can be very useful, both to reviewing files and making changes! Do not be afraid, the ODK will ensure you wont break anything. Let us look in particular at the following section of the definitions.owl file: # Class: <http://purl.obolibrary.org/obo/CATO_0000001> (http://purl.obolibrary.org/obo/PATO_0000333 coat of hair) AnnotationAssertion(<http://purl.obolibrary.org/obo/IAO_0000115> <http://purl.obolibrary.org/obo/CATO_0000001> \"A coat of hair with a http://purl.obolibrary.org/obo/PATO_0000333 colour pattern.\"^^xsd:string) AnnotationAssertion(rdfs:label <http://purl.obolibrary.org/obo/CATO_0000001> \"http://purl.obolibrary.org/obo/PATO_0000333 coat of hair\"^^xsd:string) EquivalentClasses(<http://purl.obolibrary.org/obo/CATO_0000001> ObjectIntersectionOf(<http://purl.obolibrary.org/obo/UBERON_0010166> ObjectSomeValuesFrom(<http://purl.obolibrary.org/obo/RO_0000053> <http://purl.obolibrary.org/obo/PATO_0000333>))) These are the three axioms / annotation assertions that were created by the DOSDP pipeline. The first annotation is a simple automatically generated definition. What is odd at first glance, is that the definition reads \"A coat of hair with a http://purl.obolibrary.org/obo/PATO_0000333 colour pattern.\" - what does the PATO:0000333 IRI do in the middle of our definition? Understanding this is fundamental to the DODSP pattern workflow, because it is likely that you will have to fix cases like this from time to time. The DOSDP workflow is about generating axioms automatically from existing terms. For example, in this tutorial we are trying to generate terms for different kinds of hair coats for our cats, using the colour pattern (PATO:0001533) hierarchy in the PATO ontology as a basis. The only one term we have added so far is spotted (PATO:0000333). The problem is though, that dosdp-tools , the tool which is part of the ODK and responsible for the DOSDP workflows, does not know anything about PATO:0000333 unless it is already imported into the ontology. In order to remedy this situation, lets import the term: sh run.sh make refresh-pato ODK will automatically see that you have used PATO:0000333 in your ontology, and import it for you. Next, let us make sure that the our edit file has the correct import configured. Open your ontology in a text editor, and make sure you can find the following import statement: Import(<http://purl.obolibrary.org/obo/cato/patterns/definitions.owl>) Replace cato in the PURL with whatever is the ID of your own ontology. Also, do not forget to update src/ontology/catalog-v001.xml , by adding this line: <group id=\"Folder Repository, directory=, recursive=false, Auto-Update=false, version=2\" prefer=\"public\" xml:base=\"\"> ... <uri name=\"http://purl.obolibrary.org/obo/cato/patterns/definitions.owl\" uri=\"../patterns/definitions.owl\"/> ... </group> Important: Remember that we have not yet told dosdp-tools about the freshly imported PATO:0000333 term. To do that, lets run the DOSDP pipeline again: sh run.sh make ../patterns/definitions.owl -B IMP=false A quick look at src/patterns/definitions.owl would now reveal your correctly formatted definitions: AnnotationAssertion(<http://purl.obolibrary.org/obo/IAO_0000115> <http://purl.obolibrary.org/obo/CATO_0000001> \"A coat of hair with a spotted colour pattern.\"^^xsd:string) Now, we are ready to view our ontology (the edit file, i.e. src/ontology/cato-edit.owl ) in Protege: Still a few things to iron out - there is an UBERON term that we still need to import, and our class is not a subclass of the CATO root node , but we had a good start. Re-using externally defined patterns \u00b6 Re-using terms is at the heart of the OBO philosophy, but when it comes to re-using axiom patterns, such as the ones we can define as part of a ROBOT template, we are (as of 2022) still in the early stages. One thing we can do to facilitate re-use is to share DOSDP templates between different projects. We do that by simply adding the URL at which the pattern is located to src/patterns/dosdp-patterns/external.txt . Note: if you are copying a URL from GitHub, make sure it is the raw url, i.e.: src/patterns/dosdp-patterns/external.txt https://raw.githubusercontent.com/obophenotype/bio-attribute-ontology/master/src/patterns/dosdp-patterns/entity_attribute.yaml Here, we randomly decided to import a pattern defined by the Ontology of Biological Attributes (an ontology of traits such as tail length or head size ), for example to represent cat traits in our Cat Ontology. After adding the above URL to our the external.txt file, we can add it to our pipeline: sh run.sh make update_patterns You will now see the entity_attribute.yaml template in src/patterns/dosdp-patterns . We will not do anything with this template as part of this tutorial, so you can remove it again if you wish (by removing the URL from the external.txt file and physically deleting the src/patterns/dosdp-patterns/entity_attribute.yaml file). DOSDP pipelines in ODK \u00b6 Sometimes, we want to manage more than one DOSDP pipeline at once. For example, in more than one of our projects, we have some patterns that are automatically generated by software tools, and others that are manually curated by ontology developers. In other use cases, we sometimes want to restrict the pattern pipelines to generating only logical axioms . In either case, we can add new pipelines by adding the following to the src/ontology/youront-odk.yaml file: pattern_pipelines_group: products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" - id: auto dosdp_tools_options: \"--obo-prefixes=true\" This does the following: It tells the ODK that you want Reference \u00b6 A full example ODK configuration \u00b6 id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main use_dosdps: TRUE repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: '-Xmx8G' pattern_pipelines_group: products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" - id: auto dosdp_tools_options: \"--obo-prefixes=true\" ODK configuration reference for DOSDP \u00b6 Flag Explanation use_dosdps: TRUE Activates DOSDP in your ODK repository setup pattern_pipelines_group: products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" Adding a manual pipeline to your DOSDP setup in which only logical axioms are generated.","title":"Introduction to Managing DOSDP Templates in ODK"},{"location":"tutorial/dosdp-odk/#using-dosdp-templates-in-odk-workflows","text":"","title":"Using DOSDP templates in ODK Workflows"},{"location":"tutorial/dosdp-odk/#preparation","text":"You are set up for executing ODK workflows We assume you have a modern ODK-based repository (ODK version >= 1.2.32) set up. For a tutorial on creating a new ontology repo from scratch see here . We assume you have completed at least one of the general DOSDP tutorials .","title":"Preparation"},{"location":"tutorial/dosdp-odk/#tutorial","text":"Activate DOSDP in ODK Adding a first DOSDP template","title":"Tutorial"},{"location":"tutorial/dosdp-odk/#video","text":"This is a very unprofessional video below recorded as part of one of our trainings. It walks you through this tutorial here, with some additional examples being given and a bit of Q&A.","title":"Video"},{"location":"tutorial/dosdp-odk/#glossary","text":"Template : A document with template strings that include variables which can be instantiated my a tool. For example, a ROBOT template may contain the template string SC 'part of' some % which can be instantiated by ROBOT to be transformed into an OWL axiom: SubClassOf(CATO:001 ObjectSomeValuesFrom(BFO:0000051 UBERON:123)) . Similarly, DOSDP YAML files are often referred to as \"templates\" (which is appropriate). Unfortunately, we often refer to them as \"patterns\" which is not strictly the right way to name them: they are templates that encode patterns (and that only to a limited extend). We recommend to refer to the DOSDP YAML files as \"templates\". Template string : See above: a single string with one or more slots for variables that can be instantiated and transformed into something else. The most important template string in DOSDP is the equivalentTo or subClassOf field : It tells DOSDP tools how to generate an OWL axiom, with which variable slots ( vars ). Pattern : See above, often misused to mean Template . In fact, a pattern provides a general modelling solution to a problem. Patterns can often be encoded in templates, but this is often incomplete. For example, a template typically does not care about the semantics of the properties it refers to, while this is critical for a pattern. (Avoid using this when talking about DOSDP YAML files). DOSDP template table : The spreadsheet (typically TSV or CSV) that contains the DOSDP variable data. The DOSDP template table is applied to the template string in the template to generate a set of OWL axioms and annotation assertions.","title":"Glossary"},{"location":"tutorial/dosdp-odk/#preparation_1","text":"This tutorial assumes you have set up an ODK repo with this config: id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: '-Xmx8G'","title":"Preparation"},{"location":"tutorial/dosdp-odk/#activate-dosdp-in-odk","text":"In your src/ontology/{yourontology}-odk.yaml file, simply add the following: use_dosdps: true This flag activates DOSDP in ODK - without it, none of the DOSDP workflows in ODK can be used. Technically, this flag tells ODK the following things: The src/ontology/Makefile is extended as follows: A set of pipelines , or workflows, for processing patterns, e.g. pattern_schema_checks for validating all DOSDP templates, patterns to regenerate all patterns. A new directory, src/patterns , is created with the following files: src/patterns/pattern.owl : This is an ontology of your own patterns. This can be used to browse the your pattern in the form of a class hierarchy, which can help greatly to understand how they relate logically. There are some flaws in this system, like occasional unintended equivalencies between patterns, but for most uses, it is doing ok. src/patterns/definitions.owl : This is the merged ontology of all your DOSDP generated classes. Basically, if you manage your classes across multiple DOSDP patterns and tables, their generated OWL axioms will all be added to this file. src/patterns/external.txt : This file can be used to import external patterns. Just add the (p)URL to a pattern to the file, and the DOSDP pipeline will import it when you run it. We use this a lot when sharing DOSDP templates across ontologies. Two README files: one in the directory of the default DOSDP data pipeline ( src/patterns/data/default/ ) and one in the src/patterns directory. The former points you to the place where you should put, by default, any DOSDP data tables. More about that in the next sections. To fully activate DOSDP in your ontology, please run: sh run.sh make update_repo This will: Update your ontology repository to whatever ODK you have installed in docker ( v1.3 , for example) Apply any changes to your configuration file. For example, the fact that you have activated the DOSDP pipeline in your config file will lead to the ODK extending your Makefile in certain ways","title":"Activate DOSDP in ODK"},{"location":"tutorial/dosdp-odk/#adding-a-first-simple-template","text":"(1) Create a new file src/patterns/dosdp-patterns/haircoat_colour_pattern.yaml and paste the following content: pattern_name: haircoat_colour_pattern pattern_iri: http://purl.obolibrary.org/obo/obo-academy/patterns/haircoat_colour_pattern.yaml description: \" Captures the multicoloured characteristic of the fur, i.e. spotted, dotted, motley etc.\" classes: colour_pattern: PATO:0001533 coat_of_hair: UBERON:0010166 relations: has_characteristic: RO:0000053 vars: colour_pattern: \"'colour_pattern'\" name: text: \"%s coat of hair\" vars: - colour_pattern def: text: \"A coat of hair with a %s colour pattern.\" vars: - colour_pattern equivalentTo: text: \"'coat_of_hair' and 'has_characteristic' some %s\" vars: - colour_pattern (2) Let's also create a simple template table to capture traits for our ontology. Note : the filename of the DOSDP template file ( haircoat_colour_pattern.yaml ) excluding the extension must be identical to the filename of the template table ( haircoat_colour_pattern.tsv ) excluding the extension . Let's create the new file at src/patterns/data/default/haircoat_colour_pattern.tsv . defined_class colour_pattern CATO:0000001 PATO:0000333 We are creating a minimal table here with just two columns: defined_class refers to the ID for the term that is being modelled by the template (mandatory for all DOSDP templates) colour_pattern refers to the variable of the same name specified in the vars: section of the DOSDP template YAML file.","title":"Adding a first, simple template"},{"location":"tutorial/dosdp-odk/#dosdp-generate-turning-the-template-tables-into-owl-axioms","text":"Next, we will get a bit used to various commands that help us with DOSDP-based ontology development. Lets first try to transform the simple table above to OWL using the ODK pipeline (we always use IMP=false to skip refreshing imports, which can be a lengthy process): sh run.sh make ../patterns/definitions.owl -B IMP=false This process will will create the ../patterns/definitions.owl file, which is the file that contains all axioms generated by all templates you have configured. In our simple scenario, this means a simple single pattern. Let us look at definitions.owl in your favourite text editor first. Tip: Remember, the `-B` tells `make` to run the make command no matter what - one of the advantages of `make` is that it only runs a command again if something changed, for example, you have added something to a DOSDP template table. Tip: Looking at ontologies in text editors can be very useful, both to reviewing files and making changes! Do not be afraid, the ODK will ensure you wont break anything. Let us look in particular at the following section of the definitions.owl file: # Class: <http://purl.obolibrary.org/obo/CATO_0000001> (http://purl.obolibrary.org/obo/PATO_0000333 coat of hair) AnnotationAssertion(<http://purl.obolibrary.org/obo/IAO_0000115> <http://purl.obolibrary.org/obo/CATO_0000001> \"A coat of hair with a http://purl.obolibrary.org/obo/PATO_0000333 colour pattern.\"^^xsd:string) AnnotationAssertion(rdfs:label <http://purl.obolibrary.org/obo/CATO_0000001> \"http://purl.obolibrary.org/obo/PATO_0000333 coat of hair\"^^xsd:string) EquivalentClasses(<http://purl.obolibrary.org/obo/CATO_0000001> ObjectIntersectionOf(<http://purl.obolibrary.org/obo/UBERON_0010166> ObjectSomeValuesFrom(<http://purl.obolibrary.org/obo/RO_0000053> <http://purl.obolibrary.org/obo/PATO_0000333>))) These are the three axioms / annotation assertions that were created by the DOSDP pipeline. The first annotation is a simple automatically generated definition. What is odd at first glance, is that the definition reads \"A coat of hair with a http://purl.obolibrary.org/obo/PATO_0000333 colour pattern.\" - what does the PATO:0000333 IRI do in the middle of our definition? Understanding this is fundamental to the DODSP pattern workflow, because it is likely that you will have to fix cases like this from time to time. The DOSDP workflow is about generating axioms automatically from existing terms. For example, in this tutorial we are trying to generate terms for different kinds of hair coats for our cats, using the colour pattern (PATO:0001533) hierarchy in the PATO ontology as a basis. The only one term we have added so far is spotted (PATO:0000333). The problem is though, that dosdp-tools , the tool which is part of the ODK and responsible for the DOSDP workflows, does not know anything about PATO:0000333 unless it is already imported into the ontology. In order to remedy this situation, lets import the term: sh run.sh make refresh-pato ODK will automatically see that you have used PATO:0000333 in your ontology, and import it for you. Next, let us make sure that the our edit file has the correct import configured. Open your ontology in a text editor, and make sure you can find the following import statement: Import(<http://purl.obolibrary.org/obo/cato/patterns/definitions.owl>) Replace cato in the PURL with whatever is the ID of your own ontology. Also, do not forget to update src/ontology/catalog-v001.xml , by adding this line: <group id=\"Folder Repository, directory=, recursive=false, Auto-Update=false, version=2\" prefer=\"public\" xml:base=\"\"> ... <uri name=\"http://purl.obolibrary.org/obo/cato/patterns/definitions.owl\" uri=\"../patterns/definitions.owl\"/> ... </group> Important: Remember that we have not yet told dosdp-tools about the freshly imported PATO:0000333 term. To do that, lets run the DOSDP pipeline again: sh run.sh make ../patterns/definitions.owl -B IMP=false A quick look at src/patterns/definitions.owl would now reveal your correctly formatted definitions: AnnotationAssertion(<http://purl.obolibrary.org/obo/IAO_0000115> <http://purl.obolibrary.org/obo/CATO_0000001> \"A coat of hair with a spotted colour pattern.\"^^xsd:string) Now, we are ready to view our ontology (the edit file, i.e. src/ontology/cato-edit.owl ) in Protege: Still a few things to iron out - there is an UBERON term that we still need to import, and our class is not a subclass of the CATO root node , but we had a good start.","title":"DOSDP generate: Turning the template tables into OWL axioms"},{"location":"tutorial/dosdp-odk/#re-using-externally-defined-patterns","text":"Re-using terms is at the heart of the OBO philosophy, but when it comes to re-using axiom patterns, such as the ones we can define as part of a ROBOT template, we are (as of 2022) still in the early stages. One thing we can do to facilitate re-use is to share DOSDP templates between different projects. We do that by simply adding the URL at which the pattern is located to src/patterns/dosdp-patterns/external.txt . Note: if you are copying a URL from GitHub, make sure it is the raw url, i.e.: src/patterns/dosdp-patterns/external.txt https://raw.githubusercontent.com/obophenotype/bio-attribute-ontology/master/src/patterns/dosdp-patterns/entity_attribute.yaml Here, we randomly decided to import a pattern defined by the Ontology of Biological Attributes (an ontology of traits such as tail length or head size ), for example to represent cat traits in our Cat Ontology. After adding the above URL to our the external.txt file, we can add it to our pipeline: sh run.sh make update_patterns You will now see the entity_attribute.yaml template in src/patterns/dosdp-patterns . We will not do anything with this template as part of this tutorial, so you can remove it again if you wish (by removing the URL from the external.txt file and physically deleting the src/patterns/dosdp-patterns/entity_attribute.yaml file).","title":"Re-using externally defined patterns"},{"location":"tutorial/dosdp-odk/#dosdp-pipelines-in-odk","text":"Sometimes, we want to manage more than one DOSDP pipeline at once. For example, in more than one of our projects, we have some patterns that are automatically generated by software tools, and others that are manually curated by ontology developers. In other use cases, we sometimes want to restrict the pattern pipelines to generating only logical axioms . In either case, we can add new pipelines by adding the following to the src/ontology/youront-odk.yaml file: pattern_pipelines_group: products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" - id: auto dosdp_tools_options: \"--obo-prefixes=true\" This does the following: It tells the ODK that you want","title":"DOSDP pipelines in ODK"},{"location":"tutorial/dosdp-odk/#reference","text":"","title":"Reference"},{"location":"tutorial/dosdp-odk/#a-full-example-odk-configuration","text":"id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main use_dosdps: TRUE repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: '-Xmx8G' pattern_pipelines_group: products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" - id: auto dosdp_tools_options: \"--obo-prefixes=true\"","title":"A full example ODK configuration"},{"location":"tutorial/dosdp-odk/#odk-configuration-reference-for-dosdp","text":"Flag Explanation use_dosdps: TRUE Activates DOSDP in your ODK repository setup pattern_pipelines_group: products: - id: manual dosdp_tools_options: \"--obo-prefixes=true --restrict-axioms-to=logical\" Adding a manual pipeline to your DOSDP setup in which only logical axioms are generated.","title":"ODK configuration reference for DOSDP"},{"location":"tutorial/dosdp-overview/","text":"Getting started with DOSDP templates \u00b6 Dead Simple OWL Design patterns (DOSDP) is a templating system for documenting and generating new OWL classes. The templates themselves are designed to be human readable and easy to author. Separate tables (TSV files) are used to specify individual classes. The complete DOSDP documentation can be found here http://incatools.github.io/dead_simple_owl_design_patterns/. For another DOSDP tutorial see here . Anatomy of a DOSDP file: \u00b6 A DOSDP tempaltes are written in YAML file, an easily editable format for encoding nested data structures. At the top level of nesting is a set of 'keys', which must match those specified in the DOSDP standard. The various types of key and their function are outlined below. Each key is followed by a colon and then a value, which may be a text string, a list or another set of keys. Lists items are indicated using a '-'. Nesting is achieved via indenting using some standard number of spaces (typically 3 or 4). Here's a little illustration: key1: some text key2: - first list item (text; note the indent) - second list item key3: key_under_key3: some text another_key_under_key3: - first list item (text; note the indent) - second list item yet_another_key_under_key3: key_under_yet_another_key_under_key3: some more text In the following text, keys and values together are sometimes referred to as 'fields'. Pattern level keys \u00b6 Reference doc A set of fields that specify general information about a pattern: name, description, IRI, contributors, examples etc e.g. pattern_name: abnormalAnatomicalEntity pattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml description: \"Any unspecified abnormality of an anatomical entity.\" contributors: - https://orcid.org/0000-0002-9900-7880 Dictionaries \u00b6 Reference doc A major aim of the DOSDP system is to produce self-contained, human-readable templates. Templates need IDs in order to be reliably used programatically, but templates that only use IDs are not human readable. DOSDPs therefore include a set of dictionaries that map labels to IDs. Strictly any readable name can be used, but by convention we use class labels. IDs must be OBO curie style e.g. CL:0000001). Separate dictionaries are required for classes, relations (object properties) & annotationProperties e.g. classes: quality: PATO:0000001 abnormal: PATO:0000460 anatomical entity: UBERON:0001062 relations: inheres_in_part_of: RO:0002314 has_modifier: RO:0002573 has_part: BFO:0000051 Variables \u00b6 Reference doc These fields specify the names of pattern variables (TSV column names) and map these to a range. e.g. This specifies a variable called 'anatomy' with the range 'anatomical entity': vars: anatomy: \"'anatomical entity'\" The var name (anatomy) corresponds to a column name in the table (TSV file) used in combination with this template, to generate new terms based on the template. The range specifies what type of term is allowed in this column - in this case 'anatomical entity' (UBERON:0001062; as specified in the dictionary) or one of its subclasses, e.g.- anatomy UBERON:0001154 There are various types of variables : vars are used to specify OWL classes (see example above). data_vars and data_list_vars are used to specify single pieces or data lists respectively. The range of data_vars is specified using XSD types. e.g. data_vars: number: xsd:int data_list_vars: xrefs: xsd:string A table used to specify classes following this pattern could have the following content. Note that in lists, multiple elements are separated by a '|'. number xrefs 1 pubmed:123456|DOI:10.1016/j.cell.2016.07.054 Template fields \u00b6 Template fields are where the content of classes produced by the template is specified. These mostly follow printf format : A text field has variable slots specified using %s (for strings), %d for integers and %f for floats (decimals). Variables slots are filled, in order of appearance in the text, with values coming from a list of variables in an associated vars field e.g. name: text: \"%s of %s\" vars: - neuron - brain_region If the value associated with the neuron var is (the class) 'glutamatergic neuron' and the value associated with the = 'brain region' var is 'primary motor cortext', this will generate a classes with the name (label) \"glutamatergic neuron of primary motor cortex\". OBO fields \u00b6 Reference doc DOSDPs include a set of convenience fields for annotation of classes that follow OBO conventions for field names and their mappings to OWL annotation properties. These include name , def , comment , namespace . When the value of a var is an OWL class, the name (label) of the var is used in the substitution. (see example above). The annotation axioms generated by these template fields can be annotated . One OBO field exists for this purpose: xrefs allows annotation with a list of references using the obo standard xref annotation property (curies) e.g. data_list_vars: xrefs: xsd:string def: text: \"Any %s that has a soma located in the %s\" vars: - neuron - brain_region xrefs: xrefs Logical axioms convenience fields \u00b6 Reference doc Where a single equivalent Class, subclassOf or GCI axiom is specified, you may use the keys 'EquivalentTo', 'subClassOf' or 'GCI' respectively. If multiple axioms of any type are needed, use the core field logical_axioms . Core fields \u00b6 annotations: - annotationProperty: text: vars: annotations: ... - annotationProperty: text: vars: logical_axioms: - axiom_type: subClassOf text: vars: - - - axiom_type: subClassOf text: vars: - - annotations: - ... Advanced usage: \u00b6 Optionals and multiples (0-many) \u00b6 TBA Using DOSDP templates in ODK Workflows \u00b6 The Ontology Development Kit (ODK) comes with a few pre-configured workflows involving DOSDP templates. For a detailed tutorial see here .","title":"Getting started with DOSDP templates"},{"location":"tutorial/dosdp-overview/#getting-started-with-dosdp-templates","text":"Dead Simple OWL Design patterns (DOSDP) is a templating system for documenting and generating new OWL classes. The templates themselves are designed to be human readable and easy to author. Separate tables (TSV files) are used to specify individual classes. The complete DOSDP documentation can be found here http://incatools.github.io/dead_simple_owl_design_patterns/. For another DOSDP tutorial see here .","title":"Getting started with DOSDP templates"},{"location":"tutorial/dosdp-overview/#anatomy-of-a-dosdp-file","text":"A DOSDP tempaltes are written in YAML file, an easily editable format for encoding nested data structures. At the top level of nesting is a set of 'keys', which must match those specified in the DOSDP standard. The various types of key and their function are outlined below. Each key is followed by a colon and then a value, which may be a text string, a list or another set of keys. Lists items are indicated using a '-'. Nesting is achieved via indenting using some standard number of spaces (typically 3 or 4). Here's a little illustration: key1: some text key2: - first list item (text; note the indent) - second list item key3: key_under_key3: some text another_key_under_key3: - first list item (text; note the indent) - second list item yet_another_key_under_key3: key_under_yet_another_key_under_key3: some more text In the following text, keys and values together are sometimes referred to as 'fields'.","title":"Anatomy of a DOSDP file:"},{"location":"tutorial/dosdp-overview/#pattern-level-keys","text":"Reference doc A set of fields that specify general information about a pattern: name, description, IRI, contributors, examples etc e.g. pattern_name: abnormalAnatomicalEntity pattern_iri: http://purl.obolibrary.org/obo/upheno/patterns/abnormalAnatomicalEntity.yaml description: \"Any unspecified abnormality of an anatomical entity.\" contributors: - https://orcid.org/0000-0002-9900-7880","title":"Pattern level keys"},{"location":"tutorial/dosdp-overview/#dictionaries","text":"Reference doc A major aim of the DOSDP system is to produce self-contained, human-readable templates. Templates need IDs in order to be reliably used programatically, but templates that only use IDs are not human readable. DOSDPs therefore include a set of dictionaries that map labels to IDs. Strictly any readable name can be used, but by convention we use class labels. IDs must be OBO curie style e.g. CL:0000001). Separate dictionaries are required for classes, relations (object properties) & annotationProperties e.g. classes: quality: PATO:0000001 abnormal: PATO:0000460 anatomical entity: UBERON:0001062 relations: inheres_in_part_of: RO:0002314 has_modifier: RO:0002573 has_part: BFO:0000051","title":"Dictionaries"},{"location":"tutorial/dosdp-overview/#variables","text":"Reference doc These fields specify the names of pattern variables (TSV column names) and map these to a range. e.g. This specifies a variable called 'anatomy' with the range 'anatomical entity': vars: anatomy: \"'anatomical entity'\" The var name (anatomy) corresponds to a column name in the table (TSV file) used in combination with this template, to generate new terms based on the template. The range specifies what type of term is allowed in this column - in this case 'anatomical entity' (UBERON:0001062; as specified in the dictionary) or one of its subclasses, e.g.- anatomy UBERON:0001154 There are various types of variables : vars are used to specify OWL classes (see example above). data_vars and data_list_vars are used to specify single pieces or data lists respectively. The range of data_vars is specified using XSD types. e.g. data_vars: number: xsd:int data_list_vars: xrefs: xsd:string A table used to specify classes following this pattern could have the following content. Note that in lists, multiple elements are separated by a '|'. number xrefs 1 pubmed:123456|DOI:10.1016/j.cell.2016.07.054","title":"Variables"},{"location":"tutorial/dosdp-overview/#template-fields","text":"Template fields are where the content of classes produced by the template is specified. These mostly follow printf format : A text field has variable slots specified using %s (for strings), %d for integers and %f for floats (decimals). Variables slots are filled, in order of appearance in the text, with values coming from a list of variables in an associated vars field e.g. name: text: \"%s of %s\" vars: - neuron - brain_region If the value associated with the neuron var is (the class) 'glutamatergic neuron' and the value associated with the = 'brain region' var is 'primary motor cortext', this will generate a classes with the name (label) \"glutamatergic neuron of primary motor cortex\".","title":"Template fields"},{"location":"tutorial/dosdp-overview/#obo-fields","text":"Reference doc DOSDPs include a set of convenience fields for annotation of classes that follow OBO conventions for field names and their mappings to OWL annotation properties. These include name , def , comment , namespace . When the value of a var is an OWL class, the name (label) of the var is used in the substitution. (see example above). The annotation axioms generated by these template fields can be annotated . One OBO field exists for this purpose: xrefs allows annotation with a list of references using the obo standard xref annotation property (curies) e.g. data_list_vars: xrefs: xsd:string def: text: \"Any %s that has a soma located in the %s\" vars: - neuron - brain_region xrefs: xrefs","title":"OBO fields"},{"location":"tutorial/dosdp-overview/#logical-axioms-convenience-fields","text":"Reference doc Where a single equivalent Class, subclassOf or GCI axiom is specified, you may use the keys 'EquivalentTo', 'subClassOf' or 'GCI' respectively. If multiple axioms of any type are needed, use the core field logical_axioms .","title":"Logical axioms convenience fields"},{"location":"tutorial/dosdp-overview/#core-fields","text":"annotations: - annotationProperty: text: vars: annotations: ... - annotationProperty: text: vars: logical_axioms: - axiom_type: subClassOf text: vars: - - - axiom_type: subClassOf text: vars: - - annotations: - ...","title":"Core fields"},{"location":"tutorial/dosdp-overview/#advanced-usage","text":"","title":"Advanced usage:"},{"location":"tutorial/dosdp-overview/#optionals-and-multiples-0-many","text":"TBA","title":"Optionals and multiples (0-many)"},{"location":"tutorial/dosdp-overview/#using-dosdp-templates-in-odk-workflows","text":"The Ontology Development Kit (ODK) comes with a few pre-configured workflows involving DOSDP templates. For a detailed tutorial see here .","title":"Using DOSDP templates in ODK Workflows"},{"location":"tutorial/dosdp-template/","text":"Dead Simple Ontology Design Patterns (DOSDP) \u00b6 Note: This is an updated Version of Jim Balhoff's DOSDP tutorial here . The main use case for dosdp-tools (and the DOS-DP framework ) is managing a set of ontology terms, which all follow a common logical pattern, by simply collecting the unique aspect of each term as a line in a spreadsheet. For example, we may be developing an ontology of environmental exposures. We would like to have terms in our ontology which represent exposure to a variety of stressors, such as chemicals, radiation, social stresses, etc. Creating an ontology of environmental exposures \u00b6 To maximize reuse and facilitate data integration, we can build our exposure concepts by referencing terms from domain-specific ontologies, such as the Chemical Entities of Biological Interest Ontology (ChEBI) for chemicals. By modeling each exposure concept in the same way, we can use a reasoner to leverage the chemical classification provided by ChEBI to provide a classification for our exposure concepts. Since each exposure concept has a logical definition based on our data model for exposure, there is no need to manually manage the classification hierarchy. Let's say our model for exposure concepts holds that an \"exposure\" is an event with a particular input (the thing the subject is exposed to): 'exposure to X' EquivalentTo 'exposure event' and 'has input' some X If we need an ontology class to represent 'exposure to sarin' (bad news!), we can simply use the term sarin from ChEBI, and create a logical definition: 'exposure to sarin' EquivalentTo 'exposure event' and 'has input' some sarin We can go ahead and create some other concepts we need for our exposure data: 'exposure to asbestos' EquivalentTo 'exposure event' and 'has input' some asbestos 'exposure to chemical substance' EquivalentTo 'exposure event' and 'has input' some 'chemical substance' These definitions again can reference terms provided by ChEBI: asbestos and chemical substance Classifying our concepts \u00b6 Since the three concepts we've created all follow the same logical model, their hierarchical relationship can be logically determined by the relationships of the chemicals they reference. ChEBI asserts this structure for those terms: 'chemical substance' | | -------------- | | | | sarin asbestos Based on this, an OWL reasoner can automatically tell us the relationships between our exposure concepts: 'exposure to chemical substance' | | -------------------------- | | | | 'exposure to sarin' 'exposure to asbestos' To support this, we simply need to declare the ChEBI OWL file as an owl:import in our exposure ontology, and use an OWL reasoner such as ELK. Managing terms with dosdp-tools \u00b6 Creating terms by hand like we just did works fine, and relying on the reasoner for the classification will save us a lot of trouble and maintain correctness as our ontology grows. But since all the terms use the same logical pattern, it would be nice to keep this in one place; this will help make sure we always follow the pattern correctly when we create new concepts. We really only need to store the list of inputs (e.g. chemicals) in order to create all our exposure concepts. As we will see later, we may also want to manage separate sets of terms that follow other, different, patterns. To do this with dosdp-tools , we need three main files: a pattern template , a spreadsheet of pattern fillers , and a source ontology . You will also usually need a file of prefix definitions so that the tool knows how to expand your shortened identifiers into IRIs. For our chemical exposures, getting the source ontology is easy: just download chebi.owl . Note\u2014it's about 450 MB. For our pattern fillers spreadsheet , we just need to make a tab-delimited file containing the chemical stressors for which we need exposure concepts. The file needs a column for the term IRI to be used for the generated class ( this column is always called defined_class ), and also a column for the chemical to reference ( choose a label according to your data model ). It should look like this: defined_class input EXPOSO:1 CHEBI:75701 EXPOSO:2 CHEBI:46661 EXPOSO:3 CHEBI:59999 The columns should be tab-separated\u2014you can download a correctly formatted file to follow along. For now you will just maintain this file by hand, adding chemicals by looking up their ID in ChEBI, and manually choosing the next ID for your generated classes. In the future this may be simplified using the DOS-DP table editor , which is under development. The trickiest part to DOS-DP is creating your pattern template (but it's not so hard). Pattern templates are written in YAML , a simple file format based on keys and values. The keys are text labels; values can be plain values, another key-value structure, or a list. The DOS-DP schema specifies the keys and values which can be used in a pattern file. We'll use most of the common entries in this example. Read the comments (lines starting with #) for explanation of the various fields: # We can provide a name for this pattern here. pattern_name: exposure_with_input # In 'classes', we define the terms we will use in this pattern. # In the OBO community the terms often have numeric IDs, so here # we can provide human-readable names we can use further in the pattern. # The key is the name to be used; the value is the ID in prefixed form (i.e. a CURIE). classes: exposure event: ExO:0000002 Thing: owl:Thing # Use 'relations' the same way as 'classes', # but for the object properties used in the pattern. relations: has input: RO:0002233 # The 'vars' section defines the various slots that can be # filled in for this pattern. We have only one, which we call 'input'. # The value is the range, meaning the class of things that are valid # values for this pattern. By specifying owl:Thing, we're allowing any # class to be provided as a variable filler. You need a column in your # spreadsheet for each variable defined here, in addition to the `defined class` column. vars: input: \"Thing\" # We can provide a template for an `rdfs:label` value to generate # for our new term. dosdp-tools will search the source ontology # to find the label for the filler term, and fill it into the # name template in place of the %s. name: text: \"exposure to %s\" vars: - input # This works the same as label generation, but instead creates # a definition annotation. def: text: \"A exposure event involving the interaction of an exposure receptor to %s. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input # Here we can generate a logical axiom for our new concept. Create an # expression using OWL Manchester syntax. The expression can use any # of the terms defined at the beginning of the pattern. A reference # to the variable value will be inserted in place of the %s. equivalentTo: text: \"'exposure event' and 'has input' some %s\" vars: - input Download the pattern template file to follow along. Now we only need one more file before we can run dosdp-tools . A file of prefix definitions (also in YAML format) will specify how to expand the CURIEs we used in our spreadsheet and pattern files: EXPOSO: http://example.org/exposure/ Here we are specifying how to expand our EXPOSO prefix (used in our spreadsheet defined_class column). To expand the others, we'll pass a convenience option to dosdp-tools , --obo-prefixes , which will activate some predefined prefixes such as owl: , and handle any other prefixes using the standard expansion for OBO IDs: http://purl.obolibrary.org/obo/PREFIX_ . Here's a link to the prefixes file . Now we're all set to run dosdp-tools ! If you've downloaded or created all the necessary files, run this command to generate your ontology of exposures (assuming you've added the dosdp-tools to your Unix PATH): dosdp-tools generate --obo-prefixes=true --prefixes=prefixes.yaml --infile=exposure_with_input.tsv --template=exposure_with_input.yaml --ontology=chebi.owl --outfile=exposure_with_input.owl This will apply the pattern to each line in your spreadsheet, and save the result in an ontology saved at exposure_with_input.owl (it should look something like this ). If you take a look at this ontology in a text editor or in Prot\u00e9g\u00e9, you'll see that it contains three classes, each with a generated label, text definition, and equivalent class definition. You're done! Well... you're sort of done. But wouldn't it be nice if your exposure ontology included some information about the chemicals you referenced? Without this our reasoner can't classify our exposure concepts. As we said above, we could add an owl:import declaration and load all of ChEBI, but your exposure ontology has three classes and ChEBI has over 120,000 classes. Instead, we can use the ROBOT tool to extract a module of just the relevant axioms from ChEBI. Later, we will also see how to use ROBOT to merge the outputs from multiple DOS-DP patterns into one ontology. You can download ROBOT from its homepage . Extracting a module from the source ontology \u00b6 ROBOT has a few different methods for extracting a subset from an ontology. We'll use the Syntactic Locality Module Extractor (SLME) to get a set of axioms relevant to the ChEBI terms we've referenced. ROBOT will need a file containing the list of terms. We can use a Unix command to get these out of our spreadsheet file: sed '1d' exposure_with_input.tsv | cut -f 2 >inputs.txt We'll end up with a simple list: CHEBI:75701 CHEBI:46661 CHEBI:59999 Now we can use ROBOT to extract an SLME bottom module for those terms out of ChEBI: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Our ChEBI extract only has 63 classes. Great! If you want, you can merge the ChEBI extract into your exposure ontology before releasing it to the public: robot merge --input exposure_with_input.owl --input chebi_extract.owl --output exposo.owl Now you can open exposo.owl in Prot\u00e9g\u00e9, run the reasoner, and see a correct classification for your exposure concepts! You may notice that your ontology is missing labels for ExO:0000002 ('exposure event') and RO:0002233 ('has input'). If you want, you can use ROBOT to extract that information from ExO and RO. Working with multiple patterns \u00b6 You will often want to generate ontology modules using more than one DOS-DP pattern. For example, you may want to organize environmental exposures by an additional axis of classification, such as exposure to substances with various biological roles, based on information provided by ChEBI. This requires a slightly different logical expression, so we'll make a new pattern : pattern_name: exposure_with_input_with_role classes: exposure event: ExO:0000002 Thing: owl:Thing relations: has input: RO:0002233 has role: RO:0000087 vars: input: \"Thing\" name: text: \"exposure to %s\" vars: - input def: text: \"A exposure event involving the interaction of an exposure receptor to a substance with %s role. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input equivalentTo: text: \"'exposure event' and 'has input' some ('has role' some %s)\" vars: - input Let's create an input file for this pattern, with a single filler, neurotoxin : defined_class input EXPOSO:4 CHEBI:50910 Now we can run dosdp-tools for this pattern: dosdp-tools generate --obo-prefixes --prefixes=prefixes.yaml --infile=exposure_with_input_with_role.tsv --template=exposure_with_input_with_role.yaml --ontology=chebi.owl --outfile=exposure_with_input_with_role.owl We can re-run our ChEBI module extractor, first appending the terms used for this pattern to the ones we used for the first pattern: sed '1d' exposure_with_input_with_role.tsv | cut -f 2 >>inputs.txt And then run robot extract exactly as before: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Now we just want to merge both of our generated modules, along with our ChEBI extract: robot merge --input exposure_with_input.owl --input exposure_with_input_with_role.owl --input chebi_extract.owl --output exposo.owl If you open the new exposo.owl in Prot\u00e9g\u00e9 and run the reasoner, you'll now see 'exposure to sarin' classified under both 'exposure to chemical substance' and also 'exposure to neurotoxin'. Conclusion \u00b6 By using dosdp-tools and robot together, you can effectively develop ontologies which compose parts of ontologies from multiple domains using standard patterns. You will probably want to orchestrate the types of commands used in this tutorial within a Makefile, so that you can automate this process for easy repeatability.","title":"DOSDP Templates Basic Tutorial"},{"location":"tutorial/dosdp-template/#dead-simple-ontology-design-patterns-dosdp","text":"Note: This is an updated Version of Jim Balhoff's DOSDP tutorial here . The main use case for dosdp-tools (and the DOS-DP framework ) is managing a set of ontology terms, which all follow a common logical pattern, by simply collecting the unique aspect of each term as a line in a spreadsheet. For example, we may be developing an ontology of environmental exposures. We would like to have terms in our ontology which represent exposure to a variety of stressors, such as chemicals, radiation, social stresses, etc.","title":"Dead Simple Ontology Design Patterns (DOSDP)"},{"location":"tutorial/dosdp-template/#creating-an-ontology-of-environmental-exposures","text":"To maximize reuse and facilitate data integration, we can build our exposure concepts by referencing terms from domain-specific ontologies, such as the Chemical Entities of Biological Interest Ontology (ChEBI) for chemicals. By modeling each exposure concept in the same way, we can use a reasoner to leverage the chemical classification provided by ChEBI to provide a classification for our exposure concepts. Since each exposure concept has a logical definition based on our data model for exposure, there is no need to manually manage the classification hierarchy. Let's say our model for exposure concepts holds that an \"exposure\" is an event with a particular input (the thing the subject is exposed to): 'exposure to X' EquivalentTo 'exposure event' and 'has input' some X If we need an ontology class to represent 'exposure to sarin' (bad news!), we can simply use the term sarin from ChEBI, and create a logical definition: 'exposure to sarin' EquivalentTo 'exposure event' and 'has input' some sarin We can go ahead and create some other concepts we need for our exposure data: 'exposure to asbestos' EquivalentTo 'exposure event' and 'has input' some asbestos 'exposure to chemical substance' EquivalentTo 'exposure event' and 'has input' some 'chemical substance' These definitions again can reference terms provided by ChEBI: asbestos and chemical substance","title":"Creating an ontology of environmental exposures"},{"location":"tutorial/dosdp-template/#classifying-our-concepts","text":"Since the three concepts we've created all follow the same logical model, their hierarchical relationship can be logically determined by the relationships of the chemicals they reference. ChEBI asserts this structure for those terms: 'chemical substance' | | -------------- | | | | sarin asbestos Based on this, an OWL reasoner can automatically tell us the relationships between our exposure concepts: 'exposure to chemical substance' | | -------------------------- | | | | 'exposure to sarin' 'exposure to asbestos' To support this, we simply need to declare the ChEBI OWL file as an owl:import in our exposure ontology, and use an OWL reasoner such as ELK.","title":"Classifying our concepts"},{"location":"tutorial/dosdp-template/#managing-terms-with-dosdp-tools","text":"Creating terms by hand like we just did works fine, and relying on the reasoner for the classification will save us a lot of trouble and maintain correctness as our ontology grows. But since all the terms use the same logical pattern, it would be nice to keep this in one place; this will help make sure we always follow the pattern correctly when we create new concepts. We really only need to store the list of inputs (e.g. chemicals) in order to create all our exposure concepts. As we will see later, we may also want to manage separate sets of terms that follow other, different, patterns. To do this with dosdp-tools , we need three main files: a pattern template , a spreadsheet of pattern fillers , and a source ontology . You will also usually need a file of prefix definitions so that the tool knows how to expand your shortened identifiers into IRIs. For our chemical exposures, getting the source ontology is easy: just download chebi.owl . Note\u2014it's about 450 MB. For our pattern fillers spreadsheet , we just need to make a tab-delimited file containing the chemical stressors for which we need exposure concepts. The file needs a column for the term IRI to be used for the generated class ( this column is always called defined_class ), and also a column for the chemical to reference ( choose a label according to your data model ). It should look like this: defined_class input EXPOSO:1 CHEBI:75701 EXPOSO:2 CHEBI:46661 EXPOSO:3 CHEBI:59999 The columns should be tab-separated\u2014you can download a correctly formatted file to follow along. For now you will just maintain this file by hand, adding chemicals by looking up their ID in ChEBI, and manually choosing the next ID for your generated classes. In the future this may be simplified using the DOS-DP table editor , which is under development. The trickiest part to DOS-DP is creating your pattern template (but it's not so hard). Pattern templates are written in YAML , a simple file format based on keys and values. The keys are text labels; values can be plain values, another key-value structure, or a list. The DOS-DP schema specifies the keys and values which can be used in a pattern file. We'll use most of the common entries in this example. Read the comments (lines starting with #) for explanation of the various fields: # We can provide a name for this pattern here. pattern_name: exposure_with_input # In 'classes', we define the terms we will use in this pattern. # In the OBO community the terms often have numeric IDs, so here # we can provide human-readable names we can use further in the pattern. # The key is the name to be used; the value is the ID in prefixed form (i.e. a CURIE). classes: exposure event: ExO:0000002 Thing: owl:Thing # Use 'relations' the same way as 'classes', # but for the object properties used in the pattern. relations: has input: RO:0002233 # The 'vars' section defines the various slots that can be # filled in for this pattern. We have only one, which we call 'input'. # The value is the range, meaning the class of things that are valid # values for this pattern. By specifying owl:Thing, we're allowing any # class to be provided as a variable filler. You need a column in your # spreadsheet for each variable defined here, in addition to the `defined class` column. vars: input: \"Thing\" # We can provide a template for an `rdfs:label` value to generate # for our new term. dosdp-tools will search the source ontology # to find the label for the filler term, and fill it into the # name template in place of the %s. name: text: \"exposure to %s\" vars: - input # This works the same as label generation, but instead creates # a definition annotation. def: text: \"A exposure event involving the interaction of an exposure receptor to %s. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input # Here we can generate a logical axiom for our new concept. Create an # expression using OWL Manchester syntax. The expression can use any # of the terms defined at the beginning of the pattern. A reference # to the variable value will be inserted in place of the %s. equivalentTo: text: \"'exposure event' and 'has input' some %s\" vars: - input Download the pattern template file to follow along. Now we only need one more file before we can run dosdp-tools . A file of prefix definitions (also in YAML format) will specify how to expand the CURIEs we used in our spreadsheet and pattern files: EXPOSO: http://example.org/exposure/ Here we are specifying how to expand our EXPOSO prefix (used in our spreadsheet defined_class column). To expand the others, we'll pass a convenience option to dosdp-tools , --obo-prefixes , which will activate some predefined prefixes such as owl: , and handle any other prefixes using the standard expansion for OBO IDs: http://purl.obolibrary.org/obo/PREFIX_ . Here's a link to the prefixes file . Now we're all set to run dosdp-tools ! If you've downloaded or created all the necessary files, run this command to generate your ontology of exposures (assuming you've added the dosdp-tools to your Unix PATH): dosdp-tools generate --obo-prefixes=true --prefixes=prefixes.yaml --infile=exposure_with_input.tsv --template=exposure_with_input.yaml --ontology=chebi.owl --outfile=exposure_with_input.owl This will apply the pattern to each line in your spreadsheet, and save the result in an ontology saved at exposure_with_input.owl (it should look something like this ). If you take a look at this ontology in a text editor or in Prot\u00e9g\u00e9, you'll see that it contains three classes, each with a generated label, text definition, and equivalent class definition. You're done! Well... you're sort of done. But wouldn't it be nice if your exposure ontology included some information about the chemicals you referenced? Without this our reasoner can't classify our exposure concepts. As we said above, we could add an owl:import declaration and load all of ChEBI, but your exposure ontology has three classes and ChEBI has over 120,000 classes. Instead, we can use the ROBOT tool to extract a module of just the relevant axioms from ChEBI. Later, we will also see how to use ROBOT to merge the outputs from multiple DOS-DP patterns into one ontology. You can download ROBOT from its homepage .","title":"Managing terms with dosdp-tools"},{"location":"tutorial/dosdp-template/#extracting-a-module-from-the-source-ontology","text":"ROBOT has a few different methods for extracting a subset from an ontology. We'll use the Syntactic Locality Module Extractor (SLME) to get a set of axioms relevant to the ChEBI terms we've referenced. ROBOT will need a file containing the list of terms. We can use a Unix command to get these out of our spreadsheet file: sed '1d' exposure_with_input.tsv | cut -f 2 >inputs.txt We'll end up with a simple list: CHEBI:75701 CHEBI:46661 CHEBI:59999 Now we can use ROBOT to extract an SLME bottom module for those terms out of ChEBI: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Our ChEBI extract only has 63 classes. Great! If you want, you can merge the ChEBI extract into your exposure ontology before releasing it to the public: robot merge --input exposure_with_input.owl --input chebi_extract.owl --output exposo.owl Now you can open exposo.owl in Prot\u00e9g\u00e9, run the reasoner, and see a correct classification for your exposure concepts! You may notice that your ontology is missing labels for ExO:0000002 ('exposure event') and RO:0002233 ('has input'). If you want, you can use ROBOT to extract that information from ExO and RO.","title":"Extracting a module from the source ontology"},{"location":"tutorial/dosdp-template/#working-with-multiple-patterns","text":"You will often want to generate ontology modules using more than one DOS-DP pattern. For example, you may want to organize environmental exposures by an additional axis of classification, such as exposure to substances with various biological roles, based on information provided by ChEBI. This requires a slightly different logical expression, so we'll make a new pattern : pattern_name: exposure_with_input_with_role classes: exposure event: ExO:0000002 Thing: owl:Thing relations: has input: RO:0002233 has role: RO:0000087 vars: input: \"Thing\" name: text: \"exposure to %s\" vars: - input def: text: \"A exposure event involving the interaction of an exposure receptor to a substance with %s role. Exposure may be through a variety of means, including through the air or surrounding medium, or through ingestion.\" vars: - input equivalentTo: text: \"'exposure event' and 'has input' some ('has role' some %s)\" vars: - input Let's create an input file for this pattern, with a single filler, neurotoxin : defined_class input EXPOSO:4 CHEBI:50910 Now we can run dosdp-tools for this pattern: dosdp-tools generate --obo-prefixes --prefixes=prefixes.yaml --infile=exposure_with_input_with_role.tsv --template=exposure_with_input_with_role.yaml --ontology=chebi.owl --outfile=exposure_with_input_with_role.owl We can re-run our ChEBI module extractor, first appending the terms used for this pattern to the ones we used for the first pattern: sed '1d' exposure_with_input_with_role.tsv | cut -f 2 >>inputs.txt And then run robot extract exactly as before: robot extract --method BOT --input chebi.owl --term-file inputs.txt --output chebi_extract.owl Now we just want to merge both of our generated modules, along with our ChEBI extract: robot merge --input exposure_with_input.owl --input exposure_with_input_with_role.owl --input chebi_extract.owl --output exposo.owl If you open the new exposo.owl in Prot\u00e9g\u00e9 and run the reasoner, you'll now see 'exposure to sarin' classified under both 'exposure to chemical substance' and also 'exposure to neurotoxin'.","title":"Working with multiple patterns"},{"location":"tutorial/dosdp-template/#conclusion","text":"By using dosdp-tools and robot together, you can effectively develop ontologies which compose parts of ontologies from multiple domains using standard patterns. You will probably want to orchestrate the types of commands used in this tutorial within a Makefile, so that you can automate this process for easy repeatability.","title":"Conclusion"},{"location":"tutorial/fhkb/","text":"Manchester Family History Advanced OWL \u00b6 This is a fork of the infamous Manchester Family History Advanced OWL Tutorial version 1.1, located at http://owl.cs.manchester.ac.uk/publications/talks-and-tutorials/fhkbtutorial/ The translation to markdown is not without issue, but we are making a start to making the tutorial a bit more accessible. This reproduction is done with kind permission by Robert Stevens. Original credits (Version 1.1, see pdf ): \u00b6 Authors: Robert Stevens Margaret Stevens Nicolas Matentzoglu Simon Jupp Bio-Health Informatics Group School of Computer Science University of Manchester Oxford Road Manchester United Kingdom M13 9PL robert.stevens@manchester.ac.uk Contributors \u00b6 v 1.0 Robert Stevens, Margaret Stevens, Nicolas Matentzoglu and Simon Jupp v 1.1 Robert Stevens, Nicolas Matentzoglu v 2.0 (Web version) Robert Stevens, Nicolas Matentzoglu, Shawn Tan The University of Manchester Copyright\u00a9 The University of Manchester November 25, 2015 Acknowledgements \u00b6 This tutorial was realised as part of the Semantic Web Authoring Tool (SWAT) project (see http://www.swatproject.org), which is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/G032459/1, to the University of Manchester, the University of Sussex and the Open University. Dedication \u00b6 The Stevens family\u2014all my ancestors were necessary for this to happen. Also, for my Mum who gathered all the information. Contents \u00b6 Preamble 0.1 Licencing 0.2 Reporting Errors 0.3 Acknowledgements 1. Introduction 1.1 Learning Outcomes 1.2 Why Family History? 1.3 How to use this Tutorial 1.4 FHKB Resources 1.5 Conventions used in this Tutorial 2. Adding some Individuals to the FHKB 2.1 A World of Objects 2.2 Asserting Parentage Facts 2.3 Summary 3. Ancestors and Descendants 3.1 Ancestors and Descendants 3.2 Grandparents and Great Grandparents 3.3 Summary 4. Modelling the Person Class 4.1 The Class of Person 4.2 Describing Sex in the FHKB 4.3 Defining Man and Woman 4.4 Describing Parentage in the FHKB 4.5 Who has a father? 4.6 Filling in Domains and Ranges for the FHKB Properties 4.7 Inconsistencies 4.8 Adding Some Defined Classes for Ancestors and so on 4.9 Summary 5. Siblings in the FHKB 5.1 Blood relations 5.2 Siblings: Option One 5.2.1 Brothers and Sisters 5.3 Siblings: Option two 5.3.1 Which Modelling Option to Choose for Siblings? 5.4 Half-Siblings 5.5 Aunts and Uncles 5.6 Summary 6. Individuals in Class Expressions 6.1 Richard and Robert\u2019s Parents and Ancestors 6.2 Closing Down What we Know About Parents and Siblings 6.3 Summary 7. Data Properties in the FHKB 7.1 Adding Some Data Properties for Event Years - 7.1.1 Counting Numbers of Children 7.2 The Open World Assumption 7.3 Adding Given and Family Names 7.4 Summary 8. Cousins in the FHKB 8.1 Introducing Cousins 8.2 First Cousins 8.3 Other Degrees and Removes of Cousin 8.4 Doing First Cousins Properly 8.5 Summary 9. Marriage in the FHKB 9.1 Marriage - 9.1.1 Spouses 9.2 In-Laws 9.3 Brothers and Sisters In-Law 9.4 Aunts and Uncles in-Law 9.5 Summary 10. Extending the TBox 10.1 Adding Defined Classes 10.2 Summary 11. Final remarks A FHKB Family Data Preamble \u00b6 0.1 Licencing \u00b6 The \u2018Manchester Family History Advanced OWL Tutorial\u2019 by Robert Stevens, Margaret Stevens, Nicolas Matentzoglu, Simon Jupp is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. 0.2 Reporting Errors \u00b6 This manual will almost certainly contain errors, defects and infelicities. Do report them to robert.stevens@manchester.ac.uk supplying chapter, section and some actual context in the form of words will help in fixing any of these issues. 0.3 Acknowledgements \u00b6 As well as the author list, many people have contributed to this work. Any contribution, such as reporting bugs etc., is rewarded by an acknowledgement of contribution (in alphabetical order) when the authors get around to adding them: Graham Goff; Matthew Horridge; Jared Leo; Fennie Liang; Phil Lord; Fiona McNeill; Eleni Mikroyannidi; George Moulton; Bijan Parsia; Alan Rector; Uli Sattler; Dmitry Tsarkov; Danielle Welter. Chapter 1 \u00b6 Introduction \u00b6 This tutorial introduces the tutee to many of the more advanced features of the Web Ontology Language (OWL). The topic of family history is used to take the tutee through various modelling issues and, in doing so, using many features of OWL 2 to build a Family History Knowledge Base (FHKB). The exercises are designed to maximise inference about family history through the use of an automated reasoner on an OWL knowledge base (KB) containing many members of the Stevens family. The aim, therefore, is to enable people to learn advanced features of OWL 2 in a setting that involves both classes and individuals, while attempting to maximise the use of inference within the FHKB. 1.1 Learning Outcomes \u00b6 By doing this tutorial, a tutee should be able to: Know about the separation of entities into TBox and ABox; Use classes and individuals in modelling; Write fancy class expressions; Assert facts about individuals; Use the effects of property hierarchies, property characteristics, domain/range constraints to drive inference; Use constraints and role chains on inferences about individuals; Understand and manage the consequences of the open world assumption in the TBox and ABox; Use nominals in class expressions; Appreciate some limits of OWL 2. 1.2 Why Family History? \u00b6 Building an FHKB enables us to meet our learning outcomes through a topic that is accessible to virtually everyone. Family history or genealogy is a good topic for a general tutorial on OWL 2 as it enables us to touch many features of the language and, importantly, it is a field that everyone knows. All people have a family and therefore a family history \u2013 even if they do not know their particular family history. A small caveat was put on the topic being accessible to everyone as some cultures differ, for instance, in the description of cousins and labels given to different siblings. Nevertheless, family history remains a topic that everyone can talk about. Family history is a good topic for an OWL ontology as it obviously involves both individuals \u2013 the people involved \u2013 and classes of individuals \u2013 people, men and women, cousins, etc. Also, it is an area rich in inference; from only knowing parentage and sex of an individual, it is possible to work out all family relationships \u2013 for example, sharing parents implies a sibling relationship; one\u2019s parent\u2019s brothers are one\u2019s uncles; one\u2019s parent\u2019s parents are one\u2019s grandparents. So, we should be able to construct an ontology that allows us to both express family history, but also to infer family relationships between people from knowing relatively little about them. As we will learn through the tutorial, OWL 2 cannot actually do all that is needed to create a FHKB. This is unfortunate, but we use it to our advantage to illustrate some of the limitations of OWL 2. We know that rule based systems can do family history with ease, but that is not the point here; we are not advocating OWL DL as an appropriate mechanism for doing family history, but we do use it as a good educational example. We make the following assumptions about what people know: We assume that people know OWL to the level that is known at the end of the Pizza tutorial . Some ground will be covered again, but a lot of basic OWL is assumed. We assume people know how to use Prot\u00e9g\u00e9 or their OWL environment of choice. We do not give \u2018click by click\u2019 instructions. At some places, some guidance is given, but this is not to be relied upon as Prot\u00e9g\u00e9 changes and we will not keep up to date. We make some simplifying assumptions in this tutorial: We take a conventional western view of family history. This appears to have most effects on naming of sibling and cousin relationships. We take a straight-forward view on the sex of people; this is explored further in Chapter 4; A \u2018conventional\u2019 view of marriage is taken; this is explored further in Chapter 9. We make no special treatment of time or dates; we are only interested in years and we do not do anything fancy; this is explored more in Chapter 7. We assume the ancestors of people go back for ever; obviously this is not true, eventually one would get back to a primordial soup and one\u2019s ancestors are not humans (members of the classPerson), but we don\u2019t bother with such niceties. At the end of the tutorial, you should be able to produce a property hierarchy and a TBox or class hierarchy such as shown in Figure 1.1; all supported by use of the automated reasoner and a lot of OWL 2\u2019s features. Figure 1.1: A part of the class and property hierarchy of the final FHKB. 1.3 How to use this Tutorial \u00b6 Here are some tips on using this manual to the best advantage: Start at the beginning and work towards the end. You can just read the tutorial, but building the FHKB will help you learn much more and much more easily Use the reasoner in each task; a lot of the FHKB tutorial is about using the reasoner and not doing so will detract from the learning outcomes. 1.4 FHKB Resources \u00b6 The following resources are available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial: A full version of the Stevens FHKB. Some links to papers about the FHKB. Some slides about the FHKB tutorial. A set of OWL resources for each stage of the FHKB. Some blogs about the FHKB are at http://robertdavidstevens.wordpress.com. 1.5 Conventions used in this Tutorial \u00b6 All OWL is written in Manchester Syntax. When we use FHKB entities within text, we use a sans serif typeface. We use CamelCase for classes and property names. Class names start with upper case. Individual names start with a lower case letter and internal underscores to break words. Property names usually start with \u2018is\u2019 or \u2018has\u2019 and are CamelCase with a lower case initial letter. Many classes and individuals in the FHKB have annotation properties, usually human readable labels. They show up in some of the examples in Manchester syntax, but are not made explicit as part of the tasks in this tutorial. Every object property is necessarily a sub-property of topObjectProperty. It does not have to be asserted as such. Nevertheless, there might be situations where this relationship is made explicit in this tutorial for illustrative reasons. The individuals we are dealing with represent distinct persons. Throughout the tutorial, once the respective axiom is introduced (chapter 7.1.1), the reader should make sure that all his or her individuals are always made distinct, especially when he or she adds a new one. At the end of each chapter, we note the Description Logic Language (expressivity) needed to represent the ontology and the reasoning times for a number of state of the art reasoning systems. This should get the reader a sense how difficult the FHKB becomes for reasoners to deal with over time. When there is some scary OWL or the reasoner may find the FHKB hard work, you will see a \u2018here be dragons\u2019 image. 1 1 The image comes fromhttp://ancienthomeofdragon.homestead.com/May 2012. Chapter 2 \u00b6 Adding some Individuals to the FHKB \u00b6 In this chapter we will start by creating a fresh OWL ontology and adding some individuals that will be surrogates for people in the FHKB. In particular you will: Create a new OWL ontology for the FHKB; Add some individuals that will stand for members of the Stevens family. Describe parentage of people. Add some facts to specific individuals as to their parentage; See the reasoner doing some work. At the moment we will ignore sex; sex will not happen until Chapter 4. 2.1 A World of Objects \u00b6 The \u2018world\u2019 2 or field of interest we model in an ontology is made up of objects or individuals. Such objects include, but are not limited to: People, their pets, the pizzas they eat; The processes of cooking pizzas, living, running, jumping, undertaking a journey; The spaces within a room, a bowl, an artery; The attributes of things such as colour, dimensions, speed, shape of various objects; Boundaries, love, ideas, plans, hypotheses. 2 we use \u2018world\u2019 as a synonym of \u2018field of interest\u2019 or \u2018domain\u2019. \u2018World\u2019 does not restrict us to modelling the physical world outside our consciousness. We observe these objects, either outside lying around in the world or in our heads. OWL is all about modelling such individuals. Whenever we make a statement in OWL, when we write down an axiom, we are making statements about individuals. When thinking about the axioms in an ontology it is best to think about the individuals involved, even if OWL individuals do not actually appear in the ontology. All through this tutorial we will always be returning to the individuals being described in order to help us understand what we are doing and to help us make decisions about how to do it. 2.2 Asserting Parentage Facts \u00b6 Biologically, everyone has parents; a mother and a father 3 . The starting point for family history is parentage; we need to relate the family member objects by object properties. An object property relates two objects, in this case a child object with his or her mother or father object. To do this we need to create three object properties: Task 1: Creating object properties for parentage Create a new ontology; Create an object property hasMother ; Create a property isMotherOf and give hasMother the InverseOf: isMotherOf ; Do the same for the property hasFather ; Create a property hasParent ; give it the obvious inverse; Make hasMother and hasFather sub-properties of hasParent . Run the reasoner and look at the property hierarchy. Note how the reasoner has automatically completed the sub-hierarchy for isParentOf: isMotherOf and isFatherOf are inferred to be sub-properties of isParentOf . The OWL snippet below shows some parentage fact assertions on an individual. Note that rather than being assertions to an anonymous individual via some class, we are giving an assertion to a named individual. Individual: grant_plinth Facts: hasFather mr_plinth, hasMother mrs_plinth 3 Don\u2019t quibble; it\u2019s true enough here. Task 2: Create the ABox Using the information in Table A.1 (see appendix) about parentage (so the columns about fathers and mothers), enter the fact assertions for the people which appear in rows shaded in grey. We will only use the hasMother and hasFather properties in our fact assertions. You do not need to assert names and birth years yet. This exercise will require you to create an individual for every person we want to talk about, using the Firstname_Secondname_Familyname_Birthyear pattern, as for example in Robert_David_Bright_1965 . While asserting facts about all individuals in the FHKB will be a bit tedious at times, it might be useful to at least do the task for a subset of the family members. For the impatient reader, there is a convenience snapshot of the ontology including the raw individuals available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial If you are working with Prot\u00e9g\u00e9, you may want to look at the Matrix plugin for Prot\u00e9g\u00e9 at this point. The plugin allows you to add individuals quickly in the form of a regular table, and can significantly reduce the effort of adding any type of entity to the ontology. In order to install the matrix plugin, open Prot\u00e9g\u00e9 and go to File \u00bb Check for plugins. Select the \u2018Matrix Views\u2019 plugin. Click install, wait until the the installation is confirmed, close and re-open Prot\u00e9g\u00e9; go to the \u2018Window\u2019 menu item, select \u2018Tabs\u2019 and add the \u2018Individuals matrix\u2019. Now do the following: Task 3: DL queries Classify the FHKB. Issue the DL query hasFather value David_Bright_1934 and look at the answers (remember to check the respective checkbox in Prot\u00e9g\u00e9 to include individuals in your query results). Issue the DL query isFatherOf value Robert_David_Bright_1965 . Look at the answers. 4. Look at the entailed facts on Robert_David_Bright_1965 . You should find the following: David Bright (1934) is the father of Robert David Bright (1965) and Richard John Bright (1962). Robert David Bright (1965) has David Bright 1934 as a parent. Since we have said that isFatherOf has an inverse of hasFather , and we have asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we have a simple entailment that David_Bright_1934 isFatherOf Robert_David_Bright_1965 . So, without asserting the isFatherOf facts, we have been able to ask and get answers for that DL query. As we asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we also infer that he hasParent David_Bright_1934 ; this is because hasParent is the super-property of hasFather and the sub-property implies the super-property. This works all the way up the property tree until topObjectProperty , so all individuals are related by topObjectProperty \u2014this is always true. This implication \u2018upwards\u2019 is the way to interpret how the property hierarchies work. 2.3 Summary \u00b6 We have now covered the basics of dealing with individuals in OWL ontologies. We have set up some properties, but without domains, ranges, appropriate characteristics and then arranged them in a hierarchy. From only a few assertions in our FHKB, we can already infer many facts about an individual: Simple exploitation of inverses of properties and super-properties of the asserted properties. We have also encountered some important principles: We get inverses for free. The sub-property implies the super-property. So, hasFather implies the hasParent fact between individuals. This entailment of the super-property is very important and will drive much of the inference we do with the FHKB. Upon reasoning we get the inverses of properties between named individuals for free. Lots is still open. For example, we do not know the sex of individuals and what other children, other than those described, people in the FHKB may have. The FHKB ontology at this stage of the tutorial has an expressivity of ALHI. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.026 sec (0.00001 % of final), by Pellet 2.2.0 0.144 sec (0.00116 % of final) and by FaCT++ 1.6.4 is approximately 0. sec (0.000 % of final). 0 sec indicates failure or timeout. Chapter 3 \u00b6 Ancestors and Descendants \u00b6 In this Chapter you will: Use sub-properties and the transitive property characteristic to infer ancestors of people; Add properties to the FHKB property hierarchy that will infer ancestors and descendants of a person without adding any more facts to the FHKB; Explore the use of sub-property chains for grandparents, great grandparents and so on; Place all of these new object properties in the property hierarchy and in that way learn more about the implications of the property hierarchy. Find a snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial. 3.1 Ancestors and Descendants \u00b6 The FHKB has parents established between individuals and we know that all people have two parents. A parent is an ancestor of its children; a person\u2019s parent\u2019s parents are its ancestors; and so on. So, in our FHKB, Robert\u2019s ancestors are David, Margaret, William, Iris, Charles, Violet, James, another Violet, another William, Sarah and so on. If my parent\u2019s parents are my ancestors, then what we need is a transitive version of the hasParent property. Obviously we do not want hasParent to be transitive, as Robert\u2019s grandparents (and so on) would become his parents (and that would be wrong). We can easily achieve what is necessary. We need a hasAncestor property that has a transitive characteristic. The trick is to make this a super-property of the hasParent property. As explained before, a sub-property implies its super-property. So, if individual x holds a hasParent property with an individual y , then it also holds an instance of its super-property hasAncestor with the individual y . If individual y then holds a hasParent property with another individual z , then there is also, by implication, a hasAncestor property between y and z . As hasAncestor is transitive, x and z also hold a hasAncestor relationship between them. The inverse of hasAncestor can either be isAncestorOf or hasDescendant . We choose the isAncestorOf option. Task 4: Object properties: exploiting the semantics Make a new object property hasRelation , make it symmetric. Make a new object property hasAncestor . Make it a sub-property of hasRelation and a super-property of hasParent . Make hasAncestor transitive. Create the inverse isAncestorOf . Do not \u2018stitch\u2019 it into the property hierarchy; the reasoner will sort it all out for you. Run the reasoner and issue the DL query hasAncestor value William_George_Bright_1901 . Issue the query isAncestorOf value Robert_David_Bright_1965 . The hasAncestor object property will look like this: ObjectProperty: hasAncestor SubPropertyOf: hasRelation SuperPropertyOf: hasParent, Characteristics: Transitive InverseOf: isAncestorOf As usual, it is best to think of the objects or individuals involved in the relationships. Consider the three individuals \u2013 Robert, David and William. Each has a hasFather property, linking Robert to David and then David to William. As hasFather implies its super-property hasParent , Robert also has a hasParent property with David, and David has a hasParent relation to William. Similarly, as hasParent implies hasAncestor , the Robert object has a hasAncestor relation to the David object and the David object has one to the William object. As hasAncestor is transitive, Robert not only holds this property to the David object, but also to the William object (and so on back through Robert\u2019s ancestors). 3.2 Grandparents and Great Grandparents \u00b6 We also want to use a sort of restricted transitivity in order to infer grandparents, great grandparents and so on. My grandparents are my parent\u2019s parents; my grandfathers are my parent\u2019s fathers. My great grandparents are my parent\u2019s parent\u2019s parents. My great grandmothers are my parent\u2019s parent\u2019s mothers. This is sort of like transitivity, but we want to make the paths only a certain length and, in the case of grandfathers, we want to move along two relationships \u2013 hasParent and then hasFather . We can do this with OWL 2\u2019s sub-property chains. The way to think about sub-property chains is: If we see property x followed by property y linking three objects, then it implies that property z is held between Figure 3.1: Three blobs representing objects of the classPerson. The three objects are linked by a hasParent property and this implies a hasGrandparent property. the first and third objects. Figure 3.1 shows this diagrammatically for the hasGrandfather property. For various grandparent object properties we need the following sets of implications: My parent\u2019s parents are my grandparents; My parent\u2019s fathers are my grandfathers; My parent\u2019s mothers are my grandmothers; My parent\u2019s parent\u2019s parents are my great grandparents or my grandparent\u2019s parents are my great grandparents. My parent\u2019s parent\u2019s fathers are my great grandfathers or my parent\u2019s grandfathers are my great grandfathers; My parent\u2019s parent\u2019s mothers are my great grandmothers (and so on). Notice that we can trace the paths in several ways, some have more steps than others, though the shorter paths themselves employ paths. Tracing these paths is what OWL 2\u2019s sub-property chains achieve. For the new object property hasGrandparent we write: ObjectProperty: hasGrandparent SubPropertyChain: hasParent o hasParent We read this as \u2018 hasParent followed by hasParent implies hasGrandparent \u2019. We also need to think where the hasGrandparent property fits in our growing hierarchy of object properties. Think about the implications: Does holding a hasParent property between two objects imply that they also hold a hasGrandparent property? Of course the answer is \u2018no\u2019. So, this new property is not a super-property of hasParent . Does the holding of a hasGrandparent property between two objects imply that they also hold an hasAncestor property? The answer is \u2018yes\u2019; so that should be a super-property of hasGrandparent . We need to ask such questions of our existing properties to work out where we put it in the object property hierarchy. At the moment, our hasGrandparent property will look like this: ObjectProperty: hasGrandParent SubPropertyOf: hasAncestor SubPropertyChain: hasParent o hasParent SuperPropertyOf: hasGrandmother, hasGrandfather InverseOf: isGrandParentOf Do the following task: Task 5: Grandparents object properties Make the hasGrandparent , hasGrandmother and hasGrandfather object properties and the obvious inverses (see OWL code above); Go to the individuals tabs and inspects the inferred object property assertions for Robert_David_Bright_1965 and his parents. Again, think of the objects involved. We can take the same three objects as before: Robert, David and William. Think about the properties that exist, both by assertion and implication, between these objects. We have asserted only hasFather between these objects. The inverse can be inferred between the actual individuals (remember that this is not the case for class level restrictions \u2013 that all instances of a class hold a property does not mean that the filler objects at the other end hold the inverse; the quantification on the restriction tells us this). Remember that: Robert holds a hasFather property with David; David holds a hasFather property with William; By implication through the hasParent super-property of hasFather , Robert holds a hasParent property with David, and the latter holds one with William; The sub-property chain on hasGrandfather then implies that Robert holds a hasGrandfather property to William. Use the diagram in figure 3.1 to trace the path; there is a hasParent path from Robert to William via David and this implies the hasGrandfather property between Robert and William. It is also useful to point out that the inverse of hasGrandfather also has the implication of the sub-property chain of the inverses of hasParent . That is, three objects linked by a path of two isParentOf properties implies that an isGrandfatherOf property is established between the first and third object, in this case William and Robert. As the inverses of hasFather are established by the reasoner, all the inverse implications also hold. 3.3 Summary \u00b6 It is important when dealing with property hierarchies to think in terms of properties between objects and of the implications \u2018up the hierarchy\u2019. A sub-property implies its super-property. So, in our FHKB, two person objects holding a hasParent property between them, by implication also hold an hasAncestor property between them. In turn, hasAncestor has a super-property hasRelation and the two objects in question also hold, by implication, this property between them as well. We made hasAncestor transitive. This means that my ancestor\u2019s ancestors are also my ancestors. That a sub-property is transitive does not imply that its super-property is transitive. We have seen that by manipulating the property hierarchy we can generate a lot of inferences without adding any more facts to the individuals in the FHKB. This will be a feature of the whole process \u2013 keep the work to the minimum (well, almost). In OWL 2, we can also trace \u2018paths\u2019 around objects. Again, think of the objects involved in the path of properties that link objects together. We have done simple paths so far \u2013 Robert linked to David via hasParent and David linked to William via hasFather implies the link between Robert and William of hasGrandfather . If this is true for all cases (for which you have to use your domain knowledge), one can capture this implication in the property hierarchy. Again, we are making our work easier by adding no new explicit facts, but making use of the implication that the reasoner works out for us. The FHKB ontology at this stage of the tutorial has an expressivity ofALRI+. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.262 sec (0.00014 % of final), by Pellet 2.2.0 0.030 sec (0.00024 % of final) and by FaCT++ 1.6.4 is approximately 0.004 sec (0.000 % of final). 0 sec indicates failure or timeout. Chapter 4 \u00b6 Modelling the Person Class \u00b6 In this Chapter you will: Create a Person class; Describe Sex classes; Define Man and Woman ; Ask which of the people in the FHKB has a father. Add domains and ranges to the properties in the FHKB. Make the FHKB inconsistent. Add some more defined classes about people and see some equivalence inferred between classes. These simple classes will form the structure for the whole FHKB. 4.1 The Class of Person \u00b6 For the FHKB, we start by thinking about the objects involved The people in a family \u2013 Robert, Richard, David, Margaret, William, Iris, Charles, Violet, Eileen, John and Peter; The sex of each of those people; The marriages in which they participated; The locations of their births; And many more... There is a class of Person that we will use to represent all these people objects. Task 6: Create the Person class Create a class called DomainEntity ; Create a subclass of DomainEntity called Person . We use DomainEntity as a house-keeping measure. All of our ontology goes underneath this class. We can put other classes \u2018outside\u2019 the ontology, as siblings of DomainEntity , such as \u2018probe\u2019 classes we wish to use to test our ontology. The main thing to remember about the Person class is that we are using it to represent all \u2018people\u2019 individuals. When we make statements about the Person class, we are making statements about all \u2018people\u2019 individuals. What do we know about people? All members of the Person class have: Sex \u2013 they are either male or female; Everyone has a birth year; Everyone has a mother and a father. There\u2019s a lot more we know about people, but we will not mention it here. 4.2 Describing Sex in the FHKB \u00b6 Each and every person object has a sex. In the FHKB we will take a simple view on sex \u2013 a person is either male or female, with no intersex or administrative sex and so on. Each person only has one sex. We have two straight-forward options for modelling sex: Each person object has their own sex object, which is either male or female. Thus Robert\u2019s maleness is different from David\u2019s maleness. There is only one Maleness object and one Femaleness object and each person object has a relationship to either one of these sex objects, but not both. We will take the approach of having a class of Maleness objects and a class of Femaleness objects. These are qualities or attributes of self-standing objects such as a person. These two classes are disjoint, and each is a subclass of a class called Sex . The disjointness means that any one instance of Sex cannot be both an instance of Maleness and an instance of Femaleness at once. We also want to put in a covering axiom on the class Sex , which means that any instance of Sex must be either Maleness or Femaleness ; there is no other kind of Sex . Again, notice that we have been thinking at the level of objects. We do the same when thinking about Person and their Sex . Each and every person is related to an instance of Sex . Each Person holds one relationship to a Sex object. To do this we create an object property called hasSex . We make this property functional, which means that any object can hold that property to only one distinct filler object. We make the domain of hasSex to be Person and the range to be Sex . The domain of Person means that any object holding that property will be inferred to be a member of the class Person . Putting the range of Sex on the hasSex property means that any object at the right-hand end of the hasSex property will be inferred to be of the class Sex . Again, think at the level of individuals or objects. We now put a restriction on the Person class to state that each and every instance of the class Person holds a hasSex property with an instance of the Sex class. It has an existential operator \u2018some\u2019 in the axiom, but the functional characteristic means that each Person object will hold only one hasSex property to a distinct instance of a Sex object 4 . 4 An individual could hold two hasSex properties, as long as the sex objects at the right-hand end of the property are not different. Task 7: Modelling sex Create a class called Sex ; Make it a subclass of DomainEntity ; Make Person and Sex disjoint; Create two subclasses of Sex , Maleness and Femaleness ; Make Maleness and Femaleness disjoint; Put a covering axiom on Sex such that it is equivalent to Maleness or Femaleness . Create an object property, hasSex , with the domain Person , the range Sex and give it the characteristic of \u2018Functional\u2019; Add a restriction hasSex some Sex to the class Person . The hasSex property looks like: ObjectProperty: hasSex Characteristics: Functional Domain: Person Range: Sex The Person class looks like: Class: Person SubClassOf: DomainEntity,(hasSex some Sex) DisjointWith: Sex 4.3 Defining Man and Woman \u00b6 We now have some of the foundations for the FHKB. We have the concept of Person , but we also need to have the concepts of Man and Woman . Now we have Person , together with Maleness and Femaleness , we have the necessary components to define Man and Woman . These two classes can be defined as: Any Person object that has a male sex can be recognised to be a man; any Person object that has a female sex can be recognised as a member of the class woman. Again, think about what conditions are sufficient for an object to be recognised to be a member of a class; this is how we create defined classes through the use of OWL equivalence axioms. To make the Man and Woman classes do the following: Task 8: Describe men and women Create a class Man ; Make it equivalent to a Person that hasSex some Maleness ; Do the same, but with Femaleness , to create the Woman class; A covering axiom can be put on the Person class to indicate that man and woman are the only kinds of person that can exist. (This is not strictly true due to the way Sex has been described.) Run the reasoner and take a look. Having run the reasoner, the Man and Woman classes should appear underneath Person 5 . 5 Actually in Prot\u00e9g\u00e9, this might happen without the need to run the reasoner. The Man and Woman classes will be important for use as domain and range constraints on many of the properties used in the FHKB. To achieve our aim of maximising inference, we should be able to infer that individuals are members of Man , Woman or Person by the properties held by an object. We should not have to state the type of an individual in the FHKB. The classes for Man and Woman should look like: Class: Man EquivalentTo: Person and (hasSex some Maleness) Class: Woman EquivalentTo: Person and (hasSex some Femaleness) 4.4 Describing Parentage in the FHKB \u00b6 To finish off the foundations of the FHKB we need to describe a person object\u2019s parentage. We know that each and every person has one mother and each and every person has one father. Here we are talking about biological mothers and fathers. The complexities of adoption and step parents are outside the scope of this FHKB tutorial. Task 9: Describing Parentage Add the domain Person and the range Woman to the property hasMother . Do the same for the property hasFather , but give it the range Man ; Give the property hasParent domain and range of Person ; Run the reasoner. The (inferred) property hierarchy in the FHKB should look like that shown in Figure 4.1. Notice that we have asserted the sub-property axioms on one side of the property hierarchy. Having done so, the reasoner uses those axioms, together with the inverses, to work out the property hierarchy for the \u2018other side\u2019. We make hasMother functional, as any one person object can hold only one hasMother property to a distinct Woman object. The range of hasMother is Woman , as a mother has to be a woman. The Person object holding the hasMother property can be either a man or a woman, so we have the domain constraint as Person ; this means any object holding a hasMother property will be inferred to be a Person . Similarly, any object at the right-hand end of a hasMother property will be inferred to be a Woman , which is the result we need. The same reasoning goes for hasFather and hasParent , with the sex constraints on the latter being only Person . The inverses of the two functional sub-properties of hasParent are not themselves functional. After all, a Woman can be the mother of many Person objects, but each Person object can have only one mother. Figure 4.1: The property hierarchy with the hasSex and the parentage properties Figure 4.2: the core TBox for the FHKB with the Person and Sex classes. Task 10: Restrict Person class As each and every person has a mother and each and every person has a father, place restrictions on the Person class as shown below. Class: Person SubClassOf: DomainEntity, (hasFather some Man), (hasMother some Woman), (hasSex some Sex) DisjointWith: Sex Task 11: DL queries for people and sex Issue the DL queries for Person , Man and Woman ; look at the answers and count the numbers in each class; which individuals have no sex and why? You should find that many people have been inferred to be either Man or Woman , but some are, as we will see below, only inferred to be Person . The domain and range constraints on our properties have also driven some entailments. We have not asserted that David_Bright_1934 is a member of Man , but the range constraint on hasFather (or the inferred domain constraint on the isFatherOf relation) has enabled this inference to be made. This goes for any individual that is the right-hand-side (either inferred or asserted) of either hasFather or hasMother (where the range is that of Woman ). For Robert David Bright, however, he is only the left-hand-side of an hasFather or an hasMother property, so we\u2019ve only entailed that this individual is a member of Person . 4.5 Who has a father? \u00b6 In our description of the Person class we have said that each and every instance of the class Person has a father (the same goes for mothers). So, when we ask the query \u2018which individuals have a father\u2019, we get all the instances of Person back, even though we have said nothing about the specific parentage of each Person . We do not know who their mothers and fathers are, but we know that they have one of each. We know all the individuals so far entered are members of the Person class; when asserting the type to be either Man or Woman (each of which is a subclass of Person ), we infer that each is a person. When asserting the type of each individual via the hasSex property, we know each is a Person , as the domain of hasSex is the Person class. As we have also given the right-hand side of hasSex as either Maleness or Femaleness , we have given sufficient information to recognise each of these Person instances to be members of either Man or Woman . 4.6 Filling in Domains and Ranges for the FHKB Properties \u00b6 So far we have not systematically added domains and ranges to the properties in the FHKB. As a reminder, when a property has a domain of X any object holding that property will be inferred to be a member of class X . A domain doesn\u2019t add a constraint that only members of class X hold that property; it is a strong implication of class membership. Similarly, a property holding a range implies that an object acting as right-hand-side to a property will be inferred to be of that class. We have already seen above that we can use domains and ranges to imply the sex of people within the FHKB. Do the following: Task 12: Domains and Ranges Make sure the appropriate Person , Man and Woman are domains and ranges for hasFather , hasMother and hasParent . Run the reasoner and look at the property hierarchy. Also look at the properties hasAncestor , hasGrandparent , hasUncle and so on; look to see what domains and ranges are found. Add any domains and ranges explicitly as necessary. Prot\u00e9g\u00e9 for example in its current version (November 2015) does not visualise inherited domains and ranges in the same way as it shows inferred inverse relations. We typically assert more domains and ranges than strictly necessary. For example, if we say that hasParent has the domain Person , this means that every object x that is connected to another object y via the hasParent relation must be a Person . Let us assume the only thing we said about x and y is that they are connected by a hasMother relation. Since this implies that x and y are also connected by a hasParent relation ( hasMother is a sub-property of hasParent ) we do not have to assert that hasFather has the domain of Person ; it is implied by what we know about the domain and range of hasParent . In order to remove as many assertions as possible, we may therefore choose to assert as much as we know starting from the top of the hierarchy, and only ever adding a domain if we want to constrain the already inferred domain even further (or range respectively). For example, in our case, we could have chosen to assert Person to be the domain of hasRelation . Since hasRelation is symmetric, it will also infer Person to be the range. We do not need to say anything for hasAncestor or hasParent , and only if we want to constrain the domain or range further (like in the case of hasFather by making the range Man ) do we need to actually assert something. It is worth noting that because we have built the object property hierarchy from the bottom ( hasMother etc.) we have ended up asserting more than necessary. 4.7 Inconsistencies \u00b6 From the Pizza Tutorial and other work with OWL you should have seen some unsatisfiabilities . In Prot\u00e9g\u00e9 this is highlighted by classes going \u2018red\u2019 and being subclasses ofNothing; that is, they can have no instances in that model. Task 13: Inconsistencies Add the fact Robert_David_Bright_1965 hasMother David_Bright_1934 . Run the classifier and see what happens. Remove that fact and run the classifier again. Now add the fact that Robert_David_Bright_1965 hasMother Iris_Ellen_Archer_1907 Run the classifier and see what happens. Add and remove the functional characteristic to these properties and see what happens. After asserting the first fact it should be reported by the reasoner that the ontology is inconsistent . This means, in lay terms, that the model you\u2019ve provided in the ontology cannot accommodate the facts you\u2019ve provided in the fact assertions in your ABox\u2014that is, there is an inconsistency between the facts and the ontology... The ontology is inconsistent because David_Bright_1934 is being inferred to be a Man and a Woman at the same time which is inconsistent with what we have said in the FHKB. When we, however, say that Robert David Bright has two different mothers, nothing bad happens! Our domain knowledge says that the two women are different, but the reasoner does not know this as yet... ; Iris Ellen Archer and Margaret Grace Rever may be the same person; we have to tell the reasoner that they are different. For the same reason the functional characteristic also has no effect until the reasoner \u2018knows\u2019 that the individuals are different. We will do this in Section 7.1.1 and live with this \u2018fault\u2019 for the moment. 4.8 Adding Some Defined Classes for Ancestors and so on \u00b6 Task 14: Adding defined classes Add a defined class for Ancestor , MaleAncestor , FemaleAncestor ; Add a defined class for Descendant , MaleDescendant and FemaleDescendant ; Run the reasoner and view the resulting hierarchy. The code for the classes looks like: Class: Ancestor EquivalentTo: Person and isAncestorOf some Person Class: FemaleAncestor EquivalentTo: Woman and isAncestorOf some Person Class: Descendant EquivalentTo: Person and hasAncestor some Person Class: MaleDescendant EquivalentTo: Man and hasAncestor some Person The TBox after reasoning can be seen in Figure 4.3. Notice that the reasoner has inferred that several of the classes are equivalent or \u2018the same\u2019. These are: Descendant and Person ; MaleDescendant and Man , FemaleDescendant and Woman . The reasoner has used the axioms within the ontology to infer that all the instances of Person are also instances of the class Descendant and that all the instances of Woman are also the same instances as the class Female Descendant . This is intuitively true; all people are descendants \u2013 they all have parents that have parents etc. and thus everyone is a descendant. All women are female people that have parents etc. As usual we should think about the objects within the classes and what we know about them. This time it is useful to think about the statements we have made about Person in this Chapter \u2013 that all instances of Person have a father and a mother; add to this the information from the property hierarchy and we know that all instances of Person have parents and ancestors. We have repeated all of this in our new defined classes for Ancestor and Descendant and the reasoner has highlighted this information. Figure 4.3: The defined classes from Section 4.8 in the FHKB\u2019s growing class hierarchy Task 15: More Ancestors Query for MaleDescendant . You should get Man back - they are equivalent (and this makes sense). As an additional exercise, also add in properties for forefathers and foremothers. You will follow the same pattern as for hasAncestor , but adding in, for instance, hasFather as the sub-property of the transitive super-property of hasForefather and setting the domains and ranges appropriately (or working out if they\u2019ll be inferred appropriately). Here we interpret a forefather as one\u2019s father\u2019s father etc. This isn\u2019t quite right, as a forefather is any male ancestor, but we\u2019ll do it that way anyway. You might want to play around with DL queries. Because of the blowup in inferred relationships, we decided to not include this pattern in the tutorial version of the FHKB. 4.9 Summary \u00b6 Most of what we have done in this chapter is straight-forward OWL, all of which would have been met in the pizza tutorial. It is, however, a useful revision and it sets the stage for refining the FHKB. Figure 4.2 shows the basic set-up we have in the FHKB in terms of classes; we have a class to represent person, man and woman, all set-up with a description of sex, maleness and femaleness. It is important to note, however, the approach we have taken: We have always thought in terms of the objects we are modelling. Here are some things that should now be understood upon completing this chapter: Restrictions on a class in our TBox mean we know stuff about individuals that are members of that class, even though we have asserted no facts on those individuals. We have said, for instance, that all members of the class Person have a mother, so any individual asserted to be a Person must have a mother. We do not necessarily know who they are, but we know they have one. Some precision is missing \u2013 we only know Robert David Bright is a Person , not that he is a Man . This is because, so far, he only has the domain constraint of hasMother and hasFather to help out. We can cause the ontology to be inconsistent, for example by providing facts that cannot be accommodated by the model of our ontology. In the example, David Bright was inferred to be a member of two disjoint classes. Finally, we looked at some defined classes. We inferred equivalence between some classes where the extents of the classes were inferred to be the same \u2013 in this case the extents of Person and Descendant are the same. That is, all the objects that can appear in Person will also be members of Descendant . We can check this implication intuitively \u2013 all people are descendants of someone. Perhaps not the most profound inference of all time, but we did no real work to place this observation in the FHKB. This last point is a good general observation. We can make the reasoner do work for us. The less maintenance we have to do in the FHKB the better. This will be a principle that works throughout the tutorial. The FHKB ontology at this stage of the tutorial has an expressivity of SRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.884 sec (0.00047 % of final), by Pellet 2.2.0 0.256 sec (0.00207 % of final) and by FaCT++ 1.6.4 is approximately 0.013 sec (0.000 % of final). 0 sec indicates failure or timeout. Chapter 5 \u00b6 Siblings in the FHKB \u00b6 In this chapter you will: Explore options for determining finding siblings; Meet some of the limitations in OWL; Choose one of the options explored; Add facts for siblings; Use sub-property chains to find aunts and uncles; There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial 5.1 Blood relations \u00b6 Do the following first: Task 16: The bloodrelation object property Create an hasBloodrelation object property, making it a sub-property of hasRelation . Add appropriate property characteristics. Make the already existing hasAncestor property a sub-property of hasBloodrelation . Does a blood relation of Robert have the same relationship to Robert (symmetry)? Is a blood relation of Robert\u2019s blood relation a blood relation of Robert (transitivity)? Think of an aunt by marriage; her children are my cousins and blood relations via my uncle, but my aunt is not my blood relation. My siblings share parents; male siblings are brothers and female siblings are sisters. So far we have asserted parentage facts for the Person in our ABox. Remember that our parentage properties have inverses, so if we have added an hasFather property between a Person and a Man , we infer the isFatherOf property between that Man and that Person . 5.2 Siblings: Option One \u00b6 We should have enough information within the FHKB to infer siblings. We could use a sub-property chain such as: ObjectProperty: hasSibling SubPropertyOf: hasBloodrelation Characteristics: Symmetric, transitive SubPropertyChain: hasParent o isParentOf We make a property of hasSibling and make it a sub-property of hasBloodrelation . Remember, think of the objects involved and the implications we want to follow; being a sibling implies being a blood relation, it does not imply any of the other relationships we have in the FHKB. Note that we have made hasSibling symmetric; if Robert is sibling of Richard, then Richard is sibling of Robert. We should also think about transitivity; if David is sibling of Peter and Peter is sibling of John, then David is sibling of John. So, we make hasSibling symmetric and transitive (see Figure 5.1). However, we must take care of half-siblings: child 1 and child 2 share a mother, but not a father; child 2 and child 3 share the father, but not the mother \u2013 child 1 and child 3 are not even half-siblings. However, at least for the moment, we will simply ignore this inconvenience, largely so that we can explore what happens with different modelling options. Figure 5.1: Showing the symmetry and transitivity of the hasSibling (siblingof) property by looking at the brothers David, John and Peter We also have the implication using three objects (see Figure 5.2): Robert holds a hasParent property with David; David holds an isFatherOf property with Richard; This implies that Robert holds a hasSibling property with Richard; As hasSibling is symmetric, Richard holds an hasSibling property with Robert. Figure 5.2: Tracing out the sub-property chain for hasSibling ; note that Robert is a sibling of himself by this path Do the following tasks: Task 17: Siblings Add the hasSibling property as above; Run the reasoner; Ask the DL query hasSibling value Robert_David_Bright_1965 . From this last DL query you should get the answer that both Robert and Richard are siblings of Robert. Think about the objects involved in the sub-property chain: we go from Robert to David via the hasParent and from David to Richard via the isParentOf property; so this is OK. However, we also go from Robert to David and then we can go from David back to Robert again \u2013 so Robert is a sibling of Robert. We do not want this to be true. We can add another characteristic to the hasSibling property, the one of being irreflexive . This means that an object cannot hold the property with itself. Task 18: More siblings Add the irreflexive characteristic to the hasSibling property; Run the reasoner; Note that the reasoner claims you have an inconsistent ontology (or in some cases, you might get a message box saying \"Reasoner died\"). Looking at the hasSibling property again, the reason might not be immediately obvious. The reason for the inconsistency lies in the fact that we create a logical contradiction: through the property chain, we say that every Person is a sibling of him or herself, and again disallowing just that by adding the irreflexive characteristic. A different explanation lies within the OWL specification itself: In order to maintain decidability irreflexive properties must be simple - for example, they may not be property chains 6 . 6 http://www.w3.org/TR/owl2-syntax/#The_Restrictions_on_the_Axiom_Closure 5.2.1 Brothers and Sisters \u00b6 We have only done siblings, but we obviously need to account for brothers and sisters. In an analogous way to motherhood, fatherhood and parenthood, we can talk about sex specific sibling relationships implying the sex neutral hasSibling ; holding either a hasBrother or an isSisterOf between two objects would imply that a hasSibling property is also held between those two objects. This means that we can place these two sex specific sibling properties below hasSibling with ease. Note, however, that unlike the hasSibling property, the brother and sister properties are not symmetric. Robert hasBrother Richard and vice versa , but if Daisy hasBrother William, we do not want William to hold an hasBrother property with Daisy. Instead, we create an inverse of hasBrother , isBrotherOf , and the do the same for isSisterOf . We use similar, object based, thought processes to choose whether to have transitivity as a characteristic of hasBrother . Think of some sibling objects or individuals and place hasBrother properties between them. Make it transitive and see if you get the right answers. Put in a sister to and see if it stil works. If David hasBrother Peter and Peter hasBrother John, then David hasBrother John; so, transitivity works in this case. Think of another example. Daisy hasBrother Frederick, and Frederick hasBrother William, thus Daisy hasBrother William. The inverses work in the same way; William isBrotherOf Frederick and Frederick isBrotherOf Daisy; thus William isBrotherOf Daisy. All this seems reasonable. Task 19: Brothers and sisters Create the hasBrother object property as shown below; Add hasSister in a similar manner; 3. Add appropriate inverses, domains and ranges. ObjectProperty: hasBrother SubPropertyOf: hasSibling Characteristics: Transitive InverseOf: isBrotherOf Range: Man We have some hasSibling properties (even if they are wrong). We also know the sex of many of the people in the FHKB through the domains and ranges of properties such as hasFather , hasMother and their inverses.. Can we use sub-property chains in the same way as we have used them in the hasSibling property? The issue is that of sex; the property isFatherOf is sex neutral at the child end, as is the inverse hasFather (the same obviously goes for the mother properties). We could use a sub-property chain of the form: ObjectProperty: hasBrother SubPropertyChain: hasParent o hasSon A son is a male child and thus that object is a brother of his siblings. At the moment we do not have son or daughter properties. We can construct a property hierarchy as shown in Figure 5.3. This is made up from the following properties: hasChild and isChildOf hasSon (range Man and domain Person ) and isSonOf ; hasDaughter (range Woman domain Person ) and isDaughterOf Note that hasChild is the equivalent of the existing property isParentOf ; if I have a child, then I am its parent. OWL 2 can accommodate this fact. We can add an equivalent property axiom in the following way: ObjectProperty: isChildOf EquivalentTo: hasParent We have no way of inferring the isSonOf and isDaughterOf from what already exists. What we want to happen is the implication of \u2018 Man and hasParent Person implies isSonOf \u2019. OWL 2 and its reasoners cannot do this implication. It has been called the \u2018man man problem\u2019 7 . Solutions for this have been developed [3], but are not part of OWL 2 and its reasoners. Figure 5.3: The property hierarchy for isChildOf and associated son/daughter properties 7 http://lists.w3.org/Archives/Public/public-owl-dev/2007JulSep/0177.html Child property Parent Robert David Bright 1965 isSonOf David Bright 1934, Margaret Grace Rever 1934 Richard John Bright 1962 isSonOf David Bright 1934, Margaret Grace Rever 1934 Mark Bright 1956 isSonOf John Bright 1930, Joyce Gosport Ian Bright 1959 isSonOf John Bright 1930, Joyce Gosport Janet Bright 1964 isDaughterOf John Bright 1930, Joyce Gosport William Bright 1970 isSonOf John Bright 1930, Joyce Gosport Table 5.1: Child property assertions for the FHKB Thus we must resort to hand assertions of properties to test out our new path: Task 20: Sons and daughters Add the property hierarchy shown in Figure 5.3, together with the equivalent property axiom and the obvious inverses. As a test (after running the reasoner), ask the DL query isChildOf value David_Bright_1934 and you should have the answer of Richard and Robert; Add the sub-property paths as described in the text; Add the assertions shown in Table 5.1; Run the reasoner; Ask the DL query for the brother of Robert David Bright and the sister of Janet. Of course, it works, but we see the same problem as above. As usual, think of the objects involved. Robert isSonOf David and David isParentOf Robert, so Robert is his own brother. Irreflexivity again causes problems as it does above (Task 18). 5.3 Siblings: Option two \u00b6 Our option one has lots of problems. So, we have an option of asserting the various levels of sibling. We can take the same basic structure of sibling properties as before, but just fiddle around a bit and rely on more assertion while still trying to infer as much as possible. We will take the following approach: We will take off the sub-property chains of the sibling properties as they do not work; We will assert the leaf properties of the sibling sub-hierarchy sparsely and attempt to infer as much as possible. Person Property Person Robert David Bright 1965 isBrotherOf Richard John Bright 1962 David Bright 1934 isBrotherOf John Bright 1930 David Bright 1934 isBrotherOf Peter William Bright 1941 Janet Bright 1964 isSisterOf Mark Bright 1956 Janet Bright 1964 isSisterOf Ian Bright 1959 Janet Bright 1964 isSisterOf William Bright 1970 Mark Bright 1956 isBrotherOf Ian Bright 1959 Mark Bright 1956 isBrotherOf Janet Bright 1964 Mark Bright 1956 isBrotherOf William Bright 1970 Table 5.2: The sibling relationships to add to the FHKB. Do the following: Task 21: Add sibling assertions Remove the sub-property chains of the sibling properties and the isChildOf assertions as explained above. Add the Sibling assertions shown in table 5.2; Run the reasoner; Ask isBrotherOf value Robert_David_Bright_1965 ; Ask isBrotherOf value Richard_John_Bright_1962 ; Ask hasBrother value Robert_David_Bright_1965 ; Ask hasBrother value Richard_John_Bright_1962 ; Ask isSisterOf value William_Bright_1970 ; Ask the query Man and hasSibling value Robert_David_Bright_1965 . We can see some problems with this option as well: With these properties asserted, Richard only has a hasBrother property to Robert. We would really like an isBrotherOf to Robert to hold. The query Man and hasSibling value Robert only retrieves Robert himself. Because we only asserted that Robert is a brother of Richard, and the domain of isBrotherOf is Man we know that Robert is a Man , but we do not know anything about the Sex of Richard. 5.3.1 Which Modelling Option to Choose for Siblings? \u00b6 Which of the two options gives the worse answers and which is the least effort? Option one is obviously the least effort; we only have to assert the same parentage facts as we already have; then the sub-property chains do the rest. It works OK for hasSibling , but we cannot do brothers and sisters adequately; we need Man and hasSibling \u2290 isBrotherOf and we cannot do that implication. This means we cannot ask the questions we need to ask. So, we do option two, even though it is hard work and is still not perfect for query answering, even though we have gone for a sparse assertion mode. Doing full sibling assertion would work, but is a lot of effort. We could start again and use the isSonOfandisDaughterOf option, with the sub-property chains described above. This still has the problem of everyone being their own sibling. It can get the sex specific sibling relationships, but requires a wholesale re-assertion of parentage facts. We will continue with option two, largely because it highlights some nice problems later on. 5.4 Half-Siblings \u00b6 In Section 5.2 we briefly talked about half-siblings. So far, we have assumed full-siblings (or, rather, just talked about siblings and made no distinction). Ideally, we would like to accommodate distinctions between full- and half-siblings; here we use half-siblings, where only one parent is in common between two individuals, as the example. The short-answer is, unfortunately, that OWL 2 cannot deal with half-siblings in the way that we want - that is, such that we can infer properties between named individuals indicating full- or half-sibling relationships. It is possible to find sets of half-brothers in the FHKB by writing a defined class or DL-query for a particular individual.} The following fragment of OWL defines a class that looks for the half-brothers of an individual called \u2018Percival\u2019: Class: HalfBrotherOfPercival EquivalentTo: Man and (((hasFather some (not (isFatherOf value Percival))) and (hasMother some (isMotherOf value Percival))) or ((hasFather some (isFatherOf value Percival)) and (hasMother some (not (isMotherOf value Percival))))) Here we are asking for any man that either has Percival\u2019s father but not his mother, or his mother, but not his father. This works fine, but is obviously not a general solution. The OWL description is quite complex and the writing will not scale as the number of options (hypothetically, as the number of parents increases... ) increases; it is fine for man/woman, but go any higher and it will become very tedious to write all the combinations. Another way of doing this half-brother class to find the set of half-brothers of a individual is to use cardinality constraints: Class: HalfBrotherOfPercival EquivalentTo: Man and (hasParent exactly 1 (isParentOf value Percival)) This is more succinct. We are asking for a man that has exactly one parent from the class of individuals that are the class of Percival\u2019s parents. This works, but one more constraint has to be present in the FHKB. We need to make sure that there can be only two parents (or indeed, just a specified number of parents for a person). If we leave it open as to the number of parents a person has, the reasoner cannot work out that there is a man that shares exactly one parent, as there may be other parents. We added this constraint to the FHKB in Section 6.2; try out the classes to check that they work. These two solutions have been about finding sets of half-brothers for an individual. What we really want in the FHKB is to find half-brothers between any given pair of individuals. Unfortunately we cannot, without rules, ask OWL 2 to distinguish full- and half-siblings \u2013 we cannot count the number of routes taken between siblings via different distinct intermediate parent objects. 5.5 Aunts and Uncles \u00b6 An uncle is a brother of either my mother or father. An aunt is a sister of either my mother or father. In common practice, wives and husbands of aunts and uncles are usually uncles and aunts respectively. Formally, these aunts and uncles are aunts-in-law and uncles-in-law. Whatever approach we take, we cannot fully account for aunts and uncles until we have information about marriages, which will not have until Chapter 9. We will, however, do the first part now. Look at the objects and properties between them for the following facts: Robert has father David and mother Margaret; David has brothers Peter and John; Margaret has a sister Eileen; Robert thus has the uncles John and Peter and an aunt Eileen. As we are tracing paths or \u2018chains\u2019 of objects and properties we should use sub-property chains as a solution for the aunts and uncles. We can make an hasUncle property as follows (see Figure 5.4): ObjectProperty: hasUncle SubPropertyOf: hasBloodrelation Domain: Man Range: Person SubPropertyChain: hasParent o hasBrother InverseOf: isUncleOf Figure 5.4: Tracing out the path between objects to get the hasUncle sub-property chain. Notice we have the domain of Man and range of Person . We also have an inverse. As usual, we can read this as \u2018an object that holds an hasParent property, followed by an object holding a hasBrother property, implies that the first object holds an hasUncle property with the last object\u2019. Note also where the properties (include the ones for aunt) go in the object property hierarchy. Aunts and uncles are not ancestors that are in the direct blood line of a person, but they are blood relations (in the narrower definition that we are using). Thus the aunt and uncle properties go under the hasBloodrelation property (see Figure 5.5). Again, think of the implications between objects holding a property between them; that two objects linked by a property implies that those two objects also hold all the property\u2019s super-properties as well. As long as all the super-properties are true, the place in the object property hierarchy is correct (think about the implications going up, rather than down). Figure 5.5: The object property hierarchy with the aunt and uncle properties included. On the right side, we can see the hasUncle property as shown by Prot\u00e9g\u00e9. Do the following tasks: Task 22: Uncles and Aunts Add the hasUncle property as above; Add the hasAunt property as well; Ask for the uncles of Julie_Bright_1966 and for Mark_Bright_1956 ; Add similar properties for hasGreatUncle and hasGreatAunt and place them in the property hierarchy. We can see this works \u2013 unless we have any gaps in the sibling relationships (you may have to fix these). Great aunts and uncles are simply a matter of adding another \u2018parent\u2019 leg into the sub-property chain. We are not really learning anything new with aunts and uncles, except that we keep gaining a lot for free through sub-property chains. We just add a new property with its sub-property chain and we get a whole lot more inferences on individuals. To see what we now know about Robert David Bright, do the following: Task 23: What do we know? Save the ontology and run the reasoner; Look at inferences related to the individual Robert David Bright (see warning in the beginning of this chapter). If you chose to use DL queries in Prot\u00e9g\u00e9, do not forget to tick the appropriate check-boxes. You can now see lots of facts about Robert David Bright, with only a very few actual assertions directly on Robert David Bright. 5.6 Summary \u00b6 Siblings have revealed several things for us: We can use just the parentage facts to find siblings, but everyone ends up being their own sibling; We cannot make the properties irreflexive, as the knowledge base becomes inconsistent; We would like an implication of Man and hasSibling \u2283 isBrotherOf , but OWL 2 doesn\u2019t do this implication; Whatever way we model siblings, we end up with a bit of a mess; OWL 2 cannot do half-siblings; However, we can get close enough and we can start inferring lots of facts via sub-property chains using the sibling relationships. The FHKB ontology at this stage of the tutorial has an expressivity ofSRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1355.614 sec (0.71682 % of final), by Pellet 2.2.0 0.206 sec (0.00167 % of final) and by FaCT++ 1.6.4 is approximately 0.039 sec (0.001 % of final). 0 sec indicates failure or timeout. Chapter 6 \u00b6 Individuals in Class Expressions \u00b6 In this chapter you will: Use individuals within class expressions; Make classes to find Robert and Richard\u2019s parents, ancestors, and so on; Explore equivalence of such classes; Re-visit the closed world. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial 6.1 Richard and Robert\u2019s Parents and Ancestors \u00b6 So far we have only used object properties between unspecified objects. We can, however, specify a specific individual to act at the right-hand-side of a class restriction or type assertion on an individual. The basic syntax for so-called nominals is: Class: ParentOfRobert EquivalentTo: Person and isParentOf valueRobert_David_Bright_1965 This is an equivalence axiom that recognises any individual that is a Person and a parent of Robert David Bright. Task 24: Robert and Richards parents Create the class ParentOfRobert as described above; Classify \u2013 inspect where the class is placed in the FHKB TBox and look at which individuals classify as members of the class; Do the same for a class with the value of Richard_John_Bright_1962 and classify; Finally create a class ParentOfRichardAndRobert , defining it as Person and isParentOf some {Robert_David_Bright_1965 ,Richard_John_Bright_1962 } ; again see what happens on classification. Note that the expressions isMotherOf value Robert_David_Bright_1965 and isMotherOf some {Robert_David_Bright_1965 } are practically identical. The only difference is that using value , you can only specify one individual, while some relates to a class (a set of individuals). We see that these queries work and that we can create more complex nominal based class expressions. The disjunction above is isParentOf some {Robert_David_Bright_1965, Richard_John_Bright_1965} The \u2018{\u2019 and \u2018}\u2019 are a bit of syntax that says \u2018here\u2019s a class of individual\u2019. We also see that the classes for the parents of Robert David Bright and Richard John Bright have the same members according to the FHKB, but that the two classes are not inferred to be equivalent. Our domain knowledge indicates the two classes have the same extents (members) and thus the classes are equivalent, but the automated reasoner does not make this inference. As usual, this is because the FHKB has not given the automated reasoner enough information to make such an inference. 6.2 Closing Down What we Know About Parents and Siblings \u00b6 The classes describing the parents of Richard and Robert are not equivalent, even though, as humans, we know their classes of parent are the same. We need more constraints so that it is known that the four parents are the only ones that exist. We can try this by closing down what we know about the immediate family of Robert David Bright. In Chapter 4 we described that a Person has exactly one Woman and exactly one Man as mother and father (by saying that the hasMother and hasFather properties are functional and thus only one of each may be held by any one individual to distinct individuals). The parent properties are defined in terms of hasParent , hasMother and hasFather . The latter two imply hasParent . The two sub-properties are functional, but there are no constraints on hasParent , so an individual can hold many instances of this property. So, there is no information in the FHKB to say a Person has only two parents (we say there is one mother and one father, but not that there are only two parents). Thus Robert and Richard could have other parents and other grandparents than those in the FHKB; we have to close down our descriptions so that only two parents are possible. There are two ways of doing this: Using qualified cardinality constraints in a class restriction; Putting a covering axiom on hasParent in the same way as we did for Sex in Chapter 4. Task 25: Closing the Person class Add the restriction hasParent exactly 2 Person to the classPerson ; Run the reasoner; Inspect the hierarchy to see where ParentOfRobert and ParentOfRichard are placed and whether or not they are found to be equivalent; Now add the restriction hasParent max 2 Person to the class Person ; Run the reasoner (taking note of how long the reasoning takes) and take another look. We find that these two classes are equivalent; we have supplied enough information to infer that these two classes are equivalent. So, we know that option one above works, but what about option two? This takes a bit of care to think through, but the basic thing is to think about how many ways there are to have a hasParent relationship between two individuals. We know that we can have either a hasFather or a hasMother property between two individuals; we also know that we can have only one of each of these properties between an individual and a distinct individual. However, the open world assumption tells us that there may be other ways of having a hasParent property between two individuals; we\u2019ve not closed the possibilities. By putting on the hasParent exactly 2 Person restriction on the Person class, we are effectively closing down the options for ways that a person can have parents; we know because of the functional characteristic on hasMother and hasFather that we can have only one of each of these and the two restrictions say that one of each must exist. So, we know we have two ways of having a parent on each Person individual. So, when we say that there are exactly two parents (no more and no less) we have closed down the world of having parents\u2014thus these two classes can be inferred to be equivalent. It is also worth noting that this extra axiom on the Person class will make the reasoner run much more slowly. Finally, for option 2, we have no way of placing a covering axiom on a property. What we\u2019d like to be able to state is something like: ObjectProperty: hasParent EquivalentTo: hasFather or hasMother but we can\u2019t. 6.3 Summary \u00b6 For practice, do the following: Task 26: Additional Practice Add lots more classes using members of the ABox as nominals; Make complex expressions using nominals; After each addition of a nominal, classify and see what has been inferred within the FHKB. See if you can make classes for GrandparentOfRobert and GrandparentOfRichard and make them inferred to be equivalent. In this chapter we have seen the use of individuals within class expressions. It allows us to make useful queries and class definitions. The main things to note is that it can be done and that there is some syntax involved. More importantly, some inferences may not be as expected due to the open world assumption in OWL. By now you might have noticed a significant increase in the time the reasoner needs to classify. Closing down what we know about family relationships takes its toll on the reasoner performance, especially the usage of 'hasParent exactly 2 Person'. At this point we recommend rewriting this axiom to 'hasParent max 2 Person'. It gives us most of what we need, but has a little less negative impact on the reasoning time. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 2067.273 sec (1.09313 % of final), by Pellet 2.2.0 0.529 sec (0.00428 % of final) and by FaCT++ 1.6.4 is approximately 0.147 sec (0.004 % of final). 0 sec indicates failure or timeout. Chapter 7 \u00b6 Data Properties in the FHKB \u00b6 We now have some individuals with some basic object properties between individuals. OWL 2, however, also has data properties that can relate an object or individual to some item of data. There are data about a Person , such as years of events and names etc. So, in this Chapter you will: Make some data properties to describe event years to people; Create some simple defined classes that group people by when they were born; Try counting the numbers of children people have... Deal with the open world assumption; Add given and family names to individuals in the FHKB. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial. 7.1 Adding Some Data Properties for Event Years \u00b6 Everyone has a birth year; death year; and some have a marriage year and so on. We can model these simply with data properties and an integer as a filler. OWL 2 has a DateTime datatype, where it is possible to specify a precise time and date down to a second. 7 This proves cumbersome (see http://robertdavidstevens.wordpress.com/2011/05/05/using-the-datetime-data-type-to-describe-birthdays/ for details); all we need is a simple indication of the year in which a person was born. Of course, the integer type has a zero, which the Gregorian calendar for which we use integer as a proxy does not, but integer is sufficient to our needs. Also, there are various ontological treatments of time and information about people (this extends to names etc. as well), but we gloss over that here\u2014that\u2019s another tutorial. 7 http://www.w3.org/TR/2008/WD-owl2-quick-reference-20081202/#Built-in_Datatypes_and_Facets We can have dates for birth, death and (eventually) marriage (see Chapter 9) and we can just think of these as event years. We can make a little hierarchy of event years as shown in Figure 7.1). Task 27: Create a data property hierarchy Create the data property hasEventYear with range integer and domain Person ; Create the data property hasBirthYear and make it a sub-property of hasEventYear (that way, the domain and range of hasEventYear are inherited); Create the data property hasDeathYear and make it a sub-property of hasEventYear ; For each individual add the birth years shown in Table A.1 (see appendix). You do not actually have to go back to the table\u2014it is easier to read the birth years simply off the individual names. Again, asserting birth years for all individuals can be a bit tedious. The reader can find a convenience snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial We now have an ABox with individuals with fact assertions to data indicating a birth year. We can, if we wish, also add a class restriction to the Person class saying that each and every instance of the class Person holds a data property to an integer and that this property is called \u2018hasBirthYear\u2019. As usual when deciding whether to place such a restriction upon a class, ask whether it is true that each and every instance of the class holds that property; this is exactly the same as we did for the object properties in Chapter 4. Everyone does have a birth year, even if it is not known. Once birth years have been added to our individuals, we can start asking some questions. Task 28: DL queries 1. Use a DL query to ask: Person born after 1960; Person born in the 1960s; Person born in the 1800s; Person that has fewer than three children; Person that has more than three children. The DL query for people born in the 1960s is: Person and hasBirthYear some int[>= 1960, < 1970] This kind of interval is known as a facet. 7.1.1 Counting Numbers of Children \u00b6 The last two queries in the list do not work as expected. We have asked, for instance, for Person that have more than three children, but we get no members of Person in the answer, though we know that there are some in the FHKB (e.g., John_Bright_1930 ). This is because there is not enough information in the FHKB to tell that this person has more than three different people as children. As humans we can look at the four children of John Bright and know that they are different \u2013 for instance, they all have different birth years. The automated reasoner, however, does not know that a Person can only have one birth year. Task 29: Make a functional object property Make the property hasBirthYear functional. Ask the query for Person that has more than three children again. This time the query should work. All the other event year properties should be made functional, expect hasEventYear , as one individual can have many event years. As the children have different birth year and an individual can only hold one hasBirthYear property, then these people must be distinct entities. Of course, making birth year functional is not a reliable way of ensuring that the automated reasoner knows that the individual are different. It is possible for two Person to have the same birth year within the same family \u2013 twins and so on. Peter_William_Bright_1941 has three children, two of which are twins, so will not be a member of the class of people with at least three children. So, we use the different individuals axiom. Most tools, including Prot\u00e9g\u00e9, have a feature that allows all individuals to be made different. Task 30: Make all individuals different Make all individuals different; Ask the above queries again. From now on, every time you add individuals, make sure the different individuals axiom is updated. 7.2 The Open World Assumption \u00b6 We have met again the open world assumption and its importance in the FHKB. In the use of the functional characteristic on the hasBirthYear property, we saw one way of constraining the interpretation of numbers of children. We also introduced the \u2018different individuals\u2019 axiom as a way of making all individuals in a knowledge base distinct. There are more questions, however, for which we need more ways of closing down the openness of OWL 2. Take the questions: People that have exactly two children; People that have only brothers; People that have only female children. We can only answer these questions if we locally close the world.We have said that David and Margaret have two children, Richard and Robert, but we have not said that there are not any others. As usual, try not to apply your domain knowledge too much; ask yourself what the automated reasoner actually knows. As we have the open world assumption, the reasoner will assume, unless otherwise said, that there could be more children; it simply doesn\u2019t know. Think of a railway journey enquiry system. If I ask a standard closed world system about the possible routes by rail, between Manchester and Buenos Aires, the answer will be \u2019none\u2019, as there are none described in the system. With the open world assumption, if there is no information in the system then the answer to the same question will simply be \u2018I don\u2019t know\u2019. We have to explicitly say that there is no railway route from Manchester to Buenos Aires for the right answer to come back. We have to do the same thing in OWL. We have to say that David and Margaret have only two children. We do this with a type assertion on individuals. So far we have only used fact assertions. A type assertion to close down David Bright\u2019 parentage looks like this: isParentOf only {Robert_David_Bright_1965,Richard_John_Bright_1962 } This has the same meaning as the closure axioms that you should be familiar with on classes. We are saying that the only fillers that can appear on the right-hand-side of the isParentOf property on this individual are the two individuals for Richard and Robert. We use the braces to represent the set of these two individuals. Task 31: Make a closure axiom Add the closure assertion above to David Bright; Issue the DL query isParentOf exactly 2 Person . The last query should return the answer of David Bright. Closing down the whole FHKB ABox is a chore and would really have to be done programmatically. OWL scripting languages such as the Ontology Preprocessing Language 8 (OPPL) [2] can help here. Also going directly to the OWL API [1] 9 , if you know what you are doing, is another route. Adding all these closure type assertions can slow down the reasoner; so think about the needs of your system \u2013 just adding it \u2018because it is right\u2019 is not necessarily the right route. 8 http://oppl2.sourceforge.net 9 http://owlapi.sourceforge.net/ 7.3 Adding Given and Family Names \u00b6 We also want to add some other useful data facts to people \u2013 their names. We have been putting names as part of labels on individuals, but data fact assertions make sense to separate out family and given names so that we can ask questions such as \u2018give me all people with the family name Bright and the first given name of either James or William\u2019. A person\u2019s name is a fact about that person and is more, in this case, than just a label of the representation of that person. So, we want family names and given names. A person may have more than one given name \u2013 \u2018Robert David\u2019, for instance \u2013 and an arbitrary number of given names can be held. For the FHKB, we have simply created two data properties of hasFirstGivenName and hasSecondGivenName ). Ideally, it would be good to have some index on the property to given name position, but OWL has no n-ary relationships. Otherwise, we could reify the hasGivenName property into a class of objects, such as the following: Class: GivenName SubClassOf:hasValue some String, hasPosition some Integer but it is really rather too much trouble for the resulting query potential. As already shown, we will use data properties relating instances of Person to strings. We want to distinguish family and given names, and then different positions of given names through simple conflating of position into the property name. Figure 7.1 shows the intended data property hierarchy. Figure 7.1: The event year and name data property hierarchies in the FHKB. Do the following: Task 32: Data properties Create the data properties as described in Figure 7.1; Give the hasName property the domain of Person and the range of String ; Make the leaf properties of given names functional; Add the names shown in Table A.1 (appendix); Again, it may be easier to read the names of the individual names. Ask the questions: all the people with the first given name \u2018James\u2019; all the people with the first given name \u2018William\u2019; All the people with the given name \u2018William\u2019; All the people with the given name \u2018William\u2019 and the family name \u2018Bright\u2019. The name data property hierarchy and the queries using those properties displays what now should be familiar. Sub-properties that imply the super-property. So, when we ask hasFirstGivenName value \"William\" and then the query hasGivenName value value \"William\" we can expect different answers. There are people with \u2018William\u2019 as either first or second given name and asking the question with the super-property for given names will collect both first and second given names. 7.4 Summary \u00b6 We have used data properties that link objects to data such as string, integer, floats and Booleans etc. OWL uses the XML data types. We have seen a simple use of data properties to simulate birth years. The full FHKB also uses them to place names (given and family) on individuals as strings. This means one can ask for the Person with the given name \"James\", of which there are many in the FHKB. Most importantly we have re-visited the open world assumption and its implications for querying an OWL ABox. We have looked at ways in which the ABox can be closed down \u2013 unreliably via the functional characteristic (in this particular case) and more generally via type assertions. All the DL queries used in this chapter can also serve as defined classes in the TBox. It is a useful exercise to progressively add more defined classes to the FHKB TBox. Make more complex queries, make them into defined classes and inspect where they appear in the class hierarchy. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1891.157 sec (1.00000 % of final), by Pellet 2.2.0 1.134 sec (0.00917 % of final) and by FaCT++ 1.6.4 is approximately 0.201 sec (0.006 % of final). 0 sec indicates failure or timeout. Note that we now cover the whole range of expressivity of OWL 2. HermiT at least is impossibly slow by now. This may be because HermiT does more work than the others. For now, we recommend to use either Pellet or FaCT++. Chapter 8 \u00b6 Cousins in the FHKB \u00b6 In this Chapter you will Revise or get to know about degrees and removes of cousin; Add the properties and sub-property chains for first and second cousins; Add properties and sub-property chains for some removes of cousins; Find out that the siblings debacle haunts us still; Add a defined class that does first cousins properly. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Be warned; from here on the reasoner can start running slowly! Please see warning at the beginning of the last chapter for more information. 8.1 Introducing Cousins \u00b6 Cousins can be confusing, but here is a brief summary: First cousins share a grandparent, but are not siblings; Second cousins share a great grandparent, but are not first cousins or siblings; Degrees such as first and second cousin give the distance to the nearest common ancestor; Removes give differences in generation. So, my Dad\u2019s first cousins (his generation) are my (Robert David Bright\u2019s) first cousins once removed. Simply, my first cousins are my parent\u2019s sibling\u2019s children. As usual, we can think about the objects and put in place some sub-property chains. 8.2 First Cousins \u00b6 Figure 8.1: Tracing out the sub-property chain for cousins going from a child to a parent, to its sibling, and down to its child, a cousin Figure 8.1 shows the sub-property chain for first cousins. As usual, think at the object level; to get to the first cousins of Robert David Bright, we go to the parents of Robert David Bright, to their siblings and then to their children. We go up, along and down. The OWL for this could be: ObjectProperty: hasFirstCousin SubPropertyOf: hasCousin SubPropertyChain: hasParent o hasSibling o hasChild Characteristics: Symmetric Note that we follow the definitions in Section 8.1 of first cousins sharing a grandparent, but not a parent. The sub-property chain goes up to children of a grandparent (a given person\u2019s parents), along to siblings and down to their children. We do not want this property to be transitive. One\u2019s cousins are not necessarily my cousins. The blood uncles of Robert David Bright have children that are his cousins. These first cousins, however, also have a mother that is not a blood relation of Robert David Bright and the mother\u2019s sibling\u2019s children are not cousins of Robert David Bright. We do, however, want the property to be symmetric. One\u2019s cousins have one\u2019s-self as a cousin. We need to place the cousin properties in the growing object property hierarchy. Cousins are obviously blood relations, but not ancestors, so they go off to one side, underneath hasBloodrelation . We should group the different removes and degree of cousin underneath one hasCousin property and this we will do. Do the following: Task 33: First cousins Add the property of hasCousin to the hierarchy underneath hasBloodrelation ; Add hasFirstCousin underneath this property; Add the sub-property chain as described above; Run the reasoner and look at the first cousins of Robert David Bright. You should see the following people as first cousins of Robert David Bright: Mark Anthony Heath, Nicholas Charles Heath, Mark Bright, Ian Bright, Janet Bright, William Bright, James Bright, Julie Bright, Clare Bright, Richard John Bright and Robert David Bright. The last two, as should be expected, are first cousins of Robert David Bright and this is not correct. As David Bright will be his own brother, his children are his own nieces and nephews and thus the cousins of his own children. Our inability to infer siblings correctly in the FHKB haunts us still and will continue to do so. Although the last query for the cousins of Robert David Bright should return the same results for every reasoner, we have had experiences where the results differ. 8.3 Other Degrees and Removes of Cousin \u00b6 Other degrees of cousins follow the same pattern as for first cousins; we go up, along and down. For second cousins we go up from a given individual to children of a great grandparent, along to their siblings and down to their grandchildren. The following object property declaration is for second cousins (note it uses the isGrandparentOf and its inverse properties, though the parent properties could be used) : ObjectProperty: hasSecondCousin SubPropertyOf: hasCousin SubPropertyChain: hasGrandParent o hasSibling o isGrandParentOf Characteristics: Symmetric \u2018 Removes \u2019 simply add in another \u2018leg\u2019 of either \u2018up\u2019 or \u2018down\u2019 either side of the \u2018along\u2019\u2014that is, think of the actual individuals involved and draw a little picture of blobs and lines\u2014then trace your finger up, along and down to work out the sub-property chain. The following object property declaration does it for first cousins once removed (note that this has been done by putting this extra \u2018leg\u2019 on to the hasFirstCousin property; the symmetry of the property makes it work either way around so that a given person is the first cousin once removed of his/her first cousins once removed): ObjectProperty: hasFirstCousinOnceRemoved SubPropertyOf: hasCousin SubPropertyChain: hasFirstCousin o hasChild Characteristics: Symmetric To exercise the cousin properties do the following: Task 34: Cousin properties Add properties for second degree cousins; Add removes for first and second degree cousins; Run the reasoner and check what we know about Robert David Bright\u2019 other types of cousin. You should see that we see some peculiar inferences about Robert David Bright\u2019 cousins \u2013 not only are his brother and himself his own cousins, but so are his father, mother, uncles and so on. This makes sense if we look at the general sibling problem, but also it helps to just trace the paths around. If we go up from one of Robert David Bright\u2019 true first cousins to a grandparent and down one parent relationship, we follow the first cousin once removed path and get to one of Robert David Bright\u2019 parents or uncles. This is not to be expected and we need a tighter definition that goes beyond sub-property chains so that we can exclude some implications from the FHKB. 8.4 Doing First Cousins Properly \u00b6 As far as inferring first cousin facts for Robert David Bright, we have failed. More precisely, we have recalled all Robert David Bright\u2019s cousins, but the precision is not what we would desire. What we can do is ask for Robert David Bright\u2019 cousins, but then remove the children of Robert David Bright\u2019 parents. The following DL query achieves this: Person that hasFirstCousin valueRobert_David_Bright_1965 and (not (hasFather valueDavid_Bright_1934) or not (hasMother valueMar- garet_Grace_Rever_1934) This works, but only for a named individual. We could make a defined class for this query; we could also make a defined class FirstCousin , but it is not of much utility. We would have to make sure that people whose parents are not known to have siblings with children are excluded. That is, people are not \u2018first cousins\u2019 whose only first cousins are themselves and their siblings. The following class does this: Class: FirstCousin EquivalentTo: Person that hasFirstCousin some Person Task 35: Roberts first cousins Make a defined class FirstCousin as shown above; Make a defined class FirstCousinOfRobert ; Create a DL query that looks at Robert_David_Bright_1965 first cousins and takes away the children of Robert_David_Bright_1965 \u2019 parents as shown above. This gives some practice with negation. One is making a class and then \u2018taking\u2019 some of it away \u2013 \u2018these, but not those\u2019. 8.5 Summary \u00b6 We have now expanded the FHKB to include most blood relationships. We have also found that cousins are hard to capture just using object properties and sub-property chains. Our broken sibling inferences mean that we have too many cousins inferred at the instance level. We can get cousins right at the class level by using our inference based cousins, then excluding some using negation. Perhaps not neat, but it works. We have reinforced that we can just add more and more relationships to individuals by just adding more properties to our FHKB object property hierarchy and adding more sub-property chains that use the object properties we have built up upon parentage and sibling properties; this is as it should be. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 111.395 sec (0.90085 % of final) and by FaCT++ 1.6.4 is approximately 0.868 sec (0.024 % of final). 0 sec indicates failure or timeout. Chapter 9 \u00b6 Marriage in the FHKB \u00b6 In this chapter you will: Model marriages and relationships; Establish object properties for husbands, wives and various in-laws; Re-visit aunts and uncles to do them properly; Use more than one sub-property chain on a given property. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Much of what is in this chapter is really revision; it is more of the same - making lots of properties and using lots of sub-property chains. However, it is worth it as it will test your growing skills and it also makes the reasoners and yourself work hard. There are also some good questions to ask of the FHKB as a result of adding marriages. 9.1 Marriage \u00b6 Marriage is a culturally complex situation to model. The FHKB started with a conservative model of a marriage involving only one man and one woman. 10 Later versions are more permissive; a marriage simply has a minimum of two partners. This leaves it open to numbers and sex of the people involved. In fact, \u2018marriage\u2019 is probably not the right name for it. Using BreedingRelationship as a label (the one favoured by the main author\u2019s mother) may be a little too stark and might be a little exclusive.... In any case, some more generic name is probably better and various subclasses of the FHKB\u2019s Marriage class are probably necessary. 10 There being no funny stuff in the Stevens family. To model marriage do the following: Task 36: Marriage Create a class Marriage , subclass of DomainEntity ; Create the properties: hasPartner (domain Marriage and range Person ) and isPartnerIn hasFemalePartner (domain Marriage and range Woman , sub-property of hasPartner ) and its inverse isFemalePartnerIn ; a sub-property of hasPartner has MalePartner (domain Marriage and range Man )and its inverse isMalePartnerIn ; Create the data property hasMarriageYear , making us a sub-property of hasEventYear ,make it functional; Create an individual m001 with the label Marriage of David and Margaret and add the facts: hasMalePartner David_Bright_1934 ; hasFemalePartner Margaret_Grace_Rever_1934 hasMarriageYear 1958 ; Create an individual m002 with the label Marriage of John and Joyce and add the facts: hasMalePartner John_Bright_1930 ; hasFemalePartner Joyce_Gosport (you may have to add Joyce if you did not already did that); hasMarriageYear 1955 ; Create an individual m003 with the label Marriage of Peter and Diana and add the facts: hasMalePartner Peter_William_Bright_1941 ; hasFemalePartner Diana_Pool (you may have to add Diana if you did not already did that); hasMarriageYear 1964 ; We have the basic infrastructure for marriages. We can ask the usual kinds of questions; try the following: Task 37: DL queries Ask the following DL queries: The Women partners in marriages; Marriages that happened before 1960 (see example below); Marriages that happened after 1960; Marriages that involved a man with the family name \u2018Bright\u2019. DL query: Marriage and hasMarriageYear some int[<= 1960] 9.1.1 Spouses \u00b6 This marriage infrastructure can be used to infer some slightly more interesting things for actual people. While we want marriage objects so that we can talk about marriage years and even locations, should we want to, we also want to be able to have the straight-forward spouse relationships one would expect. We can use sub-property chains in the usual manner; do the following: Task 38: Wifes and Husbands Create a property hasSpouse with two sub-properties hasHusband and hasWife . Create the inverses isSpouseOf , isWifeOf and isHusbandOf . To the hasWife property, add the sub-property chain isMalePartnerIn o hasFemalePartner . Follow the same pattern for the hasHusband property. Figure 9.1 shows what is happening with the sub-property chains. Note that the domains and ranges of the spouse properties come from the elements of the sub-property chains. Note also that the hasSpouse relationship will be implied from its sub-property chains. The following questions can now be asked: Is wife of David Bright; Has a husband born before 1940; The wife of an uncle of William Bright 1970. Figure 9.1: The sub-property chain path used to infer the spouse relationships via the marriage partnerships. and many more. This is really a chance to explore your querying abilities and make some complex nested queries that involve going up and down the hierarchy and tracing routes through the graph of relationships between the individuals you\u2019ve inferred. 9.2 In-Laws \u00b6 Now we have spouses, we can also have in-laws. The path is simple: isSpouseOf o hasMother implies hasMotherInLaw . The path involved in mother-in-laws can be seen in Figure 9.2. The following OWL code establishes the sub-property chains for hasMotherInLaw : ObjectProperty: hasMotherInLaw SubPropertyOf: hasParentInLaw SubPropertyChain: isSpouseOf o hasMother Domain: Person Range: Woman InverseOf: isMotherInLawOf Figure 9.2: Tracing out the path between objects to make the sub-property chain for mother-in-laws Do the following to make the parent in-law properties: Task 39: Parents in-law Create hasParentInLaw with two sub-properties of hasMotherInLaw and hasFatherInLaw ; Create the inverses, but remember to let the reasoner infer the hierarchy on that side of the hierarchy; Add the sub-property chains as described in the pattern for hasMotherInLaw above; Run the reasoner and check that the mother-in-law of Margaret Grace Rever is Iris Ellen Archer. 9.3 Brothers and Sisters In-Law \u00b6 Brothers and sisters in law have the interesting addition of having more than one path between objects to establish a sister or brother in law relationship. The OWL code below establishes the relationships for \u2018is sister in law of\u2019: ObjectProperty: hasSisterInLaw SubPropertyOf: hasSiblingInLaw SubPropertyChain: hasSpouse o hasSister SubPropertyChain: hasSibling o isWifeOf A wife\u2019s husband\u2019s sister is a sister in law of the wife. Figure 9.3 shows the two routes to being a sister-in-law. In addition, the wife is a sister in law of the husband\u2019s siblings. One can add as many sub-property chains to a property as one needs. You should add the properties for hasSiblingInLawOf and its obvious sub-properties following the inverse of the pattern above. Task 40: Siblings in-law Create the relationships for siblings-in-law as indicated in the owl code above. By now, chances are high that the realisation takes a long time. We recommend to remove the very computationally expensive restriction `hasParent` exactly 2 Person on the `Person` class, if you have not done it so far. Figure 9.3: The two routes to being a sister-in-law. 9.4 Aunts and Uncles in-Law \u00b6 The uncle of Robert David Bright has a wife, but she is not the aunt of Robert David Bright, she is the aunt-in-law. This is another kith relationship, not a kin relationship. The pattern has a familiar feel: ObjectProperty: isAuntInLawOf SubPropertyOf: isInLawOf SubPropertyChain: isWifeOf o isBrotherOf o isParentOf Task 41: Uncles and aunts in-law Create hasAuntInLaw and hasUncleInLaw in the usual way; Test in the usual way; Tidy up the top of the property hierarchy so that it looks like Figure 9.4. We have a top property of hasRelation and two sub-properties of isBloodRelationOf and isInLawOf to establish the kith and kin relationships respectively; All the properties created in this chapter (except for spouses) should be underneath isInLawOf . Figure 9.4: The object property hierarchy after adding the various in-law properties. 9.5 Summary \u00b6 This has really been a revision chapter; nothing new has really been introduced. We have added a lot of new object properties and one new data property. The latest object property hierarchy with the \u2018in-law\u2019 branch can be seen in Figure 9.4. Highlights have been: Having an explicit marriage object so that we can say things about the marriage itself, not just the people in the marriage; We have seen that more than one property chain can be added to a property; We have added a lot of kith relationships to join the kin or blood relationships; As usual, the reasoner can establish the hierarchy for the inverses and put a lot of the domain and ranges in for free. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 123.655 sec (1.00000 % of final) and by FaCT++ 1.6.4 is approximately 1.618 sec (0.046 % of final). 0 sec indicates failure or timeout. Chapter 10 \u00b6 Extending the TBox \u00b6 In this chapter you will: Just add lots of defined classes for all the aspects we have covered in this FHKB tutorial; You will learn that the properties used in these defined classes must be chosen with care. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial 10.1 Adding Defined Classes \u00b6 Add the following defined classes: Task 42: Adding defined classes Relation and blood relation; Forefather and Foremother; Grandparent, Grandfather and Grandmother; GreatGrandparent, GreatGrandfather and GreatGrandmother; GreatGrandparentOfRobert, GreatGrandfatherOfRobert and GreatGrandMotherOfRobert Daughter, Son, Brother, Sister, Child; Aunt, Uncle, AuntInLaw, UncleInLaw, GreatAunt and GreatUncle; FirstCousin and SecondCousin; First cousin once removed; InLaw, MotherInLaw, FatherInLaw, ParentInLaw, SiblingInLaw, SisterInLaw, BrotherInLaw; Any defined class for any property in the hierarchy and any nominal variant of these classes. The three classes of Child , Son and Daughter are of note. They are coded in the following way: Class: Child EquivalentTo: Person that hasParent Some Person Class: Son EquivalentTo: Man that hasParent Some Person Class: Daughter EquivalentTo: Woman that hasParent Some Person After running the reasoner, you will find that Person is found to be equivalent to Child ; Daughter is equivalent to Woman and that Son is equivalent to Man . This does, of course, make sense \u2013 each and every person is someone\u2019s child, each and every woman is someone\u2019s daughter. We will forget evolutionary time-scales where this might be thought to break down at some point \u2013 all Person individuals are also Descendant individuals, but do we expect some molecule in some prebiotic soup to be a member of this class? Nevertheless, within the scope of the FHKB, such inferred equivalences are not unreasonable. They are also instructive; it is possible to have different intentional descriptions of a class and for them to have the same logical extents. You can see another example of this happening in the amino acids ontology, but for different reasons. Taking Grandparent as an example class, there are two ways of writing the defined class: Class: Grandparent EquivalentTo: Person and isGrandparentOf some Person Class: Grandparent EquivalentTo: Person and (isParentOf some (Person and (is- ParentOf some Person)) Each comes out at a different place in the class hierarchy. They both capture the right individuals as members (that is, those individuals in the ABox that are holding a isGrandparentOf property), but the class hierarchy is not correct. By definition, all grandparents are also parents, but the way the object property hierarchy works means that the first way of writing the defined class (with the isGrandparentOf property) is not subsumed by the class Parent . We want this to happen in any sensible class hierarchy, so we have to use the second pattern for all the classes, spelling out the sub-property path that implies the property such as isGrandparentOf within the equivalence axiom. The reason for this need for the \u2018long-form\u2019 is that the isGrandparentOf does not imply the isParentOf property. As described in Chapter 3 if this implication were the case, being a grandparent of Robert David Bright, for instance, would also imply that the same Person were a parent of Robert David Bright; an implication we do not want. As these two properties ( isParentOf and isGrandparentOf ) do not subsume each other means that the defined classes written according to pattern one above will not subsume each other in the class hierarchy. Thus we use the second pattern. If we look at the class for grandparents of Robert: Class: GrandparentOfRobert EquivalentTo: Person that isParentOf some (Person that isParentOf value Robert David Bright) If we make the equivalent class for Richard John Bright, apply the reasoner and look at the hierarchy, we see that the two classes are not logically equivalent, even though they have the same extents of William George Bright, Iris Ellen Archer, Charles Herbert Rever and Violet Sylvia Steward. We looked at this example in Section 6.2, where there is an explanation and solutions. 10.2 Summary \u00b6 We can add defined classes based on each property we have put into the object property hierarchy. We see the expected hierarchy; as can be seen from Figure 10.1 it has an obvious symmetry based on sex. We also see a lot of equivalences inferred \u2013 all women are daughters, as well as women descendants. Perhaps not the greatest insight ever gained, but it at least makes sense; all women must be daughters. It is instructive to use the explanation feature in Prot\u00e9g\u00e9 to look at why the reasoner has made these inferences. For example, take a look at the class hasGrandmother some Woman \u2013 it is instructive to see how many there are. Like the Chapter on marriage and in-law (Chapter 9), this chapter has largely been revision. One thing of note is, however, that we must not use the object properties that are inferred through sub-property chains as definitions in the TBox; we must spell out the sub-property chain in the definition, otherwise the implications do not work properly. One thing is almost certain; the resulting TBox is rather complex and would be almost impossible to maintain by hand. Figure 10.1: The full TBox hierarchy of the FHKB The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 0.000 sec (0.00000 % of final) and by FaCT++ 1.6.4 is approximately 35.438 sec (1.000 % of final). 0 sec indicates failure or timeout. Chapter 11 \u00b6 Final remarks \u00b6 If you have done all the tasks within this tutorial, then you will have touched most parts of OWL 2. Unusually for most uses of OWL we have concentrated on individuals, rather than just on the TBox. One note of warning \u2013 the full FHKB has some 450 members of the Bright family and takes a reasonably long time to classify, even on a sensible machine. The FHKB is not scalable in its current form. One reason for this is that we have deliberately maximised inference. We have attempted not to explicitly type the individuals, but drive that through domain and range constraints. We are making the property hierarchy do lots of work. For the individual Robert David Bright, we only have a couple of assertions, but we infer some 1 500 facts between Robert David Bright and other named individuals in the FHKB\u2013displaying this in Prot\u00e9g\u00e9 causes problems. We have various complex classes in the TBox and so on. We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do. As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing.However, it serves its role to teach about OWL 2. OWL 2 on its own and using it in this style, really does not work for family history. We have seen that siblings and cousins cause problems. rules in various forms can do this kind of thing easily\u2014it is one of the primary examples for learning about Prolog. Nevertheless, the FHKB does show how much inference between named individuals can be driven from a few fact assertions and a property hierarchy. Assuming a powerful enough reasoner and the ability to deal with many individuals, it would be possible to make a family history application using the FHKB; as long as one hid the long and sometimes complex queries and manipulations that would be necessary to \u2018prune\u2019 some of the \u2018extra\u2019 facts found about individuals. However, the FHKB does usefully show the power of OWL 2, touch a great deal of the language and demonstrate some of its limitations. Appendix A \u00b6 FHKB Family Data \u00b6 Table A.1: The list of individuals in the FHKB Person First given name Second given name Family name Birth year Mother Father Alec John Archer 1927 Alec John Archer 1927 Violet Heath 1887 James Alexander Archer 1882 Charles Herbert Rever 1895 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 William Rever 1870 Charlotte Caroline Jane Bright 1894 Charlotte Caroline Jane Bright 1894 Charlotte Hewett 1863 Henry Edmund Bright 1862 Charlotte Hewett 1863 Charlotte none Hewett 1863 not specified not specified Clare Bright 1966 Clare none Bright 1966 Diana Pool Peter William Bright 1941 Diana Pool Diana none Pool none not specified not specified David Bright 1934 David none Bright 1934 Iris Ellen Archer 1906 William George Bright 1901 Dereck Heath Dereck none Heath 1927 not specified not specified Eileen Mary Rever 1929 Eileen Mary Rever 1929 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 Elizabeth Frances Jessop 1869 not specified not specified Ethel Archer 1912 Ethel none Archer 1912 Violet Heath 1887 James Alexander Archer 1882 Frederick Herbert Bright 1889 Frederick Herbert Bright 1889 Charlotte Hewett 1863 Henry Edmund Bright 1862 Henry Edmund Bright 1862 Henry Edmund Bright 1862 not specified not specified Henry Edmund Bright 1887 Henry Edmund Bright 1887 Charlotte Hewett 1863 Henry Edmund Bright 1862 Ian Bright 1959 Ian none Bright 1959 Joyce Gosport John Bright 1930 Iris Ellen Archer 1906 Iris Ellen Archer 1906 Violet Heath 1887 James Alexander Archer 1882 James Alexander Archer 1882 James Alexander Archer 1882 not specified not specified James Bright 1964 James none Bright 1964 Diana Pool Peter William Bright 1941 James Frank Hayden Bright 1891 James Frank Bright 1891 Charlotte Hewett 1863 Henry Edmund Bright 1862 Janet Bright 1964 Janet none Bright 1964 Joyce Gosport John Bright 1930 John Bright 1930 John none Bright 1930 Iris Ellen Archer 1906 William George Bright 1901 John Tacey Steward 1873 John Tacey Steward 1873 not specified not specified Joyce Archer 1921 Joyce none Archer 1921 Violet Heath 1887 James Alexander Archer 1882 Joyce Gosport Joyce none Gosport not specified not specified not specified Julie Bright 1966 Julie none Bright 1966 Diana Pool Peter William Bright 1941 Kathleen Minnie Bright 1904 Kathleen Minnie Bright 1904 Charlotte Hewett 1863 Henry Edmund Bright 1862 Leonard John Bright 1890 Leonard John Bright 1890 Charlotte Hewett 1863 Henry Edmund Bright 1862 Lois Green 1871 Lois none Green 1871 not specified not specified Margaret Grace Rever 1934 Margaret Grace Rever 1934 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Mark Anthony Heath 1960 Mark Anthony Heath 1960 Eileen Mary Rever 1929 Dereck Heath Mark Bright 1956 Mark none Bright 1956 Joyce Gosport John Bright 1930 Nicholas Charles Heath 1964 Nicholas Charles Heath 1964 Eileen Mary Rever 1929 Dereck Heath Nora Ada Bright 1899 Nora Ada Bright 1899 Charlotte Hewett 1863 Henry Edmund Bright 1862 Norman James Archer 1909 Norman James Archer 1909 Violet Heath 1887 James Alexander Archer 1882 Peter William Bright 1941 Peter William Bright 1941 Iris Ellen Archer 1906 William George Bright 1901 Richard John Bright 1962 Richard John Bright 1962 Margaret Grace Rever 1934 David Bright 1934 Robert David Bright 1965 Robert David Bright 1965 Margaret Grace Rever 1934 David Bright 1934 Violet Heath 1887 Violet none Heath 1887 not specified not specified Violet Sylvia Steward 1894 Violet Sylvia Steward 1894 Lois Green 1871 John Tacey Steward 1873 William Bright 1970 William none Bright 1970 Joyce Gosport John Bright 1930 William George Bright 1901 William George Bright 1901 Charlotte Hewett 1863 Henry Edmund Bright 1862 William Rever 1870 William none Rever 1870 not specified not specified Bibliography \u00b6 [1] M. Horridge and S. Bechhofer. The owl api: a java api for working with owl 2 ontologies. Proc. of OWL Experiences and Directions , 2009, 2009. [2] Luigi Iannone, Alan Rector, and Robert Stevens. Embedding knowledge patterns into owl. In European Semantic Web Conference (ESWC09) , pages 218\u2013232, 2009. [3] Dmitry Tsarkov, Uli Sattler, Margaret Stevens, and Robert Stevens. A Solution for the Man-Man Problem in the Family History Knowledge Base. In Sixth International Workshop on OWL: Experiences and Directions 2009 , 2009.","title":"Family History Knowledge Base (FHKB) tutorial"},{"location":"tutorial/fhkb/#manchester-family-history-advanced-owl","text":"This is a fork of the infamous Manchester Family History Advanced OWL Tutorial version 1.1, located at http://owl.cs.manchester.ac.uk/publications/talks-and-tutorials/fhkbtutorial/ The translation to markdown is not without issue, but we are making a start to making the tutorial a bit more accessible. This reproduction is done with kind permission by Robert Stevens.","title":"Manchester Family History Advanced OWL"},{"location":"tutorial/fhkb/#original-credits-version-11-see-pdf","text":"Authors: Robert Stevens Margaret Stevens Nicolas Matentzoglu Simon Jupp Bio-Health Informatics Group School of Computer Science University of Manchester Oxford Road Manchester United Kingdom M13 9PL robert.stevens@manchester.ac.uk","title":"Original credits (Version 1.1, see pdf):"},{"location":"tutorial/fhkb/#contributors","text":"v 1.0 Robert Stevens, Margaret Stevens, Nicolas Matentzoglu and Simon Jupp v 1.1 Robert Stevens, Nicolas Matentzoglu v 2.0 (Web version) Robert Stevens, Nicolas Matentzoglu, Shawn Tan The University of Manchester Copyright\u00a9 The University of Manchester November 25, 2015","title":"Contributors"},{"location":"tutorial/fhkb/#acknowledgements","text":"This tutorial was realised as part of the Semantic Web Authoring Tool (SWAT) project (see http://www.swatproject.org), which is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/G032459/1, to the University of Manchester, the University of Sussex and the Open University.","title":"Acknowledgements"},{"location":"tutorial/fhkb/#dedication","text":"The Stevens family\u2014all my ancestors were necessary for this to happen. Also, for my Mum who gathered all the information.","title":"Dedication"},{"location":"tutorial/fhkb/#contents","text":"Preamble 0.1 Licencing 0.2 Reporting Errors 0.3 Acknowledgements 1. Introduction 1.1 Learning Outcomes 1.2 Why Family History? 1.3 How to use this Tutorial 1.4 FHKB Resources 1.5 Conventions used in this Tutorial 2. Adding some Individuals to the FHKB 2.1 A World of Objects 2.2 Asserting Parentage Facts 2.3 Summary 3. Ancestors and Descendants 3.1 Ancestors and Descendants 3.2 Grandparents and Great Grandparents 3.3 Summary 4. Modelling the Person Class 4.1 The Class of Person 4.2 Describing Sex in the FHKB 4.3 Defining Man and Woman 4.4 Describing Parentage in the FHKB 4.5 Who has a father? 4.6 Filling in Domains and Ranges for the FHKB Properties 4.7 Inconsistencies 4.8 Adding Some Defined Classes for Ancestors and so on 4.9 Summary 5. Siblings in the FHKB 5.1 Blood relations 5.2 Siblings: Option One 5.2.1 Brothers and Sisters 5.3 Siblings: Option two 5.3.1 Which Modelling Option to Choose for Siblings? 5.4 Half-Siblings 5.5 Aunts and Uncles 5.6 Summary 6. Individuals in Class Expressions 6.1 Richard and Robert\u2019s Parents and Ancestors 6.2 Closing Down What we Know About Parents and Siblings 6.3 Summary 7. Data Properties in the FHKB 7.1 Adding Some Data Properties for Event Years - 7.1.1 Counting Numbers of Children 7.2 The Open World Assumption 7.3 Adding Given and Family Names 7.4 Summary 8. Cousins in the FHKB 8.1 Introducing Cousins 8.2 First Cousins 8.3 Other Degrees and Removes of Cousin 8.4 Doing First Cousins Properly 8.5 Summary 9. Marriage in the FHKB 9.1 Marriage - 9.1.1 Spouses 9.2 In-Laws 9.3 Brothers and Sisters In-Law 9.4 Aunts and Uncles in-Law 9.5 Summary 10. Extending the TBox 10.1 Adding Defined Classes 10.2 Summary 11. Final remarks A FHKB Family Data","title":"Contents"},{"location":"tutorial/fhkb/#preamble","text":"","title":"Preamble"},{"location":"tutorial/fhkb/#01-licencing","text":"The \u2018Manchester Family History Advanced OWL Tutorial\u2019 by Robert Stevens, Margaret Stevens, Nicolas Matentzoglu, Simon Jupp is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.","title":"0.1 Licencing"},{"location":"tutorial/fhkb/#02-reporting-errors","text":"This manual will almost certainly contain errors, defects and infelicities. Do report them to robert.stevens@manchester.ac.uk supplying chapter, section and some actual context in the form of words will help in fixing any of these issues.","title":"0.2 Reporting Errors"},{"location":"tutorial/fhkb/#03-acknowledgements","text":"As well as the author list, many people have contributed to this work. Any contribution, such as reporting bugs etc., is rewarded by an acknowledgement of contribution (in alphabetical order) when the authors get around to adding them: Graham Goff; Matthew Horridge; Jared Leo; Fennie Liang; Phil Lord; Fiona McNeill; Eleni Mikroyannidi; George Moulton; Bijan Parsia; Alan Rector; Uli Sattler; Dmitry Tsarkov; Danielle Welter.","title":"0.3 Acknowledgements"},{"location":"tutorial/fhkb/#chapter-1","text":"","title":"Chapter 1"},{"location":"tutorial/fhkb/#introduction","text":"This tutorial introduces the tutee to many of the more advanced features of the Web Ontology Language (OWL). The topic of family history is used to take the tutee through various modelling issues and, in doing so, using many features of OWL 2 to build a Family History Knowledge Base (FHKB). The exercises are designed to maximise inference about family history through the use of an automated reasoner on an OWL knowledge base (KB) containing many members of the Stevens family. The aim, therefore, is to enable people to learn advanced features of OWL 2 in a setting that involves both classes and individuals, while attempting to maximise the use of inference within the FHKB.","title":"Introduction"},{"location":"tutorial/fhkb/#11-learning-outcomes","text":"By doing this tutorial, a tutee should be able to: Know about the separation of entities into TBox and ABox; Use classes and individuals in modelling; Write fancy class expressions; Assert facts about individuals; Use the effects of property hierarchies, property characteristics, domain/range constraints to drive inference; Use constraints and role chains on inferences about individuals; Understand and manage the consequences of the open world assumption in the TBox and ABox; Use nominals in class expressions; Appreciate some limits of OWL 2.","title":"1.1 Learning Outcomes"},{"location":"tutorial/fhkb/#12-why-family-history","text":"Building an FHKB enables us to meet our learning outcomes through a topic that is accessible to virtually everyone. Family history or genealogy is a good topic for a general tutorial on OWL 2 as it enables us to touch many features of the language and, importantly, it is a field that everyone knows. All people have a family and therefore a family history \u2013 even if they do not know their particular family history. A small caveat was put on the topic being accessible to everyone as some cultures differ, for instance, in the description of cousins and labels given to different siblings. Nevertheless, family history remains a topic that everyone can talk about. Family history is a good topic for an OWL ontology as it obviously involves both individuals \u2013 the people involved \u2013 and classes of individuals \u2013 people, men and women, cousins, etc. Also, it is an area rich in inference; from only knowing parentage and sex of an individual, it is possible to work out all family relationships \u2013 for example, sharing parents implies a sibling relationship; one\u2019s parent\u2019s brothers are one\u2019s uncles; one\u2019s parent\u2019s parents are one\u2019s grandparents. So, we should be able to construct an ontology that allows us to both express family history, but also to infer family relationships between people from knowing relatively little about them. As we will learn through the tutorial, OWL 2 cannot actually do all that is needed to create a FHKB. This is unfortunate, but we use it to our advantage to illustrate some of the limitations of OWL 2. We know that rule based systems can do family history with ease, but that is not the point here; we are not advocating OWL DL as an appropriate mechanism for doing family history, but we do use it as a good educational example. We make the following assumptions about what people know: We assume that people know OWL to the level that is known at the end of the Pizza tutorial . Some ground will be covered again, but a lot of basic OWL is assumed. We assume people know how to use Prot\u00e9g\u00e9 or their OWL environment of choice. We do not give \u2018click by click\u2019 instructions. At some places, some guidance is given, but this is not to be relied upon as Prot\u00e9g\u00e9 changes and we will not keep up to date. We make some simplifying assumptions in this tutorial: We take a conventional western view of family history. This appears to have most effects on naming of sibling and cousin relationships. We take a straight-forward view on the sex of people; this is explored further in Chapter 4; A \u2018conventional\u2019 view of marriage is taken; this is explored further in Chapter 9. We make no special treatment of time or dates; we are only interested in years and we do not do anything fancy; this is explored more in Chapter 7. We assume the ancestors of people go back for ever; obviously this is not true, eventually one would get back to a primordial soup and one\u2019s ancestors are not humans (members of the classPerson), but we don\u2019t bother with such niceties. At the end of the tutorial, you should be able to produce a property hierarchy and a TBox or class hierarchy such as shown in Figure 1.1; all supported by use of the automated reasoner and a lot of OWL 2\u2019s features. Figure 1.1: A part of the class and property hierarchy of the final FHKB.","title":"1.2 Why Family History?"},{"location":"tutorial/fhkb/#13-how-to-use-this-tutorial","text":"Here are some tips on using this manual to the best advantage: Start at the beginning and work towards the end. You can just read the tutorial, but building the FHKB will help you learn much more and much more easily Use the reasoner in each task; a lot of the FHKB tutorial is about using the reasoner and not doing so will detract from the learning outcomes.","title":"1.3 How to use this Tutorial"},{"location":"tutorial/fhkb/#14-fhkb-resources","text":"The following resources are available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial: A full version of the Stevens FHKB. Some links to papers about the FHKB. Some slides about the FHKB tutorial. A set of OWL resources for each stage of the FHKB. Some blogs about the FHKB are at http://robertdavidstevens.wordpress.com.","title":"1.4 FHKB Resources"},{"location":"tutorial/fhkb/#15-conventions-used-in-this-tutorial","text":"All OWL is written in Manchester Syntax. When we use FHKB entities within text, we use a sans serif typeface. We use CamelCase for classes and property names. Class names start with upper case. Individual names start with a lower case letter and internal underscores to break words. Property names usually start with \u2018is\u2019 or \u2018has\u2019 and are CamelCase with a lower case initial letter. Many classes and individuals in the FHKB have annotation properties, usually human readable labels. They show up in some of the examples in Manchester syntax, but are not made explicit as part of the tasks in this tutorial. Every object property is necessarily a sub-property of topObjectProperty. It does not have to be asserted as such. Nevertheless, there might be situations where this relationship is made explicit in this tutorial for illustrative reasons. The individuals we are dealing with represent distinct persons. Throughout the tutorial, once the respective axiom is introduced (chapter 7.1.1), the reader should make sure that all his or her individuals are always made distinct, especially when he or she adds a new one. At the end of each chapter, we note the Description Logic Language (expressivity) needed to represent the ontology and the reasoning times for a number of state of the art reasoning systems. This should get the reader a sense how difficult the FHKB becomes for reasoners to deal with over time. When there is some scary OWL or the reasoner may find the FHKB hard work, you will see a \u2018here be dragons\u2019 image. 1 1 The image comes fromhttp://ancienthomeofdragon.homestead.com/May 2012.","title":"1.5 Conventions used in this Tutorial"},{"location":"tutorial/fhkb/#chapter-2","text":"","title":"Chapter 2"},{"location":"tutorial/fhkb/#adding-some-individuals-to-the-fhkb","text":"In this chapter we will start by creating a fresh OWL ontology and adding some individuals that will be surrogates for people in the FHKB. In particular you will: Create a new OWL ontology for the FHKB; Add some individuals that will stand for members of the Stevens family. Describe parentage of people. Add some facts to specific individuals as to their parentage; See the reasoner doing some work. At the moment we will ignore sex; sex will not happen until Chapter 4.","title":"Adding some Individuals to the FHKB"},{"location":"tutorial/fhkb/#21-a-world-of-objects","text":"The \u2018world\u2019 2 or field of interest we model in an ontology is made up of objects or individuals. Such objects include, but are not limited to: People, their pets, the pizzas they eat; The processes of cooking pizzas, living, running, jumping, undertaking a journey; The spaces within a room, a bowl, an artery; The attributes of things such as colour, dimensions, speed, shape of various objects; Boundaries, love, ideas, plans, hypotheses. 2 we use \u2018world\u2019 as a synonym of \u2018field of interest\u2019 or \u2018domain\u2019. \u2018World\u2019 does not restrict us to modelling the physical world outside our consciousness. We observe these objects, either outside lying around in the world or in our heads. OWL is all about modelling such individuals. Whenever we make a statement in OWL, when we write down an axiom, we are making statements about individuals. When thinking about the axioms in an ontology it is best to think about the individuals involved, even if OWL individuals do not actually appear in the ontology. All through this tutorial we will always be returning to the individuals being described in order to help us understand what we are doing and to help us make decisions about how to do it.","title":"2.1 A World of Objects"},{"location":"tutorial/fhkb/#22-asserting-parentage-facts","text":"Biologically, everyone has parents; a mother and a father 3 . The starting point for family history is parentage; we need to relate the family member objects by object properties. An object property relates two objects, in this case a child object with his or her mother or father object. To do this we need to create three object properties: Task 1: Creating object properties for parentage Create a new ontology; Create an object property hasMother ; Create a property isMotherOf and give hasMother the InverseOf: isMotherOf ; Do the same for the property hasFather ; Create a property hasParent ; give it the obvious inverse; Make hasMother and hasFather sub-properties of hasParent . Run the reasoner and look at the property hierarchy. Note how the reasoner has automatically completed the sub-hierarchy for isParentOf: isMotherOf and isFatherOf are inferred to be sub-properties of isParentOf . The OWL snippet below shows some parentage fact assertions on an individual. Note that rather than being assertions to an anonymous individual via some class, we are giving an assertion to a named individual. Individual: grant_plinth Facts: hasFather mr_plinth, hasMother mrs_plinth 3 Don\u2019t quibble; it\u2019s true enough here. Task 2: Create the ABox Using the information in Table A.1 (see appendix) about parentage (so the columns about fathers and mothers), enter the fact assertions for the people which appear in rows shaded in grey. We will only use the hasMother and hasFather properties in our fact assertions. You do not need to assert names and birth years yet. This exercise will require you to create an individual for every person we want to talk about, using the Firstname_Secondname_Familyname_Birthyear pattern, as for example in Robert_David_Bright_1965 . While asserting facts about all individuals in the FHKB will be a bit tedious at times, it might be useful to at least do the task for a subset of the family members. For the impatient reader, there is a convenience snapshot of the ontology including the raw individuals available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial If you are working with Prot\u00e9g\u00e9, you may want to look at the Matrix plugin for Prot\u00e9g\u00e9 at this point. The plugin allows you to add individuals quickly in the form of a regular table, and can significantly reduce the effort of adding any type of entity to the ontology. In order to install the matrix plugin, open Prot\u00e9g\u00e9 and go to File \u00bb Check for plugins. Select the \u2018Matrix Views\u2019 plugin. Click install, wait until the the installation is confirmed, close and re-open Prot\u00e9g\u00e9; go to the \u2018Window\u2019 menu item, select \u2018Tabs\u2019 and add the \u2018Individuals matrix\u2019. Now do the following: Task 3: DL queries Classify the FHKB. Issue the DL query hasFather value David_Bright_1934 and look at the answers (remember to check the respective checkbox in Prot\u00e9g\u00e9 to include individuals in your query results). Issue the DL query isFatherOf value Robert_David_Bright_1965 . Look at the answers. 4. Look at the entailed facts on Robert_David_Bright_1965 . You should find the following: David Bright (1934) is the father of Robert David Bright (1965) and Richard John Bright (1962). Robert David Bright (1965) has David Bright 1934 as a parent. Since we have said that isFatherOf has an inverse of hasFather , and we have asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we have a simple entailment that David_Bright_1934 isFatherOf Robert_David_Bright_1965 . So, without asserting the isFatherOf facts, we have been able to ask and get answers for that DL query. As we asserted that Robert_David_Bright_1965 hasFather David_Bright_1934 , we also infer that he hasParent David_Bright_1934 ; this is because hasParent is the super-property of hasFather and the sub-property implies the super-property. This works all the way up the property tree until topObjectProperty , so all individuals are related by topObjectProperty \u2014this is always true. This implication \u2018upwards\u2019 is the way to interpret how the property hierarchies work.","title":"2.2 Asserting Parentage Facts"},{"location":"tutorial/fhkb/#23-summary","text":"We have now covered the basics of dealing with individuals in OWL ontologies. We have set up some properties, but without domains, ranges, appropriate characteristics and then arranged them in a hierarchy. From only a few assertions in our FHKB, we can already infer many facts about an individual: Simple exploitation of inverses of properties and super-properties of the asserted properties. We have also encountered some important principles: We get inverses for free. The sub-property implies the super-property. So, hasFather implies the hasParent fact between individuals. This entailment of the super-property is very important and will drive much of the inference we do with the FHKB. Upon reasoning we get the inverses of properties between named individuals for free. Lots is still open. For example, we do not know the sex of individuals and what other children, other than those described, people in the FHKB may have. The FHKB ontology at this stage of the tutorial has an expressivity of ALHI. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.026 sec (0.00001 % of final), by Pellet 2.2.0 0.144 sec (0.00116 % of final) and by FaCT++ 1.6.4 is approximately 0. sec (0.000 % of final). 0 sec indicates failure or timeout.","title":"2.3 Summary"},{"location":"tutorial/fhkb/#chapter-3","text":"","title":"Chapter 3"},{"location":"tutorial/fhkb/#ancestors-and-descendants","text":"In this Chapter you will: Use sub-properties and the transitive property characteristic to infer ancestors of people; Add properties to the FHKB property hierarchy that will infer ancestors and descendants of a person without adding any more facts to the FHKB; Explore the use of sub-property chains for grandparents, great grandparents and so on; Place all of these new object properties in the property hierarchy and in that way learn more about the implications of the property hierarchy. Find a snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial.","title":"Ancestors and Descendants"},{"location":"tutorial/fhkb/#31-ancestors-and-descendants","text":"The FHKB has parents established between individuals and we know that all people have two parents. A parent is an ancestor of its children; a person\u2019s parent\u2019s parents are its ancestors; and so on. So, in our FHKB, Robert\u2019s ancestors are David, Margaret, William, Iris, Charles, Violet, James, another Violet, another William, Sarah and so on. If my parent\u2019s parents are my ancestors, then what we need is a transitive version of the hasParent property. Obviously we do not want hasParent to be transitive, as Robert\u2019s grandparents (and so on) would become his parents (and that would be wrong). We can easily achieve what is necessary. We need a hasAncestor property that has a transitive characteristic. The trick is to make this a super-property of the hasParent property. As explained before, a sub-property implies its super-property. So, if individual x holds a hasParent property with an individual y , then it also holds an instance of its super-property hasAncestor with the individual y . If individual y then holds a hasParent property with another individual z , then there is also, by implication, a hasAncestor property between y and z . As hasAncestor is transitive, x and z also hold a hasAncestor relationship between them. The inverse of hasAncestor can either be isAncestorOf or hasDescendant . We choose the isAncestorOf option. Task 4: Object properties: exploiting the semantics Make a new object property hasRelation , make it symmetric. Make a new object property hasAncestor . Make it a sub-property of hasRelation and a super-property of hasParent . Make hasAncestor transitive. Create the inverse isAncestorOf . Do not \u2018stitch\u2019 it into the property hierarchy; the reasoner will sort it all out for you. Run the reasoner and issue the DL query hasAncestor value William_George_Bright_1901 . Issue the query isAncestorOf value Robert_David_Bright_1965 . The hasAncestor object property will look like this: ObjectProperty: hasAncestor SubPropertyOf: hasRelation SuperPropertyOf: hasParent, Characteristics: Transitive InverseOf: isAncestorOf As usual, it is best to think of the objects or individuals involved in the relationships. Consider the three individuals \u2013 Robert, David and William. Each has a hasFather property, linking Robert to David and then David to William. As hasFather implies its super-property hasParent , Robert also has a hasParent property with David, and David has a hasParent relation to William. Similarly, as hasParent implies hasAncestor , the Robert object has a hasAncestor relation to the David object and the David object has one to the William object. As hasAncestor is transitive, Robert not only holds this property to the David object, but also to the William object (and so on back through Robert\u2019s ancestors).","title":"3.1 Ancestors and Descendants"},{"location":"tutorial/fhkb/#32-grandparents-and-great-grandparents","text":"We also want to use a sort of restricted transitivity in order to infer grandparents, great grandparents and so on. My grandparents are my parent\u2019s parents; my grandfathers are my parent\u2019s fathers. My great grandparents are my parent\u2019s parent\u2019s parents. My great grandmothers are my parent\u2019s parent\u2019s mothers. This is sort of like transitivity, but we want to make the paths only a certain length and, in the case of grandfathers, we want to move along two relationships \u2013 hasParent and then hasFather . We can do this with OWL 2\u2019s sub-property chains. The way to think about sub-property chains is: If we see property x followed by property y linking three objects, then it implies that property z is held between Figure 3.1: Three blobs representing objects of the classPerson. The three objects are linked by a hasParent property and this implies a hasGrandparent property. the first and third objects. Figure 3.1 shows this diagrammatically for the hasGrandfather property. For various grandparent object properties we need the following sets of implications: My parent\u2019s parents are my grandparents; My parent\u2019s fathers are my grandfathers; My parent\u2019s mothers are my grandmothers; My parent\u2019s parent\u2019s parents are my great grandparents or my grandparent\u2019s parents are my great grandparents. My parent\u2019s parent\u2019s fathers are my great grandfathers or my parent\u2019s grandfathers are my great grandfathers; My parent\u2019s parent\u2019s mothers are my great grandmothers (and so on). Notice that we can trace the paths in several ways, some have more steps than others, though the shorter paths themselves employ paths. Tracing these paths is what OWL 2\u2019s sub-property chains achieve. For the new object property hasGrandparent we write: ObjectProperty: hasGrandparent SubPropertyChain: hasParent o hasParent We read this as \u2018 hasParent followed by hasParent implies hasGrandparent \u2019. We also need to think where the hasGrandparent property fits in our growing hierarchy of object properties. Think about the implications: Does holding a hasParent property between two objects imply that they also hold a hasGrandparent property? Of course the answer is \u2018no\u2019. So, this new property is not a super-property of hasParent . Does the holding of a hasGrandparent property between two objects imply that they also hold an hasAncestor property? The answer is \u2018yes\u2019; so that should be a super-property of hasGrandparent . We need to ask such questions of our existing properties to work out where we put it in the object property hierarchy. At the moment, our hasGrandparent property will look like this: ObjectProperty: hasGrandParent SubPropertyOf: hasAncestor SubPropertyChain: hasParent o hasParent SuperPropertyOf: hasGrandmother, hasGrandfather InverseOf: isGrandParentOf Do the following task: Task 5: Grandparents object properties Make the hasGrandparent , hasGrandmother and hasGrandfather object properties and the obvious inverses (see OWL code above); Go to the individuals tabs and inspects the inferred object property assertions for Robert_David_Bright_1965 and his parents. Again, think of the objects involved. We can take the same three objects as before: Robert, David and William. Think about the properties that exist, both by assertion and implication, between these objects. We have asserted only hasFather between these objects. The inverse can be inferred between the actual individuals (remember that this is not the case for class level restrictions \u2013 that all instances of a class hold a property does not mean that the filler objects at the other end hold the inverse; the quantification on the restriction tells us this). Remember that: Robert holds a hasFather property with David; David holds a hasFather property with William; By implication through the hasParent super-property of hasFather , Robert holds a hasParent property with David, and the latter holds one with William; The sub-property chain on hasGrandfather then implies that Robert holds a hasGrandfather property to William. Use the diagram in figure 3.1 to trace the path; there is a hasParent path from Robert to William via David and this implies the hasGrandfather property between Robert and William. It is also useful to point out that the inverse of hasGrandfather also has the implication of the sub-property chain of the inverses of hasParent . That is, three objects linked by a path of two isParentOf properties implies that an isGrandfatherOf property is established between the first and third object, in this case William and Robert. As the inverses of hasFather are established by the reasoner, all the inverse implications also hold.","title":"3.2 Grandparents and Great Grandparents"},{"location":"tutorial/fhkb/#33-summary","text":"It is important when dealing with property hierarchies to think in terms of properties between objects and of the implications \u2018up the hierarchy\u2019. A sub-property implies its super-property. So, in our FHKB, two person objects holding a hasParent property between them, by implication also hold an hasAncestor property between them. In turn, hasAncestor has a super-property hasRelation and the two objects in question also hold, by implication, this property between them as well. We made hasAncestor transitive. This means that my ancestor\u2019s ancestors are also my ancestors. That a sub-property is transitive does not imply that its super-property is transitive. We have seen that by manipulating the property hierarchy we can generate a lot of inferences without adding any more facts to the individuals in the FHKB. This will be a feature of the whole process \u2013 keep the work to the minimum (well, almost). In OWL 2, we can also trace \u2018paths\u2019 around objects. Again, think of the objects involved in the path of properties that link objects together. We have done simple paths so far \u2013 Robert linked to David via hasParent and David linked to William via hasFather implies the link between Robert and William of hasGrandfather . If this is true for all cases (for which you have to use your domain knowledge), one can capture this implication in the property hierarchy. Again, we are making our work easier by adding no new explicit facts, but making use of the implication that the reasoner works out for us. The FHKB ontology at this stage of the tutorial has an expressivity ofALRI+. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.262 sec (0.00014 % of final), by Pellet 2.2.0 0.030 sec (0.00024 % of final) and by FaCT++ 1.6.4 is approximately 0.004 sec (0.000 % of final). 0 sec indicates failure or timeout.","title":"3.3 Summary"},{"location":"tutorial/fhkb/#chapter-4","text":"","title":"Chapter 4"},{"location":"tutorial/fhkb/#modelling-the-person-class","text":"In this Chapter you will: Create a Person class; Describe Sex classes; Define Man and Woman ; Ask which of the people in the FHKB has a father. Add domains and ranges to the properties in the FHKB. Make the FHKB inconsistent. Add some more defined classes about people and see some equivalence inferred between classes. These simple classes will form the structure for the whole FHKB.","title":"Modelling the Person Class"},{"location":"tutorial/fhkb/#41-the-class-of-person","text":"For the FHKB, we start by thinking about the objects involved The people in a family \u2013 Robert, Richard, David, Margaret, William, Iris, Charles, Violet, Eileen, John and Peter; The sex of each of those people; The marriages in which they participated; The locations of their births; And many more... There is a class of Person that we will use to represent all these people objects. Task 6: Create the Person class Create a class called DomainEntity ; Create a subclass of DomainEntity called Person . We use DomainEntity as a house-keeping measure. All of our ontology goes underneath this class. We can put other classes \u2018outside\u2019 the ontology, as siblings of DomainEntity , such as \u2018probe\u2019 classes we wish to use to test our ontology. The main thing to remember about the Person class is that we are using it to represent all \u2018people\u2019 individuals. When we make statements about the Person class, we are making statements about all \u2018people\u2019 individuals. What do we know about people? All members of the Person class have: Sex \u2013 they are either male or female; Everyone has a birth year; Everyone has a mother and a father. There\u2019s a lot more we know about people, but we will not mention it here.","title":"4.1 The Class of Person"},{"location":"tutorial/fhkb/#42-describing-sex-in-the-fhkb","text":"Each and every person object has a sex. In the FHKB we will take a simple view on sex \u2013 a person is either male or female, with no intersex or administrative sex and so on. Each person only has one sex. We have two straight-forward options for modelling sex: Each person object has their own sex object, which is either male or female. Thus Robert\u2019s maleness is different from David\u2019s maleness. There is only one Maleness object and one Femaleness object and each person object has a relationship to either one of these sex objects, but not both. We will take the approach of having a class of Maleness objects and a class of Femaleness objects. These are qualities or attributes of self-standing objects such as a person. These two classes are disjoint, and each is a subclass of a class called Sex . The disjointness means that any one instance of Sex cannot be both an instance of Maleness and an instance of Femaleness at once. We also want to put in a covering axiom on the class Sex , which means that any instance of Sex must be either Maleness or Femaleness ; there is no other kind of Sex . Again, notice that we have been thinking at the level of objects. We do the same when thinking about Person and their Sex . Each and every person is related to an instance of Sex . Each Person holds one relationship to a Sex object. To do this we create an object property called hasSex . We make this property functional, which means that any object can hold that property to only one distinct filler object. We make the domain of hasSex to be Person and the range to be Sex . The domain of Person means that any object holding that property will be inferred to be a member of the class Person . Putting the range of Sex on the hasSex property means that any object at the right-hand end of the hasSex property will be inferred to be of the class Sex . Again, think at the level of individuals or objects. We now put a restriction on the Person class to state that each and every instance of the class Person holds a hasSex property with an instance of the Sex class. It has an existential operator \u2018some\u2019 in the axiom, but the functional characteristic means that each Person object will hold only one hasSex property to a distinct instance of a Sex object 4 . 4 An individual could hold two hasSex properties, as long as the sex objects at the right-hand end of the property are not different. Task 7: Modelling sex Create a class called Sex ; Make it a subclass of DomainEntity ; Make Person and Sex disjoint; Create two subclasses of Sex , Maleness and Femaleness ; Make Maleness and Femaleness disjoint; Put a covering axiom on Sex such that it is equivalent to Maleness or Femaleness . Create an object property, hasSex , with the domain Person , the range Sex and give it the characteristic of \u2018Functional\u2019; Add a restriction hasSex some Sex to the class Person . The hasSex property looks like: ObjectProperty: hasSex Characteristics: Functional Domain: Person Range: Sex The Person class looks like: Class: Person SubClassOf: DomainEntity,(hasSex some Sex) DisjointWith: Sex","title":"4.2 Describing Sex in the FHKB"},{"location":"tutorial/fhkb/#43-defining-man-and-woman","text":"We now have some of the foundations for the FHKB. We have the concept of Person , but we also need to have the concepts of Man and Woman . Now we have Person , together with Maleness and Femaleness , we have the necessary components to define Man and Woman . These two classes can be defined as: Any Person object that has a male sex can be recognised to be a man; any Person object that has a female sex can be recognised as a member of the class woman. Again, think about what conditions are sufficient for an object to be recognised to be a member of a class; this is how we create defined classes through the use of OWL equivalence axioms. To make the Man and Woman classes do the following: Task 8: Describe men and women Create a class Man ; Make it equivalent to a Person that hasSex some Maleness ; Do the same, but with Femaleness , to create the Woman class; A covering axiom can be put on the Person class to indicate that man and woman are the only kinds of person that can exist. (This is not strictly true due to the way Sex has been described.) Run the reasoner and take a look. Having run the reasoner, the Man and Woman classes should appear underneath Person 5 . 5 Actually in Prot\u00e9g\u00e9, this might happen without the need to run the reasoner. The Man and Woman classes will be important for use as domain and range constraints on many of the properties used in the FHKB. To achieve our aim of maximising inference, we should be able to infer that individuals are members of Man , Woman or Person by the properties held by an object. We should not have to state the type of an individual in the FHKB. The classes for Man and Woman should look like: Class: Man EquivalentTo: Person and (hasSex some Maleness) Class: Woman EquivalentTo: Person and (hasSex some Femaleness)","title":"4.3 Defining Man and Woman"},{"location":"tutorial/fhkb/#44-describing-parentage-in-the-fhkb","text":"To finish off the foundations of the FHKB we need to describe a person object\u2019s parentage. We know that each and every person has one mother and each and every person has one father. Here we are talking about biological mothers and fathers. The complexities of adoption and step parents are outside the scope of this FHKB tutorial. Task 9: Describing Parentage Add the domain Person and the range Woman to the property hasMother . Do the same for the property hasFather , but give it the range Man ; Give the property hasParent domain and range of Person ; Run the reasoner. The (inferred) property hierarchy in the FHKB should look like that shown in Figure 4.1. Notice that we have asserted the sub-property axioms on one side of the property hierarchy. Having done so, the reasoner uses those axioms, together with the inverses, to work out the property hierarchy for the \u2018other side\u2019. We make hasMother functional, as any one person object can hold only one hasMother property to a distinct Woman object. The range of hasMother is Woman , as a mother has to be a woman. The Person object holding the hasMother property can be either a man or a woman, so we have the domain constraint as Person ; this means any object holding a hasMother property will be inferred to be a Person . Similarly, any object at the right-hand end of a hasMother property will be inferred to be a Woman , which is the result we need. The same reasoning goes for hasFather and hasParent , with the sex constraints on the latter being only Person . The inverses of the two functional sub-properties of hasParent are not themselves functional. After all, a Woman can be the mother of many Person objects, but each Person object can have only one mother. Figure 4.1: The property hierarchy with the hasSex and the parentage properties Figure 4.2: the core TBox for the FHKB with the Person and Sex classes. Task 10: Restrict Person class As each and every person has a mother and each and every person has a father, place restrictions on the Person class as shown below. Class: Person SubClassOf: DomainEntity, (hasFather some Man), (hasMother some Woman), (hasSex some Sex) DisjointWith: Sex Task 11: DL queries for people and sex Issue the DL queries for Person , Man and Woman ; look at the answers and count the numbers in each class; which individuals have no sex and why? You should find that many people have been inferred to be either Man or Woman , but some are, as we will see below, only inferred to be Person . The domain and range constraints on our properties have also driven some entailments. We have not asserted that David_Bright_1934 is a member of Man , but the range constraint on hasFather (or the inferred domain constraint on the isFatherOf relation) has enabled this inference to be made. This goes for any individual that is the right-hand-side (either inferred or asserted) of either hasFather or hasMother (where the range is that of Woman ). For Robert David Bright, however, he is only the left-hand-side of an hasFather or an hasMother property, so we\u2019ve only entailed that this individual is a member of Person .","title":"4.4 Describing Parentage in the FHKB"},{"location":"tutorial/fhkb/#45-who-has-a-father","text":"In our description of the Person class we have said that each and every instance of the class Person has a father (the same goes for mothers). So, when we ask the query \u2018which individuals have a father\u2019, we get all the instances of Person back, even though we have said nothing about the specific parentage of each Person . We do not know who their mothers and fathers are, but we know that they have one of each. We know all the individuals so far entered are members of the Person class; when asserting the type to be either Man or Woman (each of which is a subclass of Person ), we infer that each is a person. When asserting the type of each individual via the hasSex property, we know each is a Person , as the domain of hasSex is the Person class. As we have also given the right-hand side of hasSex as either Maleness or Femaleness , we have given sufficient information to recognise each of these Person instances to be members of either Man or Woman .","title":"4.5 Who has a father?"},{"location":"tutorial/fhkb/#46-filling-in-domains-and-ranges-for-the-fhkb-properties","text":"So far we have not systematically added domains and ranges to the properties in the FHKB. As a reminder, when a property has a domain of X any object holding that property will be inferred to be a member of class X . A domain doesn\u2019t add a constraint that only members of class X hold that property; it is a strong implication of class membership. Similarly, a property holding a range implies that an object acting as right-hand-side to a property will be inferred to be of that class. We have already seen above that we can use domains and ranges to imply the sex of people within the FHKB. Do the following: Task 12: Domains and Ranges Make sure the appropriate Person , Man and Woman are domains and ranges for hasFather , hasMother and hasParent . Run the reasoner and look at the property hierarchy. Also look at the properties hasAncestor , hasGrandparent , hasUncle and so on; look to see what domains and ranges are found. Add any domains and ranges explicitly as necessary. Prot\u00e9g\u00e9 for example in its current version (November 2015) does not visualise inherited domains and ranges in the same way as it shows inferred inverse relations. We typically assert more domains and ranges than strictly necessary. For example, if we say that hasParent has the domain Person , this means that every object x that is connected to another object y via the hasParent relation must be a Person . Let us assume the only thing we said about x and y is that they are connected by a hasMother relation. Since this implies that x and y are also connected by a hasParent relation ( hasMother is a sub-property of hasParent ) we do not have to assert that hasFather has the domain of Person ; it is implied by what we know about the domain and range of hasParent . In order to remove as many assertions as possible, we may therefore choose to assert as much as we know starting from the top of the hierarchy, and only ever adding a domain if we want to constrain the already inferred domain even further (or range respectively). For example, in our case, we could have chosen to assert Person to be the domain of hasRelation . Since hasRelation is symmetric, it will also infer Person to be the range. We do not need to say anything for hasAncestor or hasParent , and only if we want to constrain the domain or range further (like in the case of hasFather by making the range Man ) do we need to actually assert something. It is worth noting that because we have built the object property hierarchy from the bottom ( hasMother etc.) we have ended up asserting more than necessary.","title":"4.6 Filling in Domains and Ranges for the FHKB Properties"},{"location":"tutorial/fhkb/#47-inconsistencies","text":"From the Pizza Tutorial and other work with OWL you should have seen some unsatisfiabilities . In Prot\u00e9g\u00e9 this is highlighted by classes going \u2018red\u2019 and being subclasses ofNothing; that is, they can have no instances in that model. Task 13: Inconsistencies Add the fact Robert_David_Bright_1965 hasMother David_Bright_1934 . Run the classifier and see what happens. Remove that fact and run the classifier again. Now add the fact that Robert_David_Bright_1965 hasMother Iris_Ellen_Archer_1907 Run the classifier and see what happens. Add and remove the functional characteristic to these properties and see what happens. After asserting the first fact it should be reported by the reasoner that the ontology is inconsistent . This means, in lay terms, that the model you\u2019ve provided in the ontology cannot accommodate the facts you\u2019ve provided in the fact assertions in your ABox\u2014that is, there is an inconsistency between the facts and the ontology... The ontology is inconsistent because David_Bright_1934 is being inferred to be a Man and a Woman at the same time which is inconsistent with what we have said in the FHKB. When we, however, say that Robert David Bright has two different mothers, nothing bad happens! Our domain knowledge says that the two women are different, but the reasoner does not know this as yet... ; Iris Ellen Archer and Margaret Grace Rever may be the same person; we have to tell the reasoner that they are different. For the same reason the functional characteristic also has no effect until the reasoner \u2018knows\u2019 that the individuals are different. We will do this in Section 7.1.1 and live with this \u2018fault\u2019 for the moment.","title":"4.7 Inconsistencies"},{"location":"tutorial/fhkb/#48-adding-some-defined-classes-for-ancestors-and-so-on","text":"Task 14: Adding defined classes Add a defined class for Ancestor , MaleAncestor , FemaleAncestor ; Add a defined class for Descendant , MaleDescendant and FemaleDescendant ; Run the reasoner and view the resulting hierarchy. The code for the classes looks like: Class: Ancestor EquivalentTo: Person and isAncestorOf some Person Class: FemaleAncestor EquivalentTo: Woman and isAncestorOf some Person Class: Descendant EquivalentTo: Person and hasAncestor some Person Class: MaleDescendant EquivalentTo: Man and hasAncestor some Person The TBox after reasoning can be seen in Figure 4.3. Notice that the reasoner has inferred that several of the classes are equivalent or \u2018the same\u2019. These are: Descendant and Person ; MaleDescendant and Man , FemaleDescendant and Woman . The reasoner has used the axioms within the ontology to infer that all the instances of Person are also instances of the class Descendant and that all the instances of Woman are also the same instances as the class Female Descendant . This is intuitively true; all people are descendants \u2013 they all have parents that have parents etc. and thus everyone is a descendant. All women are female people that have parents etc. As usual we should think about the objects within the classes and what we know about them. This time it is useful to think about the statements we have made about Person in this Chapter \u2013 that all instances of Person have a father and a mother; add to this the information from the property hierarchy and we know that all instances of Person have parents and ancestors. We have repeated all of this in our new defined classes for Ancestor and Descendant and the reasoner has highlighted this information. Figure 4.3: The defined classes from Section 4.8 in the FHKB\u2019s growing class hierarchy Task 15: More Ancestors Query for MaleDescendant . You should get Man back - they are equivalent (and this makes sense). As an additional exercise, also add in properties for forefathers and foremothers. You will follow the same pattern as for hasAncestor , but adding in, for instance, hasFather as the sub-property of the transitive super-property of hasForefather and setting the domains and ranges appropriately (or working out if they\u2019ll be inferred appropriately). Here we interpret a forefather as one\u2019s father\u2019s father etc. This isn\u2019t quite right, as a forefather is any male ancestor, but we\u2019ll do it that way anyway. You might want to play around with DL queries. Because of the blowup in inferred relationships, we decided to not include this pattern in the tutorial version of the FHKB.","title":"4.8 Adding Some Defined Classes for Ancestors and so on"},{"location":"tutorial/fhkb/#49-summary","text":"Most of what we have done in this chapter is straight-forward OWL, all of which would have been met in the pizza tutorial. It is, however, a useful revision and it sets the stage for refining the FHKB. Figure 4.2 shows the basic set-up we have in the FHKB in terms of classes; we have a class to represent person, man and woman, all set-up with a description of sex, maleness and femaleness. It is important to note, however, the approach we have taken: We have always thought in terms of the objects we are modelling. Here are some things that should now be understood upon completing this chapter: Restrictions on a class in our TBox mean we know stuff about individuals that are members of that class, even though we have asserted no facts on those individuals. We have said, for instance, that all members of the class Person have a mother, so any individual asserted to be a Person must have a mother. We do not necessarily know who they are, but we know they have one. Some precision is missing \u2013 we only know Robert David Bright is a Person , not that he is a Man . This is because, so far, he only has the domain constraint of hasMother and hasFather to help out. We can cause the ontology to be inconsistent, for example by providing facts that cannot be accommodated by the model of our ontology. In the example, David Bright was inferred to be a member of two disjoint classes. Finally, we looked at some defined classes. We inferred equivalence between some classes where the extents of the classes were inferred to be the same \u2013 in this case the extents of Person and Descendant are the same. That is, all the objects that can appear in Person will also be members of Descendant . We can check this implication intuitively \u2013 all people are descendants of someone. Perhaps not the most profound inference of all time, but we did no real work to place this observation in the FHKB. This last point is a good general observation. We can make the reasoner do work for us. The less maintenance we have to do in the FHKB the better. This will be a principle that works throughout the tutorial. The FHKB ontology at this stage of the tutorial has an expressivity of SRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.884 sec (0.00047 % of final), by Pellet 2.2.0 0.256 sec (0.00207 % of final) and by FaCT++ 1.6.4 is approximately 0.013 sec (0.000 % of final). 0 sec indicates failure or timeout.","title":"4.9 Summary"},{"location":"tutorial/fhkb/#chapter-5","text":"","title":"Chapter 5"},{"location":"tutorial/fhkb/#siblings-in-the-fhkb","text":"In this chapter you will: Explore options for determining finding siblings; Meet some of the limitations in OWL; Choose one of the options explored; Add facts for siblings; Use sub-property chains to find aunts and uncles; There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial","title":"Siblings in the FHKB"},{"location":"tutorial/fhkb/#51-blood-relations","text":"Do the following first: Task 16: The bloodrelation object property Create an hasBloodrelation object property, making it a sub-property of hasRelation . Add appropriate property characteristics. Make the already existing hasAncestor property a sub-property of hasBloodrelation . Does a blood relation of Robert have the same relationship to Robert (symmetry)? Is a blood relation of Robert\u2019s blood relation a blood relation of Robert (transitivity)? Think of an aunt by marriage; her children are my cousins and blood relations via my uncle, but my aunt is not my blood relation. My siblings share parents; male siblings are brothers and female siblings are sisters. So far we have asserted parentage facts for the Person in our ABox. Remember that our parentage properties have inverses, so if we have added an hasFather property between a Person and a Man , we infer the isFatherOf property between that Man and that Person .","title":"5.1 Blood relations"},{"location":"tutorial/fhkb/#52-siblings-option-one","text":"We should have enough information within the FHKB to infer siblings. We could use a sub-property chain such as: ObjectProperty: hasSibling SubPropertyOf: hasBloodrelation Characteristics: Symmetric, transitive SubPropertyChain: hasParent o isParentOf We make a property of hasSibling and make it a sub-property of hasBloodrelation . Remember, think of the objects involved and the implications we want to follow; being a sibling implies being a blood relation, it does not imply any of the other relationships we have in the FHKB. Note that we have made hasSibling symmetric; if Robert is sibling of Richard, then Richard is sibling of Robert. We should also think about transitivity; if David is sibling of Peter and Peter is sibling of John, then David is sibling of John. So, we make hasSibling symmetric and transitive (see Figure 5.1). However, we must take care of half-siblings: child 1 and child 2 share a mother, but not a father; child 2 and child 3 share the father, but not the mother \u2013 child 1 and child 3 are not even half-siblings. However, at least for the moment, we will simply ignore this inconvenience, largely so that we can explore what happens with different modelling options. Figure 5.1: Showing the symmetry and transitivity of the hasSibling (siblingof) property by looking at the brothers David, John and Peter We also have the implication using three objects (see Figure 5.2): Robert holds a hasParent property with David; David holds an isFatherOf property with Richard; This implies that Robert holds a hasSibling property with Richard; As hasSibling is symmetric, Richard holds an hasSibling property with Robert. Figure 5.2: Tracing out the sub-property chain for hasSibling ; note that Robert is a sibling of himself by this path Do the following tasks: Task 17: Siblings Add the hasSibling property as above; Run the reasoner; Ask the DL query hasSibling value Robert_David_Bright_1965 . From this last DL query you should get the answer that both Robert and Richard are siblings of Robert. Think about the objects involved in the sub-property chain: we go from Robert to David via the hasParent and from David to Richard via the isParentOf property; so this is OK. However, we also go from Robert to David and then we can go from David back to Robert again \u2013 so Robert is a sibling of Robert. We do not want this to be true. We can add another characteristic to the hasSibling property, the one of being irreflexive . This means that an object cannot hold the property with itself. Task 18: More siblings Add the irreflexive characteristic to the hasSibling property; Run the reasoner; Note that the reasoner claims you have an inconsistent ontology (or in some cases, you might get a message box saying \"Reasoner died\"). Looking at the hasSibling property again, the reason might not be immediately obvious. The reason for the inconsistency lies in the fact that we create a logical contradiction: through the property chain, we say that every Person is a sibling of him or herself, and again disallowing just that by adding the irreflexive characteristic. A different explanation lies within the OWL specification itself: In order to maintain decidability irreflexive properties must be simple - for example, they may not be property chains 6 . 6 http://www.w3.org/TR/owl2-syntax/#The_Restrictions_on_the_Axiom_Closure","title":"5.2 Siblings: Option One"},{"location":"tutorial/fhkb/#521-brothers-and-sisters","text":"We have only done siblings, but we obviously need to account for brothers and sisters. In an analogous way to motherhood, fatherhood and parenthood, we can talk about sex specific sibling relationships implying the sex neutral hasSibling ; holding either a hasBrother or an isSisterOf between two objects would imply that a hasSibling property is also held between those two objects. This means that we can place these two sex specific sibling properties below hasSibling with ease. Note, however, that unlike the hasSibling property, the brother and sister properties are not symmetric. Robert hasBrother Richard and vice versa , but if Daisy hasBrother William, we do not want William to hold an hasBrother property with Daisy. Instead, we create an inverse of hasBrother , isBrotherOf , and the do the same for isSisterOf . We use similar, object based, thought processes to choose whether to have transitivity as a characteristic of hasBrother . Think of some sibling objects or individuals and place hasBrother properties between them. Make it transitive and see if you get the right answers. Put in a sister to and see if it stil works. If David hasBrother Peter and Peter hasBrother John, then David hasBrother John; so, transitivity works in this case. Think of another example. Daisy hasBrother Frederick, and Frederick hasBrother William, thus Daisy hasBrother William. The inverses work in the same way; William isBrotherOf Frederick and Frederick isBrotherOf Daisy; thus William isBrotherOf Daisy. All this seems reasonable. Task 19: Brothers and sisters Create the hasBrother object property as shown below; Add hasSister in a similar manner; 3. Add appropriate inverses, domains and ranges. ObjectProperty: hasBrother SubPropertyOf: hasSibling Characteristics: Transitive InverseOf: isBrotherOf Range: Man We have some hasSibling properties (even if they are wrong). We also know the sex of many of the people in the FHKB through the domains and ranges of properties such as hasFather , hasMother and their inverses.. Can we use sub-property chains in the same way as we have used them in the hasSibling property? The issue is that of sex; the property isFatherOf is sex neutral at the child end, as is the inverse hasFather (the same obviously goes for the mother properties). We could use a sub-property chain of the form: ObjectProperty: hasBrother SubPropertyChain: hasParent o hasSon A son is a male child and thus that object is a brother of his siblings. At the moment we do not have son or daughter properties. We can construct a property hierarchy as shown in Figure 5.3. This is made up from the following properties: hasChild and isChildOf hasSon (range Man and domain Person ) and isSonOf ; hasDaughter (range Woman domain Person ) and isDaughterOf Note that hasChild is the equivalent of the existing property isParentOf ; if I have a child, then I am its parent. OWL 2 can accommodate this fact. We can add an equivalent property axiom in the following way: ObjectProperty: isChildOf EquivalentTo: hasParent We have no way of inferring the isSonOf and isDaughterOf from what already exists. What we want to happen is the implication of \u2018 Man and hasParent Person implies isSonOf \u2019. OWL 2 and its reasoners cannot do this implication. It has been called the \u2018man man problem\u2019 7 . Solutions for this have been developed [3], but are not part of OWL 2 and its reasoners. Figure 5.3: The property hierarchy for isChildOf and associated son/daughter properties 7 http://lists.w3.org/Archives/Public/public-owl-dev/2007JulSep/0177.html Child property Parent Robert David Bright 1965 isSonOf David Bright 1934, Margaret Grace Rever 1934 Richard John Bright 1962 isSonOf David Bright 1934, Margaret Grace Rever 1934 Mark Bright 1956 isSonOf John Bright 1930, Joyce Gosport Ian Bright 1959 isSonOf John Bright 1930, Joyce Gosport Janet Bright 1964 isDaughterOf John Bright 1930, Joyce Gosport William Bright 1970 isSonOf John Bright 1930, Joyce Gosport Table 5.1: Child property assertions for the FHKB Thus we must resort to hand assertions of properties to test out our new path: Task 20: Sons and daughters Add the property hierarchy shown in Figure 5.3, together with the equivalent property axiom and the obvious inverses. As a test (after running the reasoner), ask the DL query isChildOf value David_Bright_1934 and you should have the answer of Richard and Robert; Add the sub-property paths as described in the text; Add the assertions shown in Table 5.1; Run the reasoner; Ask the DL query for the brother of Robert David Bright and the sister of Janet. Of course, it works, but we see the same problem as above. As usual, think of the objects involved. Robert isSonOf David and David isParentOf Robert, so Robert is his own brother. Irreflexivity again causes problems as it does above (Task 18).","title":"5.2.1 Brothers and Sisters"},{"location":"tutorial/fhkb/#53-siblings-option-two","text":"Our option one has lots of problems. So, we have an option of asserting the various levels of sibling. We can take the same basic structure of sibling properties as before, but just fiddle around a bit and rely on more assertion while still trying to infer as much as possible. We will take the following approach: We will take off the sub-property chains of the sibling properties as they do not work; We will assert the leaf properties of the sibling sub-hierarchy sparsely and attempt to infer as much as possible. Person Property Person Robert David Bright 1965 isBrotherOf Richard John Bright 1962 David Bright 1934 isBrotherOf John Bright 1930 David Bright 1934 isBrotherOf Peter William Bright 1941 Janet Bright 1964 isSisterOf Mark Bright 1956 Janet Bright 1964 isSisterOf Ian Bright 1959 Janet Bright 1964 isSisterOf William Bright 1970 Mark Bright 1956 isBrotherOf Ian Bright 1959 Mark Bright 1956 isBrotherOf Janet Bright 1964 Mark Bright 1956 isBrotherOf William Bright 1970 Table 5.2: The sibling relationships to add to the FHKB. Do the following: Task 21: Add sibling assertions Remove the sub-property chains of the sibling properties and the isChildOf assertions as explained above. Add the Sibling assertions shown in table 5.2; Run the reasoner; Ask isBrotherOf value Robert_David_Bright_1965 ; Ask isBrotherOf value Richard_John_Bright_1962 ; Ask hasBrother value Robert_David_Bright_1965 ; Ask hasBrother value Richard_John_Bright_1962 ; Ask isSisterOf value William_Bright_1970 ; Ask the query Man and hasSibling value Robert_David_Bright_1965 . We can see some problems with this option as well: With these properties asserted, Richard only has a hasBrother property to Robert. We would really like an isBrotherOf to Robert to hold. The query Man and hasSibling value Robert only retrieves Robert himself. Because we only asserted that Robert is a brother of Richard, and the domain of isBrotherOf is Man we know that Robert is a Man , but we do not know anything about the Sex of Richard.","title":"5.3 Siblings: Option two"},{"location":"tutorial/fhkb/#531-which-modelling-option-to-choose-for-siblings","text":"Which of the two options gives the worse answers and which is the least effort? Option one is obviously the least effort; we only have to assert the same parentage facts as we already have; then the sub-property chains do the rest. It works OK for hasSibling , but we cannot do brothers and sisters adequately; we need Man and hasSibling \u2290 isBrotherOf and we cannot do that implication. This means we cannot ask the questions we need to ask. So, we do option two, even though it is hard work and is still not perfect for query answering, even though we have gone for a sparse assertion mode. Doing full sibling assertion would work, but is a lot of effort. We could start again and use the isSonOfandisDaughterOf option, with the sub-property chains described above. This still has the problem of everyone being their own sibling. It can get the sex specific sibling relationships, but requires a wholesale re-assertion of parentage facts. We will continue with option two, largely because it highlights some nice problems later on.","title":"5.3.1 Which Modelling Option to Choose for Siblings?"},{"location":"tutorial/fhkb/#54-half-siblings","text":"In Section 5.2 we briefly talked about half-siblings. So far, we have assumed full-siblings (or, rather, just talked about siblings and made no distinction). Ideally, we would like to accommodate distinctions between full- and half-siblings; here we use half-siblings, where only one parent is in common between two individuals, as the example. The short-answer is, unfortunately, that OWL 2 cannot deal with half-siblings in the way that we want - that is, such that we can infer properties between named individuals indicating full- or half-sibling relationships. It is possible to find sets of half-brothers in the FHKB by writing a defined class or DL-query for a particular individual.} The following fragment of OWL defines a class that looks for the half-brothers of an individual called \u2018Percival\u2019: Class: HalfBrotherOfPercival EquivalentTo: Man and (((hasFather some (not (isFatherOf value Percival))) and (hasMother some (isMotherOf value Percival))) or ((hasFather some (isFatherOf value Percival)) and (hasMother some (not (isMotherOf value Percival))))) Here we are asking for any man that either has Percival\u2019s father but not his mother, or his mother, but not his father. This works fine, but is obviously not a general solution. The OWL description is quite complex and the writing will not scale as the number of options (hypothetically, as the number of parents increases... ) increases; it is fine for man/woman, but go any higher and it will become very tedious to write all the combinations. Another way of doing this half-brother class to find the set of half-brothers of a individual is to use cardinality constraints: Class: HalfBrotherOfPercival EquivalentTo: Man and (hasParent exactly 1 (isParentOf value Percival)) This is more succinct. We are asking for a man that has exactly one parent from the class of individuals that are the class of Percival\u2019s parents. This works, but one more constraint has to be present in the FHKB. We need to make sure that there can be only two parents (or indeed, just a specified number of parents for a person). If we leave it open as to the number of parents a person has, the reasoner cannot work out that there is a man that shares exactly one parent, as there may be other parents. We added this constraint to the FHKB in Section 6.2; try out the classes to check that they work. These two solutions have been about finding sets of half-brothers for an individual. What we really want in the FHKB is to find half-brothers between any given pair of individuals. Unfortunately we cannot, without rules, ask OWL 2 to distinguish full- and half-siblings \u2013 we cannot count the number of routes taken between siblings via different distinct intermediate parent objects.","title":"5.4 Half-Siblings"},{"location":"tutorial/fhkb/#55-aunts-and-uncles","text":"An uncle is a brother of either my mother or father. An aunt is a sister of either my mother or father. In common practice, wives and husbands of aunts and uncles are usually uncles and aunts respectively. Formally, these aunts and uncles are aunts-in-law and uncles-in-law. Whatever approach we take, we cannot fully account for aunts and uncles until we have information about marriages, which will not have until Chapter 9. We will, however, do the first part now. Look at the objects and properties between them for the following facts: Robert has father David and mother Margaret; David has brothers Peter and John; Margaret has a sister Eileen; Robert thus has the uncles John and Peter and an aunt Eileen. As we are tracing paths or \u2018chains\u2019 of objects and properties we should use sub-property chains as a solution for the aunts and uncles. We can make an hasUncle property as follows (see Figure 5.4): ObjectProperty: hasUncle SubPropertyOf: hasBloodrelation Domain: Man Range: Person SubPropertyChain: hasParent o hasBrother InverseOf: isUncleOf Figure 5.4: Tracing out the path between objects to get the hasUncle sub-property chain. Notice we have the domain of Man and range of Person . We also have an inverse. As usual, we can read this as \u2018an object that holds an hasParent property, followed by an object holding a hasBrother property, implies that the first object holds an hasUncle property with the last object\u2019. Note also where the properties (include the ones for aunt) go in the object property hierarchy. Aunts and uncles are not ancestors that are in the direct blood line of a person, but they are blood relations (in the narrower definition that we are using). Thus the aunt and uncle properties go under the hasBloodrelation property (see Figure 5.5). Again, think of the implications between objects holding a property between them; that two objects linked by a property implies that those two objects also hold all the property\u2019s super-properties as well. As long as all the super-properties are true, the place in the object property hierarchy is correct (think about the implications going up, rather than down). Figure 5.5: The object property hierarchy with the aunt and uncle properties included. On the right side, we can see the hasUncle property as shown by Prot\u00e9g\u00e9. Do the following tasks: Task 22: Uncles and Aunts Add the hasUncle property as above; Add the hasAunt property as well; Ask for the uncles of Julie_Bright_1966 and for Mark_Bright_1956 ; Add similar properties for hasGreatUncle and hasGreatAunt and place them in the property hierarchy. We can see this works \u2013 unless we have any gaps in the sibling relationships (you may have to fix these). Great aunts and uncles are simply a matter of adding another \u2018parent\u2019 leg into the sub-property chain. We are not really learning anything new with aunts and uncles, except that we keep gaining a lot for free through sub-property chains. We just add a new property with its sub-property chain and we get a whole lot more inferences on individuals. To see what we now know about Robert David Bright, do the following: Task 23: What do we know? Save the ontology and run the reasoner; Look at inferences related to the individual Robert David Bright (see warning in the beginning of this chapter). If you chose to use DL queries in Prot\u00e9g\u00e9, do not forget to tick the appropriate check-boxes. You can now see lots of facts about Robert David Bright, with only a very few actual assertions directly on Robert David Bright.","title":"5.5 Aunts and Uncles"},{"location":"tutorial/fhkb/#56-summary","text":"Siblings have revealed several things for us: We can use just the parentage facts to find siblings, but everyone ends up being their own sibling; We cannot make the properties irreflexive, as the knowledge base becomes inconsistent; We would like an implication of Man and hasSibling \u2283 isBrotherOf , but OWL 2 doesn\u2019t do this implication; Whatever way we model siblings, we end up with a bit of a mess; OWL 2 cannot do half-siblings; However, we can get close enough and we can start inferring lots of facts via sub-property chains using the sibling relationships. The FHKB ontology at this stage of the tutorial has an expressivity ofSRIF. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1355.614 sec (0.71682 % of final), by Pellet 2.2.0 0.206 sec (0.00167 % of final) and by FaCT++ 1.6.4 is approximately 0.039 sec (0.001 % of final). 0 sec indicates failure or timeout.","title":"5.6 Summary"},{"location":"tutorial/fhkb/#chapter-6","text":"","title":"Chapter 6"},{"location":"tutorial/fhkb/#individuals-in-class-expressions","text":"In this chapter you will: Use individuals within class expressions; Make classes to find Robert and Richard\u2019s parents, ancestors, and so on; Explore equivalence of such classes; Re-visit the closed world. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial","title":"Individuals in Class Expressions"},{"location":"tutorial/fhkb/#61-richard-and-roberts-parents-and-ancestors","text":"So far we have only used object properties between unspecified objects. We can, however, specify a specific individual to act at the right-hand-side of a class restriction or type assertion on an individual. The basic syntax for so-called nominals is: Class: ParentOfRobert EquivalentTo: Person and isParentOf valueRobert_David_Bright_1965 This is an equivalence axiom that recognises any individual that is a Person and a parent of Robert David Bright. Task 24: Robert and Richards parents Create the class ParentOfRobert as described above; Classify \u2013 inspect where the class is placed in the FHKB TBox and look at which individuals classify as members of the class; Do the same for a class with the value of Richard_John_Bright_1962 and classify; Finally create a class ParentOfRichardAndRobert , defining it as Person and isParentOf some {Robert_David_Bright_1965 ,Richard_John_Bright_1962 } ; again see what happens on classification. Note that the expressions isMotherOf value Robert_David_Bright_1965 and isMotherOf some {Robert_David_Bright_1965 } are practically identical. The only difference is that using value , you can only specify one individual, while some relates to a class (a set of individuals). We see that these queries work and that we can create more complex nominal based class expressions. The disjunction above is isParentOf some {Robert_David_Bright_1965, Richard_John_Bright_1965} The \u2018{\u2019 and \u2018}\u2019 are a bit of syntax that says \u2018here\u2019s a class of individual\u2019. We also see that the classes for the parents of Robert David Bright and Richard John Bright have the same members according to the FHKB, but that the two classes are not inferred to be equivalent. Our domain knowledge indicates the two classes have the same extents (members) and thus the classes are equivalent, but the automated reasoner does not make this inference. As usual, this is because the FHKB has not given the automated reasoner enough information to make such an inference.","title":"6.1 Richard and Robert\u2019s Parents and Ancestors"},{"location":"tutorial/fhkb/#62-closing-down-what-we-know-about-parents-and-siblings","text":"The classes describing the parents of Richard and Robert are not equivalent, even though, as humans, we know their classes of parent are the same. We need more constraints so that it is known that the four parents are the only ones that exist. We can try this by closing down what we know about the immediate family of Robert David Bright. In Chapter 4 we described that a Person has exactly one Woman and exactly one Man as mother and father (by saying that the hasMother and hasFather properties are functional and thus only one of each may be held by any one individual to distinct individuals). The parent properties are defined in terms of hasParent , hasMother and hasFather . The latter two imply hasParent . The two sub-properties are functional, but there are no constraints on hasParent , so an individual can hold many instances of this property. So, there is no information in the FHKB to say a Person has only two parents (we say there is one mother and one father, but not that there are only two parents). Thus Robert and Richard could have other parents and other grandparents than those in the FHKB; we have to close down our descriptions so that only two parents are possible. There are two ways of doing this: Using qualified cardinality constraints in a class restriction; Putting a covering axiom on hasParent in the same way as we did for Sex in Chapter 4. Task 25: Closing the Person class Add the restriction hasParent exactly 2 Person to the classPerson ; Run the reasoner; Inspect the hierarchy to see where ParentOfRobert and ParentOfRichard are placed and whether or not they are found to be equivalent; Now add the restriction hasParent max 2 Person to the class Person ; Run the reasoner (taking note of how long the reasoning takes) and take another look. We find that these two classes are equivalent; we have supplied enough information to infer that these two classes are equivalent. So, we know that option one above works, but what about option two? This takes a bit of care to think through, but the basic thing is to think about how many ways there are to have a hasParent relationship between two individuals. We know that we can have either a hasFather or a hasMother property between two individuals; we also know that we can have only one of each of these properties between an individual and a distinct individual. However, the open world assumption tells us that there may be other ways of having a hasParent property between two individuals; we\u2019ve not closed the possibilities. By putting on the hasParent exactly 2 Person restriction on the Person class, we are effectively closing down the options for ways that a person can have parents; we know because of the functional characteristic on hasMother and hasFather that we can have only one of each of these and the two restrictions say that one of each must exist. So, we know we have two ways of having a parent on each Person individual. So, when we say that there are exactly two parents (no more and no less) we have closed down the world of having parents\u2014thus these two classes can be inferred to be equivalent. It is also worth noting that this extra axiom on the Person class will make the reasoner run much more slowly. Finally, for option 2, we have no way of placing a covering axiom on a property. What we\u2019d like to be able to state is something like: ObjectProperty: hasParent EquivalentTo: hasFather or hasMother but we can\u2019t.","title":"6.2 Closing Down What we Know About Parents and Siblings"},{"location":"tutorial/fhkb/#63-summary","text":"For practice, do the following: Task 26: Additional Practice Add lots more classes using members of the ABox as nominals; Make complex expressions using nominals; After each addition of a nominal, classify and see what has been inferred within the FHKB. See if you can make classes for GrandparentOfRobert and GrandparentOfRichard and make them inferred to be equivalent. In this chapter we have seen the use of individuals within class expressions. It allows us to make useful queries and class definitions. The main things to note is that it can be done and that there is some syntax involved. More importantly, some inferences may not be as expected due to the open world assumption in OWL. By now you might have noticed a significant increase in the time the reasoner needs to classify. Closing down what we know about family relationships takes its toll on the reasoner performance, especially the usage of 'hasParent exactly 2 Person'. At this point we recommend rewriting this axiom to 'hasParent max 2 Person'. It gives us most of what we need, but has a little less negative impact on the reasoning time. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ. The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 2067.273 sec (1.09313 % of final), by Pellet 2.2.0 0.529 sec (0.00428 % of final) and by FaCT++ 1.6.4 is approximately 0.147 sec (0.004 % of final). 0 sec indicates failure or timeout.","title":"6.3 Summary"},{"location":"tutorial/fhkb/#chapter-7","text":"","title":"Chapter 7"},{"location":"tutorial/fhkb/#data-properties-in-the-fhkb","text":"We now have some individuals with some basic object properties between individuals. OWL 2, however, also has data properties that can relate an object or individual to some item of data. There are data about a Person , such as years of events and names etc. So, in this Chapter you will: Make some data properties to describe event years to people; Create some simple defined classes that group people by when they were born; Try counting the numbers of children people have... Deal with the open world assumption; Add given and family names to individuals in the FHKB. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial.","title":"Data Properties in the FHKB"},{"location":"tutorial/fhkb/#71-adding-some-data-properties-for-event-years","text":"Everyone has a birth year; death year; and some have a marriage year and so on. We can model these simply with data properties and an integer as a filler. OWL 2 has a DateTime datatype, where it is possible to specify a precise time and date down to a second. 7 This proves cumbersome (see http://robertdavidstevens.wordpress.com/2011/05/05/using-the-datetime-data-type-to-describe-birthdays/ for details); all we need is a simple indication of the year in which a person was born. Of course, the integer type has a zero, which the Gregorian calendar for which we use integer as a proxy does not, but integer is sufficient to our needs. Also, there are various ontological treatments of time and information about people (this extends to names etc. as well), but we gloss over that here\u2014that\u2019s another tutorial. 7 http://www.w3.org/TR/2008/WD-owl2-quick-reference-20081202/#Built-in_Datatypes_and_Facets We can have dates for birth, death and (eventually) marriage (see Chapter 9) and we can just think of these as event years. We can make a little hierarchy of event years as shown in Figure 7.1). Task 27: Create a data property hierarchy Create the data property hasEventYear with range integer and domain Person ; Create the data property hasBirthYear and make it a sub-property of hasEventYear (that way, the domain and range of hasEventYear are inherited); Create the data property hasDeathYear and make it a sub-property of hasEventYear ; For each individual add the birth years shown in Table A.1 (see appendix). You do not actually have to go back to the table\u2014it is easier to read the birth years simply off the individual names. Again, asserting birth years for all individuals can be a bit tedious. The reader can find a convenience snapshot of the ontology at this stage at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial We now have an ABox with individuals with fact assertions to data indicating a birth year. We can, if we wish, also add a class restriction to the Person class saying that each and every instance of the class Person holds a data property to an integer and that this property is called \u2018hasBirthYear\u2019. As usual when deciding whether to place such a restriction upon a class, ask whether it is true that each and every instance of the class holds that property; this is exactly the same as we did for the object properties in Chapter 4. Everyone does have a birth year, even if it is not known. Once birth years have been added to our individuals, we can start asking some questions. Task 28: DL queries 1. Use a DL query to ask: Person born after 1960; Person born in the 1960s; Person born in the 1800s; Person that has fewer than three children; Person that has more than three children. The DL query for people born in the 1960s is: Person and hasBirthYear some int[>= 1960, < 1970] This kind of interval is known as a facet.","title":"7.1 Adding Some Data Properties for Event Years"},{"location":"tutorial/fhkb/#711-counting-numbers-of-children","text":"The last two queries in the list do not work as expected. We have asked, for instance, for Person that have more than three children, but we get no members of Person in the answer, though we know that there are some in the FHKB (e.g., John_Bright_1930 ). This is because there is not enough information in the FHKB to tell that this person has more than three different people as children. As humans we can look at the four children of John Bright and know that they are different \u2013 for instance, they all have different birth years. The automated reasoner, however, does not know that a Person can only have one birth year. Task 29: Make a functional object property Make the property hasBirthYear functional. Ask the query for Person that has more than three children again. This time the query should work. All the other event year properties should be made functional, expect hasEventYear , as one individual can have many event years. As the children have different birth year and an individual can only hold one hasBirthYear property, then these people must be distinct entities. Of course, making birth year functional is not a reliable way of ensuring that the automated reasoner knows that the individual are different. It is possible for two Person to have the same birth year within the same family \u2013 twins and so on. Peter_William_Bright_1941 has three children, two of which are twins, so will not be a member of the class of people with at least three children. So, we use the different individuals axiom. Most tools, including Prot\u00e9g\u00e9, have a feature that allows all individuals to be made different. Task 30: Make all individuals different Make all individuals different; Ask the above queries again. From now on, every time you add individuals, make sure the different individuals axiom is updated.","title":"7.1.1 Counting Numbers of Children"},{"location":"tutorial/fhkb/#72-the-open-world-assumption","text":"We have met again the open world assumption and its importance in the FHKB. In the use of the functional characteristic on the hasBirthYear property, we saw one way of constraining the interpretation of numbers of children. We also introduced the \u2018different individuals\u2019 axiom as a way of making all individuals in a knowledge base distinct. There are more questions, however, for which we need more ways of closing down the openness of OWL 2. Take the questions: People that have exactly two children; People that have only brothers; People that have only female children. We can only answer these questions if we locally close the world.We have said that David and Margaret have two children, Richard and Robert, but we have not said that there are not any others. As usual, try not to apply your domain knowledge too much; ask yourself what the automated reasoner actually knows. As we have the open world assumption, the reasoner will assume, unless otherwise said, that there could be more children; it simply doesn\u2019t know. Think of a railway journey enquiry system. If I ask a standard closed world system about the possible routes by rail, between Manchester and Buenos Aires, the answer will be \u2019none\u2019, as there are none described in the system. With the open world assumption, if there is no information in the system then the answer to the same question will simply be \u2018I don\u2019t know\u2019. We have to explicitly say that there is no railway route from Manchester to Buenos Aires for the right answer to come back. We have to do the same thing in OWL. We have to say that David and Margaret have only two children. We do this with a type assertion on individuals. So far we have only used fact assertions. A type assertion to close down David Bright\u2019 parentage looks like this: isParentOf only {Robert_David_Bright_1965,Richard_John_Bright_1962 } This has the same meaning as the closure axioms that you should be familiar with on classes. We are saying that the only fillers that can appear on the right-hand-side of the isParentOf property on this individual are the two individuals for Richard and Robert. We use the braces to represent the set of these two individuals. Task 31: Make a closure axiom Add the closure assertion above to David Bright; Issue the DL query isParentOf exactly 2 Person . The last query should return the answer of David Bright. Closing down the whole FHKB ABox is a chore and would really have to be done programmatically. OWL scripting languages such as the Ontology Preprocessing Language 8 (OPPL) [2] can help here. Also going directly to the OWL API [1] 9 , if you know what you are doing, is another route. Adding all these closure type assertions can slow down the reasoner; so think about the needs of your system \u2013 just adding it \u2018because it is right\u2019 is not necessarily the right route. 8 http://oppl2.sourceforge.net 9 http://owlapi.sourceforge.net/","title":"7.2 The Open World Assumption"},{"location":"tutorial/fhkb/#73-adding-given-and-family-names","text":"We also want to add some other useful data facts to people \u2013 their names. We have been putting names as part of labels on individuals, but data fact assertions make sense to separate out family and given names so that we can ask questions such as \u2018give me all people with the family name Bright and the first given name of either James or William\u2019. A person\u2019s name is a fact about that person and is more, in this case, than just a label of the representation of that person. So, we want family names and given names. A person may have more than one given name \u2013 \u2018Robert David\u2019, for instance \u2013 and an arbitrary number of given names can be held. For the FHKB, we have simply created two data properties of hasFirstGivenName and hasSecondGivenName ). Ideally, it would be good to have some index on the property to given name position, but OWL has no n-ary relationships. Otherwise, we could reify the hasGivenName property into a class of objects, such as the following: Class: GivenName SubClassOf:hasValue some String, hasPosition some Integer but it is really rather too much trouble for the resulting query potential. As already shown, we will use data properties relating instances of Person to strings. We want to distinguish family and given names, and then different positions of given names through simple conflating of position into the property name. Figure 7.1 shows the intended data property hierarchy. Figure 7.1: The event year and name data property hierarchies in the FHKB. Do the following: Task 32: Data properties Create the data properties as described in Figure 7.1; Give the hasName property the domain of Person and the range of String ; Make the leaf properties of given names functional; Add the names shown in Table A.1 (appendix); Again, it may be easier to read the names of the individual names. Ask the questions: all the people with the first given name \u2018James\u2019; all the people with the first given name \u2018William\u2019; All the people with the given name \u2018William\u2019; All the people with the given name \u2018William\u2019 and the family name \u2018Bright\u2019. The name data property hierarchy and the queries using those properties displays what now should be familiar. Sub-properties that imply the super-property. So, when we ask hasFirstGivenName value \"William\" and then the query hasGivenName value value \"William\" we can expect different answers. There are people with \u2018William\u2019 as either first or second given name and asking the question with the super-property for given names will collect both first and second given names.","title":"7.3 Adding Given and Family Names"},{"location":"tutorial/fhkb/#74-summary","text":"We have used data properties that link objects to data such as string, integer, floats and Booleans etc. OWL uses the XML data types. We have seen a simple use of data properties to simulate birth years. The full FHKB also uses them to place names (given and family) on individuals as strings. This means one can ask for the Person with the given name \"James\", of which there are many in the FHKB. Most importantly we have re-visited the open world assumption and its implications for querying an OWL ABox. We have looked at ways in which the ABox can be closed down \u2013 unreliably via the functional characteristic (in this particular case) and more generally via type assertions. All the DL queries used in this chapter can also serve as defined classes in the TBox. It is a useful exercise to progressively add more defined classes to the FHKB TBox. Make more complex queries, make them into defined classes and inspect where they appear in the class hierarchy. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 1891.157 sec (1.00000 % of final), by Pellet 2.2.0 1.134 sec (0.00917 % of final) and by FaCT++ 1.6.4 is approximately 0.201 sec (0.006 % of final). 0 sec indicates failure or timeout. Note that we now cover the whole range of expressivity of OWL 2. HermiT at least is impossibly slow by now. This may be because HermiT does more work than the others. For now, we recommend to use either Pellet or FaCT++.","title":"7.4 Summary"},{"location":"tutorial/fhkb/#chapter-8","text":"","title":"Chapter 8"},{"location":"tutorial/fhkb/#cousins-in-the-fhkb","text":"In this Chapter you will Revise or get to know about degrees and removes of cousin; Add the properties and sub-property chains for first and second cousins; Add properties and sub-property chains for some removes of cousins; Find out that the siblings debacle haunts us still; Add a defined class that does first cousins properly. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Be warned; from here on the reasoner can start running slowly! Please see warning at the beginning of the last chapter for more information.","title":"Cousins in the FHKB"},{"location":"tutorial/fhkb/#81-introducing-cousins","text":"Cousins can be confusing, but here is a brief summary: First cousins share a grandparent, but are not siblings; Second cousins share a great grandparent, but are not first cousins or siblings; Degrees such as first and second cousin give the distance to the nearest common ancestor; Removes give differences in generation. So, my Dad\u2019s first cousins (his generation) are my (Robert David Bright\u2019s) first cousins once removed. Simply, my first cousins are my parent\u2019s sibling\u2019s children. As usual, we can think about the objects and put in place some sub-property chains.","title":"8.1 Introducing Cousins"},{"location":"tutorial/fhkb/#82-first-cousins","text":"Figure 8.1: Tracing out the sub-property chain for cousins going from a child to a parent, to its sibling, and down to its child, a cousin Figure 8.1 shows the sub-property chain for first cousins. As usual, think at the object level; to get to the first cousins of Robert David Bright, we go to the parents of Robert David Bright, to their siblings and then to their children. We go up, along and down. The OWL for this could be: ObjectProperty: hasFirstCousin SubPropertyOf: hasCousin SubPropertyChain: hasParent o hasSibling o hasChild Characteristics: Symmetric Note that we follow the definitions in Section 8.1 of first cousins sharing a grandparent, but not a parent. The sub-property chain goes up to children of a grandparent (a given person\u2019s parents), along to siblings and down to their children. We do not want this property to be transitive. One\u2019s cousins are not necessarily my cousins. The blood uncles of Robert David Bright have children that are his cousins. These first cousins, however, also have a mother that is not a blood relation of Robert David Bright and the mother\u2019s sibling\u2019s children are not cousins of Robert David Bright. We do, however, want the property to be symmetric. One\u2019s cousins have one\u2019s-self as a cousin. We need to place the cousin properties in the growing object property hierarchy. Cousins are obviously blood relations, but not ancestors, so they go off to one side, underneath hasBloodrelation . We should group the different removes and degree of cousin underneath one hasCousin property and this we will do. Do the following: Task 33: First cousins Add the property of hasCousin to the hierarchy underneath hasBloodrelation ; Add hasFirstCousin underneath this property; Add the sub-property chain as described above; Run the reasoner and look at the first cousins of Robert David Bright. You should see the following people as first cousins of Robert David Bright: Mark Anthony Heath, Nicholas Charles Heath, Mark Bright, Ian Bright, Janet Bright, William Bright, James Bright, Julie Bright, Clare Bright, Richard John Bright and Robert David Bright. The last two, as should be expected, are first cousins of Robert David Bright and this is not correct. As David Bright will be his own brother, his children are his own nieces and nephews and thus the cousins of his own children. Our inability to infer siblings correctly in the FHKB haunts us still and will continue to do so. Although the last query for the cousins of Robert David Bright should return the same results for every reasoner, we have had experiences where the results differ.","title":"8.2 First Cousins"},{"location":"tutorial/fhkb/#83-other-degrees-and-removes-of-cousin","text":"Other degrees of cousins follow the same pattern as for first cousins; we go up, along and down. For second cousins we go up from a given individual to children of a great grandparent, along to their siblings and down to their grandchildren. The following object property declaration is for second cousins (note it uses the isGrandparentOf and its inverse properties, though the parent properties could be used) : ObjectProperty: hasSecondCousin SubPropertyOf: hasCousin SubPropertyChain: hasGrandParent o hasSibling o isGrandParentOf Characteristics: Symmetric \u2018 Removes \u2019 simply add in another \u2018leg\u2019 of either \u2018up\u2019 or \u2018down\u2019 either side of the \u2018along\u2019\u2014that is, think of the actual individuals involved and draw a little picture of blobs and lines\u2014then trace your finger up, along and down to work out the sub-property chain. The following object property declaration does it for first cousins once removed (note that this has been done by putting this extra \u2018leg\u2019 on to the hasFirstCousin property; the symmetry of the property makes it work either way around so that a given person is the first cousin once removed of his/her first cousins once removed): ObjectProperty: hasFirstCousinOnceRemoved SubPropertyOf: hasCousin SubPropertyChain: hasFirstCousin o hasChild Characteristics: Symmetric To exercise the cousin properties do the following: Task 34: Cousin properties Add properties for second degree cousins; Add removes for first and second degree cousins; Run the reasoner and check what we know about Robert David Bright\u2019 other types of cousin. You should see that we see some peculiar inferences about Robert David Bright\u2019 cousins \u2013 not only are his brother and himself his own cousins, but so are his father, mother, uncles and so on. This makes sense if we look at the general sibling problem, but also it helps to just trace the paths around. If we go up from one of Robert David Bright\u2019 true first cousins to a grandparent and down one parent relationship, we follow the first cousin once removed path and get to one of Robert David Bright\u2019 parents or uncles. This is not to be expected and we need a tighter definition that goes beyond sub-property chains so that we can exclude some implications from the FHKB.","title":"8.3 Other Degrees and Removes of Cousin"},{"location":"tutorial/fhkb/#84-doing-first-cousins-properly","text":"As far as inferring first cousin facts for Robert David Bright, we have failed. More precisely, we have recalled all Robert David Bright\u2019s cousins, but the precision is not what we would desire. What we can do is ask for Robert David Bright\u2019 cousins, but then remove the children of Robert David Bright\u2019 parents. The following DL query achieves this: Person that hasFirstCousin valueRobert_David_Bright_1965 and (not (hasFather valueDavid_Bright_1934) or not (hasMother valueMar- garet_Grace_Rever_1934) This works, but only for a named individual. We could make a defined class for this query; we could also make a defined class FirstCousin , but it is not of much utility. We would have to make sure that people whose parents are not known to have siblings with children are excluded. That is, people are not \u2018first cousins\u2019 whose only first cousins are themselves and their siblings. The following class does this: Class: FirstCousin EquivalentTo: Person that hasFirstCousin some Person Task 35: Roberts first cousins Make a defined class FirstCousin as shown above; Make a defined class FirstCousinOfRobert ; Create a DL query that looks at Robert_David_Bright_1965 first cousins and takes away the children of Robert_David_Bright_1965 \u2019 parents as shown above. This gives some practice with negation. One is making a class and then \u2018taking\u2019 some of it away \u2013 \u2018these, but not those\u2019.","title":"8.4 Doing First Cousins Properly"},{"location":"tutorial/fhkb/#85-summary","text":"We have now expanded the FHKB to include most blood relationships. We have also found that cousins are hard to capture just using object properties and sub-property chains. Our broken sibling inferences mean that we have too many cousins inferred at the instance level. We can get cousins right at the class level by using our inference based cousins, then excluding some using negation. Perhaps not neat, but it works. We have reinforced that we can just add more and more relationships to individuals by just adding more properties to our FHKB object property hierarchy and adding more sub-property chains that use the object properties we have built up upon parentage and sibling properties; this is as it should be. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 111.395 sec (0.90085 % of final) and by FaCT++ 1.6.4 is approximately 0.868 sec (0.024 % of final). 0 sec indicates failure or timeout.","title":"8.5 Summary"},{"location":"tutorial/fhkb/#chapter-9","text":"","title":"Chapter 9"},{"location":"tutorial/fhkb/#marriage-in-the-fhkb","text":"In this chapter you will: Model marriages and relationships; Establish object properties for husbands, wives and various in-laws; Re-visit aunts and uncles to do them properly; Use more than one sub-property chain on a given property. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial Much of what is in this chapter is really revision; it is more of the same - making lots of properties and using lots of sub-property chains. However, it is worth it as it will test your growing skills and it also makes the reasoners and yourself work hard. There are also some good questions to ask of the FHKB as a result of adding marriages.","title":"Marriage in the FHKB"},{"location":"tutorial/fhkb/#91-marriage","text":"Marriage is a culturally complex situation to model. The FHKB started with a conservative model of a marriage involving only one man and one woman. 10 Later versions are more permissive; a marriage simply has a minimum of two partners. This leaves it open to numbers and sex of the people involved. In fact, \u2018marriage\u2019 is probably not the right name for it. Using BreedingRelationship as a label (the one favoured by the main author\u2019s mother) may be a little too stark and might be a little exclusive.... In any case, some more generic name is probably better and various subclasses of the FHKB\u2019s Marriage class are probably necessary. 10 There being no funny stuff in the Stevens family. To model marriage do the following: Task 36: Marriage Create a class Marriage , subclass of DomainEntity ; Create the properties: hasPartner (domain Marriage and range Person ) and isPartnerIn hasFemalePartner (domain Marriage and range Woman , sub-property of hasPartner ) and its inverse isFemalePartnerIn ; a sub-property of hasPartner has MalePartner (domain Marriage and range Man )and its inverse isMalePartnerIn ; Create the data property hasMarriageYear , making us a sub-property of hasEventYear ,make it functional; Create an individual m001 with the label Marriage of David and Margaret and add the facts: hasMalePartner David_Bright_1934 ; hasFemalePartner Margaret_Grace_Rever_1934 hasMarriageYear 1958 ; Create an individual m002 with the label Marriage of John and Joyce and add the facts: hasMalePartner John_Bright_1930 ; hasFemalePartner Joyce_Gosport (you may have to add Joyce if you did not already did that); hasMarriageYear 1955 ; Create an individual m003 with the label Marriage of Peter and Diana and add the facts: hasMalePartner Peter_William_Bright_1941 ; hasFemalePartner Diana_Pool (you may have to add Diana if you did not already did that); hasMarriageYear 1964 ; We have the basic infrastructure for marriages. We can ask the usual kinds of questions; try the following: Task 37: DL queries Ask the following DL queries: The Women partners in marriages; Marriages that happened before 1960 (see example below); Marriages that happened after 1960; Marriages that involved a man with the family name \u2018Bright\u2019. DL query: Marriage and hasMarriageYear some int[<= 1960]","title":"9.1 Marriage"},{"location":"tutorial/fhkb/#911-spouses","text":"This marriage infrastructure can be used to infer some slightly more interesting things for actual people. While we want marriage objects so that we can talk about marriage years and even locations, should we want to, we also want to be able to have the straight-forward spouse relationships one would expect. We can use sub-property chains in the usual manner; do the following: Task 38: Wifes and Husbands Create a property hasSpouse with two sub-properties hasHusband and hasWife . Create the inverses isSpouseOf , isWifeOf and isHusbandOf . To the hasWife property, add the sub-property chain isMalePartnerIn o hasFemalePartner . Follow the same pattern for the hasHusband property. Figure 9.1 shows what is happening with the sub-property chains. Note that the domains and ranges of the spouse properties come from the elements of the sub-property chains. Note also that the hasSpouse relationship will be implied from its sub-property chains. The following questions can now be asked: Is wife of David Bright; Has a husband born before 1940; The wife of an uncle of William Bright 1970. Figure 9.1: The sub-property chain path used to infer the spouse relationships via the marriage partnerships. and many more. This is really a chance to explore your querying abilities and make some complex nested queries that involve going up and down the hierarchy and tracing routes through the graph of relationships between the individuals you\u2019ve inferred.","title":"9.1.1 Spouses"},{"location":"tutorial/fhkb/#92-in-laws","text":"Now we have spouses, we can also have in-laws. The path is simple: isSpouseOf o hasMother implies hasMotherInLaw . The path involved in mother-in-laws can be seen in Figure 9.2. The following OWL code establishes the sub-property chains for hasMotherInLaw : ObjectProperty: hasMotherInLaw SubPropertyOf: hasParentInLaw SubPropertyChain: isSpouseOf o hasMother Domain: Person Range: Woman InverseOf: isMotherInLawOf Figure 9.2: Tracing out the path between objects to make the sub-property chain for mother-in-laws Do the following to make the parent in-law properties: Task 39: Parents in-law Create hasParentInLaw with two sub-properties of hasMotherInLaw and hasFatherInLaw ; Create the inverses, but remember to let the reasoner infer the hierarchy on that side of the hierarchy; Add the sub-property chains as described in the pattern for hasMotherInLaw above; Run the reasoner and check that the mother-in-law of Margaret Grace Rever is Iris Ellen Archer.","title":"9.2 In-Laws"},{"location":"tutorial/fhkb/#93-brothers-and-sisters-in-law","text":"Brothers and sisters in law have the interesting addition of having more than one path between objects to establish a sister or brother in law relationship. The OWL code below establishes the relationships for \u2018is sister in law of\u2019: ObjectProperty: hasSisterInLaw SubPropertyOf: hasSiblingInLaw SubPropertyChain: hasSpouse o hasSister SubPropertyChain: hasSibling o isWifeOf A wife\u2019s husband\u2019s sister is a sister in law of the wife. Figure 9.3 shows the two routes to being a sister-in-law. In addition, the wife is a sister in law of the husband\u2019s siblings. One can add as many sub-property chains to a property as one needs. You should add the properties for hasSiblingInLawOf and its obvious sub-properties following the inverse of the pattern above. Task 40: Siblings in-law Create the relationships for siblings-in-law as indicated in the owl code above. By now, chances are high that the realisation takes a long time. We recommend to remove the very computationally expensive restriction `hasParent` exactly 2 Person on the `Person` class, if you have not done it so far. Figure 9.3: The two routes to being a sister-in-law.","title":"9.3 Brothers and Sisters In-Law"},{"location":"tutorial/fhkb/#94-aunts-and-uncles-in-law","text":"The uncle of Robert David Bright has a wife, but she is not the aunt of Robert David Bright, she is the aunt-in-law. This is another kith relationship, not a kin relationship. The pattern has a familiar feel: ObjectProperty: isAuntInLawOf SubPropertyOf: isInLawOf SubPropertyChain: isWifeOf o isBrotherOf o isParentOf Task 41: Uncles and aunts in-law Create hasAuntInLaw and hasUncleInLaw in the usual way; Test in the usual way; Tidy up the top of the property hierarchy so that it looks like Figure 9.4. We have a top property of hasRelation and two sub-properties of isBloodRelationOf and isInLawOf to establish the kith and kin relationships respectively; All the properties created in this chapter (except for spouses) should be underneath isInLawOf . Figure 9.4: The object property hierarchy after adding the various in-law properties.","title":"9.4 Aunts and Uncles in-Law"},{"location":"tutorial/fhkb/#95-summary","text":"This has really been a revision chapter; nothing new has really been introduced. We have added a lot of new object properties and one new data property. The latest object property hierarchy with the \u2018in-law\u2019 branch can be seen in Figure 9.4. Highlights have been: Having an explicit marriage object so that we can say things about the marriage itself, not just the people in the marriage; We have seen that more than one property chain can be added to a property; We have added a lot of kith relationships to join the kin or blood relationships; As usual, the reasoner can establish the hierarchy for the inverses and put a lot of the domain and ranges in for free. The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 123.655 sec (1.00000 % of final) and by FaCT++ 1.6.4 is approximately 1.618 sec (0.046 % of final). 0 sec indicates failure or timeout.","title":"9.5 Summary"},{"location":"tutorial/fhkb/#chapter-10","text":"","title":"Chapter 10"},{"location":"tutorial/fhkb/#extending-the-tbox","text":"In this chapter you will: Just add lots of defined classes for all the aspects we have covered in this FHKB tutorial; You will learn that the properties used in these defined classes must be chosen with care. There is a snapshot of the ontology as required at this point in the tutorial available at http://owl.cs.manchester.ac.uk/tutorials/fhkbtutorial","title":"Extending the TBox"},{"location":"tutorial/fhkb/#101-adding-defined-classes","text":"Add the following defined classes: Task 42: Adding defined classes Relation and blood relation; Forefather and Foremother; Grandparent, Grandfather and Grandmother; GreatGrandparent, GreatGrandfather and GreatGrandmother; GreatGrandparentOfRobert, GreatGrandfatherOfRobert and GreatGrandMotherOfRobert Daughter, Son, Brother, Sister, Child; Aunt, Uncle, AuntInLaw, UncleInLaw, GreatAunt and GreatUncle; FirstCousin and SecondCousin; First cousin once removed; InLaw, MotherInLaw, FatherInLaw, ParentInLaw, SiblingInLaw, SisterInLaw, BrotherInLaw; Any defined class for any property in the hierarchy and any nominal variant of these classes. The three classes of Child , Son and Daughter are of note. They are coded in the following way: Class: Child EquivalentTo: Person that hasParent Some Person Class: Son EquivalentTo: Man that hasParent Some Person Class: Daughter EquivalentTo: Woman that hasParent Some Person After running the reasoner, you will find that Person is found to be equivalent to Child ; Daughter is equivalent to Woman and that Son is equivalent to Man . This does, of course, make sense \u2013 each and every person is someone\u2019s child, each and every woman is someone\u2019s daughter. We will forget evolutionary time-scales where this might be thought to break down at some point \u2013 all Person individuals are also Descendant individuals, but do we expect some molecule in some prebiotic soup to be a member of this class? Nevertheless, within the scope of the FHKB, such inferred equivalences are not unreasonable. They are also instructive; it is possible to have different intentional descriptions of a class and for them to have the same logical extents. You can see another example of this happening in the amino acids ontology, but for different reasons. Taking Grandparent as an example class, there are two ways of writing the defined class: Class: Grandparent EquivalentTo: Person and isGrandparentOf some Person Class: Grandparent EquivalentTo: Person and (isParentOf some (Person and (is- ParentOf some Person)) Each comes out at a different place in the class hierarchy. They both capture the right individuals as members (that is, those individuals in the ABox that are holding a isGrandparentOf property), but the class hierarchy is not correct. By definition, all grandparents are also parents, but the way the object property hierarchy works means that the first way of writing the defined class (with the isGrandparentOf property) is not subsumed by the class Parent . We want this to happen in any sensible class hierarchy, so we have to use the second pattern for all the classes, spelling out the sub-property path that implies the property such as isGrandparentOf within the equivalence axiom. The reason for this need for the \u2018long-form\u2019 is that the isGrandparentOf does not imply the isParentOf property. As described in Chapter 3 if this implication were the case, being a grandparent of Robert David Bright, for instance, would also imply that the same Person were a parent of Robert David Bright; an implication we do not want. As these two properties ( isParentOf and isGrandparentOf ) do not subsume each other means that the defined classes written according to pattern one above will not subsume each other in the class hierarchy. Thus we use the second pattern. If we look at the class for grandparents of Robert: Class: GrandparentOfRobert EquivalentTo: Person that isParentOf some (Person that isParentOf value Robert David Bright) If we make the equivalent class for Richard John Bright, apply the reasoner and look at the hierarchy, we see that the two classes are not logically equivalent, even though they have the same extents of William George Bright, Iris Ellen Archer, Charles Herbert Rever and Violet Sylvia Steward. We looked at this example in Section 6.2, where there is an explanation and solutions.","title":"10.1 Adding Defined Classes"},{"location":"tutorial/fhkb/#102-summary","text":"We can add defined classes based on each property we have put into the object property hierarchy. We see the expected hierarchy; as can be seen from Figure 10.1 it has an obvious symmetry based on sex. We also see a lot of equivalences inferred \u2013 all women are daughters, as well as women descendants. Perhaps not the greatest insight ever gained, but it at least makes sense; all women must be daughters. It is instructive to use the explanation feature in Prot\u00e9g\u00e9 to look at why the reasoner has made these inferences. For example, take a look at the class hasGrandmother some Woman \u2013 it is instructive to see how many there are. Like the Chapter on marriage and in-law (Chapter 9), this chapter has largely been revision. One thing of note is, however, that we must not use the object properties that are inferred through sub-property chains as definitions in the TBox; we must spell out the sub-property chain in the definition, otherwise the implications do not work properly. One thing is almost certain; the resulting TBox is rather complex and would be almost impossible to maintain by hand. Figure 10.1: The full TBox hierarchy of the FHKB The FHKB ontology at this stage of the tutorial has an expressivity of SROIQ(D). The time to reason with the FHKB at this point (in Prot\u00e9g\u00e9) on a typical desktop machine by HermiT 1.3.8 is approximately 0.000 sec (0.00000 % of final), by Pellet 2.2.0 0.000 sec (0.00000 % of final) and by FaCT++ 1.6.4 is approximately 35.438 sec (1.000 % of final). 0 sec indicates failure or timeout.","title":"10.2 Summary"},{"location":"tutorial/fhkb/#chapter-11","text":"","title":"Chapter 11"},{"location":"tutorial/fhkb/#final-remarks","text":"If you have done all the tasks within this tutorial, then you will have touched most parts of OWL 2. Unusually for most uses of OWL we have concentrated on individuals, rather than just on the TBox. One note of warning \u2013 the full FHKB has some 450 members of the Bright family and takes a reasonably long time to classify, even on a sensible machine. The FHKB is not scalable in its current form. One reason for this is that we have deliberately maximised inference. We have attempted not to explicitly type the individuals, but drive that through domain and range constraints. We are making the property hierarchy do lots of work. For the individual Robert David Bright, we only have a couple of assertions, but we infer some 1 500 facts between Robert David Bright and other named individuals in the FHKB\u2013displaying this in Prot\u00e9g\u00e9 causes problems. We have various complex classes in the TBox and so on. We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do. As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing.However, it serves its role to teach about OWL 2. OWL 2 on its own and using it in this style, really does not work for family history. We have seen that siblings and cousins cause problems. rules in various forms can do this kind of thing easily\u2014it is one of the primary examples for learning about Prolog. Nevertheless, the FHKB does show how much inference between named individuals can be driven from a few fact assertions and a property hierarchy. Assuming a powerful enough reasoner and the ability to deal with many individuals, it would be possible to make a family history application using the FHKB; as long as one hid the long and sometimes complex queries and manipulations that would be necessary to \u2018prune\u2019 some of the \u2018extra\u2019 facts found about individuals. However, the FHKB does usefully show the power of OWL 2, touch a great deal of the language and demonstrate some of its limitations.","title":"Final remarks"},{"location":"tutorial/fhkb/#appendix-a","text":"","title":"Appendix A"},{"location":"tutorial/fhkb/#fhkb-family-data","text":"Table A.1: The list of individuals in the FHKB Person First given name Second given name Family name Birth year Mother Father Alec John Archer 1927 Alec John Archer 1927 Violet Heath 1887 James Alexander Archer 1882 Charles Herbert Rever 1895 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 William Rever 1870 Charlotte Caroline Jane Bright 1894 Charlotte Caroline Jane Bright 1894 Charlotte Hewett 1863 Henry Edmund Bright 1862 Charlotte Hewett 1863 Charlotte none Hewett 1863 not specified not specified Clare Bright 1966 Clare none Bright 1966 Diana Pool Peter William Bright 1941 Diana Pool Diana none Pool none not specified not specified David Bright 1934 David none Bright 1934 Iris Ellen Archer 1906 William George Bright 1901 Dereck Heath Dereck none Heath 1927 not specified not specified Eileen Mary Rever 1929 Eileen Mary Rever 1929 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Elizabeth Frances Jessop 1869 Elizabeth Frances Jessop 1869 not specified not specified Ethel Archer 1912 Ethel none Archer 1912 Violet Heath 1887 James Alexander Archer 1882 Frederick Herbert Bright 1889 Frederick Herbert Bright 1889 Charlotte Hewett 1863 Henry Edmund Bright 1862 Henry Edmund Bright 1862 Henry Edmund Bright 1862 not specified not specified Henry Edmund Bright 1887 Henry Edmund Bright 1887 Charlotte Hewett 1863 Henry Edmund Bright 1862 Ian Bright 1959 Ian none Bright 1959 Joyce Gosport John Bright 1930 Iris Ellen Archer 1906 Iris Ellen Archer 1906 Violet Heath 1887 James Alexander Archer 1882 James Alexander Archer 1882 James Alexander Archer 1882 not specified not specified James Bright 1964 James none Bright 1964 Diana Pool Peter William Bright 1941 James Frank Hayden Bright 1891 James Frank Bright 1891 Charlotte Hewett 1863 Henry Edmund Bright 1862 Janet Bright 1964 Janet none Bright 1964 Joyce Gosport John Bright 1930 John Bright 1930 John none Bright 1930 Iris Ellen Archer 1906 William George Bright 1901 John Tacey Steward 1873 John Tacey Steward 1873 not specified not specified Joyce Archer 1921 Joyce none Archer 1921 Violet Heath 1887 James Alexander Archer 1882 Joyce Gosport Joyce none Gosport not specified not specified not specified Julie Bright 1966 Julie none Bright 1966 Diana Pool Peter William Bright 1941 Kathleen Minnie Bright 1904 Kathleen Minnie Bright 1904 Charlotte Hewett 1863 Henry Edmund Bright 1862 Leonard John Bright 1890 Leonard John Bright 1890 Charlotte Hewett 1863 Henry Edmund Bright 1862 Lois Green 1871 Lois none Green 1871 not specified not specified Margaret Grace Rever 1934 Margaret Grace Rever 1934 Violet Sylvia Steward 1894 Charles Herbert Rever 1895 Mark Anthony Heath 1960 Mark Anthony Heath 1960 Eileen Mary Rever 1929 Dereck Heath Mark Bright 1956 Mark none Bright 1956 Joyce Gosport John Bright 1930 Nicholas Charles Heath 1964 Nicholas Charles Heath 1964 Eileen Mary Rever 1929 Dereck Heath Nora Ada Bright 1899 Nora Ada Bright 1899 Charlotte Hewett 1863 Henry Edmund Bright 1862 Norman James Archer 1909 Norman James Archer 1909 Violet Heath 1887 James Alexander Archer 1882 Peter William Bright 1941 Peter William Bright 1941 Iris Ellen Archer 1906 William George Bright 1901 Richard John Bright 1962 Richard John Bright 1962 Margaret Grace Rever 1934 David Bright 1934 Robert David Bright 1965 Robert David Bright 1965 Margaret Grace Rever 1934 David Bright 1934 Violet Heath 1887 Violet none Heath 1887 not specified not specified Violet Sylvia Steward 1894 Violet Sylvia Steward 1894 Lois Green 1871 John Tacey Steward 1873 William Bright 1970 William none Bright 1970 Joyce Gosport John Bright 1930 William George Bright 1901 William George Bright 1901 Charlotte Hewett 1863 Henry Edmund Bright 1862 William Rever 1870 William none Rever 1870 not specified not specified","title":"FHKB Family Data"},{"location":"tutorial/fhkb/#bibliography","text":"[1] M. Horridge and S. Bechhofer. The owl api: a java api for working with owl 2 ontologies. Proc. of OWL Experiences and Directions , 2009, 2009. [2] Luigi Iannone, Alan Rector, and Robert Stevens. Embedding knowledge patterns into owl. In European Semantic Web Conference (ESWC09) , pages 218\u2013232, 2009. [3] Dmitry Tsarkov, Uli Sattler, Margaret Stevens, and Robert Stevens. A Solution for the Man-Man Problem in the Family History Knowledge Base. In Sixth International Workshop on OWL: Experiences and Directions 2009 , 2009.","title":"Bibliography"},{"location":"tutorial/github-fundamentals/","text":"Introduction to GitHub \u00b6 Back to Getting Started \u00b6 Back to Main Repo \u00b6 Overview: \u00b6 Getting started Organization Markdown Content types Getting started \u00b6 GitHub is increasingly used by software developers, programmers and project managers for uploading and sharing content, as well as basic project management. You build a profile, upload projects to share and connect with other users by \"following\" their accounts. Many users store programs and code projects, but you can also upload text documents or other file types in your project folders to share publicly (or privately). It is capable of storing any file type from text, to structured data, to software. And more features are being added by the day. The real power of Git, however, is less about individuals publishing content (many places can do that, including google docs etc). It is more about that content being easily shared, built upon, and credited in a way that is robust to the realities of distributed collaboration. You don't have to know how to code or use the command line. It is a powerful way to organize projects with multiple participants. Organization \u00b6 Git supports the following types of primary entities: Individual: A person who contributes to GitHub (that's you!) Example individual http://github.com/nicolevasilevsky Organization: An entity that may correspond to an actual organization (such as a university) or to a meaningful grouping of repositories. Organizations are like individuals except that they can establish teams. Example organization: https://github.com/monarch-initiative Repository: A collection of versioned files (of any type) Example repository https://github.com/monarch-initiative/mondo/ Teams : A group of individuals assembled by the administrators of an organization. An individual may participate in many teams and organizations, however a team is always bound to a single organization. Nesting teams saves time; instructions here . The relationships between any combination of these entities is many-to-many, with the nuanced exception of repositories. For our purposes today we will oversimplify by saying that a repositoy belongs either to a single organization or to a single individual . Markdown \u00b6 Content in GitHub is written using Markdown, a text-to-HTML conversion tool for web writers (ref) . For more help with Markdown, see this GitHub guide . Raw markup syntax As rendered Header - use # for H1, ## for H2, etc. # Header, ## Header (note, the header is not displaying properly in this table) Emphasis, aka italics, with *asterisks* or _underscores_. Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with **asterisks** or __underscores__. Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with **asterisks and _underscores_**. Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Strikethrough uses two tildes. ~~Scratch this.~~ Lists: To introduce line breaks in markdown, add two spaces For a bulleted list, use * or - (followed by a space) Here is an example of a list: One Two Three Here is an example of a bulleted list: One Two Three Content types \u00b6 GitHub can store any kind of content, provided it isn't too big. (And now even this is possible ). However, it is more capable for some filetypes than it is for others. Certain filetypes can be viewed 'natively' within the GitHub interface. These are: Images: png, jpg, svg GEOJSON CSV, TSV (note that files named type '.tab' will not render properly in the UI.) Markdown Software code (eg. including json, HTML, xml etc) Task - create a new GitHub repository \u00b6 Create your GitHub account if you do not already have one Customize your avatar if you haven't already Go to settings and upload any picture (it doesn't have to be your face) Create a repository Task - update the content in your README \u00b6 Go back to the repository you just created Click the pencil icon in the right corner of your README.md file Add some content to your file that includes a header, italics, bold, strikethrough, and lists You can preview your changes before committing by clicking 'Preview changes'. Commit your changes by clicking the commit button at the bottom of the page. Task - add content to your repository \u00b6 Click on the code button Click upload file Upload a file by dragging and dropping or browse for file Trying uploading an Excel file vs a TSV or CSV file. How are these displayed differently? Additional Resources \u00b6 Frequently Asked Questions Git and GitHub for Documentation Markdown Cheatsheet Git 101: Git and GitHub for Beginners Mastering Issues (10 min read) Nomi's intro to GitHub slides Udemy course: Command Line Essentials: Git Bash for Windows Udemy course: Git: Become an Expert in Git & GitHub in 4 Hours Google: Introduction to Git and GitHub Udemy: Git Started with GitHub Acknowledgements \u00b6 Adopted from CD2H MTIP tutorial","title":"GitHub Fundamentals for OBO Engineers"},{"location":"tutorial/github-fundamentals/#introduction-to-github","text":"","title":"Introduction to GitHub"},{"location":"tutorial/github-fundamentals/#back-to-getting-started","text":"","title":"Back to Getting Started"},{"location":"tutorial/github-fundamentals/#back-to-main-repo","text":"","title":"Back to Main Repo"},{"location":"tutorial/github-fundamentals/#overview","text":"Getting started Organization Markdown Content types","title":"Overview:"},{"location":"tutorial/github-fundamentals/#getting-started","text":"GitHub is increasingly used by software developers, programmers and project managers for uploading and sharing content, as well as basic project management. You build a profile, upload projects to share and connect with other users by \"following\" their accounts. Many users store programs and code projects, but you can also upload text documents or other file types in your project folders to share publicly (or privately). It is capable of storing any file type from text, to structured data, to software. And more features are being added by the day. The real power of Git, however, is less about individuals publishing content (many places can do that, including google docs etc). It is more about that content being easily shared, built upon, and credited in a way that is robust to the realities of distributed collaboration. You don't have to know how to code or use the command line. It is a powerful way to organize projects with multiple participants.","title":"Getting started"},{"location":"tutorial/github-fundamentals/#organization","text":"Git supports the following types of primary entities: Individual: A person who contributes to GitHub (that's you!) Example individual http://github.com/nicolevasilevsky Organization: An entity that may correspond to an actual organization (such as a university) or to a meaningful grouping of repositories. Organizations are like individuals except that they can establish teams. Example organization: https://github.com/monarch-initiative Repository: A collection of versioned files (of any type) Example repository https://github.com/monarch-initiative/mondo/ Teams : A group of individuals assembled by the administrators of an organization. An individual may participate in many teams and organizations, however a team is always bound to a single organization. Nesting teams saves time; instructions here . The relationships between any combination of these entities is many-to-many, with the nuanced exception of repositories. For our purposes today we will oversimplify by saying that a repositoy belongs either to a single organization or to a single individual .","title":"Organization"},{"location":"tutorial/github-fundamentals/#markdown","text":"Content in GitHub is written using Markdown, a text-to-HTML conversion tool for web writers (ref) . For more help with Markdown, see this GitHub guide . Raw markup syntax As rendered Header - use # for H1, ## for H2, etc. # Header, ## Header (note, the header is not displaying properly in this table) Emphasis, aka italics, with *asterisks* or _underscores_. Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with **asterisks** or __underscores__. Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with **asterisks and _underscores_**. Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Strikethrough uses two tildes. ~~Scratch this.~~ Lists: To introduce line breaks in markdown, add two spaces For a bulleted list, use * or - (followed by a space) Here is an example of a list: One Two Three Here is an example of a bulleted list: One Two Three","title":"Markdown"},{"location":"tutorial/github-fundamentals/#content-types","text":"GitHub can store any kind of content, provided it isn't too big. (And now even this is possible ). However, it is more capable for some filetypes than it is for others. Certain filetypes can be viewed 'natively' within the GitHub interface. These are: Images: png, jpg, svg GEOJSON CSV, TSV (note that files named type '.tab' will not render properly in the UI.) Markdown Software code (eg. including json, HTML, xml etc)","title":"Content types"},{"location":"tutorial/github-fundamentals/#task-create-a-new-github-repository","text":"Create your GitHub account if you do not already have one Customize your avatar if you haven't already Go to settings and upload any picture (it doesn't have to be your face) Create a repository","title":"Task - create a new GitHub repository"},{"location":"tutorial/github-fundamentals/#task-update-the-content-in-your-readme","text":"Go back to the repository you just created Click the pencil icon in the right corner of your README.md file Add some content to your file that includes a header, italics, bold, strikethrough, and lists You can preview your changes before committing by clicking 'Preview changes'. Commit your changes by clicking the commit button at the bottom of the page.","title":"Task - update the content in your README"},{"location":"tutorial/github-fundamentals/#task-add-content-to-your-repository","text":"Click on the code button Click upload file Upload a file by dragging and dropping or browse for file Trying uploading an Excel file vs a TSV or CSV file. How are these displayed differently?","title":"Task - add content to your repository"},{"location":"tutorial/github-fundamentals/#additional-resources","text":"Frequently Asked Questions Git and GitHub for Documentation Markdown Cheatsheet Git 101: Git and GitHub for Beginners Mastering Issues (10 min read) Nomi's intro to GitHub slides Udemy course: Command Line Essentials: Git Bash for Windows Udemy course: Git: Become an Expert in Git & GitHub in 4 Hours Google: Introduction to Git and GitHub Udemy: Git Started with GitHub","title":"Additional Resources"},{"location":"tutorial/github-fundamentals/#acknowledgements","text":"Adopted from CD2H MTIP tutorial","title":"Acknowledgements"},{"location":"tutorial/github-issues/","text":"Intro to managing and tracking issues in GitHub \u00b6 Overview \u00b6 How to create issues How to assign issues How to communicate about issues How to organize issues How to query issues How to close issues How to assign teams Where to go when you need help Miscellany that is good to know Create issues \u00b6 Back to top Why: \"Issues are a great way to keep track of tasks , enhancements , and bugs for your projects or for anyone else's. As long as you are a registered GitHub user you can log an issue, or comment on an issue for any open repo on GitHub. Issues are a bit like email\u2014except they can be shared, intelligently organized, and discussed with the rest of your team. GitHub\u2019s tracker is called Issues, and has its own section in every repository.\" (From: https://guides.github.com/features/issues/) How: How to create an issue in GitHub: We will practice creating tickets in this repository https://github.com/nicolevasilevsky/c-path-practice Click \"issues\" Click \"New Issue\" (note the word 'issue' and 'ticket' are frequently used interchangeably) Write an informative title Write a detailed explanation of your issue In the case of reporting software bugs, provide some context in which the issue was encountered (e.g. bug detected when using Google Chrome on a Mac OS) If you know the sub-tasks that are involved, list these using - [ ] markdown syntax before each bullet. Note, you can also add sub-tasks by clicking the 'add a task list' button in the tool bar. The status of the tasks in an issue (eg. https://github.com/nicolevasilevsky/c-path-practice/issues/1 will then be reflected in any summary view. Eg. https://github.com/nicolevasilevsky/c-path-practice/issues . Click Submit new issue Edit the issue (if needed) (Note that post-hoc edits will not propagate to email notifications). Your turn: Follow the instructions above to create a ticket about a hypothetical issue (such as an improvement to this tutorial) that includes a sub-task list. Assign issues \u00b6 Back to top Assign issues to people On the top right hand side, click \"Assignees\" You can assign issues to yourself or other people who are part of the repository In the box, start typing type their name or GitHub handle It is possible to assign up to 10 handles at once (assignment to a team is currently not supported) Add labels On the top right hand side, click \"Labels\" Assign a relevant label to your ticket Note, by default, every GitHub repo comes with some standard labels You can also create new labels that are specific to your project. For example see the labels on the Mondo GitHub tracker New Labels In GitHub, navigate to the Issues page or the pull requests tab Click Labels button next to the search field Click New Label to create a new label, or click Edit to edit an existing one. In the text box, type your new label name. Select a color for the label from the color bar. You can customize this color by editing the hexadecimal number above the color bar. For a list of hexadecimal numbers see HTML color codes Click Create Label to save the new label. Your turn: On the ticket you previously created: Assign the ticket to someone Add a label for an enhancement Create a new label and add it to the ticket Communicate about issues \u00b6 Back to top Comment on issues Click on an issue in the issue tracker in the https://github.com/nicolevasilevsky/c-path-practice/issues repo Scroll to the bottom of the issue, and add content in the \"Leave a comment\" field Use the top tool bar to format your text, add bold , italic , lists etc. Preview your text to see how your formatting looks Click Comment. Close issues If an issue has been addressed, click Close Issue. Best practice is to point to the work (whether code, documentation, etc) that has been done to close it. Only close the ticket if the issue has been resolved, usually someone will write a comment describing the action they did to close the issue and click Close Issue. The issue will no longer be dispalyed in the list of open issue, but will be archived. When making a change to code or documentation in GitHub, it is possible to automatically couple a change to an issue and close it. Just use 'fixes' or 'closes' followed by the issue number. Use direct @ mentions You can mention someone in a issue without assigning it to them In the comments section, type @github handle. For example, to mention Nicole, you would type @nicolevasilevsky. You can either start typing their name or GitHub handle and GitHub will autosuggest their handle. Link documents You can link documents and files by: copy and pasting URL you can attach files by dragging and dropping You can link one issue to another in the same repo by typing '#' followed by the title of the ticket This approach also works across repos but you need to use the full URL (no autocomplete available). Doing this will also cause the referent issue to display that it has been referenced. Cross reference to another ticket If your ticket is a duplicate or related to another ticket, you can cross reference another ticket Type # and you will see a list of other tickets in that repo Type #TicketNumber and that will link to the other ticket. Before saving your changes, you can preview the comment to ensure the correct formatting. Your turn: Follow the instructions above to comment on a ticket that someone created. Mention Nicole Attach a picture (such as a picture you copy from the internet, or attach a picture you have saved on your computer) Include a comment that says, 'related to #1' and link to ticket #1 Organize issues \u00b6 Back to top Milestones To create a milestone, navigate to the issues page in your repository Next to the search field, click Milestones Click New Milestone to create a new milestone, click Edit to edit an existing one Create a milestone that is broad enough to be meaningful, but specific enough to be actionable. Set a due date for the milestone (note that specific tasks can not be formally assigned due dates, though you can mention a desired due date in the narrative text of a ticket. Each ticket can only be associated to ONE milestone, however it can have as many labels as appropriate. A given issue can be part of multiple \"project\" boards (see below) Your turn Create a new milestone, and add the milestone to an existing ticket. Projects Projects is a lot like Trello, it uses cards on a list that you can name and organize as you see fit. You can create as many projects within a repository as you like To create project: Click on Projects Click New Project Name the project Write a description of the project Create columns and give them names Add 'cards' to the columns Your turn Create a new project and add columns and add cards to the columns. Query issues \u00b6 Back to top Once you start using GitHub for lots of things it is easy to get overwhelmed by the number of issues. The query dashboard https://github.com/issues allows you to filter on tickets. All issues assigned to me: https://github.com/issues/assigned All issues on which I am @ mentioned: https://github.com/issues/mentioned More complex queries are also possible. All issues either assigned to me OR on which I have commented OR am mentioned: https://github.com/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+involves%3Anicolevasilevsky+ Note, you must be signed in to GitHub to view the above links. Further reading on Issue querys Nofifications \u00b6 When you join a repository, by default, you are 'watching' all activity. You can change the setting to 'Participating and @mentions' to only get notifications that mention you You can set rules in your email to filter for emails that mention you, ie @username. Help \u00b6 Back to top You may find the answers you seek in StackOverflow , although it is primarily geared towards programmers. GitHub kind of monitors https://github.com/isaacs/github/issues/ but not with any rigor. To be safe, contact GitHub directly at https://github.com/contact , but recognize that they support literally millions of users and responsiveness is not guaranteed. Forums like isaacs sometimes offer some help because other users can help identify workarounds, for instance, as shown here . Acknowledgements \u00b6 Adopted from CD2H MTIP tutorial","title":"GitHub Issue for OBO Engineers"},{"location":"tutorial/github-issues/#intro-to-managing-and-tracking-issues-in-github","text":"","title":"Intro to managing and tracking issues in GitHub"},{"location":"tutorial/github-issues/#overview","text":"How to create issues How to assign issues How to communicate about issues How to organize issues How to query issues How to close issues How to assign teams Where to go when you need help Miscellany that is good to know","title":"Overview"},{"location":"tutorial/github-issues/#create-issues","text":"Back to top Why: \"Issues are a great way to keep track of tasks , enhancements , and bugs for your projects or for anyone else's. As long as you are a registered GitHub user you can log an issue, or comment on an issue for any open repo on GitHub. Issues are a bit like email\u2014except they can be shared, intelligently organized, and discussed with the rest of your team. GitHub\u2019s tracker is called Issues, and has its own section in every repository.\" (From: https://guides.github.com/features/issues/) How: How to create an issue in GitHub: We will practice creating tickets in this repository https://github.com/nicolevasilevsky/c-path-practice Click \"issues\" Click \"New Issue\" (note the word 'issue' and 'ticket' are frequently used interchangeably) Write an informative title Write a detailed explanation of your issue In the case of reporting software bugs, provide some context in which the issue was encountered (e.g. bug detected when using Google Chrome on a Mac OS) If you know the sub-tasks that are involved, list these using - [ ] markdown syntax before each bullet. Note, you can also add sub-tasks by clicking the 'add a task list' button in the tool bar. The status of the tasks in an issue (eg. https://github.com/nicolevasilevsky/c-path-practice/issues/1 will then be reflected in any summary view. Eg. https://github.com/nicolevasilevsky/c-path-practice/issues . Click Submit new issue Edit the issue (if needed) (Note that post-hoc edits will not propagate to email notifications). Your turn: Follow the instructions above to create a ticket about a hypothetical issue (such as an improvement to this tutorial) that includes a sub-task list.","title":"Create issues"},{"location":"tutorial/github-issues/#assign-issues","text":"Back to top Assign issues to people On the top right hand side, click \"Assignees\" You can assign issues to yourself or other people who are part of the repository In the box, start typing type their name or GitHub handle It is possible to assign up to 10 handles at once (assignment to a team is currently not supported) Add labels On the top right hand side, click \"Labels\" Assign a relevant label to your ticket Note, by default, every GitHub repo comes with some standard labels You can also create new labels that are specific to your project. For example see the labels on the Mondo GitHub tracker New Labels In GitHub, navigate to the Issues page or the pull requests tab Click Labels button next to the search field Click New Label to create a new label, or click Edit to edit an existing one. In the text box, type your new label name. Select a color for the label from the color bar. You can customize this color by editing the hexadecimal number above the color bar. For a list of hexadecimal numbers see HTML color codes Click Create Label to save the new label. Your turn: On the ticket you previously created: Assign the ticket to someone Add a label for an enhancement Create a new label and add it to the ticket","title":"Assign issues"},{"location":"tutorial/github-issues/#communicate-about-issues","text":"Back to top Comment on issues Click on an issue in the issue tracker in the https://github.com/nicolevasilevsky/c-path-practice/issues repo Scroll to the bottom of the issue, and add content in the \"Leave a comment\" field Use the top tool bar to format your text, add bold , italic , lists etc. Preview your text to see how your formatting looks Click Comment. Close issues If an issue has been addressed, click Close Issue. Best practice is to point to the work (whether code, documentation, etc) that has been done to close it. Only close the ticket if the issue has been resolved, usually someone will write a comment describing the action they did to close the issue and click Close Issue. The issue will no longer be dispalyed in the list of open issue, but will be archived. When making a change to code or documentation in GitHub, it is possible to automatically couple a change to an issue and close it. Just use 'fixes' or 'closes' followed by the issue number. Use direct @ mentions You can mention someone in a issue without assigning it to them In the comments section, type @github handle. For example, to mention Nicole, you would type @nicolevasilevsky. You can either start typing their name or GitHub handle and GitHub will autosuggest their handle. Link documents You can link documents and files by: copy and pasting URL you can attach files by dragging and dropping You can link one issue to another in the same repo by typing '#' followed by the title of the ticket This approach also works across repos but you need to use the full URL (no autocomplete available). Doing this will also cause the referent issue to display that it has been referenced. Cross reference to another ticket If your ticket is a duplicate or related to another ticket, you can cross reference another ticket Type # and you will see a list of other tickets in that repo Type #TicketNumber and that will link to the other ticket. Before saving your changes, you can preview the comment to ensure the correct formatting. Your turn: Follow the instructions above to comment on a ticket that someone created. Mention Nicole Attach a picture (such as a picture you copy from the internet, or attach a picture you have saved on your computer) Include a comment that says, 'related to #1' and link to ticket #1","title":"Communicate about issues"},{"location":"tutorial/github-issues/#organize-issues","text":"Back to top Milestones To create a milestone, navigate to the issues page in your repository Next to the search field, click Milestones Click New Milestone to create a new milestone, click Edit to edit an existing one Create a milestone that is broad enough to be meaningful, but specific enough to be actionable. Set a due date for the milestone (note that specific tasks can not be formally assigned due dates, though you can mention a desired due date in the narrative text of a ticket. Each ticket can only be associated to ONE milestone, however it can have as many labels as appropriate. A given issue can be part of multiple \"project\" boards (see below) Your turn Create a new milestone, and add the milestone to an existing ticket. Projects Projects is a lot like Trello, it uses cards on a list that you can name and organize as you see fit. You can create as many projects within a repository as you like To create project: Click on Projects Click New Project Name the project Write a description of the project Create columns and give them names Add 'cards' to the columns Your turn Create a new project and add columns and add cards to the columns.","title":"Organize issues"},{"location":"tutorial/github-issues/#query-issues","text":"Back to top Once you start using GitHub for lots of things it is easy to get overwhelmed by the number of issues. The query dashboard https://github.com/issues allows you to filter on tickets. All issues assigned to me: https://github.com/issues/assigned All issues on which I am @ mentioned: https://github.com/issues/mentioned More complex queries are also possible. All issues either assigned to me OR on which I have commented OR am mentioned: https://github.com/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+involves%3Anicolevasilevsky+ Note, you must be signed in to GitHub to view the above links. Further reading on Issue querys","title":"Query issues"},{"location":"tutorial/github-issues/#nofifications","text":"When you join a repository, by default, you are 'watching' all activity. You can change the setting to 'Participating and @mentions' to only get notifications that mention you You can set rules in your email to filter for emails that mention you, ie @username.","title":"Nofifications"},{"location":"tutorial/github-issues/#help","text":"Back to top You may find the answers you seek in StackOverflow , although it is primarily geared towards programmers. GitHub kind of monitors https://github.com/isaacs/github/issues/ but not with any rigor. To be safe, contact GitHub directly at https://github.com/contact , but recognize that they support literally millions of users and responsiveness is not guaranteed. Forums like isaacs sometimes offer some help because other users can help identify workarounds, for instance, as shown here .","title":"Help"},{"location":"tutorial/github-issues/#acknowledgements","text":"Adopted from CD2H MTIP tutorial","title":"Acknowledgements"},{"location":"tutorial/intro-cli-1/","text":"Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 1 \u00b6 As a modern ontology curator, you are an engineer - you are curating computable knowledge, testing the integrity of your curation using quality control testing, and are responsible for critical components of modern knowledge systems that directly affect user experience - the ontologies. Scientific computing is a big, scary world comprising many different tools, methodologies, training resources and philosophies, but nearly all modern workflows share one key aspect: the ability to execute commands that help you find and manipulate data with the command line. Some examples of that include: Running and Ontology Development Kit (ODK) commands like sh run.sh make prepare_release Creating a new branch in git and committing changes Downloading a file with curl or wget Finding which file in my ontology repository mentions HP:0000118 to figure out where that \"weird axiom is coming from\" Searching for strange symbols in the ontology Filtering and sorting text files Here we are doing a basic hands on tutorial which will walk you through the must-know commands. For a more comprehensives introduction into thinking about automation please see our lesson on Automating Ontology Development Workflows: Make, Shell and Automation Thinking The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an sh file in the following there exists a corresponding bat file that can be run in the windows powershell, or CMD. Prerequisites \u00b6 You have: a Mac or Linux Operating system alternatively, you have the Ontology Development Kit installed and learned how to \"get inside a running container\" . Tutorial \u00b6 Baby steps: whoami, ls and cd Working with files and directories Downloading and searching files The Dark Art of Piping and Redirects Tutorial videos Further reading Baby steps: whoami, pwd and ls \u00b6 We are not going to discuss here in any detail what the command line is. We will focus on what you can do with it: for more information skip to the further reading section . The basic idea behind the command line is that you run a command to achieve a goal. Among the most common goals relevant to you as a semantic engineer will be: Navigating the file system (changing directories, logging into remote servers and more) Reading and writing files Searching stuff Most commands result in some kind of printed statement. Lets try one. Open your terminal (a terminal is the program you use to enter commands. For a nice overview of how shell, terminal, command line and console relate, see here ). On Mac, you can type CMD+Space to search for programs and then type \"terminal\". For this tutorial we use the default Terminal.app, but there are many others, including iterm2 . For this introduction, it does not matter which terminal you use. When first opening the terminal you will see something like this: or Note that your terminal window may look slightly different, depending on your configuration. More on that later. Let's type our first command and hit enter: whoami On my machine I get (base) matentzn@mbp.local:~ $ whoami matentzn This does not seem like a useful command, but sometimes, we forget who we are, and it is good to be reminded. So, what happened here? We ran a command, named whoami and our command line executed that command which is implemented somewhere on our machine as a program. That program simply determined who I am in some way, and then printed the result again. Ok so, lets lets look a bit closer at the command prompt itself: matentzn@mbp.local:~ $ Two interesting things to not here for today: The ~ . This universally (on all Unix systems) refers to your user directory on your computer. In this case here, it tells you that in your terminal, you are \"in your user directory\". The $ sign. It simply denotes where your command line starts (everything before the $ is information provided to you, everything will be about your commands). Make sure that you do not accidentally copy based the $ sign from the examples on the web into your command prompt: (base) matentzn@mbp.local:~ $ $ whoami -bash: $: command not found (base) matentzn@mbp.local:~ $ whoami did not do anything. Ok, based on the ~ we know that we are \"in the user home directory\". Let as become a bit more confident about that and ask the command prompt where we are: matentzn@mbp.local:~ $ pwd /Users/matentzn The pwd command prints out the full path of our current location in the terminal. As you can see, the default location when opening the command prompt is, indeed, the home director, located in /Users/matentzn . We will use it later again. A word about paths. /Users/matentzn is what we call a path. On UNIX systems, / separates one directory from another. So matentzn is a directory inside of the Users directory. Let us now take a look what our current directory contains (type ls and hit enter): matentzn@mbp.local:~ $ ls Applications Library ... This command will simply list all of the files in your directory as a big list. We can do this a bit nicer: matentzn@mbp.local:~ $ ls -l total 80000 drwx------@ 4 matentzn staff 128 31 Jul 2020 Applications drwx------@ 26 matentzn staff 832 12 Sep 2021 Desktop -l is a short command line option which allows you specify that you would like print the results in a different format (a long list). We will not go into any detail here what this means but a few things to not in the output: You can see some pieces of information that are interesting, like when the file or directory was last modified (i.e. 31. July 2020), who modified it (me) and, of course, the name e.g. Applications . Before we move on to the next section, let us clear the current terminal from all the command outputs we ran: clear Your command prompt should now be empty again. Working with files and directories \u00b6 In the previous section we learned how to figure out who we are ( whoami ), where we are ( pwd ) and how to see what is inside the current directory ( ls -l ) and how to clear all the output ( clear ). Let us know look at how we can programmatically create a new directory and change the location in our terminal. First let us create a new directory: mkdir tutorial-my Now if we list the contents of our current directory again ( ls -l ), we will see our newly created directory listed! Unfortunately, we just realised that we chose the wrong name for our directory! It should have been my-tutorial instead of tutorial-my ! So, let us rename it. In the command prompt, rather than \"renaming\" files and directories, we \"move\" them ( mv ). mv tutorial-my my-tutorial Now, lets enter our newly created directory using the _c_hange _d_irectory command ( cd ), and create another sub-directory in my-tutorial , called \"data\" ( mkdir data ): cd my-tutorial mkdir data You can check again with ls -l . If you see the data directory listed, we are all set! Feel free to run clear again to get rid of all the current output on the command prompt. Let us also enter this directory now: cd data . If we want to leave the directory again, feel free to do that like this: cd .. The two dots ( .. ) mean: \"parent directory.\" This is very important to remember during your command line adventures: .. stands for \"parent directory\", and . stands for \"current/this directory\" (see more on that below). Now, let's get into something more advanced: downloading files. Downloading and searching files \u00b6 Our first task is to download the famous Human Phenotype Ontology Gene to Phenotype Annotations (aka HPOA). As you should already now, whenever we download ontologies, or ontology related files, we should always use a persistent URL, if available! This is the one for HPOA: http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt . There are two very popular commands for downloading content: curl and wget . I think most of my colleagues prefer curl , but I like wget because it simpler for beginners. So I will use it here. Lets us try downloading the file! wget http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt -O genes_to_phenotype.txt The -O parameter is optional and specifies a filename. If you do not add the parameter, wget will try to guess the filename from the URL. This does not always go so well with complex URLs, so I personally recommend basically always specifying the -O parameter. You can also use the curl equivalent of the wget command; curl -L http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt --output genes_to_phenotype.txt Try before reading on: Exercises! Move the downloaded file genes_to_phenotype.txt to the data directory you previously created. Change into the data directory. Download the OBO format version of the Human Phenotype Ontology from its PURL. Do not move on to the next step unless your data directory looks similar to this: matentzn@mbp.local:~/my-tutorial/data $ pwd /Users/matentzn/my-tutorial/data matentzn@mbp.local:~/my-tutorial/data $ ls -l total 53968 -rw-r--r-- 1 matentzn staff 19788987 11 Jun 19:09 genes_to_phenotype.txt -rw-r--r-- 1 matentzn staff 7836327 27 Jun 22:50 hp.obo Ok, let us look at the first 10 lines of genes_to_phenotype.txt using the head command: head genes_to_phenotype.txt head is a great command to familiarise yourself with a file. You can use a parameter to print more or less lines: head -3 genes_to_phenotype.txt This will print the first 3 lines of the genes_to_phenotype.txt file. There is another analogous command that allows us to look at the last lines off a file: tail genes_to_phenotype.txt head , tail . Easy to remember. Next, we will learn the most important of all standard commands on the command line: grep . grep stands for \"Global regular expression print\" and allows us to search files, and print the search results to the command line. Let us try some simple commands first. grep diabetes genes_to_phenotype.txt You will see a list of hundreds of lines out output. Each line corresponds to a line in the genes_to_phenotype.txt file which contains the word \"diabetes\". grep is case sensitive. It wont find matches like Diabetes, with capital D! Use the `-i` parameter in the grep command to instruct grep to perform case insensitive matches. There is a lot more to grep than we can cover here today, but one super cool thing is searching across an entire directory. grep -r \"Elevated circulating follicle\" . Assuming you are in the data directory, you should see something like this: ./genes_to_phenotype.txt:190 NR0B1 HP:0008232 Elevated circulating follicle stimulating hormone level - HP:0040281 orphadata ORPHA:251510 ./genes_to_phenotype.txt:57647 DHX37 HP:0008232 Elevated circulating follicle stimulating hormone level - - mim2gene OMIM:273250 ...... # Removed other results ./hp.obo:name: Elevated circulating follicle stimulating hormone level There are two new aspects to the command here: The -r option (\"recursive\") allows is to search a directory and all directories within in . The . in the beginning. Remember, in the previous use of the grep command we had the name of a file in the place where now the . is. The . means \"this directory\" - i.e. the directory you are in right now (if lost, remember pwd ). As you can see, grep does not only list the line of the file in which the match was found, it also tells us which filename it was found in! We can make this somewhat more easy to read as well by only showing filenames using the -l parameter: matentzn@mbp.local:~/my-tutorial/data $ grep -r -l \"Elevated circulating follicle\" . ./genes_to_phenotype.txt ./hp.obo The Dark Art of Piping and Redirects \u00b6 The final lesson for today is about one of the most powerful features of the command line: the ability to chain commands together . Let us start with a simple example (make sure you are inside the data directory): grep -r \"Elevated circulating follicle\" . | head -3 This results in: ./genes_to_phenotype.txt:190 NR0B1 HP:0008232 Elevated circulating follicle stimulating hormone level - HP:0040281 orphadata ORPHA:251510 ./genes_to_phenotype.txt:57647 DHX37 HP:0008232 Elevated circulating follicle stimulating hormone level - - mim2gene OMIM:273250 ./genes_to_phenotype.txt:57647 DHX37 HP:0008232 Elevated circulating follicle stimulating hormone level - HP:0040281 orphadata ORPHA:251510 So, what is happening here? First, we use the grep command to find \"Elevated circulating follicle\" in our data directory. As you may remember, there are more than 10 results for this command. So the grep command now wants to print these 10 results for you, but the | pipe symbol intercepts the result from grep and passes it on to the next command, which is head . Remember head and tail from above? Its exactly the same thing, only that, rather than printing the first lines of a file, we print the first lines of the output of the previous command . You can do incredible things with pipes. Here a taster which is beyond this first tutorial, but should give you a sense: grep \"Elevated circulating follicle\" genes_to_phenotype.txt | cut -f2 | sort | uniq | head -3 Output: AR BNC1 C14ORF39 What is happening here? grep is looking for \"Elevated circulating follicle\" in all files in the directory, then \"|\" is passing the output on to cut , which extracts the second column of the table (how cool?), then \"|\" is passing the output on to sort , which sorts the output, then \"|\" is passing the output on to uniq , which removes all duplicate values from the output, then \"|\" is passing the output on to head , which is printing only the first 3 rows of the result. Another super cool use of piping is searching your command history. Try running: history This will show you all the commands you have recently run. Now if you want to simply look for some very specific commands that you have run in the past you can combine history with grep : history | grep follicle This will print every command you ran in the last hour that contains the word \"follicle\". Super useful if you, like me, keep forgetting your commands! The last critical feature of the command line we cover today is the \"file redirect\". Instead of printing the output to file, we may chose to redirect the results to a file instead: matentzn@mbp.local:~/my-tutorial/data $ grep \"Elevated circulating follicle\" genes_to_phenotype.txt | cut -f2 | sort | uniq | head -3 > gene.txt matentzn@mbp.local:~/my-tutorial/data $ head gene.txt AR BNC1 C14ORF39 > gene.txt basically tells the command line: instead of printing the results to the command line, \"print\" them into a file which is called gene.txt . Videos \u00b6 Sam Bail: Intro to Terminal \u00b6 Sam also did here PhD in and around ontologies but has moved entirely to data engineering since. I really liked her 1 hour introduction into the terminal, this should fill some of the yawning gaps in this introduction here. Further reading \u00b6 Automating Ontology Development Workflows: Make, Shell and Automation Thinking Data Science at the Command Line : Free online book that covers everything you need to know to be a command line magician A whirlwind introduction to the command line by James Overton","title":"Basic introduction to CLI 1"},{"location":"tutorial/intro-cli-1/#tutorial-very-short-introduction-to-the-command-line-for-ontology-curators-and-semantic-engineers-part-1","text":"As a modern ontology curator, you are an engineer - you are curating computable knowledge, testing the integrity of your curation using quality control testing, and are responsible for critical components of modern knowledge systems that directly affect user experience - the ontologies. Scientific computing is a big, scary world comprising many different tools, methodologies, training resources and philosophies, but nearly all modern workflows share one key aspect: the ability to execute commands that help you find and manipulate data with the command line. Some examples of that include: Running and Ontology Development Kit (ODK) commands like sh run.sh make prepare_release Creating a new branch in git and committing changes Downloading a file with curl or wget Finding which file in my ontology repository mentions HP:0000118 to figure out where that \"weird axiom is coming from\" Searching for strange symbols in the ontology Filtering and sorting text files Here we are doing a basic hands on tutorial which will walk you through the must-know commands. For a more comprehensives introduction into thinking about automation please see our lesson on Automating Ontology Development Workflows: Make, Shell and Automation Thinking The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an sh file in the following there exists a corresponding bat file that can be run in the windows powershell, or CMD.","title":"Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 1"},{"location":"tutorial/intro-cli-1/#prerequisites","text":"You have: a Mac or Linux Operating system alternatively, you have the Ontology Development Kit installed and learned how to \"get inside a running container\" .","title":"Prerequisites"},{"location":"tutorial/intro-cli-1/#tutorial","text":"Baby steps: whoami, ls and cd Working with files and directories Downloading and searching files The Dark Art of Piping and Redirects Tutorial videos Further reading","title":"Tutorial"},{"location":"tutorial/intro-cli-1/#baby-steps-whoami-pwd-and-ls","text":"We are not going to discuss here in any detail what the command line is. We will focus on what you can do with it: for more information skip to the further reading section . The basic idea behind the command line is that you run a command to achieve a goal. Among the most common goals relevant to you as a semantic engineer will be: Navigating the file system (changing directories, logging into remote servers and more) Reading and writing files Searching stuff Most commands result in some kind of printed statement. Lets try one. Open your terminal (a terminal is the program you use to enter commands. For a nice overview of how shell, terminal, command line and console relate, see here ). On Mac, you can type CMD+Space to search for programs and then type \"terminal\". For this tutorial we use the default Terminal.app, but there are many others, including iterm2 . For this introduction, it does not matter which terminal you use. When first opening the terminal you will see something like this: or Note that your terminal window may look slightly different, depending on your configuration. More on that later. Let's type our first command and hit enter: whoami On my machine I get (base) matentzn@mbp.local:~ $ whoami matentzn This does not seem like a useful command, but sometimes, we forget who we are, and it is good to be reminded. So, what happened here? We ran a command, named whoami and our command line executed that command which is implemented somewhere on our machine as a program. That program simply determined who I am in some way, and then printed the result again. Ok so, lets lets look a bit closer at the command prompt itself: matentzn@mbp.local:~ $ Two interesting things to not here for today: The ~ . This universally (on all Unix systems) refers to your user directory on your computer. In this case here, it tells you that in your terminal, you are \"in your user directory\". The $ sign. It simply denotes where your command line starts (everything before the $ is information provided to you, everything will be about your commands). Make sure that you do not accidentally copy based the $ sign from the examples on the web into your command prompt: (base) matentzn@mbp.local:~ $ $ whoami -bash: $: command not found (base) matentzn@mbp.local:~ $ whoami did not do anything. Ok, based on the ~ we know that we are \"in the user home directory\". Let as become a bit more confident about that and ask the command prompt where we are: matentzn@mbp.local:~ $ pwd /Users/matentzn The pwd command prints out the full path of our current location in the terminal. As you can see, the default location when opening the command prompt is, indeed, the home director, located in /Users/matentzn . We will use it later again. A word about paths. /Users/matentzn is what we call a path. On UNIX systems, / separates one directory from another. So matentzn is a directory inside of the Users directory. Let us now take a look what our current directory contains (type ls and hit enter): matentzn@mbp.local:~ $ ls Applications Library ... This command will simply list all of the files in your directory as a big list. We can do this a bit nicer: matentzn@mbp.local:~ $ ls -l total 80000 drwx------@ 4 matentzn staff 128 31 Jul 2020 Applications drwx------@ 26 matentzn staff 832 12 Sep 2021 Desktop -l is a short command line option which allows you specify that you would like print the results in a different format (a long list). We will not go into any detail here what this means but a few things to not in the output: You can see some pieces of information that are interesting, like when the file or directory was last modified (i.e. 31. July 2020), who modified it (me) and, of course, the name e.g. Applications . Before we move on to the next section, let us clear the current terminal from all the command outputs we ran: clear Your command prompt should now be empty again.","title":"Baby steps: whoami, pwd and ls"},{"location":"tutorial/intro-cli-1/#working-with-files-and-directories","text":"In the previous section we learned how to figure out who we are ( whoami ), where we are ( pwd ) and how to see what is inside the current directory ( ls -l ) and how to clear all the output ( clear ). Let us know look at how we can programmatically create a new directory and change the location in our terminal. First let us create a new directory: mkdir tutorial-my Now if we list the contents of our current directory again ( ls -l ), we will see our newly created directory listed! Unfortunately, we just realised that we chose the wrong name for our directory! It should have been my-tutorial instead of tutorial-my ! So, let us rename it. In the command prompt, rather than \"renaming\" files and directories, we \"move\" them ( mv ). mv tutorial-my my-tutorial Now, lets enter our newly created directory using the _c_hange _d_irectory command ( cd ), and create another sub-directory in my-tutorial , called \"data\" ( mkdir data ): cd my-tutorial mkdir data You can check again with ls -l . If you see the data directory listed, we are all set! Feel free to run clear again to get rid of all the current output on the command prompt. Let us also enter this directory now: cd data . If we want to leave the directory again, feel free to do that like this: cd .. The two dots ( .. ) mean: \"parent directory.\" This is very important to remember during your command line adventures: .. stands for \"parent directory\", and . stands for \"current/this directory\" (see more on that below). Now, let's get into something more advanced: downloading files.","title":"Working with files and directories"},{"location":"tutorial/intro-cli-1/#downloading-and-searching-files","text":"Our first task is to download the famous Human Phenotype Ontology Gene to Phenotype Annotations (aka HPOA). As you should already now, whenever we download ontologies, or ontology related files, we should always use a persistent URL, if available! This is the one for HPOA: http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt . There are two very popular commands for downloading content: curl and wget . I think most of my colleagues prefer curl , but I like wget because it simpler for beginners. So I will use it here. Lets us try downloading the file! wget http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt -O genes_to_phenotype.txt The -O parameter is optional and specifies a filename. If you do not add the parameter, wget will try to guess the filename from the URL. This does not always go so well with complex URLs, so I personally recommend basically always specifying the -O parameter. You can also use the curl equivalent of the wget command; curl -L http://purl.obolibrary.org/obo/hp/hpoa/genes_to_phenotype.txt --output genes_to_phenotype.txt Try before reading on: Exercises! Move the downloaded file genes_to_phenotype.txt to the data directory you previously created. Change into the data directory. Download the OBO format version of the Human Phenotype Ontology from its PURL. Do not move on to the next step unless your data directory looks similar to this: matentzn@mbp.local:~/my-tutorial/data $ pwd /Users/matentzn/my-tutorial/data matentzn@mbp.local:~/my-tutorial/data $ ls -l total 53968 -rw-r--r-- 1 matentzn staff 19788987 11 Jun 19:09 genes_to_phenotype.txt -rw-r--r-- 1 matentzn staff 7836327 27 Jun 22:50 hp.obo Ok, let us look at the first 10 lines of genes_to_phenotype.txt using the head command: head genes_to_phenotype.txt head is a great command to familiarise yourself with a file. You can use a parameter to print more or less lines: head -3 genes_to_phenotype.txt This will print the first 3 lines of the genes_to_phenotype.txt file. There is another analogous command that allows us to look at the last lines off a file: tail genes_to_phenotype.txt head , tail . Easy to remember. Next, we will learn the most important of all standard commands on the command line: grep . grep stands for \"Global regular expression print\" and allows us to search files, and print the search results to the command line. Let us try some simple commands first. grep diabetes genes_to_phenotype.txt You will see a list of hundreds of lines out output. Each line corresponds to a line in the genes_to_phenotype.txt file which contains the word \"diabetes\". grep is case sensitive. It wont find matches like Diabetes, with capital D! Use the `-i` parameter in the grep command to instruct grep to perform case insensitive matches. There is a lot more to grep than we can cover here today, but one super cool thing is searching across an entire directory. grep -r \"Elevated circulating follicle\" . Assuming you are in the data directory, you should see something like this: ./genes_to_phenotype.txt:190 NR0B1 HP:0008232 Elevated circulating follicle stimulating hormone level - HP:0040281 orphadata ORPHA:251510 ./genes_to_phenotype.txt:57647 DHX37 HP:0008232 Elevated circulating follicle stimulating hormone level - - mim2gene OMIM:273250 ...... # Removed other results ./hp.obo:name: Elevated circulating follicle stimulating hormone level There are two new aspects to the command here: The -r option (\"recursive\") allows is to search a directory and all directories within in . The . in the beginning. Remember, in the previous use of the grep command we had the name of a file in the place where now the . is. The . means \"this directory\" - i.e. the directory you are in right now (if lost, remember pwd ). As you can see, grep does not only list the line of the file in which the match was found, it also tells us which filename it was found in! We can make this somewhat more easy to read as well by only showing filenames using the -l parameter: matentzn@mbp.local:~/my-tutorial/data $ grep -r -l \"Elevated circulating follicle\" . ./genes_to_phenotype.txt ./hp.obo","title":"Downloading and searching files"},{"location":"tutorial/intro-cli-1/#the-dark-art-of-piping-and-redirects","text":"The final lesson for today is about one of the most powerful features of the command line: the ability to chain commands together . Let us start with a simple example (make sure you are inside the data directory): grep -r \"Elevated circulating follicle\" . | head -3 This results in: ./genes_to_phenotype.txt:190 NR0B1 HP:0008232 Elevated circulating follicle stimulating hormone level - HP:0040281 orphadata ORPHA:251510 ./genes_to_phenotype.txt:57647 DHX37 HP:0008232 Elevated circulating follicle stimulating hormone level - - mim2gene OMIM:273250 ./genes_to_phenotype.txt:57647 DHX37 HP:0008232 Elevated circulating follicle stimulating hormone level - HP:0040281 orphadata ORPHA:251510 So, what is happening here? First, we use the grep command to find \"Elevated circulating follicle\" in our data directory. As you may remember, there are more than 10 results for this command. So the grep command now wants to print these 10 results for you, but the | pipe symbol intercepts the result from grep and passes it on to the next command, which is head . Remember head and tail from above? Its exactly the same thing, only that, rather than printing the first lines of a file, we print the first lines of the output of the previous command . You can do incredible things with pipes. Here a taster which is beyond this first tutorial, but should give you a sense: grep \"Elevated circulating follicle\" genes_to_phenotype.txt | cut -f2 | sort | uniq | head -3 Output: AR BNC1 C14ORF39 What is happening here? grep is looking for \"Elevated circulating follicle\" in all files in the directory, then \"|\" is passing the output on to cut , which extracts the second column of the table (how cool?), then \"|\" is passing the output on to sort , which sorts the output, then \"|\" is passing the output on to uniq , which removes all duplicate values from the output, then \"|\" is passing the output on to head , which is printing only the first 3 rows of the result. Another super cool use of piping is searching your command history. Try running: history This will show you all the commands you have recently run. Now if you want to simply look for some very specific commands that you have run in the past you can combine history with grep : history | grep follicle This will print every command you ran in the last hour that contains the word \"follicle\". Super useful if you, like me, keep forgetting your commands! The last critical feature of the command line we cover today is the \"file redirect\". Instead of printing the output to file, we may chose to redirect the results to a file instead: matentzn@mbp.local:~/my-tutorial/data $ grep \"Elevated circulating follicle\" genes_to_phenotype.txt | cut -f2 | sort | uniq | head -3 > gene.txt matentzn@mbp.local:~/my-tutorial/data $ head gene.txt AR BNC1 C14ORF39 > gene.txt basically tells the command line: instead of printing the results to the command line, \"print\" them into a file which is called gene.txt .","title":"The Dark Art of Piping and Redirects"},{"location":"tutorial/intro-cli-1/#videos","text":"","title":"Videos"},{"location":"tutorial/intro-cli-1/#sam-bail-intro-to-terminal","text":"Sam also did here PhD in and around ontologies but has moved entirely to data engineering since. I really liked her 1 hour introduction into the terminal, this should fill some of the yawning gaps in this introduction here.","title":"Sam Bail: Intro to Terminal"},{"location":"tutorial/intro-cli-1/#further-reading","text":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking Data Science at the Command Line : Free online book that covers everything you need to know to be a command line magician A whirlwind introduction to the command line by James Overton","title":"Further reading"},{"location":"tutorial/intro-cli-2/","text":"Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 2 \u00b6 Today we will pick up where we left off after the first CLI tutorial , and discuss some more usages of the command line. In particular, we will: Introduce you into the art of managing your shell profile Learn how to manage your path Talk about how to make your shell hacking more efficient with aliases and functions. Prerequisites \u00b6 You have: Completed the first CLI tutorial (Optional) installed the amazing ohmyzsh! - advanced CLI for managing your ZSH profile. Important : Before installing ohmyzsh, back up you ~/.zshrc file in case you have had any previous customisations you wish to preserve. Preparation \u00b6 Install https://ohmyz.sh/ (optional) For advanced windows users with docker installed, you can: Follow the instructions here to set yourself up for ODK development. Place odk.bat as instructed above in some directory on your machine (the path to the odk.bat file should have no spaces!) Create a new file .bash_profile in the same directory as your odk.bat file. Add something like -v %cd%\\.bash_profile:/root/.bash_profile to the odk.bat file (this is mounting the .bash_profile file inside your ODK container). There is already a similar -v statement in this file, just copy it right after Enter the ODK using odk.bat bash on your CMD (first, cd to the directory containing the odk.bat file). Now you can follow most of this tutorial here as well. Tutorial \u00b6 ohmyzsh! - advanced CLI (OPTIONAL) Managing the \"Path\": A first peak at your shell profile Managing aliases and functions in your bash profile ohmyzsh! - advanced CLI (OPTIONAL) \u00b6 If you have not done so, install https://ohmyz.sh/. It is not strictly speaking necessary to use ohmyzsh to follow the rest of this tutorial, but it is a nice way to managing your Zsh (z-shell) configuration. Note that the ODK is using the much older bash , but it should be fine for you to work with anyways. Managing the \"Path\": A first peak at your shell profile \u00b6 As Semantic Engineers or Ontology Curators we frequently have to install custom tools like ROBOT, owltools, and more on our computer. These are frequently downloaded from the internet as \"binaries\", for example as Java \"jar\" files. In order for our shell to \"know\" about these downloaded programs, we have to \"add them to the path\". Let us first look at what we currently have loaded in our path: echo $PATH What you see here is a list of paths. To read this list a bit more easily, let us remember our lesson on piping commands : echo $PATH | tr ':' '\\n' | sort What we do here: Using the echo command to print the contents of the $PATH variable. In Unix systems, the $ signifies the beginning of a variable name (if you are curious about what other \"environment variables\" are currently active on your system, use the printenv command). The output of the echo command is piped to the next command ( tr ). The tr \u2013 translate characters command copies the input of the previous command to the next with substitution or deletion of selected characters. Here, we substitute the : character, which is used to separate the different directory paths in the $PATH variable, with \"\\n\", which is the all important character that denotes a \"new line\". Just because, we also sort the output alphabetically to make it more readable. So, how do we change the \"$PATH\"? Let's try and install ROBOT and see! Before we download ROBOT, let us think how we will organise our custom tools moving forward. Everyone has their own preferences, but I like to create a tools directory right in my Users directory, and use this for all my tools moving forward. In this spirit, lets us first go to our user directory in the terminal, and then create a \"tools\" directory: cd ~ mkdir -p tools The -p parameter simply means: create the tools directory only if it does not exist. Now, let us go inside the tools directory ( cd ~/tools ) and continue following the instructions provided here . First, let us download the latest ROBOT release using the curl command: curl -L https://github.com/ontodev/robot/releases/latest/download/robot.jar > robot.jar ROBOT is written in the Java programming language, and packaged up as an executable JAR file. It is still quite cumbersome to directly run a command with that JAR file, but for the hell of it, let us just do it (for fun): java -jar robot.jar --version If you have worked with ROBOT before, this looks quite a bit more ugly then simply writing: robot --version If you get this (or a similar) error: zsh: permission denied: robot You will have to run the following command as well, which makes the robot wrapper script executable: chmod +x ~/tools/robot So, how can we achieve this? The answer is, we download a \"wrapper script\" and place it in the same folder as the Jar. Many tools provide such wrapper scripts, and they can sometimes do many more things than just \"running the jar file\". Let us know download the latest wrapper script: curl https://raw.githubusercontent.com/ontodev/robot/master/bin/robot > robot If everything went well, you should be able to print the contents of that file to the terminal using cat : cat robot You should see something like: #!/bin/sh ## Check for Cygwin, use grep for a case-insensitive search IS_CYGWIN=\"FALSE\" if uname | grep -iq cygwin; then IS_CYGWIN=\"TRUE\" fi # Variable to hold path to this script # Start by assuming it was the path invoked. ROBOT_SCRIPT=\"$0\" # Handle resolving symlinks to this script. # Using ls instead of readlink, because bsd and gnu flavors # have different behavior. while [ -h \"$ROBOT_SCRIPT\" ] ; do ls=`ls -ld \"$ROBOT_SCRIPT\"` # Drop everything prior to -> link=`expr \"$ls\" : '.*-> \\(.*\\)$'` if expr \"$link\" : '/.*' > /dev/null; then ROBOT_SCRIPT=\"$link\" else ROBOT_SCRIPT=`dirname \"$ROBOT_SCRIPT\"`/\"$link\" fi done # Directory that contains the this script DIR=$(dirname \"$ROBOT_SCRIPT\") if [ $IS_CYGWIN = \"TRUE\" ] then exec java $ROBOT_JAVA_ARGS -jar \"$(cygpath -w $DIR/robot.jar)\" \"$@\" else exec java $ROBOT_JAVA_ARGS -jar \"$DIR/robot.jar\" \"$@\" fi We are not getting into the details of what this wrapper script does, but note that, you can fine the actually call the the ROBOT jar file towards the end: java $ROBOT_JAVA_ARGS -jar \"$DIR/robot.jar\" \"$@\" . The cool thing is, we do not need to ever worry about this script, but it is good for use to know, as Semantic Engineers, that it exists. Now, we have downloaded the ROBOT jar file and the wrapper script into the ~/tools directory. The last step remaining is to add the ~/tools directory to your path. It makes sense to try to at least understand the basic idea behind environment variables: variables that are \"loaded\" or \"active\" in your environment (your shell). The first thing you could try to do is change the variable right here in your terminal. To do that, we can use the export command: export PATH=$PATH:~/tools What you are doing here is using the export command to set the PATH variable to $PATH:~/tools , which is the old path ( $PATH ), a colon ( : ) and the new directory we want to add ( ~/tools ). And, indeed, if we now look at our path again: echo $PATH | tr ':' '\\n' | sort We will see the path added. We can now move around to any directory on our machine and invoke the robot command. Try it before moving on! Unfortunately, the change we have now applied to the $PATH variable is not persistent: if you open a new tab in your Terminal, your $PATH variable is back to what it was. What we have to do in order to make this persistent is to add the export command to a special script which is run every time the you open a new terminal: your shell profile. There is a lot to say about your shell profiles, and we are taking a very simplistic view here that covers 95% of what we need: If you are using zsh your profile is managed using the ~/.zshrc file, and if you are using bash , your profile is managed using the ~/.bash_profile file. In this tutorial I will assume you are using zsh , and, in particular, after installing \"oh-my-zsh\". Let us look at the first 5 lines of the ~/.zshrc file: head ~/.zshrc If you have installed oh-my-zsh, the output will look something like: # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # Path to your oh-my-zsh installation. export ZSH=\"$HOME/.oh-my-zsh\" # Set name of the theme to load --- if set to \"random\", it will # load a random theme each time oh-my-zsh is loaded, in which case, # to know which specific one was loaded, run: echo $RANDOM_THEME # See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes This ~/.zshrc profile script is loaded every time you open up a new shell. What we want to do is add our export command above to this script, so that it is running every time . That is the basic concept of a shell profile: providing a series of commands that is run every time a new shell (terminal window, tab) is opened. For this tutorial, we use nano to edit the file, but feel free to use your text editor of choice. For example, you can open the profile file using TextEdit on Mac like this: open -a TextEdit ~/.zshrc We will proceed using nano , but feel free to use any editor. nano ~/.zshrc Using terminal-based editors like nano or, even worse, vim, involves a bit of a learning curve. nano is by far the least powerful and simple to use. If you typed the above command, you should see its contents on the terminal. The next step is to copy the following (remember, we already used it earlier) export PATH=$PATH:~/tools and paste it somewhere into the file . Usually, there is a specific section of the file that is concerned with setting up your path. Eventually, as you become more of an expert, you will start organising your profile according to your own preferences! Today we will just copy the command anywhere, for example: # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH export PATH=~/tutorial:$PATH # ..... other lines in the file Note that the # symbol denotes the beginning of a \"comment\" which is ignored by the shell/CLI. After you have pasted the above, you use the following keyboard key-combinations to safe and close the file: control + O This saves the file. Confirm with Enter. control + x This closes the file. Now, we need to tell the shell we are currently in that it should reload our profile we have just edited. We do that using the source command. source ~/.zshrc Great! You should be able open a new tab in your terminal (with command+t on a Mac, for example) and run the following command: robot --version Managing aliases and custom commands in your shell profile \u00b6 This section will only give a sense of the kinds of things you can do with your shell profile - in the end you will have to jump into the cold water and build your skills up yourself. Let us start with a very powerful concept: aliases. Aliases are short names for your commands you can use if you use them repeatedly but are annoyed typing them out every time. For example, tired of typing out long paths all the time to jump between your Cell Ontology and Human Phenotype Ontology directories? Instead of: cd /Users/matentzn/ws/human-phenotype-ontology/src/ontology wouldn't it be nice to be able to use, instead, cdhp or, if you are continuously checking git status , why not implement a alias gits ? Or activating your python environment ( source ~/.pyenv/versions/oak/bin/activate ) with a nice env-oak ? To achieve this we do the following: (1) Open your profile in a text editor of your choice, e.g. nano ~/.zshrc add the following lines: alias cdt='cd ~/tools' alias hg='history | grep' Save (control+o) and close (control+x) the profile. Reload the profile: source ~/.zshrc (Alternatively, just open a new tab in your Terminal.) Now, lets try our new aliases: cdt Will bring you straight to your tools directory you created in the previous lesson above. hg robot Will search your terminal command history for every command you have executed involving robot . List of ideas for aliases \u00b6 In the following, we provide a list of aliases we find super useful: alias cdt='cd ~/tools' - add shortcuts to all directories you frequently visit! alias orcid='echo '\\''https://orcid.org/0000-0002-7356-1779'\\'' | tr -d '\\''\\n'\\'' | pbcopy' - if you keep having to look up your ORCID, your favourite ontologies PURL or the your own zoom room, why not add a shortcut that copies it straight into your clipboard? alias opent='open ~/tools' - why not open your favourite directory in finder without faving to search the User Interface? You can use the same idea to open your favourite ontology from wherever you are, i.e. alias ohp='open ~/ws/human-phenotype-ontology/src/ontology/hp-edit.owl' . alias env-linkml='source ~/.pyenv/versions/linkml/bin/activate' - use simple shortcuts to active your python environments. This will become more important if you learn to master special python tools like OAK . alias update_repo='sh run.sh make update_repo' - for users of ODK - alias all your long ODK commands! Functions \u00b6 The most advanced thought we want to cover today is \"functions\". You can not only manage simple aliases, but you can actually add proper functions into your shell profile. Here is an example of one that I use: ols() { open https://www.ebi.ac.uk/ols/search?q=\"$1\" } This is a simple function in my bash profile that I can use to search on OLS: ols \"lung disorder\" It will open this search straight in my browser. rreport() { robot report -i \"$1\" --fail-on none -o /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv } This allows me to quickly run a robot report on an ontology. rreport cl.owl Why not expand the function and have it open in my atom text editor right afterwards? rreport() { robot report -i \"$1\" --fail-on none -o /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv && atom /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv } The possibilities are endless. Some power-users have hundreds of such functions in their shell profiles, and they can do amazing things with them. Let us know about your own ideas for functions on the OBOOK issue tracker . Or, why not add a function to create a new, titled issue on OBOOK? obook-issue() { open https://github.com/OBOAcademy/obook/issues/new?title=\"$1\" } and from now on run: obook-issue \"Add my awesome function\" Further reading \u00b6 Automating Ontology Development Workflows: Make, Shell and Automation Thinking Data Science at the Command Line : Free online book that covers everything you need to know to be a command line magician A whirlwind introduction to the command line by James Overton","title":"Basic introduction to CLI 2"},{"location":"tutorial/intro-cli-2/#tutorial-very-short-introduction-to-the-command-line-for-ontology-curators-and-semantic-engineers-part-2","text":"Today we will pick up where we left off after the first CLI tutorial , and discuss some more usages of the command line. In particular, we will: Introduce you into the art of managing your shell profile Learn how to manage your path Talk about how to make your shell hacking more efficient with aliases and functions.","title":"Tutorial: Very (!) short introduction to the command line for ontology curators and semantic engineers: Part 2"},{"location":"tutorial/intro-cli-2/#prerequisites","text":"You have: Completed the first CLI tutorial (Optional) installed the amazing ohmyzsh! - advanced CLI for managing your ZSH profile. Important : Before installing ohmyzsh, back up you ~/.zshrc file in case you have had any previous customisations you wish to preserve.","title":"Prerequisites"},{"location":"tutorial/intro-cli-2/#preparation","text":"Install https://ohmyz.sh/ (optional) For advanced windows users with docker installed, you can: Follow the instructions here to set yourself up for ODK development. Place odk.bat as instructed above in some directory on your machine (the path to the odk.bat file should have no spaces!) Create a new file .bash_profile in the same directory as your odk.bat file. Add something like -v %cd%\\.bash_profile:/root/.bash_profile to the odk.bat file (this is mounting the .bash_profile file inside your ODK container). There is already a similar -v statement in this file, just copy it right after Enter the ODK using odk.bat bash on your CMD (first, cd to the directory containing the odk.bat file). Now you can follow most of this tutorial here as well.","title":"Preparation"},{"location":"tutorial/intro-cli-2/#tutorial","text":"ohmyzsh! - advanced CLI (OPTIONAL) Managing the \"Path\": A first peak at your shell profile Managing aliases and functions in your bash profile","title":"Tutorial"},{"location":"tutorial/intro-cli-2/#ohmyzsh-advanced-cli-optional","text":"If you have not done so, install https://ohmyz.sh/. It is not strictly speaking necessary to use ohmyzsh to follow the rest of this tutorial, but it is a nice way to managing your Zsh (z-shell) configuration. Note that the ODK is using the much older bash , but it should be fine for you to work with anyways.","title":"ohmyzsh! - advanced CLI (OPTIONAL)"},{"location":"tutorial/intro-cli-2/#managing-the-path-a-first-peak-at-your-shell-profile","text":"As Semantic Engineers or Ontology Curators we frequently have to install custom tools like ROBOT, owltools, and more on our computer. These are frequently downloaded from the internet as \"binaries\", for example as Java \"jar\" files. In order for our shell to \"know\" about these downloaded programs, we have to \"add them to the path\". Let us first look at what we currently have loaded in our path: echo $PATH What you see here is a list of paths. To read this list a bit more easily, let us remember our lesson on piping commands : echo $PATH | tr ':' '\\n' | sort What we do here: Using the echo command to print the contents of the $PATH variable. In Unix systems, the $ signifies the beginning of a variable name (if you are curious about what other \"environment variables\" are currently active on your system, use the printenv command). The output of the echo command is piped to the next command ( tr ). The tr \u2013 translate characters command copies the input of the previous command to the next with substitution or deletion of selected characters. Here, we substitute the : character, which is used to separate the different directory paths in the $PATH variable, with \"\\n\", which is the all important character that denotes a \"new line\". Just because, we also sort the output alphabetically to make it more readable. So, how do we change the \"$PATH\"? Let's try and install ROBOT and see! Before we download ROBOT, let us think how we will organise our custom tools moving forward. Everyone has their own preferences, but I like to create a tools directory right in my Users directory, and use this for all my tools moving forward. In this spirit, lets us first go to our user directory in the terminal, and then create a \"tools\" directory: cd ~ mkdir -p tools The -p parameter simply means: create the tools directory only if it does not exist. Now, let us go inside the tools directory ( cd ~/tools ) and continue following the instructions provided here . First, let us download the latest ROBOT release using the curl command: curl -L https://github.com/ontodev/robot/releases/latest/download/robot.jar > robot.jar ROBOT is written in the Java programming language, and packaged up as an executable JAR file. It is still quite cumbersome to directly run a command with that JAR file, but for the hell of it, let us just do it (for fun): java -jar robot.jar --version If you have worked with ROBOT before, this looks quite a bit more ugly then simply writing: robot --version If you get this (or a similar) error: zsh: permission denied: robot You will have to run the following command as well, which makes the robot wrapper script executable: chmod +x ~/tools/robot So, how can we achieve this? The answer is, we download a \"wrapper script\" and place it in the same folder as the Jar. Many tools provide such wrapper scripts, and they can sometimes do many more things than just \"running the jar file\". Let us know download the latest wrapper script: curl https://raw.githubusercontent.com/ontodev/robot/master/bin/robot > robot If everything went well, you should be able to print the contents of that file to the terminal using cat : cat robot You should see something like: #!/bin/sh ## Check for Cygwin, use grep for a case-insensitive search IS_CYGWIN=\"FALSE\" if uname | grep -iq cygwin; then IS_CYGWIN=\"TRUE\" fi # Variable to hold path to this script # Start by assuming it was the path invoked. ROBOT_SCRIPT=\"$0\" # Handle resolving symlinks to this script. # Using ls instead of readlink, because bsd and gnu flavors # have different behavior. while [ -h \"$ROBOT_SCRIPT\" ] ; do ls=`ls -ld \"$ROBOT_SCRIPT\"` # Drop everything prior to -> link=`expr \"$ls\" : '.*-> \\(.*\\)$'` if expr \"$link\" : '/.*' > /dev/null; then ROBOT_SCRIPT=\"$link\" else ROBOT_SCRIPT=`dirname \"$ROBOT_SCRIPT\"`/\"$link\" fi done # Directory that contains the this script DIR=$(dirname \"$ROBOT_SCRIPT\") if [ $IS_CYGWIN = \"TRUE\" ] then exec java $ROBOT_JAVA_ARGS -jar \"$(cygpath -w $DIR/robot.jar)\" \"$@\" else exec java $ROBOT_JAVA_ARGS -jar \"$DIR/robot.jar\" \"$@\" fi We are not getting into the details of what this wrapper script does, but note that, you can fine the actually call the the ROBOT jar file towards the end: java $ROBOT_JAVA_ARGS -jar \"$DIR/robot.jar\" \"$@\" . The cool thing is, we do not need to ever worry about this script, but it is good for use to know, as Semantic Engineers, that it exists. Now, we have downloaded the ROBOT jar file and the wrapper script into the ~/tools directory. The last step remaining is to add the ~/tools directory to your path. It makes sense to try to at least understand the basic idea behind environment variables: variables that are \"loaded\" or \"active\" in your environment (your shell). The first thing you could try to do is change the variable right here in your terminal. To do that, we can use the export command: export PATH=$PATH:~/tools What you are doing here is using the export command to set the PATH variable to $PATH:~/tools , which is the old path ( $PATH ), a colon ( : ) and the new directory we want to add ( ~/tools ). And, indeed, if we now look at our path again: echo $PATH | tr ':' '\\n' | sort We will see the path added. We can now move around to any directory on our machine and invoke the robot command. Try it before moving on! Unfortunately, the change we have now applied to the $PATH variable is not persistent: if you open a new tab in your Terminal, your $PATH variable is back to what it was. What we have to do in order to make this persistent is to add the export command to a special script which is run every time the you open a new terminal: your shell profile. There is a lot to say about your shell profiles, and we are taking a very simplistic view here that covers 95% of what we need: If you are using zsh your profile is managed using the ~/.zshrc file, and if you are using bash , your profile is managed using the ~/.bash_profile file. In this tutorial I will assume you are using zsh , and, in particular, after installing \"oh-my-zsh\". Let us look at the first 5 lines of the ~/.zshrc file: head ~/.zshrc If you have installed oh-my-zsh, the output will look something like: # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # Path to your oh-my-zsh installation. export ZSH=\"$HOME/.oh-my-zsh\" # Set name of the theme to load --- if set to \"random\", it will # load a random theme each time oh-my-zsh is loaded, in which case, # to know which specific one was loaded, run: echo $RANDOM_THEME # See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes This ~/.zshrc profile script is loaded every time you open up a new shell. What we want to do is add our export command above to this script, so that it is running every time . That is the basic concept of a shell profile: providing a series of commands that is run every time a new shell (terminal window, tab) is opened. For this tutorial, we use nano to edit the file, but feel free to use your text editor of choice. For example, you can open the profile file using TextEdit on Mac like this: open -a TextEdit ~/.zshrc We will proceed using nano , but feel free to use any editor. nano ~/.zshrc Using terminal-based editors like nano or, even worse, vim, involves a bit of a learning curve. nano is by far the least powerful and simple to use. If you typed the above command, you should see its contents on the terminal. The next step is to copy the following (remember, we already used it earlier) export PATH=$PATH:~/tools and paste it somewhere into the file . Usually, there is a specific section of the file that is concerned with setting up your path. Eventually, as you become more of an expert, you will start organising your profile according to your own preferences! Today we will just copy the command anywhere, for example: # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH export PATH=~/tutorial:$PATH # ..... other lines in the file Note that the # symbol denotes the beginning of a \"comment\" which is ignored by the shell/CLI. After you have pasted the above, you use the following keyboard key-combinations to safe and close the file: control + O This saves the file. Confirm with Enter. control + x This closes the file. Now, we need to tell the shell we are currently in that it should reload our profile we have just edited. We do that using the source command. source ~/.zshrc Great! You should be able open a new tab in your terminal (with command+t on a Mac, for example) and run the following command: robot --version","title":"Managing the \"Path\": A first peak at your shell profile"},{"location":"tutorial/intro-cli-2/#managing-aliases-and-custom-commands-in-your-shell-profile","text":"This section will only give a sense of the kinds of things you can do with your shell profile - in the end you will have to jump into the cold water and build your skills up yourself. Let us start with a very powerful concept: aliases. Aliases are short names for your commands you can use if you use them repeatedly but are annoyed typing them out every time. For example, tired of typing out long paths all the time to jump between your Cell Ontology and Human Phenotype Ontology directories? Instead of: cd /Users/matentzn/ws/human-phenotype-ontology/src/ontology wouldn't it be nice to be able to use, instead, cdhp or, if you are continuously checking git status , why not implement a alias gits ? Or activating your python environment ( source ~/.pyenv/versions/oak/bin/activate ) with a nice env-oak ? To achieve this we do the following: (1) Open your profile in a text editor of your choice, e.g. nano ~/.zshrc add the following lines: alias cdt='cd ~/tools' alias hg='history | grep' Save (control+o) and close (control+x) the profile. Reload the profile: source ~/.zshrc (Alternatively, just open a new tab in your Terminal.) Now, lets try our new aliases: cdt Will bring you straight to your tools directory you created in the previous lesson above. hg robot Will search your terminal command history for every command you have executed involving robot .","title":"Managing aliases and custom commands in your shell profile"},{"location":"tutorial/intro-cli-2/#list-of-ideas-for-aliases","text":"In the following, we provide a list of aliases we find super useful: alias cdt='cd ~/tools' - add shortcuts to all directories you frequently visit! alias orcid='echo '\\''https://orcid.org/0000-0002-7356-1779'\\'' | tr -d '\\''\\n'\\'' | pbcopy' - if you keep having to look up your ORCID, your favourite ontologies PURL or the your own zoom room, why not add a shortcut that copies it straight into your clipboard? alias opent='open ~/tools' - why not open your favourite directory in finder without faving to search the User Interface? You can use the same idea to open your favourite ontology from wherever you are, i.e. alias ohp='open ~/ws/human-phenotype-ontology/src/ontology/hp-edit.owl' . alias env-linkml='source ~/.pyenv/versions/linkml/bin/activate' - use simple shortcuts to active your python environments. This will become more important if you learn to master special python tools like OAK . alias update_repo='sh run.sh make update_repo' - for users of ODK - alias all your long ODK commands!","title":"List of ideas for aliases"},{"location":"tutorial/intro-cli-2/#functions","text":"The most advanced thought we want to cover today is \"functions\". You can not only manage simple aliases, but you can actually add proper functions into your shell profile. Here is an example of one that I use: ols() { open https://www.ebi.ac.uk/ols/search?q=\"$1\" } This is a simple function in my bash profile that I can use to search on OLS: ols \"lung disorder\" It will open this search straight in my browser. rreport() { robot report -i \"$1\" --fail-on none -o /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv } This allows me to quickly run a robot report on an ontology. rreport cl.owl Why not expand the function and have it open in my atom text editor right afterwards? rreport() { robot report -i \"$1\" --fail-on none -o /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv && atom /Users/matentzn/tmp_data/report_\"$(basename -- $1)\".tsv } The possibilities are endless. Some power-users have hundreds of such functions in their shell profiles, and they can do amazing things with them. Let us know about your own ideas for functions on the OBOOK issue tracker . Or, why not add a function to create a new, titled issue on OBOOK? obook-issue() { open https://github.com/OBOAcademy/obook/issues/new?title=\"$1\" } and from now on run: obook-issue \"Add my awesome function\"","title":"Functions"},{"location":"tutorial/intro-cli-2/#further-reading","text":"Automating Ontology Development Workflows: Make, Shell and Automation Thinking Data Science at the Command Line : Free online book that covers everything you need to know to be a command line magician A whirlwind introduction to the command line by James Overton","title":"Further reading"},{"location":"tutorial/linking-data/","text":"Tutorial: From Tables to Linked Data \u00b6 These are the kinds of things that I do when I need to work with a new dataset. My goal is to have data that makes good sense and that I can integrate with other data using standard technologies: Linked Data. 0. Before \u00b6 The boss just sent me this new table to figure out: datetime investigator subject species strain sex group protocol organ disease qualifier comment 1/1/14 10:21 AM JAO 12 RAT F 344/N FEMALE 1 HISTOPATHOLOGY LUNG ADENOCARCINOMA SEVERE 1/1/14 10:30 AM JO 31 MOUSE B6C3F1 MALE 2 HISTOPATHOLOGY NOSE INFLAMMATION MILD 1/1/14 10:45 AM JAO 45 RAT F 344/N MALE 1 HISTOPATHOLOGY ADRENAL CORTEX NECROSIS MODERATE It doesn't seem too bad, but there's lots of stuff that I don't quite understand. Where to start? 1. Getting Organized \u00b6 Before I do anything else, I'm going to set up a new project for working with this data. Maybe I'll change my mind later and want to merge the new project with an existing project, but it never hurts to start from a nice clean state. I'll make a new directory in a sensible place with a sensible name. In my case I have a ~/Repositories/ directory, with subdirectories for GitHub and various GitLab servers, a local directory for projects I don't plan to share, and a temp directory for projects that I don't need to keep. I'm not sure if I'm going to share this work, so it can go in a new subdirectory of local . I'll call it \"linking-data-tutorial\" for now. Then I'll run git init to turn that directory into a git repository. For now I'm just going to work locally, but later I can make a repository on GitHub and push my local repository there. Next I'll create a README.md file where I'll keep notes for myself to read later. My preferred editor is Kakoune . So I'll open a terminal and run these commands: $ cd ~/Repositories/local/ $ mkdir linking-data-tutorial $ cd linking-data-tutorial $ git init $ kak README.md In the README I'll start writing something like this: # Linking Data Tutorial An example of how to convert a dataset to Linked Data. The source data is available from <https://github.com/jamesaoverton/obook/tree/master/03-RDF/data.csv> Maybe this information should go somewhere else eventually, but the README is a good place to start. \"Commit early, commit often\" they say, so: $ git add README.md $ git commit -m \"Initial commit\" 2. Getting Copies \u00b6 Data has an annoying tendency to get changed. You don't want it changing out from under you while you're in the middle of something. So the next thing to do is get a copy of the data and store it locally. If it's big, you can store a compressed copy. If it's too big to fit on your local machine, well keep the best notes you can of how to get to the data, and what operations you're doing on it. I'm going to make a cache directory and store all my \"upstream\" data there. I'm going to fetch the data and that's it -- I'm not going to edit these files. When I want to change the data I'll make copies in another directory. I don't want git to track the cached data, so I'll add /cache/ to .gitignore and tell git to track that . Then I'll use curl to download the file. $ mkdir cache $ echo \"/cache/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /cache/ directory\" $ cd cache $ curl -LO \"https://github.com/jamesaoverton/obook/raw/master/03-RDF/data.csv\" $ ls data.csv $ cd .. $ ls -a .gitignore data README.md 3. Getting My Bearings \u00b6 The first thing to do is look at the data. In this case I have just one table in CSV format, so I can use any number of tools to open the file and look around. I bet the majority of people would reach for Excel. My (idiosyncratic) preference is VisiData . What am I looking for? A bunch of different things: what do the rows represent? what columns do I have? for each column, what sorts of values do I have? In my README file I'll make a list of the columns like this: - datetime - investigator - subject - species - strain - sex - group - protocol - organ - disease - qualifier - comment Then I'll make some notes for myself: - datetime: American-style dates, D/M/Y or M/D/Y? - investigator: initials, ORCID? - subject: integer ID - species: common name for species, NCBITaxon? - strain: some sort of code with letters, numbers, spaces, some punctuation - sex: string female/male - group: integer ID - protocol: string, OBI? - organ: string, UBERON? - disease: string, DO/MONDO? - qualifier: string, PATO? - comment: ??? You can see that I'm trying to figure out what's in each column. I'm also thinking ahead to OBO ontologies that I know of that may have terms that I can use for each column. 4. Getting Structured \u00b6 In the end, I want to have nice, clean Linked Data. But I don't have to get there in one giant leap. Instead I'll take a bunch of small, incremental steps. There's lots of tools I can use, but this time I'll use SQLite. First I'll set up some more directories. I'll create a build directory where I'll store temporary files. I don't want git to track this directory, so I'll add it to .gitignore . $ mkdir build/ $ echo \"/build/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /build/ directory\" I'll also add a src directory to store code. I do want to track src with git. $ mkdir src $ kak src/data.sql In src/data.sql I'll add just enough to import build/data.csv : -- import build/data.csv .mode csv .import build/data.csv data_csv This will create a build/data.db file and import build/data.csv into a data_csv table. Does it work? $ sqlite3 build/data.db < src/data.sql $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|12|RAT|F 344/N|FEMALE|1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| Nice! Note that I didn't even specify a schema for data_csv . It uses the first row as the column names, and the type of every column is TEXT . Here's the schema I end up with: $ sqlite3 build/data.db <<< \".schema data_csv\" CREATE TABLE data_csv( \"datetime\" TEXT, \"investigator\" TEXT, \"subject\" TEXT, \"species\" TEXT, \"strain\" TEXT, \"sex\" TEXT, \"group\" TEXT, \"protocol\" TEXT, \"organ\" TEXT, \"disease\" TEXT, \"qualifier\" TEXT, \"comment\" TEXT ); I'm going to want to update src/data.sql then rebuild the database over and over. It's small, so this will only take a second. If it was big, then I would copy a subset into build/data.csv for now so that I the script still runs in a second or two and I can iterate quickly. I'll write a src/build.sh script to make life a little easier: #!/bin/sh rm -f build/* cp cache/data.csv build/data.csv sqlite3 build/data.db < src/data.sql Does it work? $ sh src/build.sh Nice! Time to update the README: ## Requirements - [SQLite3](https://sqlite.org/index.html) ## Usage Run `sh src/build.sh` I'll commit my work in progress: $ git add src/data.sql src/build.sh $ git add --update $ git commit -m \"Load data.csv into SQLite\" Now I have a script that executes a SQL file that loads the source data into a new database. I'll modify the src/data.sql file in a series of small steps until it has the structure that I want. 5. Getting Clean \u00b6 In the real world, data is always a mess. It takes real work to clean it up. And really, it's almost never perfectly clean. It's important to recognize that cleaning data has diminishing returns. There's low hanging fruit: easy to clean, often with code, and bringing big benefits. Then there's tough stuff that requires an expert to work through the details, row by row. The first thing to do is figure out the schema you want. I'll create a new data table and start with the default schema from data_csv . Notice that in the default schema all the column names are quoted. That's kind of annoying. But when I remove the quotation marks I realize that one of the column names is \"datetime\", but datetime is a keyword in SQLite! You can't use it as a column name without quoting. I'll rename it to \"assay_datetime\". I have the same problem with \"group\". I'll rename \"group\" to \"group_id\" and \"subject\" to \"subject_id\". The rest of the column names seem fine. I want \"assay_datetime\" to be in standard ISO datetime format, but SQLite stores these as TEXT. The \"subject\" and \"group\" columns are currently integers, but I plan to make them into URIs to CURIEs. So everything will still be TEXT. CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT ); The dates currently look like \"1/1/14 10:21 AM\". Say I know that they were done on Eastern Standard Time. How do I convert to ISO dates like \"2014-01-01 10:21:00-0500\"? Well SQLite isn't the right tool for this. The Unix date command does a nice job, though: $ date -d \"1/1/14 10:21 AM EST\" +\"%Y-%m-%d %H:%M:%S%z\" 2014-01-01 10:21:00-0500 I can run that over each line of the file using awk . So I update the src/build.sh to rework the build/data.csv before I import: #!/bin/sh rm -f build/* head -n1 cache/data.csv > build/data.csv tail -n+2 cache/data.csv \\ | awk 'BEGIN{FS=\",\"; OFS=\",\"} { \"date -d \\\"\"$1\" EST\\\" +\\\"%Y-%m-%d %H:%M:%S%z\\\"\" | getline $1; print $0 }' \\ >> build/data.csv sqlite3 build/data.db < src/data.sql One more problem I could clean up is that \"JO\" should really be \"JAO\" -- that's just a typo, and they should both refer to James A. Overton. I could make that change in src/build.sh , but I'll do it in src/data.sql instead. I'll write a query to copy all the rows of data_csv into data and then I'll update data with some fixes. -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator=\"JAO\" WHERE investigator=\"JO\"; Honestly, it took me quite a while to write that awk command. It's a very powerful tool, but I don't use it enough to remember how it works. You might prefer to write yourself a Python script, or some R code. You could use that instead of this SQL UPDATE as well. I just wanted to show you two of the thousands of ways to do this. If there's a lot of replacements like \"JO\", then you might also consider listing them in another table that you can read into your script. The important part is to automate your cleaning! Why didn't I just edit cache/data.csv in Excel? In step 2 I saved a copy of the data because I didn't want it to change while I was working on it, but I do expect it to change! By automating the cleaning process, I should be able to just update cache/data.csv run everything again, and the fixes will be applied again. I don't want to do all this work manually every time the upstream data is updated. I'll commit my work in progress: $ git add --update $ git commit -m \"Start cleaning data\" Cleaning can take a lot of work. This is example table is pretty clean already. The next hard part is sorting out your terminology. 6. Getting Connected \u00b6 It's pretty easy to convert a table structure to triples. The hard part is converting the table contents . There are some identifiers in the table that would be better as URLs, and there's a bunch of terminology that would be better if it was linked to an ontology or other system. I'll start with the identifiers that are local to this data: subject_id and group_id. I can convert them to URLs by defining a prefix and then just using that prefix. I'll use string concatenation to update the table: -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; Now I'll check my work: $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|ex:subject-12|RAT|F 344/N|FEMALE|ex:group-1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| I should take a moment to tell you, that while I was writing the Turtle conversion code later in this essay, I had to come back here and change these identifiers. The thing is that Turtle is often more strict than I expect about identifier syntax. Turtle identifiers look like CURIEs , but they're actually QNames . CURIEs are pretty much just just URLs shortened with a prefix, so almost anything goes. QNames come from XML , and Turtle identifiers have to be valid XML element names. I always remember that I need to stick to alphanumeric characters, and that I have to replace whitespace and punctuation with a - or _ . I didn't remember that the local part (aka \"suffix\", aka \"NCName\") can't start with a digit. So I tried to use \"subject:12\" and \"group:1\" as my identifiers. That worked fine until I generated Turtle. The Turtle looked fine, so it took me quite a while to figure out why it looked very wrong when I converted it into RDXML format. This kind of thing happens to me all the time. I'm almost always using a mixture of technologies based on different sets of assumptions, and there are always things that don't line up. That's why I like to work in small iterations, checking my work as I go (preferrably with automated tests), and keeping everything in version control. When I need to make a change like this one, I just circle back and iterate again. The next thing is to tackle the terminology. First I'll just make a list of the terms I'm using from the relevant columns in build/term.tsv : ```sh #collect $ sqlite3 build/data.db << EOF > build/term.tsv SELECT investigator FROM data UNION SELECT species FROM data UNION SELECT strain FROM data UNION SELECT strain FROM data UNION SELECT sex FROM data UNION SELECT protocol FROM data UNION SELECT organ FROM data UNION SELECT disease FROM data UNION SELECT qualifier FROM data; EOF It's a lot of work to go through all those terms and find good ontology terms. I'm going to do that hard work for you (just this once!) so we can keep moving. I'll add this table to `src/term.tsv` | id | code | label | | ------------------------- | -------------- | ------------------ | | obo:NCBITaxon_10116 | RAT | Rattus norvegicus | | obo:NCBITaxon_10090 | MOUSE | Mus musculus | | ex:F344N | F 344/N | F 344/N | | ex:B6C3F1 | B6C3F1 | B6C3F1 | | obo:PATO_0000383 | FEMALE | female | | obo:PATO_0000384 | MALE | male | | obo:OBI_0600020 | HISTOPATHOLOGY | histology | | obo:UBERON_0002048 | LUNG | lung | | obo:UBERON_0007827 | NOSE | external nose | | obo:UBERON_0001235 | ADRENAL CORTEX | adrenal cortex | | obo:MPATH_268 | ADENOCARCINOMA | adenocarcinoma | | obo:MPATH_212 | INFLAMMATION | inflammation | | obo:MPATH_4 | NECROSIS | necrosis | | obo:PATO_0000396 | SEVERE | severe intensity | | obo:PATO_0000394 | MILD | mild intensity | | obo:PATO_0000395 | MODERATE | moderate intensity | | orcid:0000-0001-5139-5557 | JAO | James A. Overton | And I'll add these prefixes to `src/prefix.tsv`: | prefix | base | | ------- | ------------------------------------------- | | rdf | http://www.w3.org/1999/02/22-rdf-syntax-ns# | | rdfs | http://www.w3.org/2000/01/rdf-schema# | | xsd | http://www.w3.org/2001/XMLSchema# | | owl | http://www.w3.org/2002/07/owl# | | obo | http://purl.obolibrary.org/obo/ | | orcid | http://orcid.org/ | | ex | https://example.com/ | | subject | https://example.com/subject/ | | group | https://example.com/group/ | Now I can import these tables into SQL and use the term table as a FOREIGN KEY constraint on data: ```sql .mode tabs CREATE TABLE prefix ( prefix TEXT PRIMARY KEY, base TEXT UNIQUE ); .import --skip 1 src/prefix.tsv prefix CREATE TABLE term ( id TEXT PRIMARY KEY, code TEXT UNIQUE, label TEXT UNIQUE ); .import --skip 1 src/term.tsv term CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT, FOREIGN KEY(investigator) REFERENCES term(investigator), FOREIGN KEY(species) REFERENCES term(species), FOREIGN KEY(strain) REFERENCES term(strain), FOREIGN KEY(sex) REFERENCES term(sex), FOREIGN KEY(protocol) REFERENCES term(protocol), FOREIGN KEY(organ) REFERENCES term(organ), FOREIGN KEY(disease) REFERENCES term(disease), FOREIGN KEY(qualifier) REFERENCES term(qualifier) ); -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator='JAO' WHERE investigator='JO'; -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; I'll update the README: See `src/` for: - `prefix.tsv`: shared prefixes - `term.tsv`: terminology I'll commit my work in progress: $ git add src/prefix.tsv src/term.tsv $ git add --update $ git commit -m \"Add and apply prefix and term tables\" Now all the terms are linked to controlled vocabularies of one sort or another. If I want to see the IDs for those links instead of the \"codes\" I can define a VIEW: CREATE VIEW linked_data_id AS SELECT assay_datetime, investigator_term.id AS investigator, subject_id, species_term.id AS species, strain_term.id AS strain, sex_term.id AS sex, group_id, protocol_term.id AS protocol, organ_term.id AS organ, disease_term.id AS disease, qualifier_term.id AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: $ sqlite3 build/data.db <<< \"SELECT * FROM linked_ids LIMIT 1;\" 2014-01-01 10:21:00-0500|orcid:0000-0001-5139-5557|ex:subject-12|obo:NCBITaxon_10116|ex:F344N|obo:PATO_0000383|ex:group-1|obo:OBI_0600020|obo:UBERON_0002048|obo:MPATH_268|obo:PATO_0000396 I can also define a similar view for their \"official\" labels: CREATE VIEW linked_data_label AS SELECT assay_datetime, investigator_term.label AS investigator, subject_id, species_term.label AS species, strain_term.label AS strain, sex_term.label AS sex, group_id, protocol_term.label AS protocol, organ_term.label AS organ, disease_term.label AS disease, qualifier_term.label AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: $ sqlite3 build/data.db <<< \"SELECT * FROM linked_data_label LIMIT 1;\" 2014-01-01 10:21:00-0500|James A. Overton|ex:subject-12|Rattus norvegicus|F 344/N|female|ex:group-1|histology|lung|adenocarcinoma|severe intensity I'll commit my work in progress: $ git add --update $ git commit -m \"Add linked_data tables\" Now the tables use URLs and is connected to ontologies and stuff. But are we Linked yet? 7. Getting Triples \u00b6 SQL tables aren't an official Linked Data format. Of all the RDF formats, I prefer Turtle. It's tedious but not difficult to get Turtle out of SQL. These query do what I need them to do, but note that if the literal data contained quotation marks (for instance) then I'd have to do more work to escape those. First I create a triple table: CREATE TABLE triple ( subject TEXT, predicate TEXT, object TEXT, literal INTEGER -- 0 for object IRI, 1 for object literal ); -- create triples from term table INSERT INTO triple(subject, predicate, object, literal) SELECT id, 'rdfs:label', label, 1 FROM term; -- create triples from data table INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-assay_datetime', assay_datetime, 1 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-investigator', term.id, 0 FROM data JOIN term AS term ON data.investigator = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-subject_id', subject_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-species', term.id, 0 FROM data JOIN term AS term ON data.species = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-strain', term.id, 0 FROM data JOIN term AS term ON data.strain = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-sex', term.id, 0 FROM data JOIN term AS term ON data.sex = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-group_id', group_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-protocol', term.id, 0 FROM data JOIN term AS term ON data.protocol = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-organ',term.id, 0 FROM data JOIN term AS term ON data.organ= term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-disease', term.id, 0 FROM data JOIN term AS term ON data.disease = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-qualifier', term.id, 0 FROM data JOIN term AS term ON data.qualifier = term.code; Then I can turn triples into Turtle using string concatenation: SELECT '@prefix ' || prefix || ': <' || base || '> .' FROM prefix UNION ALL SELECT '' UNION ALL SELECT subject || ' ' || predicate || ' ' || CASE literal WHEN 1 THEN '\"' || object || '\"' ELSE object END || ' . ' FROM triple; I can add this to the src/build.sh : sqlite3 build/data.db < src/turtle.sql > build/data.ttl Here's just a bit of that build/data.ttl file: @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . orcid:0000-0001-5139-5557 rdfs:label \"James A. Overton\" . assay:1 column:assay_datetime \"2014-01-01 10:21:00-0500\"^^xsd:datetime . assay:1 column:investigator orcid:0000-0001-5139-5557 . SQL is not a particularly expressive language. Building the triple table is straightforward but verbose. I could have done the same thing with much less Python code. (Or I could have been clever and generated some SQL to execute!) I'll commit my work in progress: $ git add src/turtle.sql $ git add --update $ git commit -m \"Convert to Turtle\" So technically I have a Turtle file. Linked Data! Right? Well, it's kind of \"flat\". It still looks more like a table than a graph. 8. Getting Linked \u00b6 The table I started with is very much focused on the data: there was some sort of assay done, and this is the information that someone recorded about it. The Turtle I just ended up with is basically the same. Other people may have assay data. They may have tables that they converted into Turtle. So can I just merge them? Technically yes: I can put all these triples in one graph together. But I'll still just have \"flat\" chunks of data representing rows sitting next to other rows, without really linking together. The next thing I would do with this data is reorganized it based on the thing it's talking about. I know that: there was an assay the assay was performed at a certain time, using a certain protocol there was a person who performed the assay there was a subject animal of some species, strain, and sex the subject animal belonged to a study group the subject animal had some organs the assay resulted in some measurements Most of these are things that I could point to in the world, or could have pointed to if I was in the right place at the right time. By thinking about these things, I'm stepping beyond what it was convenient for someone to record, and thinking about what happened in the world. If somebody else has some assay data, then they might have recorded it differently for whatever reason, and so it wouldn't line up with my rows. I'm trying my best to use the same terms for the same things. I also want to use the same \"shapes\" for the same things. When trying to come to an agreement about what is connected to what, life is easier if I can point to the things I want to talk about: \"See, here is the person, and the mouse came from here, and he did this and this.\" I could model the data in SQL by breaking the big table into smaller tables. I could have tables for: person group subject: species, strain, sex, group assay: date, investigator, subject, protocol measurement: assay, organ, disease, qualifier Then I would convert each table to triples more carefully. That's a good idea. Actually it's a better idea than what I'm about to do... Since we're getting near the end, I'm going to show you how you can do that modelling in SPARQL. SPARQL has a CONSTRUCT operation that you use to build triples. There's lots of tools that I could use to run SPARQL but I'll use ROBOT . I'll start with the \"flat\" triples in build/data.ttl , select them with my WHERE clause, then CONSTRUCT better triples, and save them in build/model.ttl . PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX obo: <http://purl.obolibrary.org/obo/> PREFIX ex: <https://example.com/> CONSTRUCT { ?group rdfs:label ?group_label . ?subject rdf:type ?species ; rdfs:label ?subject_label ; ex:strain ?strain ; obo:RO_0000086 ?sex ; # has quality ex:group ?group . ?sex rdf:type ?sex_type ; rdfs:label ?sex_label . ?organ rdf:type ?organ_type ; rdfs:label ?organ_label ; obo:BFO_0000050 ?subject . # part of ?assay rdf:type ?assay_type ; rdfs:label ?assay_label ; obo:OBI_0000293 ?subject ; # has specified input obo:IAO_0000136 ?organ . # is about } WHERE { ?subject_row ex:column-assay_datetime ?datetime ; ex:column-investigator ?investigator ; ex:column-subject_id ?subject ; ex:column-species ?species ; ex:column-sex ?sex_type ; ex:column-group_id ?group ; ex:column-protocol ?assay_type ; ex:column-organ ?organ_type ; ex:column-disease ?disease ; ex:column-qualifier ?qualifier . ?assay_type rdfs:label ?assay_type_label . ?sex_type rdfs:label ?sex_type_label . ?organ_type rdfs:label ?organ_type_label . BIND (URI(CONCAT(STR(?subject), \"-assay\")) AS ?assay) BIND (URI(CONCAT(STR(?subject), \"-sex\")) AS ?sex) BIND (URI(CONCAT(STR(?subject), \"-organ\")) AS ?organ) BIND (CONCAT(\"subject \", REPLACE(STR(?subject), \"^.*-\", \"\")) AS ?subject_label) BIND (CONCAT(\"group \", REPLACE(STR(?group), \"^.*-\", \"\")) AS ?group_label) BIND (CONCAT(?subject_label, \" \", ?assay_type_label) AS ?assay_label) BIND (CONCAT(?subject_label, \" sex: \", ?sex_type_label) AS ?sex_label) BIND (CONCAT(?subject_label, \" \", ?organ_type_label) AS ?organ_label) } I can add this to the src/build.sh : java -jar robot.jar query \\ --input build/data.ttl \\ --query src/model.rq build/model.ttl Then I get build/model.ttl that looks (in part) like this: ex:subject-31 a obo:NCBITaxon_10090 ; rdfs:label \"subject 31\" ; obo:RO_0000086 ex:subject-31-sex ; ex:group ex:group-2 . ex:group-2 rdfs:label \"group 2\" . Now that's what I call Linked Data! I'll update the README: ## Modelling The data refers to: - investigator - subject - group - assay - measurement data - subject organ - disease TODO: A pretty diagram. I'll commit my work in progress: $ git add src/model.rq $ git add --update $ git commit -m \"Build model.ttl\" 9. Getting It Done \u00b6 That was a lot of work for a small table. And I did all the hard work of mapping the terminology to ontology terms for you! There's lots more I can do. The SPARQL is just one big chunk, but it would be better in smaller pieces. The modelling isn't all that great yet. Before changing that I want to run it past the boss and see what she thinks. It's getting close to the end of the day. Before I quit I should update the README, clean up anything that's no longer relevant or correct, and make any necessary notes to my future self: $ git add --update $ git commit -m \"Update README\" $ quit","title":"From Tables to Linked Data"},{"location":"tutorial/linking-data/#tutorial-from-tables-to-linked-data","text":"These are the kinds of things that I do when I need to work with a new dataset. My goal is to have data that makes good sense and that I can integrate with other data using standard technologies: Linked Data.","title":"Tutorial: From Tables to Linked Data"},{"location":"tutorial/linking-data/#0-before","text":"The boss just sent me this new table to figure out: datetime investigator subject species strain sex group protocol organ disease qualifier comment 1/1/14 10:21 AM JAO 12 RAT F 344/N FEMALE 1 HISTOPATHOLOGY LUNG ADENOCARCINOMA SEVERE 1/1/14 10:30 AM JO 31 MOUSE B6C3F1 MALE 2 HISTOPATHOLOGY NOSE INFLAMMATION MILD 1/1/14 10:45 AM JAO 45 RAT F 344/N MALE 1 HISTOPATHOLOGY ADRENAL CORTEX NECROSIS MODERATE It doesn't seem too bad, but there's lots of stuff that I don't quite understand. Where to start?","title":"0. Before"},{"location":"tutorial/linking-data/#1-getting-organized","text":"Before I do anything else, I'm going to set up a new project for working with this data. Maybe I'll change my mind later and want to merge the new project with an existing project, but it never hurts to start from a nice clean state. I'll make a new directory in a sensible place with a sensible name. In my case I have a ~/Repositories/ directory, with subdirectories for GitHub and various GitLab servers, a local directory for projects I don't plan to share, and a temp directory for projects that I don't need to keep. I'm not sure if I'm going to share this work, so it can go in a new subdirectory of local . I'll call it \"linking-data-tutorial\" for now. Then I'll run git init to turn that directory into a git repository. For now I'm just going to work locally, but later I can make a repository on GitHub and push my local repository there. Next I'll create a README.md file where I'll keep notes for myself to read later. My preferred editor is Kakoune . So I'll open a terminal and run these commands: $ cd ~/Repositories/local/ $ mkdir linking-data-tutorial $ cd linking-data-tutorial $ git init $ kak README.md In the README I'll start writing something like this: # Linking Data Tutorial An example of how to convert a dataset to Linked Data. The source data is available from <https://github.com/jamesaoverton/obook/tree/master/03-RDF/data.csv> Maybe this information should go somewhere else eventually, but the README is a good place to start. \"Commit early, commit often\" they say, so: $ git add README.md $ git commit -m \"Initial commit\"","title":"1. Getting Organized"},{"location":"tutorial/linking-data/#2-getting-copies","text":"Data has an annoying tendency to get changed. You don't want it changing out from under you while you're in the middle of something. So the next thing to do is get a copy of the data and store it locally. If it's big, you can store a compressed copy. If it's too big to fit on your local machine, well keep the best notes you can of how to get to the data, and what operations you're doing on it. I'm going to make a cache directory and store all my \"upstream\" data there. I'm going to fetch the data and that's it -- I'm not going to edit these files. When I want to change the data I'll make copies in another directory. I don't want git to track the cached data, so I'll add /cache/ to .gitignore and tell git to track that . Then I'll use curl to download the file. $ mkdir cache $ echo \"/cache/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /cache/ directory\" $ cd cache $ curl -LO \"https://github.com/jamesaoverton/obook/raw/master/03-RDF/data.csv\" $ ls data.csv $ cd .. $ ls -a .gitignore data README.md","title":"2. Getting Copies"},{"location":"tutorial/linking-data/#3-getting-my-bearings","text":"The first thing to do is look at the data. In this case I have just one table in CSV format, so I can use any number of tools to open the file and look around. I bet the majority of people would reach for Excel. My (idiosyncratic) preference is VisiData . What am I looking for? A bunch of different things: what do the rows represent? what columns do I have? for each column, what sorts of values do I have? In my README file I'll make a list of the columns like this: - datetime - investigator - subject - species - strain - sex - group - protocol - organ - disease - qualifier - comment Then I'll make some notes for myself: - datetime: American-style dates, D/M/Y or M/D/Y? - investigator: initials, ORCID? - subject: integer ID - species: common name for species, NCBITaxon? - strain: some sort of code with letters, numbers, spaces, some punctuation - sex: string female/male - group: integer ID - protocol: string, OBI? - organ: string, UBERON? - disease: string, DO/MONDO? - qualifier: string, PATO? - comment: ??? You can see that I'm trying to figure out what's in each column. I'm also thinking ahead to OBO ontologies that I know of that may have terms that I can use for each column.","title":"3. Getting My Bearings"},{"location":"tutorial/linking-data/#4-getting-structured","text":"In the end, I want to have nice, clean Linked Data. But I don't have to get there in one giant leap. Instead I'll take a bunch of small, incremental steps. There's lots of tools I can use, but this time I'll use SQLite. First I'll set up some more directories. I'll create a build directory where I'll store temporary files. I don't want git to track this directory, so I'll add it to .gitignore . $ mkdir build/ $ echo \"/build/\" >> .gitignore $ git add .gitignore $ git commit -m \"Ignore /build/ directory\" I'll also add a src directory to store code. I do want to track src with git. $ mkdir src $ kak src/data.sql In src/data.sql I'll add just enough to import build/data.csv : -- import build/data.csv .mode csv .import build/data.csv data_csv This will create a build/data.db file and import build/data.csv into a data_csv table. Does it work? $ sqlite3 build/data.db < src/data.sql $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|12|RAT|F 344/N|FEMALE|1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| Nice! Note that I didn't even specify a schema for data_csv . It uses the first row as the column names, and the type of every column is TEXT . Here's the schema I end up with: $ sqlite3 build/data.db <<< \".schema data_csv\" CREATE TABLE data_csv( \"datetime\" TEXT, \"investigator\" TEXT, \"subject\" TEXT, \"species\" TEXT, \"strain\" TEXT, \"sex\" TEXT, \"group\" TEXT, \"protocol\" TEXT, \"organ\" TEXT, \"disease\" TEXT, \"qualifier\" TEXT, \"comment\" TEXT ); I'm going to want to update src/data.sql then rebuild the database over and over. It's small, so this will only take a second. If it was big, then I would copy a subset into build/data.csv for now so that I the script still runs in a second or two and I can iterate quickly. I'll write a src/build.sh script to make life a little easier: #!/bin/sh rm -f build/* cp cache/data.csv build/data.csv sqlite3 build/data.db < src/data.sql Does it work? $ sh src/build.sh Nice! Time to update the README: ## Requirements - [SQLite3](https://sqlite.org/index.html) ## Usage Run `sh src/build.sh` I'll commit my work in progress: $ git add src/data.sql src/build.sh $ git add --update $ git commit -m \"Load data.csv into SQLite\" Now I have a script that executes a SQL file that loads the source data into a new database. I'll modify the src/data.sql file in a series of small steps until it has the structure that I want.","title":"4. Getting Structured"},{"location":"tutorial/linking-data/#5-getting-clean","text":"In the real world, data is always a mess. It takes real work to clean it up. And really, it's almost never perfectly clean. It's important to recognize that cleaning data has diminishing returns. There's low hanging fruit: easy to clean, often with code, and bringing big benefits. Then there's tough stuff that requires an expert to work through the details, row by row. The first thing to do is figure out the schema you want. I'll create a new data table and start with the default schema from data_csv . Notice that in the default schema all the column names are quoted. That's kind of annoying. But when I remove the quotation marks I realize that one of the column names is \"datetime\", but datetime is a keyword in SQLite! You can't use it as a column name without quoting. I'll rename it to \"assay_datetime\". I have the same problem with \"group\". I'll rename \"group\" to \"group_id\" and \"subject\" to \"subject_id\". The rest of the column names seem fine. I want \"assay_datetime\" to be in standard ISO datetime format, but SQLite stores these as TEXT. The \"subject\" and \"group\" columns are currently integers, but I plan to make them into URIs to CURIEs. So everything will still be TEXT. CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT ); The dates currently look like \"1/1/14 10:21 AM\". Say I know that they were done on Eastern Standard Time. How do I convert to ISO dates like \"2014-01-01 10:21:00-0500\"? Well SQLite isn't the right tool for this. The Unix date command does a nice job, though: $ date -d \"1/1/14 10:21 AM EST\" +\"%Y-%m-%d %H:%M:%S%z\" 2014-01-01 10:21:00-0500 I can run that over each line of the file using awk . So I update the src/build.sh to rework the build/data.csv before I import: #!/bin/sh rm -f build/* head -n1 cache/data.csv > build/data.csv tail -n+2 cache/data.csv \\ | awk 'BEGIN{FS=\",\"; OFS=\",\"} { \"date -d \\\"\"$1\" EST\\\" +\\\"%Y-%m-%d %H:%M:%S%z\\\"\" | getline $1; print $0 }' \\ >> build/data.csv sqlite3 build/data.db < src/data.sql One more problem I could clean up is that \"JO\" should really be \"JAO\" -- that's just a typo, and they should both refer to James A. Overton. I could make that change in src/build.sh , but I'll do it in src/data.sql instead. I'll write a query to copy all the rows of data_csv into data and then I'll update data with some fixes. -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator=\"JAO\" WHERE investigator=\"JO\"; Honestly, it took me quite a while to write that awk command. It's a very powerful tool, but I don't use it enough to remember how it works. You might prefer to write yourself a Python script, or some R code. You could use that instead of this SQL UPDATE as well. I just wanted to show you two of the thousands of ways to do this. If there's a lot of replacements like \"JO\", then you might also consider listing them in another table that you can read into your script. The important part is to automate your cleaning! Why didn't I just edit cache/data.csv in Excel? In step 2 I saved a copy of the data because I didn't want it to change while I was working on it, but I do expect it to change! By automating the cleaning process, I should be able to just update cache/data.csv run everything again, and the fixes will be applied again. I don't want to do all this work manually every time the upstream data is updated. I'll commit my work in progress: $ git add --update $ git commit -m \"Start cleaning data\" Cleaning can take a lot of work. This is example table is pretty clean already. The next hard part is sorting out your terminology.","title":"5. Getting Clean"},{"location":"tutorial/linking-data/#6-getting-connected","text":"It's pretty easy to convert a table structure to triples. The hard part is converting the table contents . There are some identifiers in the table that would be better as URLs, and there's a bunch of terminology that would be better if it was linked to an ontology or other system. I'll start with the identifiers that are local to this data: subject_id and group_id. I can convert them to URLs by defining a prefix and then just using that prefix. I'll use string concatenation to update the table: -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; Now I'll check my work: $ sqlite3 build/data.db <<< \"SELECT * FROM data_csv LIMIT 1;\" 2014-01-01 10:21:00-0500|JAO|ex:subject-12|RAT|F 344/N|FEMALE|ex:group-1|HISTOPATHOLOGY|LUNG|ADENOCARCINOMA|SEVERE| I should take a moment to tell you, that while I was writing the Turtle conversion code later in this essay, I had to come back here and change these identifiers. The thing is that Turtle is often more strict than I expect about identifier syntax. Turtle identifiers look like CURIEs , but they're actually QNames . CURIEs are pretty much just just URLs shortened with a prefix, so almost anything goes. QNames come from XML , and Turtle identifiers have to be valid XML element names. I always remember that I need to stick to alphanumeric characters, and that I have to replace whitespace and punctuation with a - or _ . I didn't remember that the local part (aka \"suffix\", aka \"NCName\") can't start with a digit. So I tried to use \"subject:12\" and \"group:1\" as my identifiers. That worked fine until I generated Turtle. The Turtle looked fine, so it took me quite a while to figure out why it looked very wrong when I converted it into RDXML format. This kind of thing happens to me all the time. I'm almost always using a mixture of technologies based on different sets of assumptions, and there are always things that don't line up. That's why I like to work in small iterations, checking my work as I go (preferrably with automated tests), and keeping everything in version control. When I need to make a change like this one, I just circle back and iterate again. The next thing is to tackle the terminology. First I'll just make a list of the terms I'm using from the relevant columns in build/term.tsv : ```sh #collect $ sqlite3 build/data.db << EOF > build/term.tsv SELECT investigator FROM data UNION SELECT species FROM data UNION SELECT strain FROM data UNION SELECT strain FROM data UNION SELECT sex FROM data UNION SELECT protocol FROM data UNION SELECT organ FROM data UNION SELECT disease FROM data UNION SELECT qualifier FROM data; EOF It's a lot of work to go through all those terms and find good ontology terms. I'm going to do that hard work for you (just this once!) so we can keep moving. I'll add this table to `src/term.tsv` | id | code | label | | ------------------------- | -------------- | ------------------ | | obo:NCBITaxon_10116 | RAT | Rattus norvegicus | | obo:NCBITaxon_10090 | MOUSE | Mus musculus | | ex:F344N | F 344/N | F 344/N | | ex:B6C3F1 | B6C3F1 | B6C3F1 | | obo:PATO_0000383 | FEMALE | female | | obo:PATO_0000384 | MALE | male | | obo:OBI_0600020 | HISTOPATHOLOGY | histology | | obo:UBERON_0002048 | LUNG | lung | | obo:UBERON_0007827 | NOSE | external nose | | obo:UBERON_0001235 | ADRENAL CORTEX | adrenal cortex | | obo:MPATH_268 | ADENOCARCINOMA | adenocarcinoma | | obo:MPATH_212 | INFLAMMATION | inflammation | | obo:MPATH_4 | NECROSIS | necrosis | | obo:PATO_0000396 | SEVERE | severe intensity | | obo:PATO_0000394 | MILD | mild intensity | | obo:PATO_0000395 | MODERATE | moderate intensity | | orcid:0000-0001-5139-5557 | JAO | James A. Overton | And I'll add these prefixes to `src/prefix.tsv`: | prefix | base | | ------- | ------------------------------------------- | | rdf | http://www.w3.org/1999/02/22-rdf-syntax-ns# | | rdfs | http://www.w3.org/2000/01/rdf-schema# | | xsd | http://www.w3.org/2001/XMLSchema# | | owl | http://www.w3.org/2002/07/owl# | | obo | http://purl.obolibrary.org/obo/ | | orcid | http://orcid.org/ | | ex | https://example.com/ | | subject | https://example.com/subject/ | | group | https://example.com/group/ | Now I can import these tables into SQL and use the term table as a FOREIGN KEY constraint on data: ```sql .mode tabs CREATE TABLE prefix ( prefix TEXT PRIMARY KEY, base TEXT UNIQUE ); .import --skip 1 src/prefix.tsv prefix CREATE TABLE term ( id TEXT PRIMARY KEY, code TEXT UNIQUE, label TEXT UNIQUE ); .import --skip 1 src/term.tsv term CREATE TABLE data( assay_datetime TEXT, investigator TEXT, subject_id TEXT, species TEXT, strain TEXT, sex TEXT, group_id TEXT, protocol TEXT, organ TEXT, disease TEXT, qualifier TEXT, comment TEXT, FOREIGN KEY(investigator) REFERENCES term(investigator), FOREIGN KEY(species) REFERENCES term(species), FOREIGN KEY(strain) REFERENCES term(strain), FOREIGN KEY(sex) REFERENCES term(sex), FOREIGN KEY(protocol) REFERENCES term(protocol), FOREIGN KEY(organ) REFERENCES term(organ), FOREIGN KEY(disease) REFERENCES term(disease), FOREIGN KEY(qualifier) REFERENCES term(qualifier) ); -- copy from data_csv to data INSERT INTO data SELECT * FROM data_csv; -- clean data UPDATE data SET investigator='JAO' WHERE investigator='JO'; -- update subject and groupd IDs UPDATE data SET subject_id='ex:subject-' || subject_id; UPDATE data SET group_id='ex:group-' || group_id; I'll update the README: See `src/` for: - `prefix.tsv`: shared prefixes - `term.tsv`: terminology I'll commit my work in progress: $ git add src/prefix.tsv src/term.tsv $ git add --update $ git commit -m \"Add and apply prefix and term tables\" Now all the terms are linked to controlled vocabularies of one sort or another. If I want to see the IDs for those links instead of the \"codes\" I can define a VIEW: CREATE VIEW linked_data_id AS SELECT assay_datetime, investigator_term.id AS investigator, subject_id, species_term.id AS species, strain_term.id AS strain, sex_term.id AS sex, group_id, protocol_term.id AS protocol, organ_term.id AS organ, disease_term.id AS disease, qualifier_term.id AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: $ sqlite3 build/data.db <<< \"SELECT * FROM linked_ids LIMIT 1;\" 2014-01-01 10:21:00-0500|orcid:0000-0001-5139-5557|ex:subject-12|obo:NCBITaxon_10116|ex:F344N|obo:PATO_0000383|ex:group-1|obo:OBI_0600020|obo:UBERON_0002048|obo:MPATH_268|obo:PATO_0000396 I can also define a similar view for their \"official\" labels: CREATE VIEW linked_data_label AS SELECT assay_datetime, investigator_term.label AS investigator, subject_id, species_term.label AS species, strain_term.label AS strain, sex_term.label AS sex, group_id, protocol_term.label AS protocol, organ_term.label AS organ, disease_term.label AS disease, qualifier_term.label AS qualifier FROM data JOIN term as investigator_term ON data.investigator = investigator_term.code JOIN term as species_term ON data.species = species_term.code JOIN term as strain_term ON data.strain = strain_term.code JOIN term as sex_term ON data.sex = sex_term.code JOIN term as protocol_term ON data.protocol = protocol_term.code JOIN term as organ_term ON data.organ = organ_term.code JOIN term as disease_term ON data.disease = disease_term.code JOIN term as qualifier_term ON data.qualifier = qualifier_term.code; I'll check: $ sqlite3 build/data.db <<< \"SELECT * FROM linked_data_label LIMIT 1;\" 2014-01-01 10:21:00-0500|James A. Overton|ex:subject-12|Rattus norvegicus|F 344/N|female|ex:group-1|histology|lung|adenocarcinoma|severe intensity I'll commit my work in progress: $ git add --update $ git commit -m \"Add linked_data tables\" Now the tables use URLs and is connected to ontologies and stuff. But are we Linked yet?","title":"6. Getting Connected"},{"location":"tutorial/linking-data/#7-getting-triples","text":"SQL tables aren't an official Linked Data format. Of all the RDF formats, I prefer Turtle. It's tedious but not difficult to get Turtle out of SQL. These query do what I need them to do, but note that if the literal data contained quotation marks (for instance) then I'd have to do more work to escape those. First I create a triple table: CREATE TABLE triple ( subject TEXT, predicate TEXT, object TEXT, literal INTEGER -- 0 for object IRI, 1 for object literal ); -- create triples from term table INSERT INTO triple(subject, predicate, object, literal) SELECT id, 'rdfs:label', label, 1 FROM term; -- create triples from data table INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-assay_datetime', assay_datetime, 1 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-investigator', term.id, 0 FROM data JOIN term AS term ON data.investigator = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-subject_id', subject_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-species', term.id, 0 FROM data JOIN term AS term ON data.species = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-strain', term.id, 0 FROM data JOIN term AS term ON data.strain = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-sex', term.id, 0 FROM data JOIN term AS term ON data.sex = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-group_id', group_id, 0 FROM data; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-protocol', term.id, 0 FROM data JOIN term AS term ON data.protocol = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-organ',term.id, 0 FROM data JOIN term AS term ON data.organ= term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-disease', term.id, 0 FROM data JOIN term AS term ON data.disease = term.code; INSERT INTO triple(subject, predicate, object, literal) SELECT 'ex:assay-' || data.rowid, 'ex:column-qualifier', term.id, 0 FROM data JOIN term AS term ON data.qualifier = term.code; Then I can turn triples into Turtle using string concatenation: SELECT '@prefix ' || prefix || ': <' || base || '> .' FROM prefix UNION ALL SELECT '' UNION ALL SELECT subject || ' ' || predicate || ' ' || CASE literal WHEN 1 THEN '\"' || object || '\"' ELSE object END || ' . ' FROM triple; I can add this to the src/build.sh : sqlite3 build/data.db < src/turtle.sql > build/data.ttl Here's just a bit of that build/data.ttl file: @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . orcid:0000-0001-5139-5557 rdfs:label \"James A. Overton\" . assay:1 column:assay_datetime \"2014-01-01 10:21:00-0500\"^^xsd:datetime . assay:1 column:investigator orcid:0000-0001-5139-5557 . SQL is not a particularly expressive language. Building the triple table is straightforward but verbose. I could have done the same thing with much less Python code. (Or I could have been clever and generated some SQL to execute!) I'll commit my work in progress: $ git add src/turtle.sql $ git add --update $ git commit -m \"Convert to Turtle\" So technically I have a Turtle file. Linked Data! Right? Well, it's kind of \"flat\". It still looks more like a table than a graph.","title":"7. Getting Triples"},{"location":"tutorial/linking-data/#8-getting-linked","text":"The table I started with is very much focused on the data: there was some sort of assay done, and this is the information that someone recorded about it. The Turtle I just ended up with is basically the same. Other people may have assay data. They may have tables that they converted into Turtle. So can I just merge them? Technically yes: I can put all these triples in one graph together. But I'll still just have \"flat\" chunks of data representing rows sitting next to other rows, without really linking together. The next thing I would do with this data is reorganized it based on the thing it's talking about. I know that: there was an assay the assay was performed at a certain time, using a certain protocol there was a person who performed the assay there was a subject animal of some species, strain, and sex the subject animal belonged to a study group the subject animal had some organs the assay resulted in some measurements Most of these are things that I could point to in the world, or could have pointed to if I was in the right place at the right time. By thinking about these things, I'm stepping beyond what it was convenient for someone to record, and thinking about what happened in the world. If somebody else has some assay data, then they might have recorded it differently for whatever reason, and so it wouldn't line up with my rows. I'm trying my best to use the same terms for the same things. I also want to use the same \"shapes\" for the same things. When trying to come to an agreement about what is connected to what, life is easier if I can point to the things I want to talk about: \"See, here is the person, and the mouse came from here, and he did this and this.\" I could model the data in SQL by breaking the big table into smaller tables. I could have tables for: person group subject: species, strain, sex, group assay: date, investigator, subject, protocol measurement: assay, organ, disease, qualifier Then I would convert each table to triples more carefully. That's a good idea. Actually it's a better idea than what I'm about to do... Since we're getting near the end, I'm going to show you how you can do that modelling in SPARQL. SPARQL has a CONSTRUCT operation that you use to build triples. There's lots of tools that I could use to run SPARQL but I'll use ROBOT . I'll start with the \"flat\" triples in build/data.ttl , select them with my WHERE clause, then CONSTRUCT better triples, and save them in build/model.ttl . PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX obo: <http://purl.obolibrary.org/obo/> PREFIX ex: <https://example.com/> CONSTRUCT { ?group rdfs:label ?group_label . ?subject rdf:type ?species ; rdfs:label ?subject_label ; ex:strain ?strain ; obo:RO_0000086 ?sex ; # has quality ex:group ?group . ?sex rdf:type ?sex_type ; rdfs:label ?sex_label . ?organ rdf:type ?organ_type ; rdfs:label ?organ_label ; obo:BFO_0000050 ?subject . # part of ?assay rdf:type ?assay_type ; rdfs:label ?assay_label ; obo:OBI_0000293 ?subject ; # has specified input obo:IAO_0000136 ?organ . # is about } WHERE { ?subject_row ex:column-assay_datetime ?datetime ; ex:column-investigator ?investigator ; ex:column-subject_id ?subject ; ex:column-species ?species ; ex:column-sex ?sex_type ; ex:column-group_id ?group ; ex:column-protocol ?assay_type ; ex:column-organ ?organ_type ; ex:column-disease ?disease ; ex:column-qualifier ?qualifier . ?assay_type rdfs:label ?assay_type_label . ?sex_type rdfs:label ?sex_type_label . ?organ_type rdfs:label ?organ_type_label . BIND (URI(CONCAT(STR(?subject), \"-assay\")) AS ?assay) BIND (URI(CONCAT(STR(?subject), \"-sex\")) AS ?sex) BIND (URI(CONCAT(STR(?subject), \"-organ\")) AS ?organ) BIND (CONCAT(\"subject \", REPLACE(STR(?subject), \"^.*-\", \"\")) AS ?subject_label) BIND (CONCAT(\"group \", REPLACE(STR(?group), \"^.*-\", \"\")) AS ?group_label) BIND (CONCAT(?subject_label, \" \", ?assay_type_label) AS ?assay_label) BIND (CONCAT(?subject_label, \" sex: \", ?sex_type_label) AS ?sex_label) BIND (CONCAT(?subject_label, \" \", ?organ_type_label) AS ?organ_label) } I can add this to the src/build.sh : java -jar robot.jar query \\ --input build/data.ttl \\ --query src/model.rq build/model.ttl Then I get build/model.ttl that looks (in part) like this: ex:subject-31 a obo:NCBITaxon_10090 ; rdfs:label \"subject 31\" ; obo:RO_0000086 ex:subject-31-sex ; ex:group ex:group-2 . ex:group-2 rdfs:label \"group 2\" . Now that's what I call Linked Data! I'll update the README: ## Modelling The data refers to: - investigator - subject - group - assay - measurement data - subject organ - disease TODO: A pretty diagram. I'll commit my work in progress: $ git add src/model.rq $ git add --update $ git commit -m \"Build model.ttl\"","title":"8. Getting Linked"},{"location":"tutorial/linking-data/#9-getting-it-done","text":"That was a lot of work for a small table. And I did all the hard work of mapping the terminology to ontology terms for you! There's lots more I can do. The SPARQL is just one big chunk, but it would be better in smaller pieces. The modelling isn't all that great yet. Before changing that I want to run it past the boss and see what she thinks. It's getting close to the end of the day. Before I quit I should update the README, clean up anything that's no longer relevant or correct, and make any necessary notes to my future self: $ git add --update $ git commit -m \"Update README\" $ quit","title":"9. Getting It Done"},{"location":"tutorial/managing-dynamic-imports-odk/","text":"Managing Dynamic Imports with the Ontology Development Kit \u00b6 In this tutorial, we discuss the general workflow of managing dynamic imports, i.e. importing terms from other ontologies which can be kept up to date. Tutorial \u00b6 Follow instructions for the PATO dynamic import process here .","title":"ODK - Managing imports"},{"location":"tutorial/managing-dynamic-imports-odk/#managing-dynamic-imports-with-the-ontology-development-kit","text":"In this tutorial, we discuss the general workflow of managing dynamic imports, i.e. importing terms from other ontologies which can be kept up to date.","title":"Managing Dynamic Imports with the Ontology Development Kit"},{"location":"tutorial/managing-dynamic-imports-odk/#tutorial","text":"Follow instructions for the PATO dynamic import process here .","title":"Tutorial"},{"location":"tutorial/managing-ontology-project/","text":"Tutorial on Managing OBO Ontology Projects \u00b6 This tutorial is not about editing ontologies and managing the evolution of its content (aka ontology curation), but the general process of managing an ontology project overall. In this lesson, we will cover the following: How to effectively manage an ontology project using GitHub projects and teams How to coordinate the evolution of ontologies across projects and grants It is important to understand that the following is just one good way of doing project management for OBO ontologies, and most projects will do it slightly differently. We do however believe that thinking about your project management process and the roles involved will benefit your work in the long term, and hope that the following will help you as a starting point. Roles in OBO Ontology project management activities \u00b6 Ontology Editor (OE): manage the content of ontologies and interact with users Principal Ontology Editor (POE): coordinate the curation activities and have always fixed hours assigned to the project. Ontology Pipeline Developer (OPD): Manage the technical workflows around ontologies, such as release workflows, continuous integration and QC, and setting up data pipelines. Also helps with bulk editing activities. Principal Investigators (PI): Manage the projects that fund ontology curation activities. For an effective management of an ontology, the following criteria are recommended: There should be at least one Principal Ontology Editor for every ontology project. The importance is not whether this editor (or sometimes called 'ontology curator') has a specific number of hours per week allocated to the project (although based on our experience, 1 day per week is minimum), but whether the editor has a sense of ownership, i.e. they understand that they are the primary responsible person for maintaining the ontology moving forward. Because of potential grant overlapping issues, we recommend to have at least 1 Principal Ontology Editor for every grant/funded project that has a stake in the ontology. Every effective ontology needs at least a few hours per week from an Ontology Pipeline Developer (OPD). More on that role later. The OPD does not always have as strong a sense of ownership of the ontology project, but typically has a strong sense of responsibility to members of the curation team. There should be separate meetings for curation and technical activities - both problems are hard, and need different team members being present. We recommend at least monthly technical and biweekly curation calls, but for many of the most effective ontology projects we manage, weekly technical and weekly curation calls are normal. Without the above minimum criteria, the following recommendations will be very hard to implement. The Project Management Toolbox \u00b6 We make use of three tools in the following recommendation: Project boards : Project boards, sometimes referred to as Kanban boards, GitHub boards or agile boards, are a great way to organise outstanding tickets and help maintain a clear overview of what work needs to be done. They are usually realised with either GitHub projects or ZenHub . If you have not worked with project boards before, we highly recommend watching a quick tutorial on Youtube, such as: GitHub teams . GitHub teams , alongside with organisations, are a powerfull too to organise collaborative workflows on GitHub. They allow you to communicate and organise permissions for editing your ontology in a transparent way. You can get a sense of GitHub teams by watching one of the the numerous tutorials on GitHub, such as: Markdown-based documentation system . Writing great documentation is imperative for a sustainable project. Across many of our recent projects, were are using mkdocs , which we have also integrated with the Ontology Development Kit, but there are others to consider. We deeply recommend to complete a very short introduction to Markdown, this tutorial on YouTube . What do you need for your project? \u00b6 Every ontology or group of related ontologies (sometimes it is easier to manage multiple ontologies at once, because their scope or technical workflows are quite uniform or they are heavily interrelated) should have: at least two teams , an Editorial Team and a Technical Team, with clearly defined members. We recommend to create two teams on GitHub and keep their members always up to date (i.e. remove members that are not actively participating), but many of our projects merely maintain a \"core team\", which is a more liberal team containing everyone from stakeholders, principal investigators, editors and users (for managing write permissions see later in the \"best practice\" section) and listing the members of the Editorial and Technical Teams on a page in the documentation ( example ). Note that it is a good idea to be careful of who on your team has \"admin\" rights on your repo, so sometimes, a distinct \"admin\" team can be very helpful. Admins are allowed to do \"dangerous\" things like deleting the repository. two distinct project boards . We recommend two distinct project boards, one for the Curation/Editorial Team, and one for the Technical Team. The details on how to design the boards is up to the respective teams, but we found a simple 4 stage board with sections for To Do (issues that are important but not urgent), Priority (issues that are important and urgent), In Progress (issues that are being worked on) and Under review (issues that need review). From years of experience with project boards, we recommend against the common practice of keeping a Backlog column (issues that are neither important nor urgent nor likely to be addressed in the next 6 months), nor a Done column (to keep track of closed issues) - they just clutter the view. A documentation system (often realised using mkdocs in OBO projects) with a page listing the members of the team ( example ). This page should provide links to all related team pages from Github and their project boards, as well as a table listing all current team members with the following information: Name ORCiD Funding Information Allocated FTEs (0 if on volunteering basis) Associated teams Role Responsibilities (What kind of issues can they be assigned to review? How are they involved in the Project?) Responsibilities \u00b6 Effective Ontology Pipeline Developers (OPDs) are extremely rare and are typically active across many different projects. Therefore their attention is scattered . Understanding and accepting this is key for the following points. Principal Investigators explicitly assign target weekly hours for Ontology Editors and Ontology Pipeline Developers to the project. These should be captured on the documentation systems team page (see above). The Ontology Editors are responsible for the entire Curation Team Board and the To Do and Priority columns of the Technical Team. The later is important: it is the job of the curation team to prioritise the technical issues . The Technical Team can add tickets to the To Do and Priority columns, but this usually happens only in response to a request from the Curation Team. When the technical team meets, the Principal Ontology Editor(s) (POE) are present, i.e. the POEs are members of the technical team as well . They will help clarifying the Priority tickets. The Technical Team is responsible to assign issues and reviewers among themselves (ideally, the reviewer should be decided at the same time the issue is assigned) move issues from the Priority to the In Progress and later to the Done section. communicate through the POE to the PIs when resources are insufficient to address Priority issues. The Principal Ontology Editor is responsible for ensuring that new issues on the issue tracker are dealt with. Usually this happens in the following ways: They ensure that each external issue (i.e. an issue from anyone outside the core team) is (a) responded to in a polite manner and (b) assigned to someone appropriate or politely rejected due to lack of resources. They ensure that each internal issue is assigned to the person that made them. No issue should appear unassigned. The ensure that pull requests are (a) assigned to someone to handle and (b) merged in a timely manner. Too many open PRs cause problems with conflicts. Best Practices \u00b6 The To Do issues should first be moved to the Priority section before being addressed. This prevents focusing on easy to solve tickets in favour of important ones. Even if Google Docs are used to manage team meetings, at the end of each meetings all open issues must be captured as GitHub tickets and placed in the appropriate box on the board. We recommend that Backlog items are not added at all to the board - if they ever become important, they tend to resurface all by themselves. The single most important point of failure is the absence of an Principal Ontology Editor with a strong sense of ownership . This should be the projects priority to determine first. All new members of the project should undergo an onboarding. It is a good idea to prepare walkthroughs of the project (as video or pages in the documentation system) covering everything from Curation to Technical and Project Management. The Principal Ontology Editor responsible for dealing with external issues should be named explicitly on the team page. We recommend the following practices for write permissions: The main (formerly master ) branch should be write protected with suitable rules. For example, requiring QC to pass and 1 approving review as a minimum. The curation and technical teams are mainly for social organisation, they do not have to physically exist. However, having a small team with \"admin rights\" and a team (e.g. the core team mentioned above) with \"write\" rights greatly helps with organising the permissions in a transparent manner.","title":"Managing OBO ontology projects"},{"location":"tutorial/managing-ontology-project/#tutorial-on-managing-obo-ontology-projects","text":"This tutorial is not about editing ontologies and managing the evolution of its content (aka ontology curation), but the general process of managing an ontology project overall. In this lesson, we will cover the following: How to effectively manage an ontology project using GitHub projects and teams How to coordinate the evolution of ontologies across projects and grants It is important to understand that the following is just one good way of doing project management for OBO ontologies, and most projects will do it slightly differently. We do however believe that thinking about your project management process and the roles involved will benefit your work in the long term, and hope that the following will help you as a starting point.","title":"Tutorial on Managing OBO Ontology Projects"},{"location":"tutorial/managing-ontology-project/#roles-in-obo-ontology-project-management-activities","text":"Ontology Editor (OE): manage the content of ontologies and interact with users Principal Ontology Editor (POE): coordinate the curation activities and have always fixed hours assigned to the project. Ontology Pipeline Developer (OPD): Manage the technical workflows around ontologies, such as release workflows, continuous integration and QC, and setting up data pipelines. Also helps with bulk editing activities. Principal Investigators (PI): Manage the projects that fund ontology curation activities. For an effective management of an ontology, the following criteria are recommended: There should be at least one Principal Ontology Editor for every ontology project. The importance is not whether this editor (or sometimes called 'ontology curator') has a specific number of hours per week allocated to the project (although based on our experience, 1 day per week is minimum), but whether the editor has a sense of ownership, i.e. they understand that they are the primary responsible person for maintaining the ontology moving forward. Because of potential grant overlapping issues, we recommend to have at least 1 Principal Ontology Editor for every grant/funded project that has a stake in the ontology. Every effective ontology needs at least a few hours per week from an Ontology Pipeline Developer (OPD). More on that role later. The OPD does not always have as strong a sense of ownership of the ontology project, but typically has a strong sense of responsibility to members of the curation team. There should be separate meetings for curation and technical activities - both problems are hard, and need different team members being present. We recommend at least monthly technical and biweekly curation calls, but for many of the most effective ontology projects we manage, weekly technical and weekly curation calls are normal. Without the above minimum criteria, the following recommendations will be very hard to implement.","title":"Roles in OBO Ontology project management activities"},{"location":"tutorial/managing-ontology-project/#the-project-management-toolbox","text":"We make use of three tools in the following recommendation: Project boards : Project boards, sometimes referred to as Kanban boards, GitHub boards or agile boards, are a great way to organise outstanding tickets and help maintain a clear overview of what work needs to be done. They are usually realised with either GitHub projects or ZenHub . If you have not worked with project boards before, we highly recommend watching a quick tutorial on Youtube, such as: GitHub teams . GitHub teams , alongside with organisations, are a powerfull too to organise collaborative workflows on GitHub. They allow you to communicate and organise permissions for editing your ontology in a transparent way. You can get a sense of GitHub teams by watching one of the the numerous tutorials on GitHub, such as: Markdown-based documentation system . Writing great documentation is imperative for a sustainable project. Across many of our recent projects, were are using mkdocs , which we have also integrated with the Ontology Development Kit, but there are others to consider. We deeply recommend to complete a very short introduction to Markdown, this tutorial on YouTube .","title":"The Project Management Toolbox"},{"location":"tutorial/managing-ontology-project/#what-do-you-need-for-your-project","text":"Every ontology or group of related ontologies (sometimes it is easier to manage multiple ontologies at once, because their scope or technical workflows are quite uniform or they are heavily interrelated) should have: at least two teams , an Editorial Team and a Technical Team, with clearly defined members. We recommend to create two teams on GitHub and keep their members always up to date (i.e. remove members that are not actively participating), but many of our projects merely maintain a \"core team\", which is a more liberal team containing everyone from stakeholders, principal investigators, editors and users (for managing write permissions see later in the \"best practice\" section) and listing the members of the Editorial and Technical Teams on a page in the documentation ( example ). Note that it is a good idea to be careful of who on your team has \"admin\" rights on your repo, so sometimes, a distinct \"admin\" team can be very helpful. Admins are allowed to do \"dangerous\" things like deleting the repository. two distinct project boards . We recommend two distinct project boards, one for the Curation/Editorial Team, and one for the Technical Team. The details on how to design the boards is up to the respective teams, but we found a simple 4 stage board with sections for To Do (issues that are important but not urgent), Priority (issues that are important and urgent), In Progress (issues that are being worked on) and Under review (issues that need review). From years of experience with project boards, we recommend against the common practice of keeping a Backlog column (issues that are neither important nor urgent nor likely to be addressed in the next 6 months), nor a Done column (to keep track of closed issues) - they just clutter the view. A documentation system (often realised using mkdocs in OBO projects) with a page listing the members of the team ( example ). This page should provide links to all related team pages from Github and their project boards, as well as a table listing all current team members with the following information: Name ORCiD Funding Information Allocated FTEs (0 if on volunteering basis) Associated teams Role Responsibilities (What kind of issues can they be assigned to review? How are they involved in the Project?)","title":"What do you need for your project?"},{"location":"tutorial/managing-ontology-project/#responsibilities","text":"Effective Ontology Pipeline Developers (OPDs) are extremely rare and are typically active across many different projects. Therefore their attention is scattered . Understanding and accepting this is key for the following points. Principal Investigators explicitly assign target weekly hours for Ontology Editors and Ontology Pipeline Developers to the project. These should be captured on the documentation systems team page (see above). The Ontology Editors are responsible for the entire Curation Team Board and the To Do and Priority columns of the Technical Team. The later is important: it is the job of the curation team to prioritise the technical issues . The Technical Team can add tickets to the To Do and Priority columns, but this usually happens only in response to a request from the Curation Team. When the technical team meets, the Principal Ontology Editor(s) (POE) are present, i.e. the POEs are members of the technical team as well . They will help clarifying the Priority tickets. The Technical Team is responsible to assign issues and reviewers among themselves (ideally, the reviewer should be decided at the same time the issue is assigned) move issues from the Priority to the In Progress and later to the Done section. communicate through the POE to the PIs when resources are insufficient to address Priority issues. The Principal Ontology Editor is responsible for ensuring that new issues on the issue tracker are dealt with. Usually this happens in the following ways: They ensure that each external issue (i.e. an issue from anyone outside the core team) is (a) responded to in a polite manner and (b) assigned to someone appropriate or politely rejected due to lack of resources. They ensure that each internal issue is assigned to the person that made them. No issue should appear unassigned. The ensure that pull requests are (a) assigned to someone to handle and (b) merged in a timely manner. Too many open PRs cause problems with conflicts.","title":"Responsibilities"},{"location":"tutorial/managing-ontology-project/#best-practices","text":"The To Do issues should first be moved to the Priority section before being addressed. This prevents focusing on easy to solve tickets in favour of important ones. Even if Google Docs are used to manage team meetings, at the end of each meetings all open issues must be captured as GitHub tickets and placed in the appropriate box on the board. We recommend that Backlog items are not added at all to the board - if they ever become important, they tend to resurface all by themselves. The single most important point of failure is the absence of an Principal Ontology Editor with a strong sense of ownership . This should be the projects priority to determine first. All new members of the project should undergo an onboarding. It is a good idea to prepare walkthroughs of the project (as video or pages in the documentation system) covering everything from Curation to Technical and Project Management. The Principal Ontology Editor responsible for dealing with external issues should be named explicitly on the team page. We recommend the following practices for write permissions: The main (formerly master ) branch should be write protected with suitable rules. For example, requiring QC to pass and 1 approving review as a minimum. The curation and technical teams are mainly for social organisation, they do not have to physically exist. However, having a small team with \"admin rights\" and a team (e.g. the core team mentioned above) with \"write\" rights greatly helps with organising the permissions in a transparent manner.","title":"Best Practices"},{"location":"tutorial/managing-ontology-releases-odk/","text":"Managing Ontology Releases with the Ontology Development Kit \u00b6 In this tutorial, we discuss the general workflow of ontology releases. Tutorial \u00b6 Follow instructions for the PATO release process here .","title":"ODK - Managing ontology releases"},{"location":"tutorial/managing-ontology-releases-odk/#managing-ontology-releases-with-the-ontology-development-kit","text":"In this tutorial, we discuss the general workflow of ontology releases.","title":"Managing Ontology Releases with the Ontology Development Kit"},{"location":"tutorial/managing-ontology-releases-odk/#tutorial","text":"Follow instructions for the PATO release process here .","title":"Tutorial"},{"location":"tutorial/migrating-ontology-to-odk/","text":"Migrating your old Ontology Release System to the Ontology Development Kit \u00b6 Content TBP, recording exists on request.","title":"ODK - Migrating to ODK"},{"location":"tutorial/migrating-ontology-to-odk/#migrating-your-old-ontology-release-system-to-the-ontology-development-kit","text":"Content TBP, recording exists on request.","title":"Migrating your old Ontology Release System to the Ontology Development Kit"},{"location":"tutorial/odk-tutorial-2/","text":"ODK in 20 minutes: a complete walk through the core workflows \u00b6 The goal of this tutorial is to quickly showcase key ODK workflows . It is not geared at explaining individual steps in detail. For a much more detailed tutorial for creating a fresh ODK repo, see here for a tutorial for setting up your first workflow . We recommend to complete this tutorial before attempting this one. Tutorial \u00b6 Seeding a new ontology repo Import workflow Integration Testing Release workflow Customisable documentation Seeding \u00b6 Create a new folder. Download an example ODK config or create one yourself. Save it in the directory created above. Important : in the cato-odk.yaml change github_org to your GitHub username. If you dont do this, some ODK features wont work perfectly, like documentation. github_org: matentzn repo: cat-ontology Run the ODK seeding script. curl https://raw.githubusercontent.com/INCATools/ontology-development-kit/v1.3.1/seed-via-docker.sh | bash -s -- --clean -C cato-odk.yaml Push the newly created repo to GitHub (for example with GitHub Desktop). The import workflow \u00b6 Let us now import planned process: Open the term file src/ontology/imports/cob_terms.txt in your favourite text editor Add COB:0000082 to the term file (this is the planned process class in COB). From within the src/ontology directory, run sh run.sh make refresh-cob . Inspect the diff. Rather than importing just one term, it seems that we have important a whole bunch. This is because by default, ODK is using the SLME module extraction technique, which ensures that not only the terms we explicitly request are imported - but all the logically dependent ones as well . In src/ontology/cato-odk.yaml , locate the entry for importing cob and switch it to a different module type: filter . import_group: products: - id: ro - id: cob module_type: filter Run sh run.sh make update_repo to apply the changes. Check out the git diff to the Makefile to convince yourself that the new extraction method has been applied. Let us refresh the COB import again: From within the src/ontology directory, run sh run.sh make refresh-cob . Convince yourself that now only the planned process term is imported. Integration Testing \u00b6 Switch to a new git branch and commit your changes to Makefile , cato-odk.yaml , imports/cob_terms.txt and imports/cob_import.owl . Push the branch and create a Pull Request. After a few seconds, the automated testing should start: Feel free to click on details to see what is happening! Once the test passes (turns green) the PR is ready to be reviewed. Since this tutorial is for illustration purposes only, we just merge. The release workflow \u00b6 Great, we have done our change, now we are ready to make a release! Switch to the main branch in git . Make sure you you pull all changes ( git pull ). In src/ontology execute the release workflow: sh run.sh make prepare_release_fast (we are using fast release here which skips refreshing imports again - we just did that). Inspect the changes. You should see that the planned process class has been added to all ontology release artefacts. Create a branch and commit the changes. Push. Create pull request. Request review (skipped in this tutorial). Wait for QC to pass. Merge. On GitHub (repository front page), click on \"Create a new release\". In the next Window in the \"Choose a tag\" field select v2022-09-01 . Note the leading v . Select the correct date (the date you made the release, YYYY-MM-dd ). Fill in all the other form elements as you see fit. Click Publish release . Customisable documentation \u00b6 With our ODK setup, we also have a completely customisable documentation system installed. We just need to do a tiny change to the GitHub pages settings: On your GitHub repo page, click on \"Settings\". In the menu on the left, click on \"Pages\". On the right, under Build and deployment select Deploy from branch . Underneath, select gg-pages as the branch (this is where ODK deploys to), and /(root) as the directory. Hit Save . Wait for about 4 minutes for the site to be deployed (you can click on Actions in the main menu to follow the build process). Go back to the Pages section in Settings . You should see a button Visit site . Click on it. If everything went correctly, you should see your new page: Let's make a quick change: On the main page, click on the pen in the top right corner (this only works if you have correctly configured your github_org , see seeding ). If you have not configured your repo, go to the GitHub front page of your repo, go into the docs directory, click on index.md and edit it from here. Make a small random edit. Commit the change straight to main or do it properly, create a branch, PR, ask for reviews, merge. After the ODK updates your site, you should be able to see your changes reflected on the life site! Summary \u00b6 That's it! In about 20 minutes, we Seeded a new ontology repo. Imported a term. Made a pull request and watched the ODK Testing framework at work. Ran a release. Deployed customisable documentation pages to help our users and curators documenting processes and use instructions.","title":"ODK - 20 minute complete walk-through"},{"location":"tutorial/odk-tutorial-2/#odk-in-20-minutes-a-complete-walk-through-the-core-workflows","text":"The goal of this tutorial is to quickly showcase key ODK workflows . It is not geared at explaining individual steps in detail. For a much more detailed tutorial for creating a fresh ODK repo, see here for a tutorial for setting up your first workflow . We recommend to complete this tutorial before attempting this one.","title":"ODK in 20 minutes: a complete walk through the core workflows"},{"location":"tutorial/odk-tutorial-2/#tutorial","text":"Seeding a new ontology repo Import workflow Integration Testing Release workflow Customisable documentation","title":"Tutorial"},{"location":"tutorial/odk-tutorial-2/#seeding","text":"Create a new folder. Download an example ODK config or create one yourself. Save it in the directory created above. Important : in the cato-odk.yaml change github_org to your GitHub username. If you dont do this, some ODK features wont work perfectly, like documentation. github_org: matentzn repo: cat-ontology Run the ODK seeding script. curl https://raw.githubusercontent.com/INCATools/ontology-development-kit/v1.3.1/seed-via-docker.sh | bash -s -- --clean -C cato-odk.yaml Push the newly created repo to GitHub (for example with GitHub Desktop).","title":"Seeding"},{"location":"tutorial/odk-tutorial-2/#the-import-workflow","text":"Let us now import planned process: Open the term file src/ontology/imports/cob_terms.txt in your favourite text editor Add COB:0000082 to the term file (this is the planned process class in COB). From within the src/ontology directory, run sh run.sh make refresh-cob . Inspect the diff. Rather than importing just one term, it seems that we have important a whole bunch. This is because by default, ODK is using the SLME module extraction technique, which ensures that not only the terms we explicitly request are imported - but all the logically dependent ones as well . In src/ontology/cato-odk.yaml , locate the entry for importing cob and switch it to a different module type: filter . import_group: products: - id: ro - id: cob module_type: filter Run sh run.sh make update_repo to apply the changes. Check out the git diff to the Makefile to convince yourself that the new extraction method has been applied. Let us refresh the COB import again: From within the src/ontology directory, run sh run.sh make refresh-cob . Convince yourself that now only the planned process term is imported.","title":"The import workflow"},{"location":"tutorial/odk-tutorial-2/#integration-testing","text":"Switch to a new git branch and commit your changes to Makefile , cato-odk.yaml , imports/cob_terms.txt and imports/cob_import.owl . Push the branch and create a Pull Request. After a few seconds, the automated testing should start: Feel free to click on details to see what is happening! Once the test passes (turns green) the PR is ready to be reviewed. Since this tutorial is for illustration purposes only, we just merge.","title":"Integration Testing"},{"location":"tutorial/odk-tutorial-2/#the-release-workflow","text":"Great, we have done our change, now we are ready to make a release! Switch to the main branch in git . Make sure you you pull all changes ( git pull ). In src/ontology execute the release workflow: sh run.sh make prepare_release_fast (we are using fast release here which skips refreshing imports again - we just did that). Inspect the changes. You should see that the planned process class has been added to all ontology release artefacts. Create a branch and commit the changes. Push. Create pull request. Request review (skipped in this tutorial). Wait for QC to pass. Merge. On GitHub (repository front page), click on \"Create a new release\". In the next Window in the \"Choose a tag\" field select v2022-09-01 . Note the leading v . Select the correct date (the date you made the release, YYYY-MM-dd ). Fill in all the other form elements as you see fit. Click Publish release .","title":"The release workflow"},{"location":"tutorial/odk-tutorial-2/#customisable-documentation","text":"With our ODK setup, we also have a completely customisable documentation system installed. We just need to do a tiny change to the GitHub pages settings: On your GitHub repo page, click on \"Settings\". In the menu on the left, click on \"Pages\". On the right, under Build and deployment select Deploy from branch . Underneath, select gg-pages as the branch (this is where ODK deploys to), and /(root) as the directory. Hit Save . Wait for about 4 minutes for the site to be deployed (you can click on Actions in the main menu to follow the build process). Go back to the Pages section in Settings . You should see a button Visit site . Click on it. If everything went correctly, you should see your new page: Let's make a quick change: On the main page, click on the pen in the top right corner (this only works if you have correctly configured your github_org , see seeding ). If you have not configured your repo, go to the GitHub front page of your repo, go into the docs directory, click on index.md and edit it from here. Make a small random edit. Commit the change straight to main or do it properly, create a branch, PR, ask for reviews, merge. After the ODK updates your site, you should be able to see your changes reflected on the life site!","title":"Customisable documentation"},{"location":"tutorial/odk-tutorial-2/#summary","text":"That's it! In about 20 minutes, we Seeded a new ontology repo. Imported a term. Made a pull request and watched the ODK Testing framework at work. Ran a release. Deployed customisable documentation pages to help our users and curators documenting processes and use instructions.","title":"Summary"},{"location":"tutorial/project-ontology-development/","text":"Project Ontology Development \u00b6 Summary \u00b6 A project ontology, sometimes and controversially referred to as an application ontology , is an ontology which is composed of other ontologies for a particular use case, such as Natural Language Processing applications, Semantic Search and Knowledge Graph integration. A defining feature of a project ontology is that it is not intended to be used as a domain ontology . Concretely, this means that content from project ontologies (such as terms or axioms) is not to be re-used by domain ontologies (under no circumstances). Project ontology developers have the freedom to slice & dice, delete and add relationships, change labels etc as their use case demands it. Usually, such processing is minimal, and in a well developed environment such as OBO, new project ontology-specific terms are usually kept at a minimum. In this tutorial, we discuss the fundamental building blocks of application ontologies and show you how to build one using the Ontology Development Kit as one of several options. Prerequisites \u00b6 A basic understanding of Ontology Pipelines using ROBOT is helpful to follow this tutorial. Learning objectives \u00b6 Understand how to plan a project ontology project independent of any particular methodology Develop an application ontology using the Ontology Development Kit (ODK) Be aware off pitfalls when dealing with very large application ontologies Table of Contents \u00b6 Why do we need project ontologies? Overview The three \"ingredients\" of project ontologies The five \"phases\" of project ontology development Why do we need project ontologies? \u00b6 There are a few reasons for developing project ontologies. Here are two that are popular in our domain: Semantic integration . You have curated a lot of data using standard ontologies and now you wish to access this data using the \"semantic fabric\" provided by the ontology. Concrete examples: Adding a \"semantic layer to your knowledge graph\". For example, your data is annotated using specific anatomy terms, and you wish to query your knowledge graph through anatomical groupings, such as \"anatomical entities that are part of the cardio-vascular system\". Offering \"semantic search\". For example, you may want to restrict a certain search widget to \"diseases\" only, or try to figure out whether a user is searching for phenotypes associated with diseases. A concrete example is populating a Solr or elastic-search index using not only the labels and synonyms of an ontology, but also their relationships. Try it: https://platform.opentargets.org/, https://monarchinitiative.org/. Example ontologies: https://github.com/monarch-initiative/phenio https://github.com/EBISPOT/efo https://github.com/EBISPOT/scatlas_ontology Natural language processing (NLP) . You are developing an NLP application such as an annotator for text. Here, you may like to use ontologies to tag specific phrases in your documents, like those related to COVID. Ontologies in these cases serve essentially as more or less sophisticated dictionaries. But there are some more sophisticated uses of ontologies for NER. Example ontologies: https://github.com/berkeleybop/bero https://github.com/EBISPOT/covoc Mapping work . When developing mappings across ontologies and terminologies, it is often useful to have access to all of them at once. This helps to explore the consequences of mapping decisions, as well providing a single interface for ontology matching tools which usually operate on single ontologies. Advanced Machine Learning based approaches are used to generate graph embeddings on such merged ontologies. Example ontologies: https://github.com/monarch-initiative/mondo-ingest Basic architecture \u00b6 Three \"ingredients\" of project ontologies \u00b6 Any application ontology will be concerned with at least 3 ingredients: The seed . This is a the set of terms you wish to import into your application ontology . The seed can take many forms: a simple list of terms, e.g. MONDO:123, MONDO:231 a list of terms including additional relational selectors , e.g. MONDO:123, incl. all children a list of terms including a logical selector , MONDO:123, incl. all terms that are in some way logically related to MONDO:123 a general selector, like \"all classes\" or simply \"everything\". There are probably more, but these are the main ones we work with in the context of biomedical application ontologies. The source ontologies , often referred to as \"mirrors\" (at least by those working with ODK). These are the full ontologies which we want to use in our application ontology. For example, we may want to include anatomical entities from the Uberon ontology into our application ontology. These are usually downloaded from the internet into the application ontology workspace, and then processed by the application ontology extraction workflow (see later). Additional ontology metadata and customisations , such as axioms used to connect entities (classes) across your source ontologies to fulfil a use case, but also your regular ontology metadata (title, comments, etc). The five \"phases\" of project ontology development \u00b6 There are five phases on project ontology development which we will discuss in detail in this section: Managing the seed Extracting modules Managing metadata and customisations Merging and post-processing Validation There are other concerns, like continuous integration (basically making sure that changes to the seed or project ontology pipelines do not break anything) and release workflows which are not different from any other ontology. Managing the seed \u00b6 As described above , the seed is the set of terms that should be extracted from the source ontologies into the project ontology. The seed comprises any of the following: terms, such as MONDO:0000001 selectors, such as all, children, descendants, ancestors, annotations combinations of the two (a term with all its children) Users of ODK will be mostly familiar with term files located in the imports directory, such as src/ontology/imports/go_terms.txt . Selectors are usually hidden from the user by the ODK build system, but they are much more important now when building project ontologies. Regardless of which system you use to build your project ontology, it makes sense to carefully plan your seed management. In the following, we will discuss some examples: Using annotated data . For the Single Cell Atlas Ontology (SCAO) we already have a spreadsheet with the raw data, annotated with the ontology terms we wish to import. We only want to import these exact terms. Our strategy therefore is to extract the ontology terms from table and use these as our seed. User requests . For the Experimental Factor Ontology (EFO) , we rely on user requests. Curators provide us with individual term requests, or lists of terms they need for curating their data. We usually include these terms along with their parents to maintain rich hierarchies. Use case specific considerations . For the Phenomics Integrated Ontology (PHENIO) we wish to combine all disease, phenotype and anatomy terms together and furthermore pull in related chemicals and biological processes (and more). It makes sense to document your seed management plan. You should usually account for the possibility of changes (terms being added or removed) during the design phase. Extracting modules \u00b6 Module extraction is the process for selecting an appropriate subset from an ontology. There are many ways to extracting subsets from an ontology: Using logical modules (SLME, pronounced 'slime'): this will allow you to extract not only all the terms in your seed, but furthermore all logical axioms that could theoretically impact reasoning . SLME modules are typically much larger than what you would expect from a 'relevant' subset. Using ROBOT filter , a system to first select entities in your seed, then selectively including or excluding descendants, annotations. Using MIREOT , a technique that will extract terms and their subClass relationships, without any attempt to include other kinds of axioms. You can consult the ROBOT documentation for some details on module extraction. Let's be honest - none of these module extraction techniques are really ideal for project ontologies. SLME modules are typically used for domain ontology development to ensure logical consistency with imported ontologies, but otherwise contain too much information (for most project ontology use cases). ROBOT filter has a hard time with dealing with closures of existential restrictions: for example you cant be sure that, if you import \"endocardial endothelium\" and \"heart\" using filter, that the one former is still part of the latter (it is only indirectly a part) - a lot of research and work has being going on to make this easier. The next version of ROBOT (1.8.5) is going to contain a new module extraction command which will ensure that such links are not broken. One of the design confusions in this part of the process is that most use cases of application ontologies really do not care at all about OWL. Remember, OWL really only matters for the design of domain ontologies, to ensure a consistent representation of the domain and enable reasoning-based classification. So it is, at least slightly, unsatisfactory that we have to use OWL tools to do something that may as well be done by something simpler, more akin to \"graph-walking\". Managing metadata and customisations \u00b6 Just like any other ontology, a project ontology should be well annotated according to the standards of FAIR Semantics , for example using the OBO Foundry conventions . In particular, project ontologies should be annotated with a title, a license, a description versioned and annotated with versionIRIs get associated with some PURL system (OBO Foundry often wont accept such ontologies, but other free options like https://w3id.org/ exist) Furthermore, it is often necessary to add additional terms to the ontology which are not covered by other upstream ontologies. Here we need to distinguish two cases: The need to quickly add terms that belong somewhere else Adding terms that have no obvious home in any of your declared source ontologies. With our OBO hat on, if you start adding terms \"quickly\", you should develop a procedure to get these terms into suitable upstream ontologies at a later stage. This is not so much a necessity as a matter of \"open data ethics\": if you use other people's work to make your life easier, its good to give back! Lastly, our use cases sometimes require us to add additional links between the terms in our ontologies. For example, we may have to add subClassOf links between classes of different ontologies that cover the same domain. Or we want to add additional information. As with \"quickly adding terms\", if the information is generally useful, you should consider to add them to the respective upstream source ontology (synonyms of disease terms from Mondo , for example). We often manage such axioms as ROBOT templates and curate them as simple to read tables. Merging and post-processing \u00b6 Just like with most ontologies, the last part of the process is merging the various pieces (modules from external sources, customisations, metadata) together into a single whole. During this phase a few things can happen, but these are the most common ones: Merging : All separate parts are merged into one file. Restructure : Sometimes, we run additional processes to update the structure of the final ontology. One common post-processing step is to remove obsolete classes that may have come in during the extraction phase, add additional links between classes using approaches such as relation graph or prune away unsatisfiability-causing axioms such as disjointness axioms and negation. The latter is sometimes necessary when terms from multiple logically incompatible ontologies are imported. Annotate version information : owl:versionInfo and versionIRI annotations are added to the merged ontology. Validation \u00b6 One thing to remember is that you are not building a domain ontology . You are usually not concerned with typical issues in ontology engineering, such as logical consistency (or coherence, i.e. the absence of unsatisfiable classes). The key for validating an application ontology comes from its intended use case: Can the ontology deliver the use case it promised? There are many approaches to ensure that, chief among them competency questions . What we usually do is try to express competency questions as SPARQL queries, and ensure that there is at least one result. For example, for one of the project ontologies the author is involved with (CPONT) , we have developed a synthetic data generator, which we combine with the ontology to ask questions such as: \"Give me all patients which has a recorded diagnosis of scoliosis\" ( SPARQL ). So the ontology does a \"good job\" if it is able to return, say, at least 100 patients in our synthetic data for which we know that they are diagnoses with scoliosis or one of its subtypes. Frameworks for building project ontologies \u00b6 The perfect framework for building project ontologies does not exist yet. The Ontology Development Kit (ODK) has all the tools you need set up a basic application ontology, but the absence of a \"perfect\" module extraction algorithm for this use case is still unsatisfactory. However, for many use cases, filter modules like the ones described above are actually good enough. Here we will go through a simple example. An alternative framework for application ontology development based on a Web User Interface and tables for managing the seed is developed by James Overton at ( ontodev ). Another potential alternative is to go all the way to graph-land and build the application ontology with KGX and LinkML. See here for an example. Creating a project ontology this way feels more like a Knowledge Graph ETL task than building an ontology! Example application ontology with ODK \u00b6 Set up a basic ODK ontology. We are not covering this again in this tutorial, please refer to the tutorial on setting up your ODK repo . Dealing with large imports \u00b6 Many of the larger imports in application ontologies do not fit into the normal GitHub file size limit. In this cases it is better to attach them to a GitHub release rather than to check them into version control. TBD Additional materials and resources \u00b6 TBD Contributors \u00b6 Nicolas Matentzoglu","title":"Project Ontology Development"},{"location":"tutorial/project-ontology-development/#project-ontology-development","text":"","title":"Project Ontology Development"},{"location":"tutorial/project-ontology-development/#summary","text":"A project ontology, sometimes and controversially referred to as an application ontology , is an ontology which is composed of other ontologies for a particular use case, such as Natural Language Processing applications, Semantic Search and Knowledge Graph integration. A defining feature of a project ontology is that it is not intended to be used as a domain ontology . Concretely, this means that content from project ontologies (such as terms or axioms) is not to be re-used by domain ontologies (under no circumstances). Project ontology developers have the freedom to slice & dice, delete and add relationships, change labels etc as their use case demands it. Usually, such processing is minimal, and in a well developed environment such as OBO, new project ontology-specific terms are usually kept at a minimum. In this tutorial, we discuss the fundamental building blocks of application ontologies and show you how to build one using the Ontology Development Kit as one of several options.","title":"Summary"},{"location":"tutorial/project-ontology-development/#prerequisites","text":"A basic understanding of Ontology Pipelines using ROBOT is helpful to follow this tutorial.","title":"Prerequisites"},{"location":"tutorial/project-ontology-development/#learning-objectives","text":"Understand how to plan a project ontology project independent of any particular methodology Develop an application ontology using the Ontology Development Kit (ODK) Be aware off pitfalls when dealing with very large application ontologies","title":"Learning objectives"},{"location":"tutorial/project-ontology-development/#table-of-contents","text":"Why do we need project ontologies? Overview The three \"ingredients\" of project ontologies The five \"phases\" of project ontology development","title":"Table of Contents"},{"location":"tutorial/project-ontology-development/#why-do-we-need-project-ontologies","text":"There are a few reasons for developing project ontologies. Here are two that are popular in our domain: Semantic integration . You have curated a lot of data using standard ontologies and now you wish to access this data using the \"semantic fabric\" provided by the ontology. Concrete examples: Adding a \"semantic layer to your knowledge graph\". For example, your data is annotated using specific anatomy terms, and you wish to query your knowledge graph through anatomical groupings, such as \"anatomical entities that are part of the cardio-vascular system\". Offering \"semantic search\". For example, you may want to restrict a certain search widget to \"diseases\" only, or try to figure out whether a user is searching for phenotypes associated with diseases. A concrete example is populating a Solr or elastic-search index using not only the labels and synonyms of an ontology, but also their relationships. Try it: https://platform.opentargets.org/, https://monarchinitiative.org/. Example ontologies: https://github.com/monarch-initiative/phenio https://github.com/EBISPOT/efo https://github.com/EBISPOT/scatlas_ontology Natural language processing (NLP) . You are developing an NLP application such as an annotator for text. Here, you may like to use ontologies to tag specific phrases in your documents, like those related to COVID. Ontologies in these cases serve essentially as more or less sophisticated dictionaries. But there are some more sophisticated uses of ontologies for NER. Example ontologies: https://github.com/berkeleybop/bero https://github.com/EBISPOT/covoc Mapping work . When developing mappings across ontologies and terminologies, it is often useful to have access to all of them at once. This helps to explore the consequences of mapping decisions, as well providing a single interface for ontology matching tools which usually operate on single ontologies. Advanced Machine Learning based approaches are used to generate graph embeddings on such merged ontologies. Example ontologies: https://github.com/monarch-initiative/mondo-ingest","title":"Why do we need project ontologies?"},{"location":"tutorial/project-ontology-development/#basic-architecture","text":"","title":"Basic architecture"},{"location":"tutorial/project-ontology-development/#three-ingredients-of-project-ontologies","text":"Any application ontology will be concerned with at least 3 ingredients: The seed . This is a the set of terms you wish to import into your application ontology . The seed can take many forms: a simple list of terms, e.g. MONDO:123, MONDO:231 a list of terms including additional relational selectors , e.g. MONDO:123, incl. all children a list of terms including a logical selector , MONDO:123, incl. all terms that are in some way logically related to MONDO:123 a general selector, like \"all classes\" or simply \"everything\". There are probably more, but these are the main ones we work with in the context of biomedical application ontologies. The source ontologies , often referred to as \"mirrors\" (at least by those working with ODK). These are the full ontologies which we want to use in our application ontology. For example, we may want to include anatomical entities from the Uberon ontology into our application ontology. These are usually downloaded from the internet into the application ontology workspace, and then processed by the application ontology extraction workflow (see later). Additional ontology metadata and customisations , such as axioms used to connect entities (classes) across your source ontologies to fulfil a use case, but also your regular ontology metadata (title, comments, etc).","title":"Three \"ingredients\" of project ontologies"},{"location":"tutorial/project-ontology-development/#the-five-phases-of-project-ontology-development","text":"There are five phases on project ontology development which we will discuss in detail in this section: Managing the seed Extracting modules Managing metadata and customisations Merging and post-processing Validation There are other concerns, like continuous integration (basically making sure that changes to the seed or project ontology pipelines do not break anything) and release workflows which are not different from any other ontology.","title":"The five \"phases\" of project ontology development"},{"location":"tutorial/project-ontology-development/#managing-the-seed","text":"As described above , the seed is the set of terms that should be extracted from the source ontologies into the project ontology. The seed comprises any of the following: terms, such as MONDO:0000001 selectors, such as all, children, descendants, ancestors, annotations combinations of the two (a term with all its children) Users of ODK will be mostly familiar with term files located in the imports directory, such as src/ontology/imports/go_terms.txt . Selectors are usually hidden from the user by the ODK build system, but they are much more important now when building project ontologies. Regardless of which system you use to build your project ontology, it makes sense to carefully plan your seed management. In the following, we will discuss some examples: Using annotated data . For the Single Cell Atlas Ontology (SCAO) we already have a spreadsheet with the raw data, annotated with the ontology terms we wish to import. We only want to import these exact terms. Our strategy therefore is to extract the ontology terms from table and use these as our seed. User requests . For the Experimental Factor Ontology (EFO) , we rely on user requests. Curators provide us with individual term requests, or lists of terms they need for curating their data. We usually include these terms along with their parents to maintain rich hierarchies. Use case specific considerations . For the Phenomics Integrated Ontology (PHENIO) we wish to combine all disease, phenotype and anatomy terms together and furthermore pull in related chemicals and biological processes (and more). It makes sense to document your seed management plan. You should usually account for the possibility of changes (terms being added or removed) during the design phase.","title":"Managing the seed"},{"location":"tutorial/project-ontology-development/#extracting-modules","text":"Module extraction is the process for selecting an appropriate subset from an ontology. There are many ways to extracting subsets from an ontology: Using logical modules (SLME, pronounced 'slime'): this will allow you to extract not only all the terms in your seed, but furthermore all logical axioms that could theoretically impact reasoning . SLME modules are typically much larger than what you would expect from a 'relevant' subset. Using ROBOT filter , a system to first select entities in your seed, then selectively including or excluding descendants, annotations. Using MIREOT , a technique that will extract terms and their subClass relationships, without any attempt to include other kinds of axioms. You can consult the ROBOT documentation for some details on module extraction. Let's be honest - none of these module extraction techniques are really ideal for project ontologies. SLME modules are typically used for domain ontology development to ensure logical consistency with imported ontologies, but otherwise contain too much information (for most project ontology use cases). ROBOT filter has a hard time with dealing with closures of existential restrictions: for example you cant be sure that, if you import \"endocardial endothelium\" and \"heart\" using filter, that the one former is still part of the latter (it is only indirectly a part) - a lot of research and work has being going on to make this easier. The next version of ROBOT (1.8.5) is going to contain a new module extraction command which will ensure that such links are not broken. One of the design confusions in this part of the process is that most use cases of application ontologies really do not care at all about OWL. Remember, OWL really only matters for the design of domain ontologies, to ensure a consistent representation of the domain and enable reasoning-based classification. So it is, at least slightly, unsatisfactory that we have to use OWL tools to do something that may as well be done by something simpler, more akin to \"graph-walking\".","title":"Extracting modules"},{"location":"tutorial/project-ontology-development/#managing-metadata-and-customisations","text":"Just like any other ontology, a project ontology should be well annotated according to the standards of FAIR Semantics , for example using the OBO Foundry conventions . In particular, project ontologies should be annotated with a title, a license, a description versioned and annotated with versionIRIs get associated with some PURL system (OBO Foundry often wont accept such ontologies, but other free options like https://w3id.org/ exist) Furthermore, it is often necessary to add additional terms to the ontology which are not covered by other upstream ontologies. Here we need to distinguish two cases: The need to quickly add terms that belong somewhere else Adding terms that have no obvious home in any of your declared source ontologies. With our OBO hat on, if you start adding terms \"quickly\", you should develop a procedure to get these terms into suitable upstream ontologies at a later stage. This is not so much a necessity as a matter of \"open data ethics\": if you use other people's work to make your life easier, its good to give back! Lastly, our use cases sometimes require us to add additional links between the terms in our ontologies. For example, we may have to add subClassOf links between classes of different ontologies that cover the same domain. Or we want to add additional information. As with \"quickly adding terms\", if the information is generally useful, you should consider to add them to the respective upstream source ontology (synonyms of disease terms from Mondo , for example). We often manage such axioms as ROBOT templates and curate them as simple to read tables.","title":"Managing metadata and customisations"},{"location":"tutorial/project-ontology-development/#merging-and-post-processing","text":"Just like with most ontologies, the last part of the process is merging the various pieces (modules from external sources, customisations, metadata) together into a single whole. During this phase a few things can happen, but these are the most common ones: Merging : All separate parts are merged into one file. Restructure : Sometimes, we run additional processes to update the structure of the final ontology. One common post-processing step is to remove obsolete classes that may have come in during the extraction phase, add additional links between classes using approaches such as relation graph or prune away unsatisfiability-causing axioms such as disjointness axioms and negation. The latter is sometimes necessary when terms from multiple logically incompatible ontologies are imported. Annotate version information : owl:versionInfo and versionIRI annotations are added to the merged ontology.","title":"Merging and post-processing"},{"location":"tutorial/project-ontology-development/#validation","text":"One thing to remember is that you are not building a domain ontology . You are usually not concerned with typical issues in ontology engineering, such as logical consistency (or coherence, i.e. the absence of unsatisfiable classes). The key for validating an application ontology comes from its intended use case: Can the ontology deliver the use case it promised? There are many approaches to ensure that, chief among them competency questions . What we usually do is try to express competency questions as SPARQL queries, and ensure that there is at least one result. For example, for one of the project ontologies the author is involved with (CPONT) , we have developed a synthetic data generator, which we combine with the ontology to ask questions such as: \"Give me all patients which has a recorded diagnosis of scoliosis\" ( SPARQL ). So the ontology does a \"good job\" if it is able to return, say, at least 100 patients in our synthetic data for which we know that they are diagnoses with scoliosis or one of its subtypes.","title":"Validation"},{"location":"tutorial/project-ontology-development/#frameworks-for-building-project-ontologies","text":"The perfect framework for building project ontologies does not exist yet. The Ontology Development Kit (ODK) has all the tools you need set up a basic application ontology, but the absence of a \"perfect\" module extraction algorithm for this use case is still unsatisfactory. However, for many use cases, filter modules like the ones described above are actually good enough. Here we will go through a simple example. An alternative framework for application ontology development based on a Web User Interface and tables for managing the seed is developed by James Overton at ( ontodev ). Another potential alternative is to go all the way to graph-land and build the application ontology with KGX and LinkML. See here for an example. Creating a project ontology this way feels more like a Knowledge Graph ETL task than building an ontology!","title":"Frameworks for building project ontologies"},{"location":"tutorial/project-ontology-development/#example-application-ontology-with-odk","text":"Set up a basic ODK ontology. We are not covering this again in this tutorial, please refer to the tutorial on setting up your ODK repo .","title":"Example application ontology with ODK"},{"location":"tutorial/project-ontology-development/#dealing-with-large-imports","text":"Many of the larger imports in application ontologies do not fit into the normal GitHub file size limit. In this cases it is better to attach them to a GitHub release rather than to check them into version control. TBD","title":"Dealing with large imports"},{"location":"tutorial/project-ontology-development/#additional-materials-and-resources","text":"TBD","title":"Additional materials and resources"},{"location":"tutorial/project-ontology-development/#contributors","text":"Nicolas Matentzoglu","title":"Contributors"},{"location":"tutorial/pull-requests/","text":"Pull Requests \u00b6 Prerequisites \u00b6 Participants will need to have access to the following resources and tools prior to the training: GitHub account - register for a free GitHub account here Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop Protege - Install Protege 5.5, download it here Preparation ( optional ) \u00b6 Review tutorial on Contributing to Ontologies See 'How to' guide on Pull Requests What is delivered as part of the course \u00b6 Description: How to create and manage pull requests to ontology files in GitHub. Learning objectives \u00b6 How to create a really good pull request GitHub Pull Request Workflow How to find a reviewer for your pull request in an open source environment How to review a pull request How to change a pull request in response to review How to update from master Resolve conflicts on branch Contributors \u00b6 Nicole Vasilevsky Nico Matentzoglu How to create a really good pull request \u00b6 What is a Pull Request? \u00b6 A pull request (PR) is an event in Git where a contributor (you!) asks a maintainer of a Git repository to review changes (e.g. edits to an ontology file) they want to merge into a project (e.g. the owl file) (see reference ). A contributor creates a pull request to propose and collaborate on changes to a repository. These changes are proposed in a branch, which ensures that the default branch only contains finished and approved work. See more details here . How to write a great descriptive title \u00b6 When committing a pull request, you must include a title and a description (more details in the workflow below.) Tips below (adapted from Hugo Dias ): The title of the PR should be self-explanatory Do : Describe what was changed in the pull request Example: Add new term: MONDO:0100503 DPH5-related diphthamide-deficiency syndrome` Don't : write a vague title that has very little meaning. Example: Add new term Don't : use the branch name in the pull request (sometimes GitHub will offer this as a default name) Example: issue-5024 What kind of information to include in the description \u00b6 Describe what was changed in the pull request Explain why this PR exists Make it clear how it does what it sets out to do. E.g., Does it edit the ontology-edit.owl file? Does it edit another file(s)? What was your motivation for the chosen solution? Use screenshots to demonstrate what has changed. See How to guide on creating screenshots Example : General tips \u00b6 Do : Follow the Single Responsibility Principle : The pull request should do only one thing. Note : sometimes a small edit can change a lot of code, for example, if you want to change all of the created_by annotations to dc:creator. That's okay. The pull request should be atomic: it should be small and self contained with simple changes that affect a little code a possible Whenever possible, break pull-requests into smaller ones Commit early, commit often Include specific information like the ID and label for terms changed. Note, you can easily obtain term metadata like OBO ID, IRI, or the term label in Protege by clicking the three lines above the Annotations box (next to the term name), see screenshot below. You can also copy the IRI in markdown, which is really convenient for pasting into GitHub. Don't : Make additional changes on a single PR that goes beyond the scope of the ticket or PR. For example, if you are adding a new term, don't also fix definitions or formatting for other terms. GitHub Pull Request Workflow \u00b6 Update the local copy of the ontology \u00b6 In GitHub Desktop, navigate to your local ontology directory of your ontology Make sure you are on the master/main branch and click Pull origin (or Fetch origin) Create a New Working Branch \u00b6 When starting to work on a ticket or making edits to an ontology, you should create a new branch of the repository to edit the ontology file. Make sure you are on the master branch before creating a new branch. Please do not create a new branch off of an existing branch (unless the situation explicitly calls for it). To create a new branch, click on Current Branch and select New Branch Name your branch. Some recommended best practices for branch name are to name the branch after the issue number, for example issue-206. If you are not addressing a ticket per se, you could name the branch: 'initals-edits-date', e.g. nv-edits_2022-07-12, or give it a name specific to what you are doing, e.g. fix-typos-2022-07-12. Continuing work on an existing Working Branch \u00b6 If you are continuing to do work on an existing branch, in addition to updating master, go to your branch by selecting Current Branch in GitHub Desktop and either search for or browse for the branch name. Video Explanation \u00b6 A video is below. OPTIONAL: To update the working branch with respect to the current version of the ontology, select Branch from the top menu, Update from master. This step is optional because it is not necessary to work on the current version of the ontology; all changes will be synchronized when git merge is performed. Editing an ontology on a branch \u00b6 Create a new branch, open Protege. Protege will display your branch name in the lower left corner (or it will show Git: master ) Make necessary edits in Protege. Committing, pushing and making pull requests \u00b6 Review: GitHub Desktop will display the diff or changes made to the ontology. Before committing, view the diff and ensure the changes were intended. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask for help instead or consider discarding your changes and starting the edits again. To discard changes, right click on the changed file name and select Discard changes. Example diffs: Example 1 (Cell Ontology): Example 2 (Mondo): Write a good commit messages \u00b6 Commit message: Before Committing, you must add a commit message. In GitHub Desktop in the Commit field in the lower left, there is a subject line and a description. Give a very descriptive title: Add a descriptive title in the subject line. For example: add new class ONTOLOGY:ID [term name] (e.g. add new class MONDO:0000006 heart disease) Write a detailed summary of what the change is in the Description box, referring to the issue. The sentence should clearly state how the issue is addressed. NOTE : You can use the word \u2018fixes\u2019 or \u2018closes\u2019 in the commit message - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . \u2018Fixes\u2019 and \u201cCloses\u2019 is case-insensitive and can be plural or singular (fixes, closes, fix, close). If you don\u2019t want to close the ticket, just refer to the ticket # without the word \u2018fixes\u2019 or use \u2018adresses\u2019 or 'addresses'. The commit will be associated with the correct ticket but the ticket will remain open. Push : To incorporate the changes into the remote repository, click Commit to [branch name], then click Push. Make a pull request (PR) \u00b6 You can either make a pull request (PR) directly from GitHub Desktop, or via the GitHub web browser. To make a PR from GitHub Desktop, click the button 'Create Pull Request'. You will be directed to your web browser and GitHub repo. Click Create Pull Request. If your PR is a work-in-progress and not ready for review, you can save it as a draft PR and convert it to a PR when it is ready for review. If you do not create a PR directly from GitHub Dekstop, you can go to your GitHub repo and you will see a yellow banner on top that notifies you of a pending PR. Navigate to the tab labeled as \u2018Code\u2019. You should see your commit listed at the top of the page in a light yellow box. If you don\u2019t see it, click on the \u2018Branches\u2019 link to reveal it in the list, and click on it. Click the green button \u2018Compare & pull request\u2019 on the right. You may now add comments, if applicable and request a reviewer. See section below on reviewers. You can see the diff for your file by clicking 'Files Changed'. Examine it as a sanity check. Go back to the Conversation tab. Click on the green box \u2018Pull request\u2019 to generate a pull request. How to find a reviewer for your pull request in an open source environment \u00b6 Publicly managed ontologies do not have a structure in place to automatically deal with PRs. If you make a PR to an open source project, you should open a separate social channel to the developers to notify them of your PR (e.g. find the project mailing list, Slack, etc.). You should introduce yourself and give some context. Depending on the level of your permissions for the repository, you may or may not be able to assign a reviewer yourself. If you have write access to the repository, you can assign a reviewer. Otherwise, you can tag people in the description of your pull request. Tips for finding reviewers: An ontology repository should have an owner assigned. This may be described in the ReadMe file or on the OBO Foundry website . For example, the contact person for Mondo is Nicole Vasilevsky. The primary owner can likely review your PR or triage your request to the appropriate person. If you are addressing a specific ticket, you may want to assign the person who created the ticket to review. How to review a pull request (PR) \u00b6 If you are assigned to review a pull request, you should receive an email notification. You can also check for PRs assigned to you by going to https://github.com/pulls/assigned . What kind of person do we need for what kind of pull request? \u00b6 It depends on what the pull request is addressing. Remember the QC checks will check for things like unsatisfiable classes and many other checks (that vary between ontologies). Your job as a reviewer is to check for things that the QC checks won't pick up and need human judgement. If it is content changes, like adding new terms, or reclassifying a term, an ontology curator could review your PR. If the PR is addressing quality control or technical aspects, one of the ontology semantic engineers would probably be a good fit. If you don't know who to assign, we recommend assigning the ontology contact person and they can triage the request. To review a PR, you should view the 'Files changed' and view the diff(s). You can review changes in a pull request one file at a time. While reviewing the files in a pull request, you can leave individual comments on specific changes. Example : Things to look out for when reviewing a PR: \u00b6 Make sure the changes made address the ticket. In the example above, Sabrina addressed a ticket that requested adding a new term to Mondo, which is what she did on the PR (see https://github.com/monarch-initiative/mondo/pull/5078 ). Examples of things to look for in content changes (like adding new terms or revising existing terms): Poorly written definitions Missing or misformatted database cross-references Incorrectly scoped synonyms appropriate annotations Make sure there are not any unintended or unwanted changes on the PR. See example below. Protege reordered the location of a term in the file. Check that the logic is correct. This can be a difficult thing to do. Some tips: Open the branch in Protege and examine the hierarchy in Protege Compare the logic that was use to the logic used in an existing term If the ontology uses Design Patterns, ensure the logic is consistent with the Design Patterns Ask an expert in ontology logic to help review the PR Remember there is no magic bullet to ensuring an ontology is logically sound, but do the best you can Adding your review \u00b6 After you finish reviewing each file, you can mark the file as viewed. This collapses the file, helping you identify the files you still need to review. A progress bar in the pull request header shows the number of files you've viewed. You can leave comments and requests for changes on the PR inline for on the PR when viewing the 'Files changed'. You can add a single comment, or start a review if you have multiple comments. After reviewing the file(s), you can approve the pull request or request additional changes by submitting your review with a summary comment. Comment (Submit general feedback without explicit approval) Approve (Submit feedback and approve merging these changes) Request changes (Submit feedback that must be addressed before merging) In addition or instead of adding inline comments, you can leave comments on the Conversation page. The conversation page is a good place to discuss the PR, and for the original creator to respond to the reviewer comments. Inline commits \u00b6 GitHub added a 'suggested Changes' feature that allows a PR reviewer to suggest an exact change in a comment in a PR. You can add inline comments and commit your comment using 'inline commits'. Read more about it here . Go to the 'Files changed' tab of a pull request Hover over the line you want to fix, and a blue box with a plus sign appears near the gutter on the left Click that to display the normal line comment form Click the button with a plus and minus sign, it adds a suggestion block to the comment text area with the existing text You can make changes to the text inside the suggestion box. Note that you can add context for your suggested changes outside of the suggestion block When you create the comment, it will show up to the maintainer as a diff The maintainer can see what changes you are suggesting and accept them with a click When are you done with your review? \u00b6 If you review the PR and the changes properly address what was described in the description, then it should be sufficient. Not every PR needs comments, it can be approved without any comments or requests for changes. Feel free to ask for help with your review, and/or assign additional reviewers. Some of the content above was adapted from GitHub Docs . How to change a pull request in response to review \u00b6 Check out your branch in GitHub Desktop and open the file in Protege. Make the suggested changes. Check the diff. Commit your changes on your branch. Note, you do not need to create another PR, your commits will show up on the same PR. Resolve the comments on the PR. Notify the reviewer that your PR is ready for re-review. How to update from master \u00b6 In GitHub Desktop, navigate to your branch. In the top file menu, select Branch -> Update from master. Resolve conflicts on branch \u00b6 Conflicts arise when edits are made on two separate branches to the same line in a file. ( reference ). When editing an ontology file (owl file or obo file), conflicts often arise when adding new terms to an ontology file on separate branches, or when there are a lot of open pull requests. Conflicts in ontology files can be fixed either on the command line or using GitHub Desktop. In this lesson, we describe how to fix conflicts using GitHub Desktop. Fix conflicts in GitHub desktop \u00b6 In GitHub Desktop, go to your master/main branch and fetch pull. Go to branch with conflict. Pull branch. Branch -> update from master. Open in Sublime or Atom. Make changes in file (open the ontology file in a text editor (like Sublime) and search for the conflicts. These are usually preceded by <<<<<. Fix the conflicts, then save). In GitHub Desktop, continue merge. Push. In terminal: open [ontology file name] (e.g. open mondo-edit.obo ) or open in Protege manually. Save as (nothing should have changed in the diff). Check the diff in GitHub online. Video Explanation \u00b6 Watch a video below with an example fixing a conflict in the Mondo ontology file. Some examples of conflicts that Nicole fixed in Mondo are below: Further regarding \u00b6 Gene Ontology Daily Workflow \u00b6 Gene Ontology Editing Guide GitHub Merge Conflicts \u00b6 Resolving a merge conflict on GitHub Git merge conflicts The anatomy of a perfect pull request \u00b6 Blog post by Hugo Dias Suggesting Changes on GitHub - includes description of how to make inline commits \u00b6","title":"Pull requests"},{"location":"tutorial/pull-requests/#pull-requests","text":"","title":"Pull Requests"},{"location":"tutorial/pull-requests/#prerequisites","text":"Participants will need to have access to the following resources and tools prior to the training: GitHub account - register for a free GitHub account here Install GitHub Desktop Please make sure you have some kind of git client installed on your machine. If you are new to Git, please install GitHub Desktop Protege - Install Protege 5.5, download it here","title":"Prerequisites"},{"location":"tutorial/pull-requests/#preparation-optional","text":"Review tutorial on Contributing to Ontologies See 'How to' guide on Pull Requests","title":"Preparation (optional)"},{"location":"tutorial/pull-requests/#what-is-delivered-as-part-of-the-course","text":"Description: How to create and manage pull requests to ontology files in GitHub.","title":"What is delivered as part of the course"},{"location":"tutorial/pull-requests/#learning-objectives","text":"How to create a really good pull request GitHub Pull Request Workflow How to find a reviewer for your pull request in an open source environment How to review a pull request How to change a pull request in response to review How to update from master Resolve conflicts on branch","title":"Learning objectives"},{"location":"tutorial/pull-requests/#contributors","text":"Nicole Vasilevsky Nico Matentzoglu","title":"Contributors"},{"location":"tutorial/pull-requests/#how-to-create-a-really-good-pull-request","text":"","title":"How to create a really good pull request"},{"location":"tutorial/pull-requests/#what-is-a-pull-request","text":"A pull request (PR) is an event in Git where a contributor (you!) asks a maintainer of a Git repository to review changes (e.g. edits to an ontology file) they want to merge into a project (e.g. the owl file) (see reference ). A contributor creates a pull request to propose and collaborate on changes to a repository. These changes are proposed in a branch, which ensures that the default branch only contains finished and approved work. See more details here .","title":"What is a Pull Request?"},{"location":"tutorial/pull-requests/#how-to-write-a-great-descriptive-title","text":"When committing a pull request, you must include a title and a description (more details in the workflow below.) Tips below (adapted from Hugo Dias ): The title of the PR should be self-explanatory Do : Describe what was changed in the pull request Example: Add new term: MONDO:0100503 DPH5-related diphthamide-deficiency syndrome` Don't : write a vague title that has very little meaning. Example: Add new term Don't : use the branch name in the pull request (sometimes GitHub will offer this as a default name) Example: issue-5024","title":"How to write a great descriptive title"},{"location":"tutorial/pull-requests/#what-kind-of-information-to-include-in-the-description","text":"Describe what was changed in the pull request Explain why this PR exists Make it clear how it does what it sets out to do. E.g., Does it edit the ontology-edit.owl file? Does it edit another file(s)? What was your motivation for the chosen solution? Use screenshots to demonstrate what has changed. See How to guide on creating screenshots Example :","title":"What kind of information to include in the description"},{"location":"tutorial/pull-requests/#general-tips","text":"Do : Follow the Single Responsibility Principle : The pull request should do only one thing. Note : sometimes a small edit can change a lot of code, for example, if you want to change all of the created_by annotations to dc:creator. That's okay. The pull request should be atomic: it should be small and self contained with simple changes that affect a little code a possible Whenever possible, break pull-requests into smaller ones Commit early, commit often Include specific information like the ID and label for terms changed. Note, you can easily obtain term metadata like OBO ID, IRI, or the term label in Protege by clicking the three lines above the Annotations box (next to the term name), see screenshot below. You can also copy the IRI in markdown, which is really convenient for pasting into GitHub. Don't : Make additional changes on a single PR that goes beyond the scope of the ticket or PR. For example, if you are adding a new term, don't also fix definitions or formatting for other terms.","title":"General tips"},{"location":"tutorial/pull-requests/#github-pull-request-workflow","text":"","title":"GitHub Pull Request Workflow"},{"location":"tutorial/pull-requests/#update-the-local-copy-of-the-ontology","text":"In GitHub Desktop, navigate to your local ontology directory of your ontology Make sure you are on the master/main branch and click Pull origin (or Fetch origin)","title":"Update the local copy of the ontology"},{"location":"tutorial/pull-requests/#create-a-new-working-branch","text":"When starting to work on a ticket or making edits to an ontology, you should create a new branch of the repository to edit the ontology file. Make sure you are on the master branch before creating a new branch. Please do not create a new branch off of an existing branch (unless the situation explicitly calls for it). To create a new branch, click on Current Branch and select New Branch Name your branch. Some recommended best practices for branch name are to name the branch after the issue number, for example issue-206. If you are not addressing a ticket per se, you could name the branch: 'initals-edits-date', e.g. nv-edits_2022-07-12, or give it a name specific to what you are doing, e.g. fix-typos-2022-07-12.","title":"Create a New Working Branch"},{"location":"tutorial/pull-requests/#continuing-work-on-an-existing-working-branch","text":"If you are continuing to do work on an existing branch, in addition to updating master, go to your branch by selecting Current Branch in GitHub Desktop and either search for or browse for the branch name.","title":"Continuing work on an existing Working Branch"},{"location":"tutorial/pull-requests/#video-explanation","text":"A video is below. OPTIONAL: To update the working branch with respect to the current version of the ontology, select Branch from the top menu, Update from master. This step is optional because it is not necessary to work on the current version of the ontology; all changes will be synchronized when git merge is performed.","title":"Video Explanation"},{"location":"tutorial/pull-requests/#editing-an-ontology-on-a-branch","text":"Create a new branch, open Protege. Protege will display your branch name in the lower left corner (or it will show Git: master ) Make necessary edits in Protege.","title":"Editing an ontology on a branch"},{"location":"tutorial/pull-requests/#committing-pushing-and-making-pull-requests","text":"Review: GitHub Desktop will display the diff or changes made to the ontology. Before committing, view the diff and ensure the changes were intended. Examples of a diff are pasted below. Large diffs are a sign that something went wrong. In this case, do not commit the changes and ask for help instead or consider discarding your changes and starting the edits again. To discard changes, right click on the changed file name and select Discard changes. Example diffs: Example 1 (Cell Ontology): Example 2 (Mondo):","title":"Committing, pushing and making pull requests"},{"location":"tutorial/pull-requests/#write-a-good-commit-messages","text":"Commit message: Before Committing, you must add a commit message. In GitHub Desktop in the Commit field in the lower left, there is a subject line and a description. Give a very descriptive title: Add a descriptive title in the subject line. For example: add new class ONTOLOGY:ID [term name] (e.g. add new class MONDO:0000006 heart disease) Write a detailed summary of what the change is in the Description box, referring to the issue. The sentence should clearly state how the issue is addressed. NOTE : You can use the word \u2018fixes\u2019 or \u2018closes\u2019 in the commit message - these are magic words in GitHub; when used in combination with the ticket number, it will automatically close the ticket. Learn more on this GitHub Help Documentation page about Closing issues via commit messages . \u2018Fixes\u2019 and \u201cCloses\u2019 is case-insensitive and can be plural or singular (fixes, closes, fix, close). If you don\u2019t want to close the ticket, just refer to the ticket # without the word \u2018fixes\u2019 or use \u2018adresses\u2019 or 'addresses'. The commit will be associated with the correct ticket but the ticket will remain open. Push : To incorporate the changes into the remote repository, click Commit to [branch name], then click Push.","title":"Write a good commit messages"},{"location":"tutorial/pull-requests/#make-a-pull-request-pr","text":"You can either make a pull request (PR) directly from GitHub Desktop, or via the GitHub web browser. To make a PR from GitHub Desktop, click the button 'Create Pull Request'. You will be directed to your web browser and GitHub repo. Click Create Pull Request. If your PR is a work-in-progress and not ready for review, you can save it as a draft PR and convert it to a PR when it is ready for review. If you do not create a PR directly from GitHub Dekstop, you can go to your GitHub repo and you will see a yellow banner on top that notifies you of a pending PR. Navigate to the tab labeled as \u2018Code\u2019. You should see your commit listed at the top of the page in a light yellow box. If you don\u2019t see it, click on the \u2018Branches\u2019 link to reveal it in the list, and click on it. Click the green button \u2018Compare & pull request\u2019 on the right. You may now add comments, if applicable and request a reviewer. See section below on reviewers. You can see the diff for your file by clicking 'Files Changed'. Examine it as a sanity check. Go back to the Conversation tab. Click on the green box \u2018Pull request\u2019 to generate a pull request.","title":"Make a pull request (PR)"},{"location":"tutorial/pull-requests/#how-to-find-a-reviewer-for-your-pull-request-in-an-open-source-environment","text":"Publicly managed ontologies do not have a structure in place to automatically deal with PRs. If you make a PR to an open source project, you should open a separate social channel to the developers to notify them of your PR (e.g. find the project mailing list, Slack, etc.). You should introduce yourself and give some context. Depending on the level of your permissions for the repository, you may or may not be able to assign a reviewer yourself. If you have write access to the repository, you can assign a reviewer. Otherwise, you can tag people in the description of your pull request. Tips for finding reviewers: An ontology repository should have an owner assigned. This may be described in the ReadMe file or on the OBO Foundry website . For example, the contact person for Mondo is Nicole Vasilevsky. The primary owner can likely review your PR or triage your request to the appropriate person. If you are addressing a specific ticket, you may want to assign the person who created the ticket to review.","title":"How to find a reviewer for your pull request in an open source environment"},{"location":"tutorial/pull-requests/#how-to-review-a-pull-request-pr","text":"If you are assigned to review a pull request, you should receive an email notification. You can also check for PRs assigned to you by going to https://github.com/pulls/assigned .","title":"How to review a pull request (PR)"},{"location":"tutorial/pull-requests/#what-kind-of-person-do-we-need-for-what-kind-of-pull-request","text":"It depends on what the pull request is addressing. Remember the QC checks will check for things like unsatisfiable classes and many other checks (that vary between ontologies). Your job as a reviewer is to check for things that the QC checks won't pick up and need human judgement. If it is content changes, like adding new terms, or reclassifying a term, an ontology curator could review your PR. If the PR is addressing quality control or technical aspects, one of the ontology semantic engineers would probably be a good fit. If you don't know who to assign, we recommend assigning the ontology contact person and they can triage the request. To review a PR, you should view the 'Files changed' and view the diff(s). You can review changes in a pull request one file at a time. While reviewing the files in a pull request, you can leave individual comments on specific changes. Example :","title":"What kind of person do we need for what kind of pull request?"},{"location":"tutorial/pull-requests/#things-to-look-out-for-when-reviewing-a-pr","text":"Make sure the changes made address the ticket. In the example above, Sabrina addressed a ticket that requested adding a new term to Mondo, which is what she did on the PR (see https://github.com/monarch-initiative/mondo/pull/5078 ). Examples of things to look for in content changes (like adding new terms or revising existing terms): Poorly written definitions Missing or misformatted database cross-references Incorrectly scoped synonyms appropriate annotations Make sure there are not any unintended or unwanted changes on the PR. See example below. Protege reordered the location of a term in the file. Check that the logic is correct. This can be a difficult thing to do. Some tips: Open the branch in Protege and examine the hierarchy in Protege Compare the logic that was use to the logic used in an existing term If the ontology uses Design Patterns, ensure the logic is consistent with the Design Patterns Ask an expert in ontology logic to help review the PR Remember there is no magic bullet to ensuring an ontology is logically sound, but do the best you can","title":"Things to look out for when reviewing a PR:"},{"location":"tutorial/pull-requests/#adding-your-review","text":"After you finish reviewing each file, you can mark the file as viewed. This collapses the file, helping you identify the files you still need to review. A progress bar in the pull request header shows the number of files you've viewed. You can leave comments and requests for changes on the PR inline for on the PR when viewing the 'Files changed'. You can add a single comment, or start a review if you have multiple comments. After reviewing the file(s), you can approve the pull request or request additional changes by submitting your review with a summary comment. Comment (Submit general feedback without explicit approval) Approve (Submit feedback and approve merging these changes) Request changes (Submit feedback that must be addressed before merging) In addition or instead of adding inline comments, you can leave comments on the Conversation page. The conversation page is a good place to discuss the PR, and for the original creator to respond to the reviewer comments.","title":"Adding your review"},{"location":"tutorial/pull-requests/#inline-commits","text":"GitHub added a 'suggested Changes' feature that allows a PR reviewer to suggest an exact change in a comment in a PR. You can add inline comments and commit your comment using 'inline commits'. Read more about it here . Go to the 'Files changed' tab of a pull request Hover over the line you want to fix, and a blue box with a plus sign appears near the gutter on the left Click that to display the normal line comment form Click the button with a plus and minus sign, it adds a suggestion block to the comment text area with the existing text You can make changes to the text inside the suggestion box. Note that you can add context for your suggested changes outside of the suggestion block When you create the comment, it will show up to the maintainer as a diff The maintainer can see what changes you are suggesting and accept them with a click","title":"Inline commits"},{"location":"tutorial/pull-requests/#when-are-you-done-with-your-review","text":"If you review the PR and the changes properly address what was described in the description, then it should be sufficient. Not every PR needs comments, it can be approved without any comments or requests for changes. Feel free to ask for help with your review, and/or assign additional reviewers. Some of the content above was adapted from GitHub Docs .","title":"When are you done with your review?"},{"location":"tutorial/pull-requests/#how-to-change-a-pull-request-in-response-to-review","text":"Check out your branch in GitHub Desktop and open the file in Protege. Make the suggested changes. Check the diff. Commit your changes on your branch. Note, you do not need to create another PR, your commits will show up on the same PR. Resolve the comments on the PR. Notify the reviewer that your PR is ready for re-review.","title":"How to change a pull request in response to review"},{"location":"tutorial/pull-requests/#how-to-update-from-master","text":"In GitHub Desktop, navigate to your branch. In the top file menu, select Branch -> Update from master.","title":"How to update from master"},{"location":"tutorial/pull-requests/#resolve-conflicts-on-branch","text":"Conflicts arise when edits are made on two separate branches to the same line in a file. ( reference ). When editing an ontology file (owl file or obo file), conflicts often arise when adding new terms to an ontology file on separate branches, or when there are a lot of open pull requests. Conflicts in ontology files can be fixed either on the command line or using GitHub Desktop. In this lesson, we describe how to fix conflicts using GitHub Desktop.","title":"Resolve conflicts on branch"},{"location":"tutorial/pull-requests/#fix-conflicts-in-github-desktop","text":"In GitHub Desktop, go to your master/main branch and fetch pull. Go to branch with conflict. Pull branch. Branch -> update from master. Open in Sublime or Atom. Make changes in file (open the ontology file in a text editor (like Sublime) and search for the conflicts. These are usually preceded by <<<<<. Fix the conflicts, then save). In GitHub Desktop, continue merge. Push. In terminal: open [ontology file name] (e.g. open mondo-edit.obo ) or open in Protege manually. Save as (nothing should have changed in the diff). Check the diff in GitHub online.","title":"Fix conflicts in GitHub desktop"},{"location":"tutorial/pull-requests/#video-explanation_1","text":"Watch a video below with an example fixing a conflict in the Mondo ontology file. Some examples of conflicts that Nicole fixed in Mondo are below:","title":"Video Explanation"},{"location":"tutorial/pull-requests/#further-regarding","text":"","title":"Further regarding"},{"location":"tutorial/pull-requests/#gene-ontology-daily-workflow","text":"Gene Ontology Editing Guide","title":"Gene Ontology Daily Workflow"},{"location":"tutorial/pull-requests/#github-merge-conflicts","text":"Resolving a merge conflict on GitHub Git merge conflicts","title":"GitHub Merge Conflicts"},{"location":"tutorial/pull-requests/#the-anatomy-of-a-perfect-pull-request","text":"Blog post by Hugo Dias","title":"The anatomy of a perfect pull request"},{"location":"tutorial/pull-requests/#suggesting-changes-on-github-includes-description-of-how-to-make-inline-commits","text":"","title":"Suggesting Changes on GitHub - includes description of how to make inline commits"},{"location":"tutorial/robot-tutorial-1/","text":"ROBOT Mini-Tutorial 1: Convert, Extract and Template \u00b6 Last week, we were introduced to ROBOT for quality control and generating reports about terms in an ontology. This week, we will learn about three new ROBOT commands: Convert Extract Template Before starting this tutorial, either: make sure Docker is running and you are in the container download and install ROBOT for your operating system We will be using the files from the Ontologies 101 Tutorial . In your terminal, navigate to the repository that you cloned and then into the BDK14_exercises folder. Convert \u00b6 So far, we have been saving our ontologies in Protege using the default RDF/XML syntax, but there are many flavors of OWL. We will discuss each of these serializations in more detail during the class session, but ROBOT supports the following: owl - RDF/XML owx - OWL/XML ttl - Turtle obo - OBO Format ofn - OWL Functional omn - OWL Manchester json - obographs JSON Let's Try It! \u00b6 Navigate to the basic-subclass/ folder. Open chromosome-parts.owl in your text editor and you will see it's in RDF/XML format. We're going to take this file and convert it to Turtle ( ttl ) serialization. Return to your terminal and enter the following command: robot convert --input chromosome-parts.owl --format ttl --output chromosome-parts.ttl ROBOT convert is smart about detecting formats, so since the output file ends with .ttl , the --format ttl parameter isn't really required. If you wanted to use a different file ending, say .owl , you will need to include the format flag to force ROBOT to write Turtle. Now open chromosome-parts.ttl in your text editor and see what's changed! RDF/XML and Turtle are very different serializations, but the actual data that is contained in these two files is exactly the same. On Your Own \u00b6 Convert chromosome-parts.owl into the following formats: obo (OBO Format), ofn (OWL Functional), and omn (OWL Manchester). Open each file and take a minute to scroll through (we don't expect you to be able to read these, they're mostly meant for computers!) Why do you think we need these different serializations? What do you think the purpose of OWL Manchester vs. RDF/XML is? Extract \u00b6 Sometimes we only want to browse or share a subset of an ontology, especially with some of the larger OBO Foundry ontologies. There are two main methods for creating subsets: MIREOT SLME Right now, we will use use MIREOT and talk more about SLME in our class session. MIREOT makes sure that you have the minimal amount of information you need to reuse an existing ontology term. It allows us to extract a small portion of the class hierarchy by specifying upper and lower boundaries, which you will see in the example below. We need to know the identifiers (as CURIEs) of the terms that we want to set as our boundaries. Let's Try It! \u00b6 Open chromosome-parts.owl in Protege and open the Class hierarchy. We are going to create a subset relevant to the term \"chromosome\". First, we will find the CURIE of our desired term. Search for \"chromosome\" and find the \"id\" annotation property. This will be our lower term. Right now, we won't set an upper boundary. That means this subset will go all the way up to the top-level ancestor of \"chromosome\". Return to your terminal and enter the following command (where the --lower-term is the CURIE that we just found): robot extract --method MIREOT --input chromosome-parts.owl --lower-term GO:0005694 --output chromosome-full.owl Now open chromosome-full.owl in Protege and open the Class hierarchy. When you open the \"cellular_component\" node, you'll notice that most of the terms are gone! Both \"organelle\" and \"intracellular part\" remain because they are in the path between \"chromosome\" and the top-level \"cellular_component\". Keep clicking down and you'll find \"chromosome\" at the very bottom. Since \"chromosome\" has two named parents, both of those parents are included, which is why we ended up with \"organelle\" and \"intracellular part\". Now let's try it with an upper term. This time, we want \"organelle\" to be the upper boundary. Find the CURIE for \"organelle\". Return to your terminal and enter the following command (where the --upper-term is the new CURIE we just found): robot extract --method MIREOT \\ --input chromosome-parts.owl \\ --lower-term GO:0005694 \\ --upper-term GO:0043226 \\ --output chromosome.owl Open chromosome.owl and again return to the Class hierarchy. This time, we see \"organelle\" directly below owl:Thing . \"intracellular part\" is also now missing because it does not fall under \"organelle\". On Your Own \u00b6 Play with different upper- and lower-level terms to create different subsets Compare the terms that are in the subsets to the terms in the original chromosome-parts.owl file. What is missing from the terms in the subsets? What has been included as our \"minimal\" information? Template \u00b6 Most of the knowledge encapsulated in ontologies comes from domain experts. Often, these domain experts are not computer scientists and are not familiar with the command line. Luckily, most domain experts are familiar with spreadsheets! ROBOT provides a way to convert spreadsheets into OWL ontologies using template strings . We'll get more into these during the class session, but if you want to get a head start, they are all documented here . Essentially, the first row of a ROBOT template is a human-readable header. The second row is the ROBOT template string. Each row below that represents an entity to be created in the output ontology. We can create new entities by giving them new IDs, but we can also reference existing entities just by label. For now, we're going to create a new, small ontology with new terms using a template. Let's Try It! \u00b6 Download (or copy/paste) the animals.tsv file and move it to the basic-subclass/ folder (or whatever folder you would like to work in; we will not be using any of the Ontology 101 files anymore). This contains the following data: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000001 animal Any animal in the world. obo:0000002 canine animal A member of the genus Canis. obo:0000003 feline animal A member of the genus Felis. In the first column, we use the special ID keyword to say that this is our term's unique identifier. The second column contains the LABEL keyword which is a shortcut for the rdfs:label annotation property. The third column uses the SC keyword to state that this column will be a subclass statement. The % sign is replaced by the value in the cell. We'll talk more about this keyword and the % symbol during the class session. Finally, the last column begins with A to denote that this will be an annotation, and then is followed by the annotation property we're using. Just looking at the template, you can begin to predict what a class hierarchy using these terms would look like in an ontology. We can turn this into reality! In your terminal, enter the following command: robot template --template animals.tsv --output animals.owl Note that in this command, we don't use the --input parameter. That parameter is reserved for input ontologies, and we are not using one right now. More on this later. Open animals.owl in Protege, and you'll be able to see the class hierarchy we defined in the template as an actual structure. Now let's make another small ontology that reuses some terms from our animals.owl file. Download (or copy/paste) animals2.tsv into the same folder. This contains the following: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000004 dog canine A member of the subspecies Canis lupus familiaris. obo:0000005 cat feline A member of the species Felis catus. You'll notice that we are referencing two terms from our other spreadsheet in this one. In your terminal, enter the following command: robot template --input animals.owl --template animals2.tsv --output animals2.owl This time, we did use the --input parameter and provided the animals ontology we just created. This allows us to use any term in the animals.owl file in our animals2.tsv template and ROBOT will know what we're talking about. Go ahead and open animals2.owl in Protege. What's missing? The parent classes for \"dog\" and \"cat\" don't have labels, and the \"animal\" term is missing entirely. This is because, even though ROBOT knew about these classes, we didn't ask for the original ontology to be included in the output, so no axioms from that ontology can be found in this newly-created one. Next week, we'll learn about combining ontologies with the Merge command. For now, let's add the original animals.owl file as an import: Go to the \"Active ontology\" tab and find the \"Imported ontologies\" section at the bottom Click the + next to \"Direct imports\" Select \"Import an ontology contained in a local file\" and click Continue Browse for the path to animals.owl , click Continue, and then click Finish Prot\u00e9g\u00e9 will now load animals.owl as an import. When you return to the Entities tab, you'll see all those upper-level terms. Note the difference in how the terms are displayed in the class hierarchy. On Your Own \u00b6 Try adding another class or two to the animals.tsv template and regenerating animals.owl . Can you create your own template?","title":"ROBOT Mini-Tutorial 1 - Convert, Extract and Template"},{"location":"tutorial/robot-tutorial-1/#robot-mini-tutorial-1-convert-extract-and-template","text":"Last week, we were introduced to ROBOT for quality control and generating reports about terms in an ontology. This week, we will learn about three new ROBOT commands: Convert Extract Template Before starting this tutorial, either: make sure Docker is running and you are in the container download and install ROBOT for your operating system We will be using the files from the Ontologies 101 Tutorial . In your terminal, navigate to the repository that you cloned and then into the BDK14_exercises folder.","title":"ROBOT Mini-Tutorial 1: Convert, Extract and Template"},{"location":"tutorial/robot-tutorial-1/#convert","text":"So far, we have been saving our ontologies in Protege using the default RDF/XML syntax, but there are many flavors of OWL. We will discuss each of these serializations in more detail during the class session, but ROBOT supports the following: owl - RDF/XML owx - OWL/XML ttl - Turtle obo - OBO Format ofn - OWL Functional omn - OWL Manchester json - obographs JSON","title":"Convert"},{"location":"tutorial/robot-tutorial-1/#lets-try-it","text":"Navigate to the basic-subclass/ folder. Open chromosome-parts.owl in your text editor and you will see it's in RDF/XML format. We're going to take this file and convert it to Turtle ( ttl ) serialization. Return to your terminal and enter the following command: robot convert --input chromosome-parts.owl --format ttl --output chromosome-parts.ttl ROBOT convert is smart about detecting formats, so since the output file ends with .ttl , the --format ttl parameter isn't really required. If you wanted to use a different file ending, say .owl , you will need to include the format flag to force ROBOT to write Turtle. Now open chromosome-parts.ttl in your text editor and see what's changed! RDF/XML and Turtle are very different serializations, but the actual data that is contained in these two files is exactly the same.","title":"Let's Try It!"},{"location":"tutorial/robot-tutorial-1/#on-your-own","text":"Convert chromosome-parts.owl into the following formats: obo (OBO Format), ofn (OWL Functional), and omn (OWL Manchester). Open each file and take a minute to scroll through (we don't expect you to be able to read these, they're mostly meant for computers!) Why do you think we need these different serializations? What do you think the purpose of OWL Manchester vs. RDF/XML is?","title":"On Your Own"},{"location":"tutorial/robot-tutorial-1/#extract","text":"Sometimes we only want to browse or share a subset of an ontology, especially with some of the larger OBO Foundry ontologies. There are two main methods for creating subsets: MIREOT SLME Right now, we will use use MIREOT and talk more about SLME in our class session. MIREOT makes sure that you have the minimal amount of information you need to reuse an existing ontology term. It allows us to extract a small portion of the class hierarchy by specifying upper and lower boundaries, which you will see in the example below. We need to know the identifiers (as CURIEs) of the terms that we want to set as our boundaries.","title":"Extract"},{"location":"tutorial/robot-tutorial-1/#lets-try-it_1","text":"Open chromosome-parts.owl in Protege and open the Class hierarchy. We are going to create a subset relevant to the term \"chromosome\". First, we will find the CURIE of our desired term. Search for \"chromosome\" and find the \"id\" annotation property. This will be our lower term. Right now, we won't set an upper boundary. That means this subset will go all the way up to the top-level ancestor of \"chromosome\". Return to your terminal and enter the following command (where the --lower-term is the CURIE that we just found): robot extract --method MIREOT --input chromosome-parts.owl --lower-term GO:0005694 --output chromosome-full.owl Now open chromosome-full.owl in Protege and open the Class hierarchy. When you open the \"cellular_component\" node, you'll notice that most of the terms are gone! Both \"organelle\" and \"intracellular part\" remain because they are in the path between \"chromosome\" and the top-level \"cellular_component\". Keep clicking down and you'll find \"chromosome\" at the very bottom. Since \"chromosome\" has two named parents, both of those parents are included, which is why we ended up with \"organelle\" and \"intracellular part\". Now let's try it with an upper term. This time, we want \"organelle\" to be the upper boundary. Find the CURIE for \"organelle\". Return to your terminal and enter the following command (where the --upper-term is the new CURIE we just found): robot extract --method MIREOT \\ --input chromosome-parts.owl \\ --lower-term GO:0005694 \\ --upper-term GO:0043226 \\ --output chromosome.owl Open chromosome.owl and again return to the Class hierarchy. This time, we see \"organelle\" directly below owl:Thing . \"intracellular part\" is also now missing because it does not fall under \"organelle\".","title":"Let's Try It!"},{"location":"tutorial/robot-tutorial-1/#on-your-own_1","text":"Play with different upper- and lower-level terms to create different subsets Compare the terms that are in the subsets to the terms in the original chromosome-parts.owl file. What is missing from the terms in the subsets? What has been included as our \"minimal\" information?","title":"On Your Own"},{"location":"tutorial/robot-tutorial-1/#template","text":"Most of the knowledge encapsulated in ontologies comes from domain experts. Often, these domain experts are not computer scientists and are not familiar with the command line. Luckily, most domain experts are familiar with spreadsheets! ROBOT provides a way to convert spreadsheets into OWL ontologies using template strings . We'll get more into these during the class session, but if you want to get a head start, they are all documented here . Essentially, the first row of a ROBOT template is a human-readable header. The second row is the ROBOT template string. Each row below that represents an entity to be created in the output ontology. We can create new entities by giving them new IDs, but we can also reference existing entities just by label. For now, we're going to create a new, small ontology with new terms using a template.","title":"Template"},{"location":"tutorial/robot-tutorial-1/#lets-try-it_2","text":"Download (or copy/paste) the animals.tsv file and move it to the basic-subclass/ folder (or whatever folder you would like to work in; we will not be using any of the Ontology 101 files anymore). This contains the following data: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000001 animal Any animal in the world. obo:0000002 canine animal A member of the genus Canis. obo:0000003 feline animal A member of the genus Felis. In the first column, we use the special ID keyword to say that this is our term's unique identifier. The second column contains the LABEL keyword which is a shortcut for the rdfs:label annotation property. The third column uses the SC keyword to state that this column will be a subclass statement. The % sign is replaced by the value in the cell. We'll talk more about this keyword and the % symbol during the class session. Finally, the last column begins with A to denote that this will be an annotation, and then is followed by the annotation property we're using. Just looking at the template, you can begin to predict what a class hierarchy using these terms would look like in an ontology. We can turn this into reality! In your terminal, enter the following command: robot template --template animals.tsv --output animals.owl Note that in this command, we don't use the --input parameter. That parameter is reserved for input ontologies, and we are not using one right now. More on this later. Open animals.owl in Protege, and you'll be able to see the class hierarchy we defined in the template as an actual structure. Now let's make another small ontology that reuses some terms from our animals.owl file. Download (or copy/paste) animals2.tsv into the same folder. This contains the following: CURIE Label Parent Comment ID LABEL SC % A rdfs:comment obo:0000004 dog canine A member of the subspecies Canis lupus familiaris. obo:0000005 cat feline A member of the species Felis catus. You'll notice that we are referencing two terms from our other spreadsheet in this one. In your terminal, enter the following command: robot template --input animals.owl --template animals2.tsv --output animals2.owl This time, we did use the --input parameter and provided the animals ontology we just created. This allows us to use any term in the animals.owl file in our animals2.tsv template and ROBOT will know what we're talking about. Go ahead and open animals2.owl in Protege. What's missing? The parent classes for \"dog\" and \"cat\" don't have labels, and the \"animal\" term is missing entirely. This is because, even though ROBOT knew about these classes, we didn't ask for the original ontology to be included in the output, so no axioms from that ontology can be found in this newly-created one. Next week, we'll learn about combining ontologies with the Merge command. For now, let's add the original animals.owl file as an import: Go to the \"Active ontology\" tab and find the \"Imported ontologies\" section at the bottom Click the + next to \"Direct imports\" Select \"Import an ontology contained in a local file\" and click Continue Browse for the path to animals.owl , click Continue, and then click Finish Prot\u00e9g\u00e9 will now load animals.owl as an import. When you return to the Entities tab, you'll see all those upper-level terms. Note the difference in how the terms are displayed in the class hierarchy.","title":"Let's Try It!"},{"location":"tutorial/robot-tutorial-1/#on-your-own_2","text":"Try adding another class or two to the animals.tsv template and regenerating animals.owl . Can you create your own template?","title":"On Your Own"},{"location":"tutorial/robot-tutorial-2/","text":"ROBOT Tutorial 2: Annotate, Merge, Reason and Diff \u00b6 In week 6, we got some hands-on experience with ROBOT using convert , extract , and template . This week, we will learn four new ROBOT commands: Annotate Merge Reason Diff The goal of these and previous commands is to build up to creating an ontology release workflow. Before starting this tutorial, either: make sure Docker is running and you are in the container download and install ROBOT for your operating system To start, we will be working in the same folder as the first ROBOT Mini-Tutorial . Navigate to this folder in your terminal and list the contents of the current directory by running ls . You should see catalog-v001.xml listed as one of these files. We want to delete this so that we can fix the ontology IRI problem we ran into last week! Before going any further with this tutorial, do this by running either del catalog-v001.xml for Windows or rm catalog-v001.xml if you're using Docker, MacOS, or other Linux system. Annotate \u00b6 The annotate command allows you to attach metadata to your ontology in the form of IRIs and ontology annotations. Like the annotations on a term, ontology annotations help users to understand how they can use the ontology. Ontology IRIs \u00b6 As we discussed during previous parts of the course, ontology IRIs are very important! We saw how importing an ontology without an IRI into another ontology without an IRI can cause some problems in the catalog-v001.xml file. We're going to fix that problem by giving IRIs to both our animals.owl and animals2.owl files. Let's start with animals.owl : robot annotate --input animals.owl \\ --ontology-iri http://example.com/animals.owl \\ --output animals.owl You'll notice we gave the same file name as the input file; we're just updating our previous file so we don't need to do this in a separate OWL file. On your own, give animals2.owl the ontology IRI http://example.com/animals2.owl . Remember that, in reality, we always want our ontology IRIs to be resolvable , so these would be pretty bad IRIs for an actual ontology. Let's fix our import statement now. Open animals2.owl in Prot\u00e9g\u00e9 and go to the Entities tab. You'll see that even though we still have the import statement in the Active ontology tab, the top-level terms are no longer labeled. Since we changed the ontology IRI, Prot\u00e9g\u00e9 can no longer resolve our local file (because the catalog-v001.xml file was not updated). Go back to the Active ontology tab and click the X to the right of our original import. Then, re-add animals.owl as an import using the same steps as last time. When you return to the Entities tab, you'll once again see the labels of the top-level terms. Version IRIs \u00b6 When we release our ontologies, we want to make sure to include a version IRI. Like the ontology IRI, this should always resolve to the version of the ontology at the time of the release. For clarity, we usually use dates in our version IRIs in the OBO Foundry. That way, you know when you navigate to a specific version IRI, that's what the ontology looked like on that date. (Note: edit files don't usually have version IRIs as they are always changing, and we don't expect to be able to point to a stable version) While you can add a version IRI in Prot\u00e9g\u00e9, if you're trying to create an automated release workflow, this is a manual step you don't want to have to include. Keeping it in your release workflow also makes sure that the verion IRIs are consistent (we'll see how to do this with make later). For now, let's add a version IRI to animals.owl (feel free to replace the 2021-05-20 with today's date): robot annotate --input animals.owl \\ --version-iri http://example.com/animals/2021-05-20/animals.owl \\ --output animals.owl Let's break down this version IRI. We have the host ( http://example.com/ ) followed by our ontology's namespace ( animals ). Next, we provided the date in the format of YYYY-MM-DD . Finally, we have the name of the file. This is standard for OBO Foundry, except with a different host. For example, you can find a release of OBI from April 6, 2021 at http://purl.obolibrary.org/obo/obi/2021-04-06/obi.owl . In this case, the host is http://purl.obolibrary.org/obo/ . Of course, you may see different patterns in non-OBO-Foundry ontologies, but they should always resolve (hopefully!). Go ahead and open or reload animals.owl in Protege. You'll see in the Active Ontology tab that now both the ontology IRI and version IRI fields are filled out. Ontology Annotations \u00b6 In addition to ontology and version IRIs, you may also want to add some other metadata to your ontology. For example, when we were introduced to report , we added a description to the ontology to fix one of the report problems. The three ontology annotations that are required by the OBO Foundry are: Title ( dc11:title ) License ( dc:license ) Description ( dc11:description ) These three annotation properties all come from the Dublin Core , but they have slightly different namespaces. This is because DC is split into two parts: the /terms/ and /elements/1.1/ namespaces. Just remember to double check that you're using the correct namespace. If you click on the DC link, you can find the complete list of DC terms in their respective namespaces. ROBOT contains some built-in prefixes, which can be found here . The prefix dc: corresponds to the /terms/ namespace and dc11: to /elements/1.1/ . You may see different prefixes used (for example, /terms/ is sometimes dcterms: or just terms: ), but the full namespace is what really matters as long as the prefix is defined somewhere. Let's go ahead and add a title and description to our animals.owl file. We'll do this using the --annotation option, which expects two arguments: (1) the CURIE of the annotation property, (2) the value of the annotation. The value of the annotation must be enclosed in double quotes if there are spaces. You can use any annotation property you want here, and include as many as you want! For now, we'll start with two: robot annotate --input animals.owl \\ --annotation dc11:title \"Animal Ontology\" \\ --annotation dc11:description \"An ontology about animals\" \\ --output animals.owl --annotation adds these as strings, but remember that an annotation can also point to an link or IRI. We want our license to be a link, so we'll use --link-annotation instead to add that: robot annotate --input animals.owl \\ --link-annotation dc:license https://creativecommons.org/licenses/by/4.0/ \\ --output animals.owl OBO Foundry recommends using Creative Commons for all licenses. We just gave our ontology the most permissive of these, CC-BY . When you open animals.owl in Prot\u00e9g\u00e9 again, you'll see these annotations added to the Active ontology tab. You can also click on the CC-BY link! Merge \u00b6 We've already learned how to include external ontologies as imports. Usually, for the released version of an ontology, the imports are merged in so that all contents are in one file. Another reason you may want to merge two ontologies is if you're adding new terms to an ontology using template , like how we created new animal terms in animals2.tsv last time. We're going to demonstrate two methods of merging now. The first involves merging two (or more!) separate files and the second involves merging all imports into the current input ontology. Merging Multiple Files \u00b6 First, copy animals2.owl to animals-new.owl . In Windows, this command is copy animals2.owl animals-new.owl . For Docker and other Linux operating systems, this is cp animals2.owl animals-new.owl . Open animals-new.owl in Prot\u00e9g\u00e9 and remove the import we added last time. This is done in the Imported ontologies section of the Active ontology tab. Just click the X on the right side of the imported animals ontology. Don't forget to save! Continuing with the animals.owl file we created last week, now run the following command: robot merge --input animals.owl --input animals-new.owl --output animals-full.owl When you just import an external ontology into your ontology, you'll notice in the Prot\u00e9g\u00e9 class hierarchy that all terms from the external ontology are a less-bold text than internal terms. This can be seen when you open animals2.owl , where we imported animals.owl . This is simply Prot\u00e9g\u00e9's way of telling us that these terms are not part of your current ontology . Now that we've merged these two ontologies together, when you open animals-full.owl in Prot\u00e9g\u00e9, you'll see that all the terms are bold. By default, the output ontology will get the ontology IRI of the first input ontology. We picked animals.owl as our first ontology here because this is the ontology that we're adding terms to, so we want our new output ontology to replace the original while keeping the same IRI. merge will also copy over all the ontology annotations from animals.owl (the first input) into the new file. The annotations from animals2.owl are ignored, but we'll talk more about this in our class session. If we were editing an ontology in the wild, we'd probably now replace the original with this new file using cp or copy . For now, don't replace animals.owl because we'll need it for this next part. IMPORTANT : Be very careful to check that the format is the same if you're replacing a file! Remember, you can always output OWL Functional syntax or another syntax by ending your output with .ofn , for example: --output animals-full.ofn . Merging Imports \u00b6 When we want to merge all our imports into our working ontology, we call this collapsing the import closure . Luckily (since we're lazy), you don't need to type out each of your imports as an input to do this. We already have animals.owl imported into animals2.owl . Let's collapse the import closure: robot merge --input animals2.owl --collapse-import-closure true --output animals-full-2.owl Even though we gave this a different file name, if you open animals-full-2.owl in Prot\u00e9g\u00e9, you'll notice that it's exactly the same as animals-full.owl ! This is because we merged the same files together, just in a slightly different way. This time, though, the ontology IRI is the one for animals2.owl , not animals.owl . That is because that was our first input file. Reason \u00b6 As we saw in the prepwork for Week 5, running a reasoner in Prot\u00e9g\u00e9 creates an inferred class hierarchy. In the OBO Foundry, releases versions of ontologies usually have this inferred hierarchy asserted , so you see the full inferred hierarchy when you open the ontology without running the reasoner. ROBOT reason allows us to output a version of the ontology with these inferences asserted. As we discussed, ELK and HermiT are the two main reasoners you'll be using. Instead of using our example ontologies (the asserted and inferred hierarchies for these will look exactly the same), we're going to use another ontology from the Ontologies 101 tutorial from week 5. Navigate back to that directory and then navigate to BDK14_exercises/basic-classification . Like running the reasoner in Prot\u00e9g\u00e9, running reason does three things: Check for inconsistency Check for unsatisfiable classes Assert the inferred class hierarchy Remember, when we run the reasoner in Prot\u00e9g\u00e9, if the ontology is inconsistent, reason will fail. If there are unsatisfiable classes, these will be asserted as owl:Nothing . ROBOT will always fail in both cases, but has some tools to help us figure out why. Let's introduce an unsatifiable class into our test and see what happens. First, let's make a copy of ubiq-ligase-complex.owl and call this new file unreasoned.owl ( copy or cp ). Open unreasoned.owl in Prot\u00e9g\u00e9 and follow the steps below. These are things we've covered in past exercises, but if you get stuck, please don't hesitate to reach out. Find 'organelle' in the class hierarchy below 'cellular_component' (or just search for it by label) Make 'organelle' disjoint with 'organelle part' (either use the class hierarchy or type it in the expression editor) Find 'intracellular organelle part' below 'intracellular part' or 'organelle part' (or search for it by label) Add 'organelle' as a parent class to 'intracellular organelle part' (remember that you only need to include the single quotes if the label has spaces) Like we did in the Disjointness part of the Ontologies 101 tutorial, we've made 'intracellular organelle part' a subclass of two classes that should have no overlap based on the disjointness axiom. Save the ontology and return to your terminal. Now, we'll run reason . The default reasoner is ELK, but you can specify the reasoner you want to use with the --reasoner option. For now, we'll just use ELK. robot reason --input unreasoned.owl --output unsatisfiable.owl You'll notice that ROBOT printed an error message telling us that the term with the IRI http://purl.obolibrary.org/obo/GO_0044446 is unsatisfiable and ROBOT didn't create unsatisfiable.owl . This is ideal for automated pipelines where we don't want to be releasing unsatisfiable classes. We can still use ROBOT to investigate the issue, though. It already gave us the IRI, but we can get more details using the --dump-unsatisfiable option. We won't provide an output this time because we know it won't succeed. robot reason --input unreasoned.owl --dump-unsatisfiable unsatisfiable.owl You can open unsatisfiable.owl in Prot\u00e9g\u00e9 and see that 'intracellular organelle part' is not the only term included, even though it was the only unsatisfiable class. Like with the SLME method of extraction, all the terms used in unsatisfiable class or classes logic are included in this unsatisfiable module. We can then use Prot\u00e9g\u00e9 to dig a little deeper in this small module. This is especially useful when working with large ontologies and/or the HermiT reasoner, which both can take quite some time. By extracting a smaller module, we can run the reasoner again in Prot\u00e9g\u00e9 to get detailed explanations. In this case, we already know the problem, so we don't need to investigate any more. Now let's reason over the original ubiq-ligase-complex.owl and see what happens: robot reason --input ubiq-ligase-complex.owl --output reasoned.owl If you just open reasoned.owl in Prot\u00e9g\u00e9, you won't really notice a different between this and the input file unless you do some digging. This takes us to our next command... Diff \u00b6 The diff command can be used to compare the axioms in two ontologies to see what has been added and what has been removed. While the diffs on GitHub are useful for seeing what changed, it can be really tough for a human to read the raw OWL formats. Using ROBOT, we can output these diffs in a few different formats (using the --format option): plain : plain text with just the added and removed axioms listed in OWL functional syntax (still tough for a human to read, but could be good for passing to other scripts) pretty : similar to plain , but the IRIs are replaced with CURIEs and labels where available (still hard to read) html : a nice, sharable HTML file with the diffs sorted by term markdown : like the HTML diff, but in markdown for easy sharing on platforms like GitHub (perfect for pull requests!) We're going to generate an HTML diff of ubiq-ligase-complex.owl compared to the new reasoned.owl file to see what inferences have been asserted. diff takes a left (\"original\") and a right (\"new\") input to compare. robot diff --left ubiq-ligase-complex.owl \\ --right reasoned.owl \\ --format html \\ --output diff.html Open diff.html in your browser side-by-side with reasoned.owl and you can see how the changes look in both. Homework question : Running reason should assert inferences, yet there are some removed axioms in our diff. Why do you think these axioms were removed?","title":"ROBOT Mini-Tutorial 2 - Annotate, Merge, Reason and Diff"},{"location":"tutorial/robot-tutorial-2/#robot-tutorial-2-annotate-merge-reason-and-diff","text":"In week 6, we got some hands-on experience with ROBOT using convert , extract , and template . This week, we will learn four new ROBOT commands: Annotate Merge Reason Diff The goal of these and previous commands is to build up to creating an ontology release workflow. Before starting this tutorial, either: make sure Docker is running and you are in the container download and install ROBOT for your operating system To start, we will be working in the same folder as the first ROBOT Mini-Tutorial . Navigate to this folder in your terminal and list the contents of the current directory by running ls . You should see catalog-v001.xml listed as one of these files. We want to delete this so that we can fix the ontology IRI problem we ran into last week! Before going any further with this tutorial, do this by running either del catalog-v001.xml for Windows or rm catalog-v001.xml if you're using Docker, MacOS, or other Linux system.","title":"ROBOT Tutorial 2: Annotate, Merge, Reason and Diff"},{"location":"tutorial/robot-tutorial-2/#annotate","text":"The annotate command allows you to attach metadata to your ontology in the form of IRIs and ontology annotations. Like the annotations on a term, ontology annotations help users to understand how they can use the ontology.","title":"Annotate"},{"location":"tutorial/robot-tutorial-2/#ontology-iris","text":"As we discussed during previous parts of the course, ontology IRIs are very important! We saw how importing an ontology without an IRI into another ontology without an IRI can cause some problems in the catalog-v001.xml file. We're going to fix that problem by giving IRIs to both our animals.owl and animals2.owl files. Let's start with animals.owl : robot annotate --input animals.owl \\ --ontology-iri http://example.com/animals.owl \\ --output animals.owl You'll notice we gave the same file name as the input file; we're just updating our previous file so we don't need to do this in a separate OWL file. On your own, give animals2.owl the ontology IRI http://example.com/animals2.owl . Remember that, in reality, we always want our ontology IRIs to be resolvable , so these would be pretty bad IRIs for an actual ontology. Let's fix our import statement now. Open animals2.owl in Prot\u00e9g\u00e9 and go to the Entities tab. You'll see that even though we still have the import statement in the Active ontology tab, the top-level terms are no longer labeled. Since we changed the ontology IRI, Prot\u00e9g\u00e9 can no longer resolve our local file (because the catalog-v001.xml file was not updated). Go back to the Active ontology tab and click the X to the right of our original import. Then, re-add animals.owl as an import using the same steps as last time. When you return to the Entities tab, you'll once again see the labels of the top-level terms.","title":"Ontology IRIs"},{"location":"tutorial/robot-tutorial-2/#version-iris","text":"When we release our ontologies, we want to make sure to include a version IRI. Like the ontology IRI, this should always resolve to the version of the ontology at the time of the release. For clarity, we usually use dates in our version IRIs in the OBO Foundry. That way, you know when you navigate to a specific version IRI, that's what the ontology looked like on that date. (Note: edit files don't usually have version IRIs as they are always changing, and we don't expect to be able to point to a stable version) While you can add a version IRI in Prot\u00e9g\u00e9, if you're trying to create an automated release workflow, this is a manual step you don't want to have to include. Keeping it in your release workflow also makes sure that the verion IRIs are consistent (we'll see how to do this with make later). For now, let's add a version IRI to animals.owl (feel free to replace the 2021-05-20 with today's date): robot annotate --input animals.owl \\ --version-iri http://example.com/animals/2021-05-20/animals.owl \\ --output animals.owl Let's break down this version IRI. We have the host ( http://example.com/ ) followed by our ontology's namespace ( animals ). Next, we provided the date in the format of YYYY-MM-DD . Finally, we have the name of the file. This is standard for OBO Foundry, except with a different host. For example, you can find a release of OBI from April 6, 2021 at http://purl.obolibrary.org/obo/obi/2021-04-06/obi.owl . In this case, the host is http://purl.obolibrary.org/obo/ . Of course, you may see different patterns in non-OBO-Foundry ontologies, but they should always resolve (hopefully!). Go ahead and open or reload animals.owl in Protege. You'll see in the Active Ontology tab that now both the ontology IRI and version IRI fields are filled out.","title":"Version IRIs"},{"location":"tutorial/robot-tutorial-2/#ontology-annotations","text":"In addition to ontology and version IRIs, you may also want to add some other metadata to your ontology. For example, when we were introduced to report , we added a description to the ontology to fix one of the report problems. The three ontology annotations that are required by the OBO Foundry are: Title ( dc11:title ) License ( dc:license ) Description ( dc11:description ) These three annotation properties all come from the Dublin Core , but they have slightly different namespaces. This is because DC is split into two parts: the /terms/ and /elements/1.1/ namespaces. Just remember to double check that you're using the correct namespace. If you click on the DC link, you can find the complete list of DC terms in their respective namespaces. ROBOT contains some built-in prefixes, which can be found here . The prefix dc: corresponds to the /terms/ namespace and dc11: to /elements/1.1/ . You may see different prefixes used (for example, /terms/ is sometimes dcterms: or just terms: ), but the full namespace is what really matters as long as the prefix is defined somewhere. Let's go ahead and add a title and description to our animals.owl file. We'll do this using the --annotation option, which expects two arguments: (1) the CURIE of the annotation property, (2) the value of the annotation. The value of the annotation must be enclosed in double quotes if there are spaces. You can use any annotation property you want here, and include as many as you want! For now, we'll start with two: robot annotate --input animals.owl \\ --annotation dc11:title \"Animal Ontology\" \\ --annotation dc11:description \"An ontology about animals\" \\ --output animals.owl --annotation adds these as strings, but remember that an annotation can also point to an link or IRI. We want our license to be a link, so we'll use --link-annotation instead to add that: robot annotate --input animals.owl \\ --link-annotation dc:license https://creativecommons.org/licenses/by/4.0/ \\ --output animals.owl OBO Foundry recommends using Creative Commons for all licenses. We just gave our ontology the most permissive of these, CC-BY . When you open animals.owl in Prot\u00e9g\u00e9 again, you'll see these annotations added to the Active ontology tab. You can also click on the CC-BY link!","title":"Ontology Annotations"},{"location":"tutorial/robot-tutorial-2/#merge","text":"We've already learned how to include external ontologies as imports. Usually, for the released version of an ontology, the imports are merged in so that all contents are in one file. Another reason you may want to merge two ontologies is if you're adding new terms to an ontology using template , like how we created new animal terms in animals2.tsv last time. We're going to demonstrate two methods of merging now. The first involves merging two (or more!) separate files and the second involves merging all imports into the current input ontology.","title":"Merge"},{"location":"tutorial/robot-tutorial-2/#merging-multiple-files","text":"First, copy animals2.owl to animals-new.owl . In Windows, this command is copy animals2.owl animals-new.owl . For Docker and other Linux operating systems, this is cp animals2.owl animals-new.owl . Open animals-new.owl in Prot\u00e9g\u00e9 and remove the import we added last time. This is done in the Imported ontologies section of the Active ontology tab. Just click the X on the right side of the imported animals ontology. Don't forget to save! Continuing with the animals.owl file we created last week, now run the following command: robot merge --input animals.owl --input animals-new.owl --output animals-full.owl When you just import an external ontology into your ontology, you'll notice in the Prot\u00e9g\u00e9 class hierarchy that all terms from the external ontology are a less-bold text than internal terms. This can be seen when you open animals2.owl , where we imported animals.owl . This is simply Prot\u00e9g\u00e9's way of telling us that these terms are not part of your current ontology . Now that we've merged these two ontologies together, when you open animals-full.owl in Prot\u00e9g\u00e9, you'll see that all the terms are bold. By default, the output ontology will get the ontology IRI of the first input ontology. We picked animals.owl as our first ontology here because this is the ontology that we're adding terms to, so we want our new output ontology to replace the original while keeping the same IRI. merge will also copy over all the ontology annotations from animals.owl (the first input) into the new file. The annotations from animals2.owl are ignored, but we'll talk more about this in our class session. If we were editing an ontology in the wild, we'd probably now replace the original with this new file using cp or copy . For now, don't replace animals.owl because we'll need it for this next part. IMPORTANT : Be very careful to check that the format is the same if you're replacing a file! Remember, you can always output OWL Functional syntax or another syntax by ending your output with .ofn , for example: --output animals-full.ofn .","title":"Merging Multiple Files"},{"location":"tutorial/robot-tutorial-2/#merging-imports","text":"When we want to merge all our imports into our working ontology, we call this collapsing the import closure . Luckily (since we're lazy), you don't need to type out each of your imports as an input to do this. We already have animals.owl imported into animals2.owl . Let's collapse the import closure: robot merge --input animals2.owl --collapse-import-closure true --output animals-full-2.owl Even though we gave this a different file name, if you open animals-full-2.owl in Prot\u00e9g\u00e9, you'll notice that it's exactly the same as animals-full.owl ! This is because we merged the same files together, just in a slightly different way. This time, though, the ontology IRI is the one for animals2.owl , not animals.owl . That is because that was our first input file.","title":"Merging Imports"},{"location":"tutorial/robot-tutorial-2/#reason","text":"As we saw in the prepwork for Week 5, running a reasoner in Prot\u00e9g\u00e9 creates an inferred class hierarchy. In the OBO Foundry, releases versions of ontologies usually have this inferred hierarchy asserted , so you see the full inferred hierarchy when you open the ontology without running the reasoner. ROBOT reason allows us to output a version of the ontology with these inferences asserted. As we discussed, ELK and HermiT are the two main reasoners you'll be using. Instead of using our example ontologies (the asserted and inferred hierarchies for these will look exactly the same), we're going to use another ontology from the Ontologies 101 tutorial from week 5. Navigate back to that directory and then navigate to BDK14_exercises/basic-classification . Like running the reasoner in Prot\u00e9g\u00e9, running reason does three things: Check for inconsistency Check for unsatisfiable classes Assert the inferred class hierarchy Remember, when we run the reasoner in Prot\u00e9g\u00e9, if the ontology is inconsistent, reason will fail. If there are unsatisfiable classes, these will be asserted as owl:Nothing . ROBOT will always fail in both cases, but has some tools to help us figure out why. Let's introduce an unsatifiable class into our test and see what happens. First, let's make a copy of ubiq-ligase-complex.owl and call this new file unreasoned.owl ( copy or cp ). Open unreasoned.owl in Prot\u00e9g\u00e9 and follow the steps below. These are things we've covered in past exercises, but if you get stuck, please don't hesitate to reach out. Find 'organelle' in the class hierarchy below 'cellular_component' (or just search for it by label) Make 'organelle' disjoint with 'organelle part' (either use the class hierarchy or type it in the expression editor) Find 'intracellular organelle part' below 'intracellular part' or 'organelle part' (or search for it by label) Add 'organelle' as a parent class to 'intracellular organelle part' (remember that you only need to include the single quotes if the label has spaces) Like we did in the Disjointness part of the Ontologies 101 tutorial, we've made 'intracellular organelle part' a subclass of two classes that should have no overlap based on the disjointness axiom. Save the ontology and return to your terminal. Now, we'll run reason . The default reasoner is ELK, but you can specify the reasoner you want to use with the --reasoner option. For now, we'll just use ELK. robot reason --input unreasoned.owl --output unsatisfiable.owl You'll notice that ROBOT printed an error message telling us that the term with the IRI http://purl.obolibrary.org/obo/GO_0044446 is unsatisfiable and ROBOT didn't create unsatisfiable.owl . This is ideal for automated pipelines where we don't want to be releasing unsatisfiable classes. We can still use ROBOT to investigate the issue, though. It already gave us the IRI, but we can get more details using the --dump-unsatisfiable option. We won't provide an output this time because we know it won't succeed. robot reason --input unreasoned.owl --dump-unsatisfiable unsatisfiable.owl You can open unsatisfiable.owl in Prot\u00e9g\u00e9 and see that 'intracellular organelle part' is not the only term included, even though it was the only unsatisfiable class. Like with the SLME method of extraction, all the terms used in unsatisfiable class or classes logic are included in this unsatisfiable module. We can then use Prot\u00e9g\u00e9 to dig a little deeper in this small module. This is especially useful when working with large ontologies and/or the HermiT reasoner, which both can take quite some time. By extracting a smaller module, we can run the reasoner again in Prot\u00e9g\u00e9 to get detailed explanations. In this case, we already know the problem, so we don't need to investigate any more. Now let's reason over the original ubiq-ligase-complex.owl and see what happens: robot reason --input ubiq-ligase-complex.owl --output reasoned.owl If you just open reasoned.owl in Prot\u00e9g\u00e9, you won't really notice a different between this and the input file unless you do some digging. This takes us to our next command...","title":"Reason"},{"location":"tutorial/robot-tutorial-2/#diff","text":"The diff command can be used to compare the axioms in two ontologies to see what has been added and what has been removed. While the diffs on GitHub are useful for seeing what changed, it can be really tough for a human to read the raw OWL formats. Using ROBOT, we can output these diffs in a few different formats (using the --format option): plain : plain text with just the added and removed axioms listed in OWL functional syntax (still tough for a human to read, but could be good for passing to other scripts) pretty : similar to plain , but the IRIs are replaced with CURIEs and labels where available (still hard to read) html : a nice, sharable HTML file with the diffs sorted by term markdown : like the HTML diff, but in markdown for easy sharing on platforms like GitHub (perfect for pull requests!) We're going to generate an HTML diff of ubiq-ligase-complex.owl compared to the new reasoned.owl file to see what inferences have been asserted. diff takes a left (\"original\") and a right (\"new\") input to compare. robot diff --left ubiq-ligase-complex.owl \\ --right reasoned.owl \\ --format html \\ --output diff.html Open diff.html in your browser side-by-side with reasoned.owl and you can see how the changes look in both. Homework question : Running reason should assert inferences, yet there are some removed axioms in our diff. Why do you think these axioms were removed?","title":"Diff"},{"location":"tutorial/robot-tutorial-qc/","text":"ROBOT Tutorial: Quality Control with ROBOT \u00b6 In this tutorial you will learn how to set up your QC pipeline with ROBOT report , verify , validate-profile and reason . Preparation \u00b6 You should know how to run ROBOT commands on your machine You should have a basic understanding of OWL and reasoning Overview \u00b6 Quality control is a very large concern in ontologies. For example, we want to make sure that our editors use the right annotation properties to attach metadata to terms (such as a date, or a label), or to make sure that our last edit did not accidentally introduce a logical error. In ROBOT, we have four commands that help us in particular to ensure the quality of our ontologies: ROBOT validate-profile : Ensures that your ontology is a syntactically valid OWL ontology. ROBOT verify : Define \"bad examples\", i.e. situations you want to avoid as SPARQL queries and use verify to ensure they do not appear in your ontology. ROBOT report : Use dozens of time tested best practice checks curated by the OBO Technical Working Group to check your ontology for typical errors, like missing labels or wrong license declarations. ROBOT reason : Use reason to ensure that your ontology is consistent and coherent and test the \" unique name assumption \". In the following, we will learn about all of these and how they fit in the wider concerns of ontology quality control. Download test ontology \u00b6 Download example.owl , or get it via the command line: curl https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/robot_tutorial_qc/example.owl > example.owl Let us ensure we are using the same ROBOT version: robot --version We see: ROBOT version 1.8.3 ROBOT validate-profile \u00b6 ROBOT validate-profile : Ensures that your ontology is a syntactically valid OWL ontology. This is the absolute minimum check - some \"violations\" to OWL 2 DL validity cause the reasoner to behave in unexpected and wrong ways! robot validate-profile --profile DL -i example.owl Thankfully, our test ontology is in valid OWL DL: OWL 2 DL Profile Report: [Ontology and imports closure in profile] This check is overlooked by a lot of OWL Ontology developers despite its importance to ensure both a predictable behaviour of the reasoner and of parsing tools. See here for an example where an ontology was not in OWL DL profile, causing various problems for parsing and computation: https://github.com/Orphanet/ORDO/issues/32. ROBOT report \u00b6 Let us generate a simple report: robot report -i example.owl -o report.html ROBOT report will do two things: It will print out the number of errors (violations) and an indication that the report failed : Violations: 11 ----------------- ERROR: 5 WARN: 4 INFO: 2 ERROR Report failed! And it will provide you with a report file, report.html Let us look at the file in a browser (simply double-click on the html file the way you would open a PDF). Your report should look similar to this: While there are other formats you can export your report to , HTML is a great format which not only offers useful colour coding, but also allows us to click on the related classes and properties and, more importantly, the checks to find our what they mean (for an overview of all ROBOT report checks see here ). Exercise \u00b6 We will leave it to the reader as an exercise to try and fix all the errors indicated by the report! Advanced usage of ROBOT report \u00b6 Customisation \u00b6 While by far the most widely spread usage of ROBOT report is to check for OBO best practices, it is possible to customise the report by removing certain OBO ontology checks and adding custom ones. Lets first create a simple profile.txt in our directory and add the following lines: WARN annotation_whitespace ERROR missing_ontology_description ERROR missing_definition ERROR missing_ontology_license ERROR missing_ontology_title ERROR misused_obsolete_label ERROR multiple_labels Now we tell ROBOT to run the command using our custom profile rather than the default ROBOT profile: robot report -i example.owl --profile profile.txt -o report.html The resulting report looks different: In particular, some checks like missing_superclass which we did not care about for our use case are not shown at all anymore, and others, such as missing_definition are now considered ERROR (red) rather than WARN (warning, yellow) because for our use case, we have decided that definitions on terms are mandatory. ROBOT verify \u00b6 ROBOT verify allows us to define QC checks for undesirable situation (we sometimes call this \"anti-pattern\") using the SPARQL query language. The idea is simple: we write a SPARQL query for the thing we do not want . For example, we can use SPARQL to look for classes with more than one label. Then, we feed this query to ROBOT verify. ROBOT verify than ensures that the query has no answers , i.e the thing we do not want actually does not happen: PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT DISTINCT ?entity ?property ?value WHERE { VALUES ?property { rdfs:label } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) . FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) } ORDER BY ?entity Let us safe this query now in our working directory as bad_labels.sparql and run the following: robot verify -i example.owl --queries bad_labels.sparql ROBOT will output this to tell us which terms have violations: FAIL Rule bad_labels.sparql: 2 violation(s) entity,property,value http://purl.obolibrary.org/obo/OBI_0002986,http://www.w3.org/2000/01/rdf-schema#label,CT scan http://purl.obolibrary.org/obo/OBI_0002986,http://www.w3.org/2000/01/rdf-schema#label,computed tomography imaging assay Now the cool thing with verify is that we can basically feed SPARQL SELECT queries in whatever shape or form we want. To make error messages more readable for curators, you can even encode a proper error message: PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT DISTINCT ?error WHERE { VALUES ?property { rdfs:label } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) . FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) BIND(CONCAT(\"Entity \",STR(?entity),\" uses two different labels: (1) \",STR(?value),\" and (2) \",STR(?value2)) as ?error) } ORDER BY ?entity This time, when running the query, we get: FAIL Rule bad_labels.sparql: 2 violation(s) error Entity http://purl.obolibrary.org/obo/OBI_0002986 uses two different labels: (1) CT scan and (2) computed tomography imaging assay Entity http://purl.obolibrary.org/obo/OBI_0002986 uses two different labels: (1) computed tomography imaging assay and (2) CT scan Which appears much more readable! You can tweak the output in whatever way you think is best. Two things about this: You cannot do this very well with ROBOT report : despite the ability to include custom queries, all queries must start with: SELECT DISTINCT ?entity ?property ?value WHERE This is one of the reasons we still like using ROBOT verify, despite the fact that ROBOT report can also be extended with custom checks. Another cool thing about verify is that you can add the --output-dir results/ parameter to your query to get ROBOT to export the query results as TSV files. This can be useful if you have many QC queries and need to work with them independently of the checks. Note that ROBOT notices two errors despite there being only 1, technically speaking. This is because the WHERE clause in SPARQL which twice (one where label 1 is first, one where label 2 is first). You can be smart about it and get around it by sorting your results before binding them, but for most use cases this hack is hardly worth the effort. ROBOT reason \u00b6 This is not an exhaustive tutorial for ROBOT reason (for more, see here ). We only want to get across two checks that we feel absolutely every ontology developer should know about. The \"distinct scope\" assumption \u00b6 In most cases, we do not want to define the exact same concept twice. There are some exceptions, for example when we align ontologies such as CHEBI and GO which may have overlapping scope, but in 99.9% of the cases, having the reasoner infer that two classes are the same points to a mistake in the axiomatisation. Let us check that we do not have any such unintended equivalencies in our ontology: robot reason -i example.owl --equivalent-classes-allowed none ROBOT will note that: ERROR No equivalent class axioms are allowed ERROR Equivalence: <http://purl.obolibrary.org/obo/TEST_0600047> == <http://purl.obolibrary.org/obo/OBI_0600047> Further investigation in Protege will reveal that TEST_0600047 and OBI_0600047 are subclasses of each other, which causes the reasoner to infer that they are equivalent.","title":"ROBOT Mini-Tutorial QC - Quality Control with report, verify and query"},{"location":"tutorial/robot-tutorial-qc/#robot-tutorial-quality-control-with-robot","text":"In this tutorial you will learn how to set up your QC pipeline with ROBOT report , verify , validate-profile and reason .","title":"ROBOT Tutorial: Quality Control with ROBOT"},{"location":"tutorial/robot-tutorial-qc/#preparation","text":"You should know how to run ROBOT commands on your machine You should have a basic understanding of OWL and reasoning","title":"Preparation"},{"location":"tutorial/robot-tutorial-qc/#overview","text":"Quality control is a very large concern in ontologies. For example, we want to make sure that our editors use the right annotation properties to attach metadata to terms (such as a date, or a label), or to make sure that our last edit did not accidentally introduce a logical error. In ROBOT, we have four commands that help us in particular to ensure the quality of our ontologies: ROBOT validate-profile : Ensures that your ontology is a syntactically valid OWL ontology. ROBOT verify : Define \"bad examples\", i.e. situations you want to avoid as SPARQL queries and use verify to ensure they do not appear in your ontology. ROBOT report : Use dozens of time tested best practice checks curated by the OBO Technical Working Group to check your ontology for typical errors, like missing labels or wrong license declarations. ROBOT reason : Use reason to ensure that your ontology is consistent and coherent and test the \" unique name assumption \". In the following, we will learn about all of these and how they fit in the wider concerns of ontology quality control.","title":"Overview"},{"location":"tutorial/robot-tutorial-qc/#download-test-ontology","text":"Download example.owl , or get it via the command line: curl https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/robot_tutorial_qc/example.owl > example.owl Let us ensure we are using the same ROBOT version: robot --version We see: ROBOT version 1.8.3","title":"Download test ontology"},{"location":"tutorial/robot-tutorial-qc/#robot-validate-profile","text":"ROBOT validate-profile : Ensures that your ontology is a syntactically valid OWL ontology. This is the absolute minimum check - some \"violations\" to OWL 2 DL validity cause the reasoner to behave in unexpected and wrong ways! robot validate-profile --profile DL -i example.owl Thankfully, our test ontology is in valid OWL DL: OWL 2 DL Profile Report: [Ontology and imports closure in profile] This check is overlooked by a lot of OWL Ontology developers despite its importance to ensure both a predictable behaviour of the reasoner and of parsing tools. See here for an example where an ontology was not in OWL DL profile, causing various problems for parsing and computation: https://github.com/Orphanet/ORDO/issues/32.","title":"ROBOT validate-profile"},{"location":"tutorial/robot-tutorial-qc/#robot-report","text":"Let us generate a simple report: robot report -i example.owl -o report.html ROBOT report will do two things: It will print out the number of errors (violations) and an indication that the report failed : Violations: 11 ----------------- ERROR: 5 WARN: 4 INFO: 2 ERROR Report failed! And it will provide you with a report file, report.html Let us look at the file in a browser (simply double-click on the html file the way you would open a PDF). Your report should look similar to this: While there are other formats you can export your report to , HTML is a great format which not only offers useful colour coding, but also allows us to click on the related classes and properties and, more importantly, the checks to find our what they mean (for an overview of all ROBOT report checks see here ).","title":"ROBOT report"},{"location":"tutorial/robot-tutorial-qc/#exercise","text":"We will leave it to the reader as an exercise to try and fix all the errors indicated by the report!","title":"Exercise"},{"location":"tutorial/robot-tutorial-qc/#advanced-usage-of-robot-report","text":"","title":"Advanced usage of ROBOT report"},{"location":"tutorial/robot-tutorial-qc/#customisation","text":"While by far the most widely spread usage of ROBOT report is to check for OBO best practices, it is possible to customise the report by removing certain OBO ontology checks and adding custom ones. Lets first create a simple profile.txt in our directory and add the following lines: WARN annotation_whitespace ERROR missing_ontology_description ERROR missing_definition ERROR missing_ontology_license ERROR missing_ontology_title ERROR misused_obsolete_label ERROR multiple_labels Now we tell ROBOT to run the command using our custom profile rather than the default ROBOT profile: robot report -i example.owl --profile profile.txt -o report.html The resulting report looks different: In particular, some checks like missing_superclass which we did not care about for our use case are not shown at all anymore, and others, such as missing_definition are now considered ERROR (red) rather than WARN (warning, yellow) because for our use case, we have decided that definitions on terms are mandatory.","title":"Customisation"},{"location":"tutorial/robot-tutorial-qc/#robot-verify","text":"ROBOT verify allows us to define QC checks for undesirable situation (we sometimes call this \"anti-pattern\") using the SPARQL query language. The idea is simple: we write a SPARQL query for the thing we do not want . For example, we can use SPARQL to look for classes with more than one label. Then, we feed this query to ROBOT verify. ROBOT verify than ensures that the query has no answers , i.e the thing we do not want actually does not happen: PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT DISTINCT ?entity ?property ?value WHERE { VALUES ?property { rdfs:label } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) . FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) } ORDER BY ?entity Let us safe this query now in our working directory as bad_labels.sparql and run the following: robot verify -i example.owl --queries bad_labels.sparql ROBOT will output this to tell us which terms have violations: FAIL Rule bad_labels.sparql: 2 violation(s) entity,property,value http://purl.obolibrary.org/obo/OBI_0002986,http://www.w3.org/2000/01/rdf-schema#label,CT scan http://purl.obolibrary.org/obo/OBI_0002986,http://www.w3.org/2000/01/rdf-schema#label,computed tomography imaging assay Now the cool thing with verify is that we can basically feed SPARQL SELECT queries in whatever shape or form we want. To make error messages more readable for curators, you can even encode a proper error message: PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT DISTINCT ?error WHERE { VALUES ?property { rdfs:label } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) . FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) BIND(CONCAT(\"Entity \",STR(?entity),\" uses two different labels: (1) \",STR(?value),\" and (2) \",STR(?value2)) as ?error) } ORDER BY ?entity This time, when running the query, we get: FAIL Rule bad_labels.sparql: 2 violation(s) error Entity http://purl.obolibrary.org/obo/OBI_0002986 uses two different labels: (1) CT scan and (2) computed tomography imaging assay Entity http://purl.obolibrary.org/obo/OBI_0002986 uses two different labels: (1) computed tomography imaging assay and (2) CT scan Which appears much more readable! You can tweak the output in whatever way you think is best. Two things about this: You cannot do this very well with ROBOT report : despite the ability to include custom queries, all queries must start with: SELECT DISTINCT ?entity ?property ?value WHERE This is one of the reasons we still like using ROBOT verify, despite the fact that ROBOT report can also be extended with custom checks. Another cool thing about verify is that you can add the --output-dir results/ parameter to your query to get ROBOT to export the query results as TSV files. This can be useful if you have many QC queries and need to work with them independently of the checks. Note that ROBOT notices two errors despite there being only 1, technically speaking. This is because the WHERE clause in SPARQL which twice (one where label 1 is first, one where label 2 is first). You can be smart about it and get around it by sorting your results before binding them, but for most use cases this hack is hardly worth the effort.","title":"ROBOT verify"},{"location":"tutorial/robot-tutorial-qc/#robot-reason","text":"This is not an exhaustive tutorial for ROBOT reason (for more, see here ). We only want to get across two checks that we feel absolutely every ontology developer should know about.","title":"ROBOT reason"},{"location":"tutorial/robot-tutorial-qc/#the-distinct-scope-assumption","text":"In most cases, we do not want to define the exact same concept twice. There are some exceptions, for example when we align ontologies such as CHEBI and GO which may have overlapping scope, but in 99.9% of the cases, having the reasoner infer that two classes are the same points to a mistake in the axiomatisation. Let us check that we do not have any such unintended equivalencies in our ontology: robot reason -i example.owl --equivalent-classes-allowed none ROBOT will note that: ERROR No equivalent class axioms are allowed ERROR Equivalence: <http://purl.obolibrary.org/obo/TEST_0600047> == <http://purl.obolibrary.org/obo/OBI_0600047> Further investigation in Protege will reveal that TEST_0600047 and OBI_0600047 are subclasses of each other, which causes the reasoner to infer that they are equivalent.","title":"The \"distinct scope\" assumption"},{"location":"tutorial/setting-up-project-odk/","text":"Tutorial: How to get started with your own ODK-style repository \u00b6 Preparation: Installing docker, installing ODK and setting memory. Follow the steps here . Creating your first ontology repository The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an sh file in the following there exists a corresponding bat file that can be run in the windows powershell, or CMD. Prerequisites \u00b6 You have: A Github account Completed the \"Preparation\" steps above Video \u00b6 A recording of a demo of creating a ODK-repo is available here Your first repository \u00b6 Create temporary directory to get started On your machine, create a new folder somewhere: cd ~ mkdir odk_tutorial cd odk_tutorial Download the seed-my-repo wrapper script Now download the seed-my-repo wrapper script from the ODK GitHub repository. A detailed explanation of how to do that can be found here . For simplicity, we just use wget here to download the seed-my-repo file, but you can do it manually: wget https://raw.githubusercontent.com/INCATools/ontology-development-kit/master/seed-via-docker.sh Download a basic config to start from and start building your own The last ingredient we need is an ODK config file. While you can, in theory, create an empty repo entirely without a config file (one will be generated for you), we recommend to just start right with one. You can find many examples of configs here . For the sake of this tutorial, we will start with a simple config: id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: \"-Xmx8G\" Safe this config file as in your temporary directory, e.g. ~/odk_tutorial/cato-odk.yaml . Most of your work managing your ODK in the future will involve editing this file. There are dozens of cool options that do magical things in there. For now, lets focus on the most essential: General config: \u00b6 id: cato title: \"Cat Anatomy Ontology\" The id is essential, as it will determine how files will be named, which default term IDs to assume, and many more. It should be a lowercase string which is, by convention at least 4 characters long - 5 is not unheard of. The title field is used to generate various default values in the repository, like the README and others. There are other fields, like description , but let's start minimal for now. A full list of elements can be found in this schema: https://github.com/INCATools/ontology-development-kit/blob/master/schema/project-schema.json Git config: \u00b6 github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology The github_org (the GitHub or GitLab organisation) and the repo (repository name) will be used for some basic config of the git repo. Enter your own github_org here rather than obophenotype . Your default github_org is your GitHub username. If you are not creating a new repo, but working on a repo that predates renaming the GitHub main branch from master to main , you may want to set the git_main_branch as well. Pipeline configuration \u00b6 release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json With this configuration, we tell the ODK that we wish to automatically generate the base, full and simple release files for our ontology. We also say that we want the primary_release to be the full release (which is also the default). The primary release will be materialised as cato.owl , and is what most users of your ontology will interact with. More information and what these are can be found here . We always want to create a base , i.e. the release variant that contains all the axioms that belong to the ontology, and none of the imported ones, but we do not want to make it the primary_release , because it will be unclassified and missing a lot of the important inferences. We also configure export products: we always want to export to OWL ( owl ), but we can also chose to export to OBO ( obo ) format and OBOGraphs JSON ( json ). Imports config: \u00b6 import_group: products: - id: ro - id: pato - id: omo This is a central part of the ODK, and the section of the config file you will interact with the most. Please see here for details. What we are asking the ODK here, in essence, to set us up for dynamically importing from the Relation Ontology (RO), the Phenotype And Trait Ontology (PATO) and the OBO Metadata Ontology (OMO). Memory management: \u00b6 robot_java_args: '-Xmx8G' Here we say that we allow ROBOT to consume up to 8GB of memory. Make sure that your docker is set up to permit at least ~20% more memory than that, i.e. 9GB or 10GB, otherwise, some cryptic Docker errors may come up. Generate the repo \u00b6 Run the following: cd ~/odk_tutorial sh seed-via-docker.sh -c -C cato-odk.yaml This will create a basic layout of your repo under target/cato/* Note: after this run, you wont need cato-odk.yaml anymore as it will have been added to your ontology repo, which we will see later. Publish on GitHub \u00b6 You can now move the target/cato directory to a more suitable location. For the sake of this tutorial we will move it to the Home directory. mv target/cato ~/ Using GitHub Desktop \u00b6 If you use GitHub Desktop, you can now simply add this repo by selecting File -> Add local repository and select the directory you moved the repo to (as an aside, you should really have a nice workspace directory like ~/git or ~/ws or some such to organise your projects). Then click Publish the repository on Using the Command Line \u00b6 Follow the instructions you see on the Terminal (they are printed after your seed-my-repo run). Finish! \u00b6 Congratulations, you have successfully jump-started your very own ODK repository and can start developing. Next steps: \u00b6 Start editing ~/cato/src/ontology/cato-edit.owl using Protege. Run a release","title":"ODK - Getting started with your own repo"},{"location":"tutorial/setting-up-project-odk/#tutorial-how-to-get-started-with-your-own-odk-style-repository","text":"Preparation: Installing docker, installing ODK and setting memory. Follow the steps here . Creating your first ontology repository The tutorial uses example tailored for users of UNIX systems, like Mac and Linux. Users of Windows generally have analogous steps - wherever we talk about an sh file in the following there exists a corresponding bat file that can be run in the windows powershell, or CMD.","title":"Tutorial: How to get started with your own ODK-style repository"},{"location":"tutorial/setting-up-project-odk/#prerequisites","text":"You have: A Github account Completed the \"Preparation\" steps above","title":"Prerequisites"},{"location":"tutorial/setting-up-project-odk/#video","text":"A recording of a demo of creating a ODK-repo is available here","title":"Video"},{"location":"tutorial/setting-up-project-odk/#your-first-repository","text":"Create temporary directory to get started On your machine, create a new folder somewhere: cd ~ mkdir odk_tutorial cd odk_tutorial Download the seed-my-repo wrapper script Now download the seed-my-repo wrapper script from the ODK GitHub repository. A detailed explanation of how to do that can be found here . For simplicity, we just use wget here to download the seed-my-repo file, but you can do it manually: wget https://raw.githubusercontent.com/INCATools/ontology-development-kit/master/seed-via-docker.sh Download a basic config to start from and start building your own The last ingredient we need is an ODK config file. While you can, in theory, create an empty repo entirely without a config file (one will be generated for you), we recommend to just start right with one. You can find many examples of configs here . For the sake of this tutorial, we will start with a simple config: id: cato title: \"Cat Anatomy Ontology\" github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json import_group: products: - id: ro - id: pato - id: omo robot_java_args: \"-Xmx8G\" Safe this config file as in your temporary directory, e.g. ~/odk_tutorial/cato-odk.yaml . Most of your work managing your ODK in the future will involve editing this file. There are dozens of cool options that do magical things in there. For now, lets focus on the most essential:","title":"Your first repository"},{"location":"tutorial/setting-up-project-odk/#general-config","text":"id: cato title: \"Cat Anatomy Ontology\" The id is essential, as it will determine how files will be named, which default term IDs to assume, and many more. It should be a lowercase string which is, by convention at least 4 characters long - 5 is not unheard of. The title field is used to generate various default values in the repository, like the README and others. There are other fields, like description , but let's start minimal for now. A full list of elements can be found in this schema: https://github.com/INCATools/ontology-development-kit/blob/master/schema/project-schema.json","title":"General config:"},{"location":"tutorial/setting-up-project-odk/#git-config","text":"github_org: obophenotype git_main_branch: main repo: cat_anatomy_ontology The github_org (the GitHub or GitLab organisation) and the repo (repository name) will be used for some basic config of the git repo. Enter your own github_org here rather than obophenotype . Your default github_org is your GitHub username. If you are not creating a new repo, but working on a repo that predates renaming the GitHub main branch from master to main , you may want to set the git_main_branch as well.","title":"Git config:"},{"location":"tutorial/setting-up-project-odk/#pipeline-configuration","text":"release_artefacts: - base - full - simple primary_release: full export_formats: - owl - obo - json With this configuration, we tell the ODK that we wish to automatically generate the base, full and simple release files for our ontology. We also say that we want the primary_release to be the full release (which is also the default). The primary release will be materialised as cato.owl , and is what most users of your ontology will interact with. More information and what these are can be found here . We always want to create a base , i.e. the release variant that contains all the axioms that belong to the ontology, and none of the imported ones, but we do not want to make it the primary_release , because it will be unclassified and missing a lot of the important inferences. We also configure export products: we always want to export to OWL ( owl ), but we can also chose to export to OBO ( obo ) format and OBOGraphs JSON ( json ).","title":"Pipeline configuration"},{"location":"tutorial/setting-up-project-odk/#imports-config","text":"import_group: products: - id: ro - id: pato - id: omo This is a central part of the ODK, and the section of the config file you will interact with the most. Please see here for details. What we are asking the ODK here, in essence, to set us up for dynamically importing from the Relation Ontology (RO), the Phenotype And Trait Ontology (PATO) and the OBO Metadata Ontology (OMO).","title":"Imports config:"},{"location":"tutorial/setting-up-project-odk/#memory-management","text":"robot_java_args: '-Xmx8G' Here we say that we allow ROBOT to consume up to 8GB of memory. Make sure that your docker is set up to permit at least ~20% more memory than that, i.e. 9GB or 10GB, otherwise, some cryptic Docker errors may come up.","title":"Memory management:"},{"location":"tutorial/setting-up-project-odk/#generate-the-repo","text":"Run the following: cd ~/odk_tutorial sh seed-via-docker.sh -c -C cato-odk.yaml This will create a basic layout of your repo under target/cato/* Note: after this run, you wont need cato-odk.yaml anymore as it will have been added to your ontology repo, which we will see later.","title":"Generate the repo"},{"location":"tutorial/setting-up-project-odk/#publish-on-github","text":"You can now move the target/cato directory to a more suitable location. For the sake of this tutorial we will move it to the Home directory. mv target/cato ~/","title":"Publish on GitHub"},{"location":"tutorial/setting-up-project-odk/#using-github-desktop","text":"If you use GitHub Desktop, you can now simply add this repo by selecting File -> Add local repository and select the directory you moved the repo to (as an aside, you should really have a nice workspace directory like ~/git or ~/ws or some such to organise your projects). Then click Publish the repository on","title":"Using GitHub Desktop"},{"location":"tutorial/setting-up-project-odk/#using-the-command-line","text":"Follow the instructions you see on the Terminal (they are printed after your seed-my-repo run).","title":"Using the Command Line"},{"location":"tutorial/setting-up-project-odk/#finish","text":"Congratulations, you have successfully jump-started your very own ODK repository and can start developing.","title":"Finish!"},{"location":"tutorial/setting-up-project-odk/#next-steps","text":"Start editing ~/cato/src/ontology/cato-edit.owl using Protege. Run a release","title":"Next steps:"},{"location":"tutorial/sparql-report-odk/","text":"Generating SPARQL table reports with ODK \u00b6 This tutorial will teach you how to create report tables using SPARQL and the ODK. Report tables are TSV files that can be viewed by programs such as Excel or Google Sheets. For a tutorial on how to generate reports independent of ODK please see here . Preparation \u00b6 You are set up for executing ODK workflows We assume you have a modern ODK-based repository (ODK version >= 1.2.32) set up. For a tutorial on creating a new ontology repo from scratch see here . Finish the ROBOT tutorial on queries Tutorial \u00b6 Adding a configuration to the ODK YAML file: \u00b6 robot_report: custom_sparql_exports: - basic-report - my-cat-report This will tell the ODK that you no longer wish to generate the ODK default reports (synonyms, xrefs, etc), but instead: One of the custom reports ( basic-report ) and a new custom report, called my-cat-report . Now, we can apply these changes as usual: sh run.sh make update_repo Adding the actual table report \u00b6 Similar to our ROBOT tutorial on queries , let us now add a simple table report for the terms and labels in our ontology. To do that, let us safe the following file in our src/sparql directory (standard ODK setup), i.e. src/sparql/my-cat-report.sparql (you must use the same name as the one you speciefied in your ODK yaml file above): PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?term ?property ?value WHERE { ?term a owl:Class ; rdfs:label ?value . } Now, let's generate our report (you have to be, as always, in src/ontology/ ): sh run.sh make custom_reports This will generate all custom reports you have configured in one go and save them in the src/ontology/reports directory. reports/my-cat-report.tsv looks probably something like this for you: ?term ?property ?value <http://purl.obolibrary.org/obo/CATO_0000000> \"root node\"@en ... That is all there is. You can configure as many reports as you want, and they will all be generated with the custom_reports command above, or as part of your ontology releases.","title":"ODK - Adding SPARQL table reports"},{"location":"tutorial/sparql-report-odk/#generating-sparql-table-reports-with-odk","text":"This tutorial will teach you how to create report tables using SPARQL and the ODK. Report tables are TSV files that can be viewed by programs such as Excel or Google Sheets. For a tutorial on how to generate reports independent of ODK please see here .","title":"Generating SPARQL table reports with ODK"},{"location":"tutorial/sparql-report-odk/#preparation","text":"You are set up for executing ODK workflows We assume you have a modern ODK-based repository (ODK version >= 1.2.32) set up. For a tutorial on creating a new ontology repo from scratch see here . Finish the ROBOT tutorial on queries","title":"Preparation"},{"location":"tutorial/sparql-report-odk/#tutorial","text":"","title":"Tutorial"},{"location":"tutorial/sparql-report-odk/#adding-a-configuration-to-the-odk-yaml-file","text":"robot_report: custom_sparql_exports: - basic-report - my-cat-report This will tell the ODK that you no longer wish to generate the ODK default reports (synonyms, xrefs, etc), but instead: One of the custom reports ( basic-report ) and a new custom report, called my-cat-report . Now, we can apply these changes as usual: sh run.sh make update_repo","title":"Adding a configuration to the ODK YAML file:"},{"location":"tutorial/sparql-report-odk/#adding-the-actual-table-report","text":"Similar to our ROBOT tutorial on queries , let us now add a simple table report for the terms and labels in our ontology. To do that, let us safe the following file in our src/sparql directory (standard ODK setup), i.e. src/sparql/my-cat-report.sparql (you must use the same name as the one you speciefied in your ODK yaml file above): PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?term ?property ?value WHERE { ?term a owl:Class ; rdfs:label ?value . } Now, let's generate our report (you have to be, as always, in src/ontology/ ): sh run.sh make custom_reports This will generate all custom reports you have configured in one go and save them in the src/ontology/reports directory. reports/my-cat-report.tsv looks probably something like this for you: ?term ?property ?value <http://purl.obolibrary.org/obo/CATO_0000000> \"root node\"@en ... That is all there is. You can configure as many reports as you want, and they will all be generated with the custom_reports command above, or as part of your ontology releases.","title":"Adding the actual table report"},{"location":"tutorial/sparql-report-robot/","text":"Generating SPARQL table reports with ROBOT \u00b6 Preparation \u00b6 You should be able to run ROBOT. Overview: \u00b6 Creating table outputs from your ontology helps with many issues, for example during ontology curation (it is often easier to look at tables of related ontology terms rather than a hierarchy), for data aggregation (you want to know how many synonyms there are, and which) and simply to share \"a list of all terms with labels\". There are two major tools to help here: ROBOT export : Exporting standardised tables for typical use cases, like labels, definitions and similar. For details, please look at the documentation which should provide all the information for producing table reports. ROBOT query : Generating reports using SPARQL. This is the focus of the tutorial here. Download test ontology \u00b6 Download example.owl , or get it via the command line: curl https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/robot_tutorial_qc/example.owl > example.owl Let us ensure we are using the same ROBOT version: robot --version We see: ROBOT version 1.8.3 Generating a simple report \u00b6 Very frequently, we wish need to create summary tables (for a more detailed motivation see here ). Here, lets generate a simple report table by specifying a query: PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?term ?property ?value WHERE { ?term rdfs:label ?value . } Let us safe the query as labels.sparql in our working directory. Let's now generate the report: robot query -i example.owl --query labels.sparql labels.tsv When looking at labels.tsv (in a text editor, or Excel, or whatever table editor you prefer), we notice that some properties are included in our list and decide to change that by restricting the results to classes: PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?term ?property ?value WHERE { ?term a owl:Class ; rdfs:label ?value . } Now, when running the robot query command again, we see only the terms we want. Note that you could have achieved all this with a simple ROBOT export command. However, there are many cool ways you can tweak your reports when you learn how to build them manually during SPARQL. Your only limit is essentially SPARQL itself, which gives you access too most things in your ontology, aside from perhaps complex logical axioms.","title":"ROBOT - Generating SPARQL table reports"},{"location":"tutorial/sparql-report-robot/#generating-sparql-table-reports-with-robot","text":"","title":"Generating SPARQL table reports with ROBOT"},{"location":"tutorial/sparql-report-robot/#preparation","text":"You should be able to run ROBOT.","title":"Preparation"},{"location":"tutorial/sparql-report-robot/#overview","text":"Creating table outputs from your ontology helps with many issues, for example during ontology curation (it is often easier to look at tables of related ontology terms rather than a hierarchy), for data aggregation (you want to know how many synonyms there are, and which) and simply to share \"a list of all terms with labels\". There are two major tools to help here: ROBOT export : Exporting standardised tables for typical use cases, like labels, definitions and similar. For details, please look at the documentation which should provide all the information for producing table reports. ROBOT query : Generating reports using SPARQL. This is the focus of the tutorial here.","title":"Overview:"},{"location":"tutorial/sparql-report-robot/#download-test-ontology","text":"Download example.owl , or get it via the command line: curl https://raw.githubusercontent.com/OBOAcademy/obook/master/docs/tutorial/robot_tutorial_qc/example.owl > example.owl Let us ensure we are using the same ROBOT version: robot --version We see: ROBOT version 1.8.3","title":"Download test ontology"},{"location":"tutorial/sparql-report-robot/#generating-a-simple-report","text":"Very frequently, we wish need to create summary tables (for a more detailed motivation see here ). Here, lets generate a simple report table by specifying a query: PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?term ?property ?value WHERE { ?term rdfs:label ?value . } Let us safe the query as labels.sparql in our working directory. Let's now generate the report: robot query -i example.owl --query labels.sparql labels.tsv When looking at labels.tsv (in a text editor, or Excel, or whatever table editor you prefer), we notice that some properties are included in our list and decide to change that by restricting the results to classes: PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?term ?property ?value WHERE { ?term a owl:Class ; rdfs:label ?value . } Now, when running the robot query command again, we see only the terms we want. Note that you could have achieved all this with a simple ROBOT export command. However, there are many cool ways you can tweak your reports when you learn how to build them manually during SPARQL. Your only limit is essentially SPARQL itself, which gives you access too most things in your ontology, aside from perhaps complex logical axioms.","title":"Generating a simple report"},{"location":"tutorial/sparql/","text":"Basic SPARQL for OBO Engineers \u00b6 In this tutorial we introduce SPARQL, with a particular spin on how we use it across OBO ontologies. Following this tutorial should give you a sense of how we use SPARQL across OBO, without going too much into technical details. You can find concrete tutorials on how to generate reports or QC checks with ROBOT and ODK towards the end of this page. Preparation \u00b6 Watch Linked Data Engineering: Querying RDF with SPARQL Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour) SPARQL tools for OBO Engineers \u00b6 RENCI Ubergraph Endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies. Yasgui : Yasgui is a simple and beautiful front-end for SPARQL endpoints which can be used not only to query, but also to share queries with others. For example this simple SPARQL query runs across the RENCI Ubergraph Endpoint. GTF : A UI that allows one to run SPARQL queries on TTL files on the web, or upload them. Looks like its based on Yasgui, as it shares the same share functionality. ROBOT query : ROBOT method to generate TSV reports from SPARQL queries, and applying data transformations ( --update ). ROBOT uses Jena internally to execute SPARQL queries. ROBOT verify : ROBOT method to run SPARQL QC queries. If the query returns a result, the QC test fails. ROBOT report : ROBOT report is a more powerful approach to running OBO QC queries. The default OBO report which ships with ROBOT can be customised by changing the error level, removing a test entirely and even extending the report to custom (SPARQL) checks. Robot report can generate beautiful HTML reports which are easy to read. SPARQL in the OBO-sphere \u00b6 SPARQL has many uses in the OBO-sphere, but the following in particular: Quality control checking Creating summary tables for ontologies Sophisticated data transformations in ontology pipelines We will discuss each of these in the following and give examples. An informal discussion of SPARQL in OBO can be followed in video below. Quality control checking \u00b6 For us, ROBOT + SPARQL were a game changer for our quality control (QC) pipelines. This is how it works. First, we encode the error in the form of a SPARQL query (we sometimes call this \"anti-pattern\", i.e. an undesirable (anti-) representation). For example, the following check simply looks for entities that have more than one definition : PREFIX obo: <http://purl.obolibrary.org/obo/> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?entity ?property ?value WHERE { VALUES ?property { obo:IAO_0000115 obo:IAO_0000600 } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) } ORDER BY ?entity This is a typical workflow. Think of an ontology editor working on an ontology. Often, that curator notices that the same problem happens repeatedly and tell us, the Ontology Pipeline Developer, that they would like a check to prevent the error. We then capture the erroneous situation as a SPARQL query. Then, we add it to our ontology repository , and execute it with ROBOT report or ROBOT verify (see above) in our CI pipelines, usually based on GitHub actions or Travis. Note that the Ontology Development Kit provides a built-in framework for for such queries build on ROBOT verify and report. Creating summary tables for ontologies \u00b6 Many times, we need to create tabular reports of our ontologies to share with stakeholders or to help with internal reviews, e.g.: create lists of ontology terms with their definitions and labels create summaries of ontologies, like aggregate statistics Sometimes using Yasgui , for example in conjunction with the RENCI Ubergraph Endpoint, is enough, but often, using ROBOT query is the better choice, especially if you want to make sure the right version of the ontology is used (Ubergraph occasionally is out of date). Using ROBOT in conjunction with a Workflows Automation system like Github actions helps with generating up-to-date reports. Here is an example of a GitHub action that generates a few reports with ROBOT and pushes them back to the repository. A note for Data Scientists \u00b6 In many cases we are asked how to best \"load an ontology\" into a python notebook or similar. Very often the answer is that it is best to first extract the content of the ontology into a table form, and then load it using a CSV reader like pandas . In this scenario, the workflow for interacting with ontologies is: Define the information you want in the form of a SPARQL query. Extract the the information as a TSV table using ROBOT query. Load the information into your notebook. If combined with for example a Makefile, you can always ensure that the report generation process is fully reproducible as well. Sophisticated data transformations in ontology pipelines \u00b6 Lastly, we use ROBOT query to implement complex ontology transformation processes. For example the following complex query transforms related synonyms to exact synonyms if some complex condition is met: prefix owl: <http://www.w3.org/2002/07/owl#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> DELETE { ?term oboInOwl:hasRelatedSynonym ?related . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } INSERT { ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } WHERE { { ?term oboInOwl:hasRelatedSynonym ?related ; oboInOwl:hasExactSynonym ?exact ; a owl:Class . ?exax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?exact ; oboInOwl:hasDbXref ?xref1 . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . FILTER (str(?related)=str(?exact)) FILTER (isIRI(?term) && regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\")) } } This can be a very useful tool for bulk editing the ontology, in particular where it is difficult or impossible to achieve the same using regular expressions or other forms of \"replacement\"-techniques. Here are some example queries we collected to do such mass operations in Mondo. Related tutorials \u00b6 QC checks with ROBOT Generating SPARQL table reports with ROBOT Generating SPARQL table reports with ODK","title":"Basic SPARQL for OBO Engineers"},{"location":"tutorial/sparql/#basic-sparql-for-obo-engineers","text":"In this tutorial we introduce SPARQL, with a particular spin on how we use it across OBO ontologies. Following this tutorial should give you a sense of how we use SPARQL across OBO, without going too much into technical details. You can find concrete tutorials on how to generate reports or QC checks with ROBOT and ODK towards the end of this page.","title":"Basic SPARQL for OBO Engineers"},{"location":"tutorial/sparql/#preparation","text":"Watch Linked Data Engineering: Querying RDF with SPARQL Complete Running Basic SPARQL Queries tutorial (~45 minutes - 1 hour)","title":"Preparation"},{"location":"tutorial/sparql/#sparql-tools-for-obo-engineers","text":"RENCI Ubergraph Endpoint : Many key OBO ontologies are loaded here with lots of materialised inferences ( docs ). Ontobee SPARQL endpoint : Useful to run queries across all OBO Foundry ontologies. Yasgui : Yasgui is a simple and beautiful front-end for SPARQL endpoints which can be used not only to query, but also to share queries with others. For example this simple SPARQL query runs across the RENCI Ubergraph Endpoint. GTF : A UI that allows one to run SPARQL queries on TTL files on the web, or upload them. Looks like its based on Yasgui, as it shares the same share functionality. ROBOT query : ROBOT method to generate TSV reports from SPARQL queries, and applying data transformations ( --update ). ROBOT uses Jena internally to execute SPARQL queries. ROBOT verify : ROBOT method to run SPARQL QC queries. If the query returns a result, the QC test fails. ROBOT report : ROBOT report is a more powerful approach to running OBO QC queries. The default OBO report which ships with ROBOT can be customised by changing the error level, removing a test entirely and even extending the report to custom (SPARQL) checks. Robot report can generate beautiful HTML reports which are easy to read.","title":"SPARQL tools for OBO Engineers"},{"location":"tutorial/sparql/#sparql-in-the-obo-sphere","text":"SPARQL has many uses in the OBO-sphere, but the following in particular: Quality control checking Creating summary tables for ontologies Sophisticated data transformations in ontology pipelines We will discuss each of these in the following and give examples. An informal discussion of SPARQL in OBO can be followed in video below.","title":"SPARQL in the OBO-sphere"},{"location":"tutorial/sparql/#quality-control-checking","text":"For us, ROBOT + SPARQL were a game changer for our quality control (QC) pipelines. This is how it works. First, we encode the error in the form of a SPARQL query (we sometimes call this \"anti-pattern\", i.e. an undesirable (anti-) representation). For example, the following check simply looks for entities that have more than one definition : PREFIX obo: <http://purl.obolibrary.org/obo/> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT DISTINCT ?entity ?property ?value WHERE { VALUES ?property { obo:IAO_0000115 obo:IAO_0000600 } ?entity ?property ?value . ?entity ?property ?value2 . FILTER (?value != ?value2) FILTER NOT EXISTS { ?entity owl:deprecated true } FILTER (!isBlank(?entity)) } ORDER BY ?entity This is a typical workflow. Think of an ontology editor working on an ontology. Often, that curator notices that the same problem happens repeatedly and tell us, the Ontology Pipeline Developer, that they would like a check to prevent the error. We then capture the erroneous situation as a SPARQL query. Then, we add it to our ontology repository , and execute it with ROBOT report or ROBOT verify (see above) in our CI pipelines, usually based on GitHub actions or Travis. Note that the Ontology Development Kit provides a built-in framework for for such queries build on ROBOT verify and report.","title":"Quality control checking"},{"location":"tutorial/sparql/#creating-summary-tables-for-ontologies","text":"Many times, we need to create tabular reports of our ontologies to share with stakeholders or to help with internal reviews, e.g.: create lists of ontology terms with their definitions and labels create summaries of ontologies, like aggregate statistics Sometimes using Yasgui , for example in conjunction with the RENCI Ubergraph Endpoint, is enough, but often, using ROBOT query is the better choice, especially if you want to make sure the right version of the ontology is used (Ubergraph occasionally is out of date). Using ROBOT in conjunction with a Workflows Automation system like Github actions helps with generating up-to-date reports. Here is an example of a GitHub action that generates a few reports with ROBOT and pushes them back to the repository.","title":"Creating summary tables for ontologies"},{"location":"tutorial/sparql/#a-note-for-data-scientists","text":"In many cases we are asked how to best \"load an ontology\" into a python notebook or similar. Very often the answer is that it is best to first extract the content of the ontology into a table form, and then load it using a CSV reader like pandas . In this scenario, the workflow for interacting with ontologies is: Define the information you want in the form of a SPARQL query. Extract the the information as a TSV table using ROBOT query. Load the information into your notebook. If combined with for example a Makefile, you can always ensure that the report generation process is fully reproducible as well.","title":"A note for Data Scientists"},{"location":"tutorial/sparql/#sophisticated-data-transformations-in-ontology-pipelines","text":"Lastly, we use ROBOT query to implement complex ontology transformation processes. For example the following complex query transforms related synonyms to exact synonyms if some complex condition is met: prefix owl: <http://www.w3.org/2002/07/owl#> prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> DELETE { ?term oboInOwl:hasRelatedSynonym ?related . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } INSERT { ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . } WHERE { { ?term oboInOwl:hasRelatedSynonym ?related ; oboInOwl:hasExactSynonym ?exact ; a owl:Class . ?exax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasExactSynonym ; owl:annotatedTarget ?exact ; oboInOwl:hasDbXref ?xref1 . ?relax a owl:Axiom ; owl:annotatedSource ?term ; owl:annotatedProperty oboInOwl:hasRelatedSynonym ; owl:annotatedTarget ?related ; oboInOwl:hasDbXref ?xref2 . FILTER (str(?related)=str(?exact)) FILTER (isIRI(?term) && regex(str(?term), \"^http://purl.obolibrary.org/obo/MONDO_\")) } } This can be a very useful tool for bulk editing the ontology, in particular where it is difficult or impossible to achieve the same using regular expressions or other forms of \"replacement\"-techniques. Here are some example queries we collected to do such mass operations in Mondo.","title":"Sophisticated data transformations in ontology pipelines"},{"location":"tutorial/sparql/#related-tutorials","text":"QC checks with ROBOT Generating SPARQL table reports with ROBOT Generating SPARQL table reports with ODK","title":"Related tutorials"},{"location":"tutorial/sssom-manual/","text":"Mappings with SSSOM: Tutorials \u00b6 Curating mappings with SSSOM: an introduction","title":"Basic Tutorial"},{"location":"tutorial/sssom-manual/#mappings-with-sssom-tutorials","text":"Curating mappings with SSSOM: an introduction","title":"Mappings with SSSOM: Tutorials"}]}